2023-04-27 05:19:35,754 - INFO - Config:
Namespace(config='configs/implicit_mlp.yaml', dir='/user/delden/acoustics/spp_ai_acoustics/acousticNN/experiments/arch/no_encoding/implicitmlp', epochs=100, device='cuda', seed=3, parameter_list='configs/data.yaml', encoding='none', dir_name='arch/no_encoding/implicitmlp')
2023-04-27 05:19:35,755 - INFO - Config:
Munch({'optimizer': Munch({'type': 'AdamW', 'kwargs': Munch({'lr': 0.001, 'weight_decay': 0.005, 'betas': [0.9, 0.95]})}), 'scheduler': Munch({'type': 'CosLR', 'kwargs': Munch({'epochs': 500, 'initial_epochs': 25})}), 'model': Munch({'name': 'ImplicitMLP', 'input_encoding': False, 'encoding_dim': 16, 'embed_dim': 64, 'depth': 7, 'mlp_width': 256}), 'generation_frequency': 5001, 'validation_frequency': 10, 'batch_size': 16, 'epochs': 500, 'gradient_clip': 10})
2023-04-27 05:19:49,904 - INFO - ==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ImplicitMLP                              [16, 200, 4]              --
├─GroupwiseProjection: 1-1               [3200, 15, 64]            --
│    └─ModuleList: 2-1                   --                        --
│    │    └─Linear: 3-1                  [3200, 4, 64]             128
│    │    └─Linear: 3-2                  [3200, 5, 64]             128
│    │    └─Linear: 3-3                  [3200, 5, 64]             128
│    │    └─Linear: 3-4                  [3200, 1, 64]             128
├─MLP: 1-2                               [3200, 256]               --
│    └─Linear: 2-2                       [3200, 256]               246,016
│    └─ReLU: 2-3                         [3200, 256]               --
│    └─Dropout: 2-4                      [3200, 256]               --
│    └─Linear: 2-5                       [3200, 256]               65,792
│    └─ReLU: 2-6                         [3200, 256]               --
│    └─Dropout: 2-7                      [3200, 256]               --
│    └─Linear: 2-8                       [3200, 256]               65,792
│    └─ReLU: 2-9                         [3200, 256]               --
│    └─Dropout: 2-10                     [3200, 256]               --
│    └─Linear: 2-11                      [3200, 256]               65,792
│    └─ReLU: 2-12                        [3200, 256]               --
│    └─Dropout: 2-13                     [3200, 256]               --
│    └─Linear: 2-14                      [3200, 256]               65,792
│    └─ReLU: 2-15                        [3200, 256]               --
│    └─Dropout: 2-16                     [3200, 256]               --
│    └─Linear: 2-17                      [3200, 256]               65,792
│    └─ReLU: 2-18                        [3200, 256]               --
│    └─Dropout: 2-19                     [3200, 256]               --
│    └─Linear: 2-20                      [3200, 256]               65,792
│    └─Dropout: 2-21                     [3200, 256]               --
├─Linear: 1-3                            [3200, 4]                 1,028
==========================================================================================
Total params: 642,308
Trainable params: 642,308
Non-trainable params: 0
Total mult-adds (G): 2.06
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 70.55
Params size (MB): 2.57
Estimated Total Size (MB): 73.14
==========================================================================================
2023-04-27 05:19:50,687 - INFO - Epoch 0 training loss = 3.281e+03
2023-04-27 05:19:50,753 - INFO - Validation loss = 3.195e+03
2023-04-27 05:19:50,753 - INFO - best model
2023-04-27 05:19:51,521 - INFO - Epoch 1 training loss = 3.055e+03
2023-04-27 05:19:52,277 - INFO - Epoch 2 training loss = 653.5
2023-04-27 05:19:53,034 - INFO - Epoch 3 training loss = 124.1
2023-04-27 05:19:53,788 - INFO - Epoch 4 training loss = 97.99
2023-04-27 05:19:54,543 - INFO - Epoch 5 training loss = 85.41
2023-04-27 05:19:55,298 - INFO - Epoch 6 training loss = 72.55
2023-04-27 05:19:56,050 - INFO - Epoch 7 training loss = 60.65
2023-04-27 05:19:56,802 - INFO - Epoch 8 training loss = 51.04
2023-04-27 05:19:57,554 - INFO - Epoch 9 training loss = 42.06
2023-04-27 05:19:58,306 - INFO - Epoch 10 training loss = 36.57
2023-04-27 05:19:58,370 - INFO - Validation loss = 59.16
2023-04-27 05:19:58,370 - INFO - best model
2023-04-27 05:19:59,136 - INFO - Epoch 11 training loss = 33.75
2023-04-27 05:19:59,891 - INFO - Epoch 12 training loss = 30.4
2023-04-27 05:20:00,644 - INFO - Epoch 13 training loss = 27.26
2023-04-27 05:20:01,396 - INFO - Epoch 14 training loss = 25.72
2023-04-27 05:20:02,151 - INFO - Epoch 15 training loss = 23.85
2023-04-27 05:20:02,903 - INFO - Epoch 16 training loss = 20.29
2023-04-27 05:20:03,655 - INFO - Epoch 17 training loss = 20.37
2023-04-27 05:20:04,407 - INFO - Epoch 18 training loss = 18.71
2023-04-27 05:20:05,160 - INFO - Epoch 19 training loss = 18.21
2023-04-27 05:20:05,912 - INFO - Epoch 20 training loss = 16.51
2023-04-27 05:20:05,973 - INFO - Validation loss = 16.76
2023-04-27 05:20:05,974 - INFO - best model
2023-04-27 05:20:06,739 - INFO - Epoch 21 training loss = 15.87
2023-04-27 05:20:07,495 - INFO - Epoch 22 training loss = 16.69
2023-04-27 05:20:08,247 - INFO - Epoch 23 training loss = 15.54
2023-04-27 05:20:08,999 - INFO - Epoch 24 training loss = 14.7
2023-04-27 05:20:09,751 - INFO - Epoch 25 training loss = 15.29
2023-04-27 05:20:10,504 - INFO - Epoch 26 training loss = 15.01
2023-04-27 05:20:11,256 - INFO - Epoch 27 training loss = 14.34
2023-04-27 05:20:12,009 - INFO - Epoch 28 training loss = 12.54
2023-04-27 05:20:12,761 - INFO - Epoch 29 training loss = 11.16
2023-04-27 05:20:13,514 - INFO - Epoch 30 training loss = 10.94
2023-04-27 05:20:13,576 - INFO - Validation loss = 15.63
2023-04-27 05:20:13,576 - INFO - best model
2023-04-27 05:20:14,340 - INFO - Epoch 31 training loss = 11.44
2023-04-27 05:20:15,092 - INFO - Epoch 32 training loss = 9.497
2023-04-27 05:20:15,843 - INFO - Epoch 33 training loss = 9.308
2023-04-27 05:20:16,595 - INFO - Epoch 34 training loss = 8.803
2023-04-27 05:20:17,347 - INFO - Epoch 35 training loss = 9.798
2023-04-27 05:20:18,099 - INFO - Epoch 36 training loss = 8.275
2023-04-27 05:20:18,850 - INFO - Epoch 37 training loss = 7.695
2023-04-27 05:20:19,601 - INFO - Epoch 38 training loss = 8.066
2023-04-27 05:20:20,352 - INFO - Epoch 39 training loss = 7.622
2023-04-27 05:20:21,104 - INFO - Epoch 40 training loss = 7.505
2023-04-27 05:20:21,164 - INFO - Validation loss = 11.21
2023-04-27 05:20:21,165 - INFO - best model
2023-04-27 05:20:21,927 - INFO - Epoch 41 training loss = 7.317
2023-04-27 05:20:22,679 - INFO - Epoch 42 training loss = 7.145
2023-04-27 05:20:23,431 - INFO - Epoch 43 training loss = 6.552
2023-04-27 05:20:24,182 - INFO - Epoch 44 training loss = 6.667
2023-04-27 05:20:24,934 - INFO - Epoch 45 training loss = 6.46
2023-04-27 05:20:25,685 - INFO - Epoch 46 training loss = 6.619
2023-04-27 05:20:26,437 - INFO - Epoch 47 training loss = 6.13
2023-04-27 05:20:27,189 - INFO - Epoch 48 training loss = 6.152
2023-04-27 05:20:27,940 - INFO - Epoch 49 training loss = 5.614
2023-04-27 05:20:28,692 - INFO - Epoch 50 training loss = 5.875
2023-04-27 05:20:28,754 - INFO - Validation loss = 6.256
2023-04-27 05:20:28,755 - INFO - best model
2023-04-27 05:20:29,517 - INFO - Epoch 51 training loss = 5.487
2023-04-27 05:20:30,269 - INFO - Epoch 52 training loss = 5.923
2023-04-27 05:20:31,020 - INFO - Epoch 53 training loss = 5.504
2023-04-27 05:20:31,772 - INFO - Epoch 54 training loss = 5.552
2023-04-27 05:20:32,524 - INFO - Epoch 55 training loss = 4.994
2023-04-27 05:20:33,275 - INFO - Epoch 56 training loss = 5.224
2023-04-27 05:20:34,027 - INFO - Epoch 57 training loss = 5.477
2023-04-27 05:20:34,778 - INFO - Epoch 58 training loss = 5.123
2023-04-27 05:20:35,530 - INFO - Epoch 59 training loss = 4.78
2023-04-27 05:20:36,281 - INFO - Epoch 60 training loss = 4.97
2023-04-27 05:20:36,342 - INFO - Validation loss = 5.896
2023-04-27 05:20:36,343 - INFO - best model
2023-04-27 05:20:37,104 - INFO - Epoch 61 training loss = 5.24
2023-04-27 05:20:37,857 - INFO - Epoch 62 training loss = 5.015
2023-04-27 05:20:38,610 - INFO - Epoch 63 training loss = 4.55
2023-04-27 05:20:39,362 - INFO - Epoch 64 training loss = 4.586
2023-04-27 05:20:40,117 - INFO - Epoch 65 training loss = 4.574
2023-04-27 05:20:40,873 - INFO - Epoch 66 training loss = 4.205
2023-04-27 05:20:41,627 - INFO - Epoch 67 training loss = 4.465
2023-04-27 05:20:42,386 - INFO - Epoch 68 training loss = 4.499
2023-04-27 05:20:43,144 - INFO - Epoch 69 training loss = 4.063
2023-04-27 05:20:43,903 - INFO - Epoch 70 training loss = 4.413
2023-04-27 05:20:43,964 - INFO - Validation loss = 5.362
2023-04-27 05:20:43,965 - INFO - best model
2023-04-27 05:20:44,734 - INFO - Epoch 71 training loss = 4.041
2023-04-27 05:20:45,492 - INFO - Epoch 72 training loss = 4.304
2023-04-27 05:20:46,250 - INFO - Epoch 73 training loss = 4.368
2023-04-27 05:20:47,001 - INFO - Epoch 74 training loss = 3.983
2023-04-27 05:20:47,757 - INFO - Epoch 75 training loss = 3.748
2023-04-27 05:20:48,516 - INFO - Epoch 76 training loss = 3.785
2023-04-27 05:20:49,273 - INFO - Epoch 77 training loss = 3.744
2023-04-27 05:20:50,032 - INFO - Epoch 78 training loss = 3.579
2023-04-27 05:20:50,785 - INFO - Epoch 79 training loss = 3.691
2023-04-27 05:20:51,538 - INFO - Epoch 80 training loss = 3.654
2023-04-27 05:20:51,599 - INFO - Validation loss = 3.958
2023-04-27 05:20:51,599 - INFO - best model
2023-04-27 05:20:52,362 - INFO - Epoch 81 training loss = 3.701
2023-04-27 05:20:53,115 - INFO - Epoch 82 training loss = 3.75
2023-04-27 05:20:53,866 - INFO - Epoch 83 training loss = 3.63
2023-04-27 05:20:54,618 - INFO - Epoch 84 training loss = 3.514
2023-04-27 05:20:55,370 - INFO - Epoch 85 training loss = 3.448
2023-04-27 05:20:56,128 - INFO - Epoch 86 training loss = 3.421
2023-04-27 05:20:56,880 - INFO - Epoch 87 training loss = 3.382
2023-04-27 05:20:57,632 - INFO - Epoch 88 training loss = 3.289
2023-04-27 05:20:58,383 - INFO - Epoch 89 training loss = 3.303
2023-04-27 05:20:59,135 - INFO - Epoch 90 training loss = 3.547
2023-04-27 05:20:59,196 - INFO - Validation loss = 4.153
2023-04-27 05:20:59,948 - INFO - Epoch 91 training loss = 3.186
2023-04-27 05:21:00,700 - INFO - Epoch 92 training loss = 3.277
2023-04-27 05:21:01,452 - INFO - Epoch 93 training loss = 3.113
2023-04-27 05:21:02,203 - INFO - Epoch 94 training loss = 2.833
2023-04-27 05:21:02,955 - INFO - Epoch 95 training loss = 3.215
2023-04-27 05:21:03,708 - INFO - Epoch 96 training loss = 2.986
2023-04-27 05:21:04,460 - INFO - Epoch 97 training loss = 3.112
2023-04-27 05:21:05,213 - INFO - Epoch 98 training loss = 3.192
2023-04-27 05:21:05,965 - INFO - Epoch 99 training loss = 2.977
2023-04-27 05:21:06,718 - INFO - Epoch 100 training loss = 2.996
2023-04-27 05:21:06,779 - INFO - Validation loss = 3.033
2023-04-27 05:21:06,779 - INFO - best model
2023-04-27 05:21:07,544 - INFO - Epoch 101 training loss = 3.11
2023-04-27 05:21:08,296 - INFO - Epoch 102 training loss = 2.897
2023-04-27 05:21:09,048 - INFO - Epoch 103 training loss = 2.997
2023-04-27 05:21:09,801 - INFO - Epoch 104 training loss = 3.03
2023-04-27 05:21:10,554 - INFO - Epoch 105 training loss = 2.697
2023-04-27 05:21:11,313 - INFO - Epoch 106 training loss = 2.871
2023-04-27 05:21:12,071 - INFO - Epoch 107 training loss = 2.798
2023-04-27 05:21:12,830 - INFO - Epoch 108 training loss = 2.664
2023-04-27 05:21:13,586 - INFO - Epoch 109 training loss = 2.827
2023-04-27 05:21:14,338 - INFO - Epoch 110 training loss = 2.733
2023-04-27 05:21:14,399 - INFO - Validation loss = 4.21
2023-04-27 05:21:15,152 - INFO - Epoch 111 training loss = 2.721
2023-04-27 05:21:15,911 - INFO - Epoch 112 training loss = 2.673
2023-04-27 05:21:16,669 - INFO - Epoch 113 training loss = 2.651
2023-04-27 05:21:17,428 - INFO - Epoch 114 training loss = 2.654
2023-04-27 05:21:18,187 - INFO - Epoch 115 training loss = 2.606
2023-04-27 05:21:18,942 - INFO - Epoch 116 training loss = 2.528
2023-04-27 05:21:19,695 - INFO - Epoch 117 training loss = 2.68
2023-04-27 05:21:20,447 - INFO - Epoch 118 training loss = 2.418
2023-04-27 05:21:21,200 - INFO - Epoch 119 training loss = 2.51
2023-04-27 05:21:21,952 - INFO - Epoch 120 training loss = 2.341
2023-04-27 05:21:22,013 - INFO - Validation loss = 2.944
2023-04-27 05:21:22,013 - INFO - best model
2023-04-27 05:21:22,777 - INFO - Epoch 121 training loss = 2.629
2023-04-27 05:21:23,530 - INFO - Epoch 122 training loss = 2.316
2023-04-27 05:21:24,283 - INFO - Epoch 123 training loss = 2.484
2023-04-27 05:21:25,035 - INFO - Epoch 124 training loss = 2.561
2023-04-27 05:21:25,788 - INFO - Epoch 125 training loss = 2.726
2023-04-27 05:21:26,540 - INFO - Epoch 126 training loss = 2.453
2023-04-27 05:21:27,293 - INFO - Epoch 127 training loss = 2.442
2023-04-27 05:21:28,052 - INFO - Epoch 128 training loss = 2.272
2023-04-27 05:21:28,810 - INFO - Epoch 129 training loss = 2.294
2023-04-27 05:21:29,568 - INFO - Epoch 130 training loss = 2.167
2023-04-27 05:21:29,629 - INFO - Validation loss = 2.454
2023-04-27 05:21:29,630 - INFO - best model
2023-04-27 05:21:30,398 - INFO - Epoch 131 training loss = 2.233
2023-04-27 05:21:31,156 - INFO - Epoch 132 training loss = 2.366
2023-04-27 05:21:31,914 - INFO - Epoch 133 training loss = 2.17
2023-04-27 05:21:32,672 - INFO - Epoch 134 training loss = 2.376
2023-04-27 05:21:33,431 - INFO - Epoch 135 training loss = 2.17
2023-04-27 05:21:34,189 - INFO - Epoch 136 training loss = 2.197
2023-04-27 05:21:34,947 - INFO - Epoch 137 training loss = 2.049
2023-04-27 05:21:35,705 - INFO - Epoch 138 training loss = 2.138
2023-04-27 05:21:36,463 - INFO - Epoch 139 training loss = 2.144
2023-04-27 05:21:37,222 - INFO - Epoch 140 training loss = 2.092
2023-04-27 05:21:37,283 - INFO - Validation loss = 2.308
2023-04-27 05:21:37,283 - INFO - best model
2023-04-27 05:21:38,054 - INFO - Epoch 141 training loss = 2.171
2023-04-27 05:21:38,812 - INFO - Epoch 142 training loss = 2.006
2023-04-27 05:21:39,568 - INFO - Epoch 143 training loss = 1.977
2023-04-27 05:21:40,319 - INFO - Epoch 144 training loss = 2.163
2023-04-27 05:21:41,070 - INFO - Epoch 145 training loss = 2.001
2023-04-27 05:21:41,822 - INFO - Epoch 146 training loss = 2.055
2023-04-27 05:21:42,574 - INFO - Epoch 147 training loss = 2.132
2023-04-27 05:21:43,326 - INFO - Epoch 148 training loss = 2.111
2023-04-27 05:21:44,079 - INFO - Epoch 149 training loss = 1.941
2023-04-27 05:21:44,830 - INFO - Epoch 150 training loss = 1.921
2023-04-27 05:21:44,891 - INFO - Validation loss = 2.085
2023-04-27 05:21:44,892 - INFO - best model
2023-04-27 05:21:45,659 - INFO - Epoch 151 training loss = 1.933
2023-04-27 05:21:46,418 - INFO - Epoch 152 training loss = 1.999
2023-04-27 05:21:47,176 - INFO - Epoch 153 training loss = 1.793
2023-04-27 05:21:47,933 - INFO - Epoch 154 training loss = 1.871
2023-04-27 05:21:48,692 - INFO - Epoch 155 training loss = 1.772
2023-04-27 05:21:49,449 - INFO - Epoch 156 training loss = 1.837
2023-04-27 05:21:50,207 - INFO - Epoch 157 training loss = 1.849
2023-04-27 05:21:50,964 - INFO - Epoch 158 training loss = 1.876
2023-04-27 05:21:51,722 - INFO - Epoch 159 training loss = 1.752
2023-04-27 05:21:52,480 - INFO - Epoch 160 training loss = 1.72
2023-04-27 05:21:52,541 - INFO - Validation loss = 2.31
2023-04-27 05:21:53,299 - INFO - Epoch 161 training loss = 1.942
2023-04-27 05:21:54,056 - INFO - Epoch 162 training loss = 1.842
2023-04-27 05:21:54,814 - INFO - Epoch 163 training loss = 1.716
2023-04-27 05:21:55,571 - INFO - Epoch 164 training loss = 1.788
2023-04-27 05:21:56,329 - INFO - Epoch 165 training loss = 1.793
2023-04-27 05:21:57,086 - INFO - Epoch 166 training loss = 1.67
2023-04-27 05:21:57,844 - INFO - Epoch 167 training loss = 1.596
2023-04-27 05:21:58,601 - INFO - Epoch 168 training loss = 1.64
2023-04-27 05:21:59,358 - INFO - Epoch 169 training loss = 1.655
2023-04-27 05:22:00,116 - INFO - Epoch 170 training loss = 1.625
2023-04-27 05:22:00,177 - INFO - Validation loss = 2.375
2023-04-27 05:22:00,935 - INFO - Epoch 171 training loss = 1.629
2023-04-27 05:22:01,693 - INFO - Epoch 172 training loss = 1.685
2023-04-27 05:22:02,451 - INFO - Epoch 173 training loss = 1.594
2023-04-27 05:22:03,208 - INFO - Epoch 174 training loss = 1.608
2023-04-27 05:22:03,966 - INFO - Epoch 175 training loss = 1.593
2023-04-27 05:22:04,722 - INFO - Epoch 176 training loss = 1.582
2023-04-27 05:22:05,477 - INFO - Epoch 177 training loss = 1.548
2023-04-27 05:22:06,231 - INFO - Epoch 178 training loss = 1.662
2023-04-27 05:22:06,983 - INFO - Epoch 179 training loss = 1.512
2023-04-27 05:22:07,736 - INFO - Epoch 180 training loss = 1.657
2023-04-27 05:22:07,797 - INFO - Validation loss = 2.045
2023-04-27 05:22:07,798 - INFO - best model
2023-04-27 05:22:08,559 - INFO - Epoch 181 training loss =  1.5
2023-04-27 05:22:09,312 - INFO - Epoch 182 training loss = 1.514
2023-04-27 05:22:10,066 - INFO - Epoch 183 training loss = 1.515
2023-04-27 05:22:10,819 - INFO - Epoch 184 training loss = 1.402
2023-04-27 05:22:11,571 - INFO - Epoch 185 training loss = 1.471
2023-04-27 05:22:12,322 - INFO - Epoch 186 training loss = 1.515
2023-04-27 05:22:13,073 - INFO - Epoch 187 training loss = 1.553
2023-04-27 05:22:13,825 - INFO - Epoch 188 training loss = 1.377
2023-04-27 05:22:14,577 - INFO - Epoch 189 training loss =  1.4
2023-04-27 05:22:15,328 - INFO - Epoch 190 training loss = 1.377
2023-04-27 05:22:15,389 - INFO - Validation loss =  1.9
2023-04-27 05:22:15,390 - INFO - best model
2023-04-27 05:22:16,152 - INFO - Epoch 191 training loss = 1.397
2023-04-27 05:22:16,904 - INFO - Epoch 192 training loss = 1.433
2023-04-27 05:22:17,656 - INFO - Epoch 193 training loss = 1.339
2023-04-27 05:22:18,408 - INFO - Epoch 194 training loss = 1.421
2023-04-27 05:22:19,160 - INFO - Epoch 195 training loss = 1.264
2023-04-27 05:22:19,911 - INFO - Epoch 196 training loss = 1.325
2023-04-27 05:22:20,662 - INFO - Epoch 197 training loss = 1.378
2023-04-27 05:22:21,414 - INFO - Epoch 198 training loss = 1.367
2023-04-27 05:22:22,166 - INFO - Epoch 199 training loss = 1.319
2023-04-27 05:22:22,918 - INFO - Epoch 200 training loss = 1.322
2023-04-27 05:22:22,979 - INFO - Validation loss = 2.336
2023-04-27 05:22:23,731 - INFO - Epoch 201 training loss = 1.356
2023-04-27 05:22:24,483 - INFO - Epoch 202 training loss = 1.267
2023-04-27 05:22:25,234 - INFO - Epoch 203 training loss = 1.216
2023-04-27 05:22:25,986 - INFO - Epoch 204 training loss = 1.252
2023-04-27 05:22:26,737 - INFO - Epoch 205 training loss = 1.149
2023-04-27 05:22:27,489 - INFO - Epoch 206 training loss = 1.239
2023-04-27 05:22:28,240 - INFO - Epoch 207 training loss = 1.254
2023-04-27 05:22:28,992 - INFO - Epoch 208 training loss = 1.204
2023-04-27 05:22:29,744 - INFO - Epoch 209 training loss = 1.246
2023-04-27 05:22:30,496 - INFO - Epoch 210 training loss = 1.22
2023-04-27 05:22:30,556 - INFO - Validation loss = 1.777
2023-04-27 05:22:30,557 - INFO - best model
2023-04-27 05:22:31,319 - INFO - Epoch 211 training loss = 1.205
2023-04-27 05:22:32,070 - INFO - Epoch 212 training loss = 1.233
2023-04-27 05:22:32,822 - INFO - Epoch 213 training loss = 1.176
2023-04-27 05:22:33,574 - INFO - Epoch 214 training loss = 1.106
2023-04-27 05:22:34,325 - INFO - Epoch 215 training loss = 1.258
2023-04-27 05:22:35,077 - INFO - Epoch 216 training loss = 1.126
2023-04-27 05:22:35,829 - INFO - Epoch 217 training loss = 1.103
2023-04-27 05:22:36,581 - INFO - Epoch 218 training loss = 1.14
2023-04-27 05:22:37,333 - INFO - Epoch 219 training loss = 1.15
2023-04-27 05:22:38,089 - INFO - Epoch 220 training loss = 1.094
2023-04-27 05:22:38,150 - INFO - Validation loss = 1.635
2023-04-27 05:22:38,150 - INFO - best model
2023-04-27 05:22:38,915 - INFO - Epoch 221 training loss = 1.084
2023-04-27 05:22:39,669 - INFO - Epoch 222 training loss = 1.259
2023-04-27 05:22:40,423 - INFO - Epoch 223 training loss = 1.013
2023-04-27 05:22:41,178 - INFO - Epoch 224 training loss = 1.046
2023-04-27 05:22:41,933 - INFO - Epoch 225 training loss = 1.035
2023-04-27 05:22:42,686 - INFO - Epoch 226 training loss = 1.029
2023-04-27 05:22:43,437 - INFO - Epoch 227 training loss = 1.051
2023-04-27 05:22:44,189 - INFO - Epoch 228 training loss = 1.136
2023-04-27 05:22:44,942 - INFO - Epoch 229 training loss = 0.9671
2023-04-27 05:22:45,693 - INFO - Epoch 230 training loss = 0.9888
2023-04-27 05:22:45,754 - INFO - Validation loss = 1.609
2023-04-27 05:22:45,755 - INFO - best model
2023-04-27 05:22:46,516 - INFO - Epoch 231 training loss = 0.9594
2023-04-27 05:22:47,268 - INFO - Epoch 232 training loss = 0.9811
2023-04-27 05:22:48,020 - INFO - Epoch 233 training loss = 0.9916
2023-04-27 05:22:48,771 - INFO - Epoch 234 training loss = 0.9731
2023-04-27 05:22:49,522 - INFO - Epoch 235 training loss = 1.008
2023-04-27 05:22:50,273 - INFO - Epoch 236 training loss = 0.9372
2023-04-27 05:22:51,024 - INFO - Epoch 237 training loss = 0.9391
2023-04-27 05:22:51,776 - INFO - Epoch 238 training loss = 0.9284
2023-04-27 05:22:52,528 - INFO - Epoch 239 training loss = 0.9306
2023-04-27 05:22:53,279 - INFO - Epoch 240 training loss = 0.8889
2023-04-27 05:22:53,340 - INFO - Validation loss = 1.434
2023-04-27 05:22:53,340 - INFO - best model
2023-04-27 05:22:54,102 - INFO - Epoch 241 training loss = 0.9517
2023-04-27 05:22:54,853 - INFO - Epoch 242 training loss = 0.9012
2023-04-27 05:22:55,604 - INFO - Epoch 243 training loss = 0.849
2023-04-27 05:22:56,355 - INFO - Epoch 244 training loss = 0.9088
2023-04-27 05:22:57,106 - INFO - Epoch 245 training loss = 0.8911
2023-04-27 05:22:57,857 - INFO - Epoch 246 training loss = 0.8451
2023-04-27 05:22:58,609 - INFO - Epoch 247 training loss = 0.922
2023-04-27 05:22:59,360 - INFO - Epoch 248 training loss = 0.8701
2023-04-27 05:23:00,111 - INFO - Epoch 249 training loss = 0.8311
2023-04-27 05:23:00,863 - INFO - Epoch 250 training loss = 0.7893
2023-04-27 05:23:00,924 - INFO - Validation loss = 1.321
2023-04-27 05:23:00,924 - INFO - best model
2023-04-27 05:23:01,686 - INFO - Epoch 251 training loss = 0.8182
2023-04-27 05:23:02,438 - INFO - Epoch 252 training loss = 0.8347
2023-04-27 05:23:03,189 - INFO - Epoch 253 training loss = 0.8107
2023-04-27 05:23:03,941 - INFO - Epoch 254 training loss = 0.866
2023-04-27 05:23:04,692 - INFO - Epoch 255 training loss = 0.7801
2023-04-27 05:23:05,445 - INFO - Epoch 256 training loss = 0.7718
2023-04-27 05:23:06,199 - INFO - Epoch 257 training loss = 0.7863
2023-04-27 05:23:06,953 - INFO - Epoch 258 training loss = 0.7594
2023-04-27 05:23:07,708 - INFO - Epoch 259 training loss = 0.7748
2023-04-27 05:23:08,462 - INFO - Epoch 260 training loss = 0.8007
2023-04-27 05:23:08,523 - INFO - Validation loss = 1.357
2023-04-27 05:23:09,277 - INFO - Epoch 261 training loss = 0.774
2023-04-27 05:23:10,031 - INFO - Epoch 262 training loss = 0.7296
2023-04-27 05:23:10,785 - INFO - Epoch 263 training loss = 0.7662
2023-04-27 05:23:11,539 - INFO - Epoch 264 training loss = 0.7979
2023-04-27 05:23:12,294 - INFO - Epoch 265 training loss = 0.7283
2023-04-27 05:23:13,048 - INFO - Epoch 266 training loss = 0.7222
2023-04-27 05:23:13,802 - INFO - Epoch 267 training loss = 0.7245
2023-04-27 05:23:14,556 - INFO - Epoch 268 training loss = 0.6906
2023-04-27 05:23:15,311 - INFO - Epoch 269 training loss = 0.7045
2023-04-27 05:23:16,065 - INFO - Epoch 270 training loss = 0.6937
2023-04-27 05:23:16,126 - INFO - Validation loss = 1.377
2023-04-27 05:23:16,881 - INFO - Epoch 271 training loss = 0.6665
2023-04-27 05:23:17,635 - INFO - Epoch 272 training loss = 0.6609
2023-04-27 05:23:18,390 - INFO - Epoch 273 training loss = 0.6789
2023-04-27 05:23:19,144 - INFO - Epoch 274 training loss = 0.6798
2023-04-27 05:23:19,898 - INFO - Epoch 275 training loss = 0.6266
2023-04-27 05:23:20,653 - INFO - Epoch 276 training loss = 0.6463
2023-04-27 05:23:21,407 - INFO - Epoch 277 training loss = 0.7065
2023-04-27 05:23:22,162 - INFO - Epoch 278 training loss = 0.6342
2023-04-27 05:23:22,916 - INFO - Epoch 279 training loss = 0.6523
2023-04-27 05:23:23,667 - INFO - Epoch 280 training loss = 0.6082
2023-04-27 05:23:23,730 - INFO - Validation loss = 1.503
2023-04-27 05:23:24,481 - INFO - Epoch 281 training loss = 0.6291
2023-04-27 05:23:25,233 - INFO - Epoch 282 training loss = 0.6287
2023-04-27 05:23:25,984 - INFO - Epoch 283 training loss = 0.6251
2023-04-27 05:23:26,735 - INFO - Epoch 284 training loss = 0.6003
2023-04-27 05:23:27,486 - INFO - Epoch 285 training loss = 0.5592
2023-04-27 05:23:28,238 - INFO - Epoch 286 training loss = 0.5932
2023-04-27 05:23:28,989 - INFO - Epoch 287 training loss = 0.5557
2023-04-27 05:23:29,740 - INFO - Epoch 288 training loss = 0.5862
2023-04-27 05:23:30,491 - INFO - Epoch 289 training loss = 0.5761
2023-04-27 05:23:31,242 - INFO - Epoch 290 training loss = 0.559
2023-04-27 05:23:31,303 - INFO - Validation loss = 1.241
2023-04-27 05:23:31,303 - INFO - best model
2023-04-27 05:23:32,065 - INFO - Epoch 291 training loss = 0.5821
2023-04-27 05:23:32,816 - INFO - Epoch 292 training loss = 0.5537
2023-04-27 05:23:33,567 - INFO - Epoch 293 training loss = 0.5537
2023-04-27 05:23:34,318 - INFO - Epoch 294 training loss = 0.5599
2023-04-27 05:23:35,070 - INFO - Epoch 295 training loss = 0.5799
2023-04-27 05:23:35,821 - INFO - Epoch 296 training loss = 0.5504
2023-04-27 05:23:36,573 - INFO - Epoch 297 training loss = 0.509
2023-04-27 05:23:37,324 - INFO - Epoch 298 training loss = 0.5325
2023-04-27 05:23:38,079 - INFO - Epoch 299 training loss = 0.5093
2023-04-27 05:23:38,833 - INFO - Epoch 300 training loss = 0.4949
2023-04-27 05:23:38,894 - INFO - Validation loss = 1.561
2023-04-27 05:23:39,648 - INFO - Epoch 301 training loss = 0.518
2023-04-27 05:23:40,402 - INFO - Epoch 302 training loss = 0.5146
2023-04-27 05:23:41,155 - INFO - Epoch 303 training loss = 0.495
2023-04-27 05:23:41,910 - INFO - Epoch 304 training loss = 0.5132
2023-04-27 05:23:42,665 - INFO - Epoch 305 training loss = 0.4757
2023-04-27 05:23:43,420 - INFO - Epoch 306 training loss = 0.4614
2023-04-27 05:23:44,174 - INFO - Epoch 307 training loss = 0.4824
2023-04-27 05:23:44,928 - INFO - Epoch 308 training loss = 0.4911
2023-04-27 05:23:45,683 - INFO - Epoch 309 training loss = 0.4911
2023-04-27 05:23:46,438 - INFO - Epoch 310 training loss = 0.4849
2023-04-27 05:23:46,499 - INFO - Validation loss = 1.32
2023-04-27 05:23:47,253 - INFO - Epoch 311 training loss = 0.4663
2023-04-27 05:23:48,007 - INFO - Epoch 312 training loss = 0.4684
2023-04-27 05:23:48,765 - INFO - Epoch 313 training loss = 0.4653
2023-04-27 05:23:49,523 - INFO - Epoch 314 training loss = 0.4514
2023-04-27 05:23:50,280 - INFO - Epoch 315 training loss = 0.4441
2023-04-27 05:23:51,038 - INFO - Epoch 316 training loss = 0.4486
2023-04-27 05:23:51,796 - INFO - Epoch 317 training loss = 0.421
2023-04-27 05:23:52,553 - INFO - Epoch 318 training loss = 0.4377
2023-04-27 05:23:53,304 - INFO - Epoch 319 training loss = 0.4091
2023-04-27 05:23:54,055 - INFO - Epoch 320 training loss = 0.4443
2023-04-27 05:23:54,116 - INFO - Validation loss = 1.139
2023-04-27 05:23:54,116 - INFO - best model
2023-04-27 05:23:54,878 - INFO - Epoch 321 training loss = 0.4194
2023-04-27 05:23:55,629 - INFO - Epoch 322 training loss = 0.4306
2023-04-27 05:23:56,380 - INFO - Epoch 323 training loss = 0.4242
2023-04-27 05:23:57,131 - INFO - Epoch 324 training loss = 0.4019
2023-04-27 05:23:57,883 - INFO - Epoch 325 training loss = 0.4165
2023-04-27 05:23:58,641 - INFO - Epoch 326 training loss = 0.3911
2023-04-27 05:23:59,399 - INFO - Epoch 327 training loss = 0.3825
2023-04-27 05:24:00,157 - INFO - Epoch 328 training loss = 0.3798
2023-04-27 05:24:00,910 - INFO - Epoch 329 training loss = 0.3832
2023-04-27 05:24:01,662 - INFO - Epoch 330 training loss = 0.3742
2023-04-27 05:24:01,723 - INFO - Validation loss = 1.091
2023-04-27 05:24:01,723 - INFO - best model
2023-04-27 05:24:02,485 - INFO - Epoch 331 training loss = 0.3649
2023-04-27 05:24:03,236 - INFO - Epoch 332 training loss = 0.3846
2023-04-27 05:24:03,988 - INFO - Epoch 333 training loss = 0.3689
2023-04-27 05:24:04,739 - INFO - Epoch 334 training loss = 0.3792
2023-04-27 05:24:05,491 - INFO - Epoch 335 training loss = 0.3613
2023-04-27 05:24:06,242 - INFO - Epoch 336 training loss = 0.3657
2023-04-27 05:24:06,993 - INFO - Epoch 337 training loss = 0.355
2023-04-27 05:24:07,745 - INFO - Epoch 338 training loss = 0.3504
2023-04-27 05:24:08,497 - INFO - Epoch 339 training loss = 0.3393
2023-04-27 05:24:09,248 - INFO - Epoch 340 training loss = 0.3449
2023-04-27 05:24:09,309 - INFO - Validation loss = 1.069
2023-04-27 05:24:09,309 - INFO - best model
2023-04-27 05:24:10,073 - INFO - Epoch 341 training loss = 0.3491
2023-04-27 05:24:10,824 - INFO - Epoch 342 training loss = 0.3351
2023-04-27 05:24:11,578 - INFO - Epoch 343 training loss = 0.3303
2023-04-27 05:24:12,336 - INFO - Epoch 344 training loss = 0.3287
2023-04-27 05:24:13,094 - INFO - Epoch 345 training loss = 0.3202
2023-04-27 05:24:13,852 - INFO - Epoch 346 training loss = 0.3309
2023-04-27 05:24:14,610 - INFO - Epoch 347 training loss = 0.3219
2023-04-27 05:24:15,368 - INFO - Epoch 348 training loss = 0.3196
2023-04-27 05:24:16,126 - INFO - Epoch 349 training loss = 0.3252
2023-04-27 05:24:16,883 - INFO - Epoch 350 training loss = 0.3188
2023-04-27 05:24:16,944 - INFO - Validation loss = 1.032
2023-04-27 05:24:16,944 - INFO - best model
2023-04-27 05:24:17,713 - INFO - Epoch 351 training loss = 0.3153
2023-04-27 05:24:18,471 - INFO - Epoch 352 training loss = 0.3028
2023-04-27 05:24:19,229 - INFO - Epoch 353 training loss = 0.3048
2023-04-27 05:24:19,987 - INFO - Epoch 354 training loss = 0.2966
2023-04-27 05:24:20,745 - INFO - Epoch 355 training loss = 0.3044
2023-04-27 05:24:21,503 - INFO - Epoch 356 training loss = 0.2962
2023-04-27 05:24:22,261 - INFO - Epoch 357 training loss = 0.2847
2023-04-27 05:24:23,014 - INFO - Epoch 358 training loss = 0.2854
2023-04-27 05:24:23,771 - INFO - Epoch 359 training loss = 0.2872
2023-04-27 05:24:24,529 - INFO - Epoch 360 training loss = 0.2845
2023-04-27 05:24:24,590 - INFO - Validation loss = 1.023
2023-04-27 05:24:24,591 - INFO - best model
2023-04-27 05:24:25,358 - INFO - Epoch 361 training loss = 0.2802
2023-04-27 05:24:26,114 - INFO - Epoch 362 training loss = 0.2811
2023-04-27 05:24:26,865 - INFO - Epoch 363 training loss = 0.2865
2023-04-27 05:24:27,617 - INFO - Epoch 364 training loss = 0.2826
2023-04-27 05:24:28,368 - INFO - Epoch 365 training loss = 0.2822
2023-04-27 05:24:29,119 - INFO - Epoch 366 training loss = 0.2711
2023-04-27 05:24:29,870 - INFO - Epoch 367 training loss = 0.2695
2023-04-27 05:24:30,621 - INFO - Epoch 368 training loss = 0.2581
2023-04-27 05:24:31,372 - INFO - Epoch 369 training loss = 0.2582
2023-04-27 05:24:32,123 - INFO - Epoch 370 training loss = 0.2637
2023-04-27 05:24:32,184 - INFO - Validation loss = 1.038
2023-04-27 05:24:32,935 - INFO - Epoch 371 training loss = 0.2633
2023-04-27 05:24:33,691 - INFO - Epoch 372 training loss = 0.2568
2023-04-27 05:24:34,449 - INFO - Epoch 373 training loss = 0.2543
2023-04-27 05:24:35,207 - INFO - Epoch 374 training loss = 0.2535
2023-04-27 05:24:35,965 - INFO - Epoch 375 training loss = 0.25
2023-04-27 05:24:36,723 - INFO - Epoch 376 training loss = 0.2467
2023-04-27 05:24:37,480 - INFO - Epoch 377 training loss = 0.2416
2023-04-27 05:24:38,233 - INFO - Epoch 378 training loss = 0.2431
2023-04-27 05:24:38,991 - INFO - Epoch 379 training loss = 0.2428
2023-04-27 05:24:39,749 - INFO - Epoch 380 training loss = 0.2359
2023-04-27 05:24:39,810 - INFO - Validation loss = 0.9934
2023-04-27 05:24:39,810 - INFO - best model
2023-04-27 05:24:40,579 - INFO - Epoch 381 training loss = 0.2329
2023-04-27 05:24:41,337 - INFO - Epoch 382 training loss = 0.2355
2023-04-27 05:24:42,096 - INFO - Epoch 383 training loss = 0.2363
2023-04-27 05:24:42,854 - INFO - Epoch 384 training loss = 0.2375
2023-04-27 05:24:43,613 - INFO - Epoch 385 training loss = 0.2368
2023-04-27 05:24:44,372 - INFO - Epoch 386 training loss = 0.2244
2023-04-27 05:24:45,131 - INFO - Epoch 387 training loss = 0.2276
2023-04-27 05:24:45,889 - INFO - Epoch 388 training loss = 0.2264
2023-04-27 05:24:46,648 - INFO - Epoch 389 training loss = 0.2222
2023-04-27 05:24:47,406 - INFO - Epoch 390 training loss = 0.2239
2023-04-27 05:24:47,467 - INFO - Validation loss = 0.9676
2023-04-27 05:24:47,467 - INFO - best model
2023-04-27 05:24:48,229 - INFO - Epoch 391 training loss = 0.2149
2023-04-27 05:24:48,980 - INFO - Epoch 392 training loss = 0.2164
2023-04-27 05:24:49,731 - INFO - Epoch 393 training loss = 0.2257
2023-04-27 05:24:50,482 - INFO - Epoch 394 training loss = 0.2159
2023-04-27 05:24:51,233 - INFO - Epoch 395 training loss = 0.2146
2023-04-27 05:24:51,985 - INFO - Epoch 396 training loss = 0.2147
2023-04-27 05:24:52,738 - INFO - Epoch 397 training loss = 0.2101
2023-04-27 05:24:53,490 - INFO - Epoch 398 training loss = 0.2133
2023-04-27 05:24:54,243 - INFO - Epoch 399 training loss = 0.21
2023-04-27 05:24:54,995 - INFO - Epoch 400 training loss = 0.2066
2023-04-27 05:24:55,056 - INFO - Validation loss = 0.9806
2023-04-27 05:24:55,808 - INFO - Epoch 401 training loss = 0.2072
2023-04-27 05:24:56,561 - INFO - Epoch 402 training loss = 0.2009
2023-04-27 05:24:57,313 - INFO - Epoch 403 training loss = 0.2005
2023-04-27 05:24:58,065 - INFO - Epoch 404 training loss = 0.2042
2023-04-27 05:24:58,818 - INFO - Epoch 405 training loss = 0.1995
2023-04-27 05:24:59,570 - INFO - Epoch 406 training loss = 0.1981
2023-04-27 05:25:00,323 - INFO - Epoch 407 training loss = 0.1969
2023-04-27 05:25:01,075 - INFO - Epoch 408 training loss = 0.1997
2023-04-27 05:25:01,827 - INFO - Epoch 409 training loss = 0.1919
2023-04-27 05:25:02,580 - INFO - Epoch 410 training loss = 0.1949
2023-04-27 05:25:02,641 - INFO - Validation loss = 0.9475
2023-04-27 05:25:02,641 - INFO - best model
2023-04-27 05:25:03,404 - INFO - Epoch 411 training loss = 0.1935
2023-04-27 05:25:04,156 - INFO - Epoch 412 training loss = 0.1896
2023-04-27 05:25:04,908 - INFO - Epoch 413 training loss = 0.191
2023-04-27 05:25:05,659 - INFO - Epoch 414 training loss = 0.1896
2023-04-27 05:25:06,411 - INFO - Epoch 415 training loss = 0.1883
2023-04-27 05:25:07,162 - INFO - Epoch 416 training loss = 0.189
2023-04-27 05:25:07,915 - INFO - Epoch 417 training loss = 0.1897
2023-04-27 05:25:08,667 - INFO - Epoch 418 training loss = 0.1851
2023-04-27 05:25:09,419 - INFO - Epoch 419 training loss = 0.1857
2023-04-27 05:25:10,171 - INFO - Epoch 420 training loss = 0.1858
2023-04-27 05:25:10,231 - INFO - Validation loss = 0.9444
2023-04-27 05:25:10,232 - INFO - best model
2023-04-27 05:25:10,994 - INFO - Epoch 421 training loss = 0.1826
2023-04-27 05:25:11,747 - INFO - Epoch 422 training loss = 0.181
2023-04-27 05:25:12,501 - INFO - Epoch 423 training loss = 0.1816
2023-04-27 05:25:13,253 - INFO - Epoch 424 training loss = 0.1801
2023-04-27 05:25:14,006 - INFO - Epoch 425 training loss = 0.1783
2023-04-27 05:25:14,759 - INFO - Epoch 426 training loss = 0.1777
2023-04-27 05:25:15,512 - INFO - Epoch 427 training loss = 0.1784
2023-04-27 05:25:16,265 - INFO - Epoch 428 training loss = 0.1785
2023-04-27 05:25:17,017 - INFO - Epoch 429 training loss = 0.1772
2023-04-27 05:25:17,771 - INFO - Epoch 430 training loss = 0.1748
2023-04-27 05:25:17,831 - INFO - Validation loss = 0.9418
2023-04-27 05:25:17,832 - INFO - best model
2023-04-27 05:25:18,598 - INFO - Epoch 431 training loss = 0.1744
2023-04-27 05:25:19,353 - INFO - Epoch 432 training loss = 0.1762
2023-04-27 05:25:20,107 - INFO - Epoch 433 training loss = 0.1744
2023-04-27 05:25:20,862 - INFO - Epoch 434 training loss = 0.1725
2023-04-27 05:25:21,616 - INFO - Epoch 435 training loss = 0.172
2023-04-27 05:25:22,371 - INFO - Epoch 436 training loss = 0.172
2023-04-27 05:25:23,126 - INFO - Epoch 437 training loss = 0.1707
2023-04-27 05:25:23,881 - INFO - Epoch 438 training loss = 0.1696
2023-04-27 05:25:24,636 - INFO - Epoch 439 training loss = 0.1695
2023-04-27 05:25:25,391 - INFO - Epoch 440 training loss = 0.168
2023-04-27 05:25:25,452 - INFO - Validation loss = 0.9262
2023-04-27 05:25:25,452 - INFO - best model
2023-04-27 05:25:26,217 - INFO - Epoch 441 training loss = 0.1684
2023-04-27 05:25:26,971 - INFO - Epoch 442 training loss = 0.1679
2023-04-27 05:25:27,725 - INFO - Epoch 443 training loss = 0.1667
2023-04-27 05:25:28,479 - INFO - Epoch 444 training loss = 0.1656
2023-04-27 05:25:29,233 - INFO - Epoch 445 training loss = 0.1649
2023-04-27 05:25:29,987 - INFO - Epoch 446 training loss = 0.165
2023-04-27 05:25:30,741 - INFO - Epoch 447 training loss = 0.1641
2023-04-27 05:25:31,495 - INFO - Epoch 448 training loss = 0.164
2023-04-27 05:25:32,249 - INFO - Epoch 449 training loss = 0.1635
2023-04-27 05:25:33,003 - INFO - Epoch 450 training loss = 0.163
2023-04-27 05:25:33,064 - INFO - Validation loss = 0.9264
2023-04-27 05:25:33,819 - INFO - Epoch 451 training loss = 0.1623
2023-04-27 05:25:34,573 - INFO - Epoch 452 training loss = 0.1616
2023-04-27 05:25:35,327 - INFO - Epoch 453 training loss = 0.1619
2023-04-27 05:25:36,081 - INFO - Epoch 454 training loss = 0.1613
2023-04-27 05:25:36,835 - INFO - Epoch 455 training loss = 0.1601
2023-04-27 05:25:37,590 - INFO - Epoch 456 training loss = 0.1603
2023-04-27 05:25:38,345 - INFO - Epoch 457 training loss = 0.1594
2023-04-27 05:25:39,099 - INFO - Epoch 458 training loss = 0.1596
2023-04-27 05:25:39,853 - INFO - Epoch 459 training loss = 0.1586
2023-04-27 05:25:40,608 - INFO - Epoch 460 training loss = 0.1584
2023-04-27 05:25:40,669 - INFO - Validation loss = 0.9365
2023-04-27 05:25:41,423 - INFO - Epoch 461 training loss = 0.1581
2023-04-27 05:25:42,178 - INFO - Epoch 462 training loss = 0.1579
2023-04-27 05:25:42,933 - INFO - Epoch 463 training loss = 0.1574
2023-04-27 05:25:43,689 - INFO - Epoch 464 training loss = 0.1573
2023-04-27 05:25:44,443 - INFO - Epoch 465 training loss = 0.1572
2023-04-27 05:25:45,198 - INFO - Epoch 466 training loss = 0.1564
2023-04-27 05:25:45,953 - INFO - Epoch 467 training loss = 0.1559
2023-04-27 05:25:46,709 - INFO - Epoch 468 training loss = 0.1558
2023-04-27 05:25:47,464 - INFO - Epoch 469 training loss = 0.1558
2023-04-27 05:25:48,219 - INFO - Epoch 470 training loss = 0.1554
2023-04-27 05:25:48,280 - INFO - Validation loss = 0.9229
2023-04-27 05:25:48,281 - INFO - best model
2023-04-27 05:25:49,046 - INFO - Epoch 471 training loss = 0.1549
2023-04-27 05:25:49,801 - INFO - Epoch 472 training loss = 0.1546
2023-04-27 05:25:50,555 - INFO - Epoch 473 training loss = 0.1545
2023-04-27 05:25:51,310 - INFO - Epoch 474 training loss = 0.154
2023-04-27 05:25:52,065 - INFO - Epoch 475 training loss = 0.1541
2023-04-27 05:25:52,820 - INFO - Epoch 476 training loss = 0.1536
2023-04-27 05:25:53,574 - INFO - Epoch 477 training loss = 0.1533
2023-04-27 05:25:54,328 - INFO - Epoch 478 training loss = 0.1532
2023-04-27 05:25:55,082 - INFO - Epoch 479 training loss = 0.1531
2023-04-27 05:25:55,836 - INFO - Epoch 480 training loss = 0.1528
2023-04-27 05:25:55,897 - INFO - Validation loss = 0.9234
2023-04-27 05:25:56,652 - INFO - Epoch 481 training loss = 0.1529
2023-04-27 05:25:57,407 - INFO - Epoch 482 training loss = 0.1524
2023-04-27 05:25:58,161 - INFO - Epoch 483 training loss = 0.1522
2023-04-27 05:25:58,915 - INFO - Epoch 484 training loss = 0.152
2023-04-27 05:25:59,670 - INFO - Epoch 485 training loss = 0.152
2023-04-27 05:26:00,425 - INFO - Epoch 486 training loss = 0.1518
2023-04-27 05:26:01,180 - INFO - Epoch 487 training loss = 0.1517
2023-04-27 05:26:01,935 - INFO - Epoch 488 training loss = 0.1516
2023-04-27 05:26:02,690 - INFO - Epoch 489 training loss = 0.1514
2023-04-27 05:26:03,445 - INFO - Epoch 490 training loss = 0.1512
2023-04-27 05:26:03,506 - INFO - Validation loss = 0.9222
2023-04-27 05:26:03,507 - INFO - best model
2023-04-27 05:26:04,272 - INFO - Epoch 491 training loss = 0.1512
2023-04-27 05:26:05,027 - INFO - Epoch 492 training loss = 0.1511
2023-04-27 05:26:05,782 - INFO - Epoch 493 training loss = 0.151
2023-04-27 05:26:06,537 - INFO - Epoch 494 training loss = 0.1509
2023-04-27 05:26:07,292 - INFO - Epoch 495 training loss = 0.1508
2023-04-27 05:26:08,045 - INFO - Epoch 496 training loss = 0.1508
2023-04-27 05:26:08,798 - INFO - Epoch 497 training loss = 0.1508
2023-04-27 05:26:09,550 - INFO - Epoch 498 training loss = 0.1507
2023-04-27 05:26:10,303 - INFO - Epoch 499 training loss = 0.1507
2023-04-27 05:26:10,353 - INFO - Validation loss = 0.9208
