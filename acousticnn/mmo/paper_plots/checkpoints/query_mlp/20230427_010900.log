2023-04-27 01:09:00,317 - INFO - Config:
Namespace(config='configs/implicit_mlp.yaml', dir='/user/delden/acoustics/spp_ai_acoustics/acousticNN/experiments/arch/no_encoding/implicitmlp', epochs=100, device='cuda', seed=2, parameter_list='configs/data.yaml', encoding='none', dir_name='arch/no_encoding/implicitmlp')
2023-04-27 01:09:00,318 - INFO - Config:
Munch({'optimizer': Munch({'type': 'AdamW', 'kwargs': Munch({'lr': 0.001, 'weight_decay': 0.005, 'betas': [0.9, 0.95]})}), 'scheduler': Munch({'type': 'CosLR', 'kwargs': Munch({'epochs': 500, 'initial_epochs': 25})}), 'model': Munch({'name': 'ImplicitMLP', 'input_encoding': False, 'encoding_dim': 16, 'embed_dim': 64, 'depth': 7, 'mlp_width': 256}), 'generation_frequency': 5001, 'validation_frequency': 10, 'batch_size': 16, 'epochs': 500, 'gradient_clip': 10})
2023-04-27 01:09:14,443 - INFO - ==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ImplicitMLP                              [16, 200, 4]              --
├─GroupwiseProjection: 1-1               [3200, 15, 64]            --
│    └─ModuleList: 2-1                   --                        --
│    │    └─Linear: 3-1                  [3200, 4, 64]             128
│    │    └─Linear: 3-2                  [3200, 5, 64]             128
│    │    └─Linear: 3-3                  [3200, 5, 64]             128
│    │    └─Linear: 3-4                  [3200, 1, 64]             128
├─MLP: 1-2                               [3200, 256]               --
│    └─Linear: 2-2                       [3200, 256]               246,016
│    └─ReLU: 2-3                         [3200, 256]               --
│    └─Dropout: 2-4                      [3200, 256]               --
│    └─Linear: 2-5                       [3200, 256]               65,792
│    └─ReLU: 2-6                         [3200, 256]               --
│    └─Dropout: 2-7                      [3200, 256]               --
│    └─Linear: 2-8                       [3200, 256]               65,792
│    └─ReLU: 2-9                         [3200, 256]               --
│    └─Dropout: 2-10                     [3200, 256]               --
│    └─Linear: 2-11                      [3200, 256]               65,792
│    └─ReLU: 2-12                        [3200, 256]               --
│    └─Dropout: 2-13                     [3200, 256]               --
│    └─Linear: 2-14                      [3200, 256]               65,792
│    └─ReLU: 2-15                        [3200, 256]               --
│    └─Dropout: 2-16                     [3200, 256]               --
│    └─Linear: 2-17                      [3200, 256]               65,792
│    └─ReLU: 2-18                        [3200, 256]               --
│    └─Dropout: 2-19                     [3200, 256]               --
│    └─Linear: 2-20                      [3200, 256]               65,792
│    └─Dropout: 2-21                     [3200, 256]               --
├─Linear: 1-3                            [3200, 4]                 1,028
==========================================================================================
Total params: 642,308
Trainable params: 642,308
Non-trainable params: 0
Total mult-adds (G): 2.06
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 70.55
Params size (MB): 2.57
Estimated Total Size (MB): 73.14
==========================================================================================
2023-04-27 01:09:15,237 - INFO - Epoch 0 training loss = 3.314e+03
2023-04-27 01:09:15,303 - INFO - Validation loss = 3.237e+03
2023-04-27 01:09:15,303 - INFO - best model
2023-04-27 01:09:16,076 - INFO - Epoch 1 training loss = 3.103e+03
2023-04-27 01:09:16,835 - INFO - Epoch 2 training loss = 731.3
2023-04-27 01:09:17,594 - INFO - Epoch 3 training loss = 123.5
2023-04-27 01:09:18,351 - INFO - Epoch 4 training loss = 97.63
2023-04-27 01:09:19,109 - INFO - Epoch 5 training loss = 85.81
2023-04-27 01:09:19,866 - INFO - Epoch 6 training loss = 71.98
2023-04-27 01:09:20,622 - INFO - Epoch 7 training loss = 60.09
2023-04-27 01:09:21,379 - INFO - Epoch 8 training loss = 50.42
2023-04-27 01:09:22,136 - INFO - Epoch 9 training loss = 41.32
2023-04-27 01:09:22,894 - INFO - Epoch 10 training loss = 35.61
2023-04-27 01:09:22,955 - INFO - Validation loss = 34.9
2023-04-27 01:09:22,956 - INFO - best model
2023-04-27 01:09:23,722 - INFO - Epoch 11 training loss = 32.29
2023-04-27 01:09:24,479 - INFO - Epoch 12 training loss = 31.07
2023-04-27 01:09:25,236 - INFO - Epoch 13 training loss = 26.2
2023-04-27 01:09:25,993 - INFO - Epoch 14 training loss = 25.63
2023-04-27 01:09:26,750 - INFO - Epoch 15 training loss = 23.61
2023-04-27 01:09:27,507 - INFO - Epoch 16 training loss = 20.97
2023-04-27 01:09:28,265 - INFO - Epoch 17 training loss = 19.34
2023-04-27 01:09:29,021 - INFO - Epoch 18 training loss = 18.84
2023-04-27 01:09:29,778 - INFO - Epoch 19 training loss = 17.65
2023-04-27 01:09:30,535 - INFO - Epoch 20 training loss = 17.28
2023-04-27 01:09:30,596 - INFO - Validation loss = 16.51
2023-04-27 01:09:30,596 - INFO - best model
2023-04-27 01:09:31,364 - INFO - Epoch 21 training loss = 16.02
2023-04-27 01:09:32,122 - INFO - Epoch 22 training loss = 15.67
2023-04-27 01:09:32,880 - INFO - Epoch 23 training loss = 16.58
2023-04-27 01:09:33,636 - INFO - Epoch 24 training loss = 14.61
2023-04-27 01:09:34,393 - INFO - Epoch 25 training loss = 15.68
2023-04-27 01:09:35,151 - INFO - Epoch 26 training loss = 13.7
2023-04-27 01:09:35,908 - INFO - Epoch 27 training loss = 12.19
2023-04-27 01:09:36,665 - INFO - Epoch 28 training loss = 12.42
2023-04-27 01:09:37,421 - INFO - Epoch 29 training loss = 11.58
2023-04-27 01:09:38,178 - INFO - Epoch 30 training loss = 10.68
2023-04-27 01:09:38,242 - INFO - Validation loss = 12.46
2023-04-27 01:09:38,242 - INFO - best model
2023-04-27 01:09:39,010 - INFO - Epoch 31 training loss = 10.4
2023-04-27 01:09:39,766 - INFO - Epoch 32 training loss = 9.962
2023-04-27 01:09:40,523 - INFO - Epoch 33 training loss = 9.76
2023-04-27 01:09:41,281 - INFO - Epoch 34 training loss = 9.616
2023-04-27 01:09:42,038 - INFO - Epoch 35 training loss = 9.048
2023-04-27 01:09:42,795 - INFO - Epoch 36 training loss =  8.6
2023-04-27 01:09:43,552 - INFO - Epoch 37 training loss = 8.193
2023-04-27 01:09:44,310 - INFO - Epoch 38 training loss = 8.178
2023-04-27 01:09:45,067 - INFO - Epoch 39 training loss = 7.304
2023-04-27 01:09:45,825 - INFO - Epoch 40 training loss = 7.968
2023-04-27 01:09:45,886 - INFO - Validation loss = 7.018
2023-04-27 01:09:45,886 - INFO - best model
2023-04-27 01:09:46,654 - INFO - Epoch 41 training loss = 7.638
2023-04-27 01:09:47,415 - INFO - Epoch 42 training loss = 6.735
2023-04-27 01:09:48,176 - INFO - Epoch 43 training loss = 7.135
2023-04-27 01:09:48,934 - INFO - Epoch 44 training loss = 6.636
2023-04-27 01:09:49,690 - INFO - Epoch 45 training loss = 6.572
2023-04-27 01:09:50,447 - INFO - Epoch 46 training loss = 6.456
2023-04-27 01:09:51,205 - INFO - Epoch 47 training loss = 6.496
2023-04-27 01:09:51,961 - INFO - Epoch 48 training loss = 6.59
2023-04-27 01:09:52,718 - INFO - Epoch 49 training loss = 6.331
2023-04-27 01:09:53,475 - INFO - Epoch 50 training loss = 5.691
2023-04-27 01:09:53,536 - INFO - Validation loss = 5.605
2023-04-27 01:09:53,537 - INFO - best model
2023-04-27 01:09:54,304 - INFO - Epoch 51 training loss = 5.585
2023-04-27 01:09:55,060 - INFO - Epoch 52 training loss = 5.589
2023-04-27 01:09:55,817 - INFO - Epoch 53 training loss = 5.851
2023-04-27 01:09:56,574 - INFO - Epoch 54 training loss = 5.496
2023-04-27 01:09:57,331 - INFO - Epoch 55 training loss = 5.491
2023-04-27 01:09:58,088 - INFO - Epoch 56 training loss = 5.543
2023-04-27 01:09:58,845 - INFO - Epoch 57 training loss = 5.348
2023-04-27 01:09:59,602 - INFO - Epoch 58 training loss = 4.749
2023-04-27 01:10:00,359 - INFO - Epoch 59 training loss = 4.995
2023-04-27 01:10:01,116 - INFO - Epoch 60 training loss = 5.101
2023-04-27 01:10:01,177 - INFO - Validation loss = 4.61
2023-04-27 01:10:01,177 - INFO - best model
2023-04-27 01:10:01,948 - INFO - Epoch 61 training loss = 5.063
2023-04-27 01:10:02,706 - INFO - Epoch 62 training loss = 4.84
2023-04-27 01:10:03,463 - INFO - Epoch 63 training loss = 4.632
2023-04-27 01:10:04,221 - INFO - Epoch 64 training loss = 4.489
2023-04-27 01:10:04,978 - INFO - Epoch 65 training loss = 4.845
2023-04-27 01:10:05,735 - INFO - Epoch 66 training loss = 4.517
2023-04-27 01:10:06,492 - INFO - Epoch 67 training loss = 4.458
2023-04-27 01:10:07,250 - INFO - Epoch 68 training loss = 4.373
2023-04-27 01:10:08,007 - INFO - Epoch 69 training loss = 4.63
2023-04-27 01:10:08,764 - INFO - Epoch 70 training loss = 4.298
2023-04-27 01:10:08,825 - INFO - Validation loss = 5.642
2023-04-27 01:10:09,582 - INFO - Epoch 71 training loss = 4.103
2023-04-27 01:10:10,340 - INFO - Epoch 72 training loss = 4.233
2023-04-27 01:10:11,097 - INFO - Epoch 73 training loss = 4.187
2023-04-27 01:10:11,854 - INFO - Epoch 74 training loss = 4.076
2023-04-27 01:10:12,611 - INFO - Epoch 75 training loss = 3.99
2023-04-27 01:10:13,368 - INFO - Epoch 76 training loss = 4.028
2023-04-27 01:10:14,126 - INFO - Epoch 77 training loss = 4.037
2023-04-27 01:10:14,883 - INFO - Epoch 78 training loss = 3.844
2023-04-27 01:10:15,640 - INFO - Epoch 79 training loss = 3.972
2023-04-27 01:10:16,397 - INFO - Epoch 80 training loss = 4.005
2023-04-27 01:10:16,458 - INFO - Validation loss = 4.147
2023-04-27 01:10:16,458 - INFO - best model
2023-04-27 01:10:17,226 - INFO - Epoch 81 training loss = 3.924
2023-04-27 01:10:17,981 - INFO - Epoch 82 training loss = 3.965
2023-04-27 01:10:18,738 - INFO - Epoch 83 training loss = 3.703
2023-04-27 01:10:19,495 - INFO - Epoch 84 training loss = 3.544
2023-04-27 01:10:20,252 - INFO - Epoch 85 training loss = 3.452
2023-04-27 01:10:21,009 - INFO - Epoch 86 training loss = 3.488
2023-04-27 01:10:21,766 - INFO - Epoch 87 training loss = 3.387
2023-04-27 01:10:22,522 - INFO - Epoch 88 training loss = 3.415
2023-04-27 01:10:23,279 - INFO - Epoch 89 training loss = 3.447
2023-04-27 01:10:24,036 - INFO - Epoch 90 training loss = 3.519
2023-04-27 01:10:24,097 - INFO - Validation loss = 3.826
2023-04-27 01:10:24,097 - INFO - best model
2023-04-27 01:10:24,864 - INFO - Epoch 91 training loss = 3.313
2023-04-27 01:10:25,621 - INFO - Epoch 92 training loss = 3.514
2023-04-27 01:10:26,378 - INFO - Epoch 93 training loss = 3.064
2023-04-27 01:10:27,135 - INFO - Epoch 94 training loss = 3.709
2023-04-27 01:10:27,891 - INFO - Epoch 95 training loss = 3.23
2023-04-27 01:10:28,648 - INFO - Epoch 96 training loss = 3.084
2023-04-27 01:10:29,405 - INFO - Epoch 97 training loss = 3.046
2023-04-27 01:10:30,161 - INFO - Epoch 98 training loss = 3.072
2023-04-27 01:10:30,918 - INFO - Epoch 99 training loss = 3.076
2023-04-27 01:10:31,675 - INFO - Epoch 100 training loss = 3.091
2023-04-27 01:10:31,736 - INFO - Validation loss = 3.534
2023-04-27 01:10:31,736 - INFO - best model
2023-04-27 01:10:32,505 - INFO - Epoch 101 training loss = 3.174
2023-04-27 01:10:33,262 - INFO - Epoch 102 training loss = 3.17
2023-04-27 01:10:34,019 - INFO - Epoch 103 training loss = 3.233
2023-04-27 01:10:34,776 - INFO - Epoch 104 training loss = 3.086
2023-04-27 01:10:35,533 - INFO - Epoch 105 training loss = 3.002
2023-04-27 01:10:36,290 - INFO - Epoch 106 training loss = 2.961
2023-04-27 01:10:37,047 - INFO - Epoch 107 training loss = 3.144
2023-04-27 01:10:37,804 - INFO - Epoch 108 training loss = 2.973
2023-04-27 01:10:38,561 - INFO - Epoch 109 training loss = 2.955
2023-04-27 01:10:39,318 - INFO - Epoch 110 training loss = 3.058
2023-04-27 01:10:39,379 - INFO - Validation loss = 4.082
2023-04-27 01:10:40,136 - INFO - Epoch 111 training loss = 2.811
2023-04-27 01:10:40,893 - INFO - Epoch 112 training loss = 2.914
2023-04-27 01:10:41,651 - INFO - Epoch 113 training loss = 2.638
2023-04-27 01:10:42,408 - INFO - Epoch 114 training loss = 2.741
2023-04-27 01:10:43,166 - INFO - Epoch 115 training loss = 2.716
2023-04-27 01:10:43,923 - INFO - Epoch 116 training loss = 2.702
2023-04-27 01:10:44,680 - INFO - Epoch 117 training loss = 2.738
2023-04-27 01:10:45,438 - INFO - Epoch 118 training loss = 2.746
2023-04-27 01:10:46,195 - INFO - Epoch 119 training loss = 2.616
2023-04-27 01:10:46,953 - INFO - Epoch 120 training loss = 2.47
2023-04-27 01:10:47,014 - INFO - Validation loss = 3.245
2023-04-27 01:10:47,014 - INFO - best model
2023-04-27 01:10:47,783 - INFO - Epoch 121 training loss = 2.638
2023-04-27 01:10:48,540 - INFO - Epoch 122 training loss = 2.489
2023-04-27 01:10:49,298 - INFO - Epoch 123 training loss = 2.433
2023-04-27 01:10:50,055 - INFO - Epoch 124 training loss = 2.763
2023-04-27 01:10:50,812 - INFO - Epoch 125 training loss = 2.393
2023-04-27 01:10:51,569 - INFO - Epoch 126 training loss = 2.61
2023-04-27 01:10:52,326 - INFO - Epoch 127 training loss = 2.642
2023-04-27 01:10:53,082 - INFO - Epoch 128 training loss = 2.556
2023-04-27 01:10:53,839 - INFO - Epoch 129 training loss = 2.378
2023-04-27 01:10:54,597 - INFO - Epoch 130 training loss = 2.398
2023-04-27 01:10:54,658 - INFO - Validation loss =  3.1
2023-04-27 01:10:54,658 - INFO - best model
2023-04-27 01:10:55,425 - INFO - Epoch 131 training loss = 2.269
2023-04-27 01:10:56,182 - INFO - Epoch 132 training loss = 2.325
2023-04-27 01:10:56,939 - INFO - Epoch 133 training loss = 2.268
2023-04-27 01:10:57,696 - INFO - Epoch 134 training loss = 2.425
2023-04-27 01:10:58,452 - INFO - Epoch 135 training loss = 2.226
2023-04-27 01:10:59,209 - INFO - Epoch 136 training loss = 2.195
2023-04-27 01:10:59,966 - INFO - Epoch 137 training loss = 2.427
2023-04-27 01:11:00,723 - INFO - Epoch 138 training loss = 2.506
2023-04-27 01:11:01,480 - INFO - Epoch 139 training loss = 2.198
2023-04-27 01:11:02,238 - INFO - Epoch 140 training loss = 2.194
2023-04-27 01:11:02,299 - INFO - Validation loss = 2.73
2023-04-27 01:11:02,299 - INFO - best model
2023-04-27 01:11:03,067 - INFO - Epoch 141 training loss = 2.106
2023-04-27 01:11:03,824 - INFO - Epoch 142 training loss = 2.127
2023-04-27 01:11:04,581 - INFO - Epoch 143 training loss = 2.217
2023-04-27 01:11:05,338 - INFO - Epoch 144 training loss = 2.162
2023-04-27 01:11:06,095 - INFO - Epoch 145 training loss = 2.157
2023-04-27 01:11:06,853 - INFO - Epoch 146 training loss = 1.96
2023-04-27 01:11:07,610 - INFO - Epoch 147 training loss = 2.035
2023-04-27 01:11:08,367 - INFO - Epoch 148 training loss = 2.116
2023-04-27 01:11:09,124 - INFO - Epoch 149 training loss = 2.164
2023-04-27 01:11:09,882 - INFO - Epoch 150 training loss = 2.029
2023-04-27 01:11:09,943 - INFO - Validation loss = 3.349
2023-04-27 01:11:10,700 - INFO - Epoch 151 training loss = 2.035
2023-04-27 01:11:11,458 - INFO - Epoch 152 training loss = 2.071
2023-04-27 01:11:12,215 - INFO - Epoch 153 training loss = 1.884
2023-04-27 01:11:12,971 - INFO - Epoch 154 training loss = 1.983
2023-04-27 01:11:13,725 - INFO - Epoch 155 training loss = 1.86
2023-04-27 01:11:14,480 - INFO - Epoch 156 training loss = 1.945
2023-04-27 01:11:15,234 - INFO - Epoch 157 training loss = 1.935
2023-04-27 01:11:15,989 - INFO - Epoch 158 training loss = 2.124
2023-04-27 01:11:16,744 - INFO - Epoch 159 training loss = 1.872
2023-04-27 01:11:17,502 - INFO - Epoch 160 training loss = 1.905
2023-04-27 01:11:17,563 - INFO - Validation loss = 2.765
2023-04-27 01:11:18,322 - INFO - Epoch 161 training loss = 1.79
2023-04-27 01:11:19,079 - INFO - Epoch 162 training loss = 1.767
2023-04-27 01:11:19,836 - INFO - Epoch 163 training loss = 1.942
2023-04-27 01:11:20,594 - INFO - Epoch 164 training loss = 1.711
2023-04-27 01:11:21,349 - INFO - Epoch 165 training loss = 1.805
2023-04-27 01:11:22,103 - INFO - Epoch 166 training loss = 1.768
2023-04-27 01:11:22,858 - INFO - Epoch 167 training loss = 1.716
2023-04-27 01:11:23,615 - INFO - Epoch 168 training loss = 1.82
2023-04-27 01:11:24,373 - INFO - Epoch 169 training loss = 1.777
2023-04-27 01:11:25,130 - INFO - Epoch 170 training loss = 1.708
2023-04-27 01:11:25,191 - INFO - Validation loss = 2.617
2023-04-27 01:11:25,191 - INFO - best model
2023-04-27 01:11:25,959 - INFO - Epoch 171 training loss = 1.85
2023-04-27 01:11:26,716 - INFO - Epoch 172 training loss = 1.698
2023-04-27 01:11:27,473 - INFO - Epoch 173 training loss = 1.691
2023-04-27 01:11:28,227 - INFO - Epoch 174 training loss = 1.672
2023-04-27 01:11:28,981 - INFO - Epoch 175 training loss = 1.652
2023-04-27 01:11:29,735 - INFO - Epoch 176 training loss = 1.625
2023-04-27 01:11:30,489 - INFO - Epoch 177 training loss = 1.696
2023-04-27 01:11:31,242 - INFO - Epoch 178 training loss = 1.593
2023-04-27 01:11:31,998 - INFO - Epoch 179 training loss = 1.691
2023-04-27 01:11:32,756 - INFO - Epoch 180 training loss = 1.578
2023-04-27 01:11:32,817 - INFO - Validation loss = 2.189
2023-04-27 01:11:32,817 - INFO - best model
2023-04-27 01:11:33,586 - INFO - Epoch 181 training loss = 1.664
2023-04-27 01:11:34,342 - INFO - Epoch 182 training loss = 1.541
2023-04-27 01:11:35,096 - INFO - Epoch 183 training loss = 1.552
2023-04-27 01:11:35,850 - INFO - Epoch 184 training loss = 1.573
2023-04-27 01:11:36,605 - INFO - Epoch 185 training loss = 1.545
2023-04-27 01:11:37,359 - INFO - Epoch 186 training loss = 1.519
2023-04-27 01:11:38,112 - INFO - Epoch 187 training loss = 1.474
2023-04-27 01:11:38,866 - INFO - Epoch 188 training loss = 1.487
2023-04-27 01:11:39,620 - INFO - Epoch 189 training loss = 1.483
2023-04-27 01:11:40,376 - INFO - Epoch 190 training loss = 1.459
2023-04-27 01:11:40,438 - INFO - Validation loss = 2.527
2023-04-27 01:11:41,194 - INFO - Epoch 191 training loss = 1.476
2023-04-27 01:11:41,952 - INFO - Epoch 192 training loss = 1.395
2023-04-27 01:11:42,710 - INFO - Epoch 193 training loss = 1.337
2023-04-27 01:11:43,468 - INFO - Epoch 194 training loss = 1.36
2023-04-27 01:11:44,226 - INFO - Epoch 195 training loss = 1.452
2023-04-27 01:11:44,985 - INFO - Epoch 196 training loss = 1.446
2023-04-27 01:11:45,744 - INFO - Epoch 197 training loss = 1.507
2023-04-27 01:11:46,502 - INFO - Epoch 198 training loss = 1.396
2023-04-27 01:11:47,261 - INFO - Epoch 199 training loss = 1.407
2023-04-27 01:11:48,020 - INFO - Epoch 200 training loss = 1.352
2023-04-27 01:11:48,081 - INFO - Validation loss = 1.973
2023-04-27 01:11:48,081 - INFO - best model
2023-04-27 01:11:48,850 - INFO - Epoch 201 training loss = 1.395
2023-04-27 01:11:49,608 - INFO - Epoch 202 training loss = 1.296
2023-04-27 01:11:50,366 - INFO - Epoch 203 training loss = 1.346
2023-04-27 01:11:51,124 - INFO - Epoch 204 training loss = 1.392
2023-04-27 01:11:51,878 - INFO - Epoch 205 training loss = 1.357
2023-04-27 01:11:52,632 - INFO - Epoch 206 training loss = 1.315
2023-04-27 01:11:53,386 - INFO - Epoch 207 training loss = 1.318
2023-04-27 01:11:54,141 - INFO - Epoch 208 training loss = 1.331
2023-04-27 01:11:54,896 - INFO - Epoch 209 training loss = 1.274
2023-04-27 01:11:55,651 - INFO - Epoch 210 training loss = 1.196
2023-04-27 01:11:55,712 - INFO - Validation loss = 2.09
2023-04-27 01:11:56,468 - INFO - Epoch 211 training loss = 1.254
2023-04-27 01:11:57,223 - INFO - Epoch 212 training loss = 1.159
2023-04-27 01:11:57,978 - INFO - Epoch 213 training loss = 1.234
2023-04-27 01:11:58,733 - INFO - Epoch 214 training loss = 1.154
2023-04-27 01:11:59,489 - INFO - Epoch 215 training loss = 1.192
2023-04-27 01:12:00,244 - INFO - Epoch 216 training loss = 1.13
2023-04-27 01:12:01,000 - INFO - Epoch 217 training loss = 1.21
2023-04-27 01:12:01,755 - INFO - Epoch 218 training loss = 1.322
2023-04-27 01:12:02,513 - INFO - Epoch 219 training loss = 1.249
2023-04-27 01:12:03,272 - INFO - Epoch 220 training loss = 1.066
2023-04-27 01:12:03,333 - INFO - Validation loss = 1.839
2023-04-27 01:12:03,333 - INFO - best model
2023-04-27 01:12:04,101 - INFO - Epoch 221 training loss = 1.145
2023-04-27 01:12:04,857 - INFO - Epoch 222 training loss = 1.083
2023-04-27 01:12:05,612 - INFO - Epoch 223 training loss = 1.175
2023-04-27 01:12:06,367 - INFO - Epoch 224 training loss = 1.069
2023-04-27 01:12:07,123 - INFO - Epoch 225 training loss = 1.149
2023-04-27 01:12:07,878 - INFO - Epoch 226 training loss = 1.182
2023-04-27 01:12:08,633 - INFO - Epoch 227 training loss = 1.13
2023-04-27 01:12:09,389 - INFO - Epoch 228 training loss = 1.062
2023-04-27 01:12:10,145 - INFO - Epoch 229 training loss = 1.047
2023-04-27 01:12:10,903 - INFO - Epoch 230 training loss = 1.079
2023-04-27 01:12:10,964 - INFO - Validation loss = 1.668
2023-04-27 01:12:10,964 - INFO - best model
2023-04-27 01:12:11,729 - INFO - Epoch 231 training loss = 1.007
2023-04-27 01:12:12,484 - INFO - Epoch 232 training loss = 1.05
2023-04-27 01:12:13,240 - INFO - Epoch 233 training loss = 1.072
2023-04-27 01:12:13,998 - INFO - Epoch 234 training loss = 1.005
2023-04-27 01:12:14,757 - INFO - Epoch 235 training loss = 1.042
2023-04-27 01:12:15,516 - INFO - Epoch 236 training loss = 1.064
2023-04-27 01:12:16,275 - INFO - Epoch 237 training loss = 1.024
2023-04-27 01:12:17,033 - INFO - Epoch 238 training loss = 1.024
2023-04-27 01:12:17,792 - INFO - Epoch 239 training loss = 0.9665
2023-04-27 01:12:18,550 - INFO - Epoch 240 training loss = 0.9556
2023-04-27 01:12:18,611 - INFO - Validation loss = 1.835
2023-04-27 01:12:19,369 - INFO - Epoch 241 training loss = 0.9488
2023-04-27 01:12:20,128 - INFO - Epoch 242 training loss = 1.01
2023-04-27 01:12:20,886 - INFO - Epoch 243 training loss = 0.9169
2023-04-27 01:12:21,645 - INFO - Epoch 244 training loss = 0.9305
2023-04-27 01:12:22,403 - INFO - Epoch 245 training loss = 0.9919
2023-04-27 01:12:23,161 - INFO - Epoch 246 training loss = 0.8843
2023-04-27 01:12:23,920 - INFO - Epoch 247 training loss = 0.9766
2023-04-27 01:12:24,678 - INFO - Epoch 248 training loss = 0.9362
2023-04-27 01:12:25,436 - INFO - Epoch 249 training loss = 0.918
2023-04-27 01:12:26,195 - INFO - Epoch 250 training loss = 0.8644
2023-04-27 01:12:26,256 - INFO - Validation loss = 2.018
2023-04-27 01:12:27,014 - INFO - Epoch 251 training loss = 0.8573
2023-04-27 01:12:27,772 - INFO - Epoch 252 training loss = 0.9319
2023-04-27 01:12:28,531 - INFO - Epoch 253 training loss = 0.9795
2023-04-27 01:12:29,288 - INFO - Epoch 254 training loss = 0.8644
2023-04-27 01:12:30,046 - INFO - Epoch 255 training loss = 0.8463
2023-04-27 01:12:30,803 - INFO - Epoch 256 training loss = 0.8531
2023-04-27 01:12:31,560 - INFO - Epoch 257 training loss = 0.8522
2023-04-27 01:12:32,319 - INFO - Epoch 258 training loss = 0.8247
2023-04-27 01:12:33,075 - INFO - Epoch 259 training loss = 0.9123
2023-04-27 01:12:33,828 - INFO - Epoch 260 training loss = 0.7905
2023-04-27 01:12:33,889 - INFO - Validation loss = 1.609
2023-04-27 01:12:33,889 - INFO - best model
2023-04-27 01:12:34,655 - INFO - Epoch 261 training loss = 0.7773
2023-04-27 01:12:35,410 - INFO - Epoch 262 training loss = 0.8286
2023-04-27 01:12:36,169 - INFO - Epoch 263 training loss = 0.7717
2023-04-27 01:12:36,927 - INFO - Epoch 264 training loss = 0.7672
2023-04-27 01:12:37,685 - INFO - Epoch 265 training loss = 0.7771
2023-04-27 01:12:38,443 - INFO - Epoch 266 training loss = 0.7611
2023-04-27 01:12:39,202 - INFO - Epoch 267 training loss = 0.7361
2023-04-27 01:12:39,960 - INFO - Epoch 268 training loss = 0.7296
2023-04-27 01:12:40,718 - INFO - Epoch 269 training loss = 0.774
2023-04-27 01:12:41,476 - INFO - Epoch 270 training loss = 0.7298
2023-04-27 01:12:41,538 - INFO - Validation loss = 1.457
2023-04-27 01:12:41,538 - INFO - best model
2023-04-27 01:12:42,308 - INFO - Epoch 271 training loss = 0.7689
2023-04-27 01:12:43,067 - INFO - Epoch 272 training loss = 0.7057
2023-04-27 01:12:43,825 - INFO - Epoch 273 training loss = 0.7132
2023-04-27 01:12:44,584 - INFO - Epoch 274 training loss = 0.715
2023-04-27 01:12:45,343 - INFO - Epoch 275 training loss = 0.778
2023-04-27 01:12:46,101 - INFO - Epoch 276 training loss = 0.6927
2023-04-27 01:12:46,859 - INFO - Epoch 277 training loss = 0.7159
2023-04-27 01:12:47,619 - INFO - Epoch 278 training loss = 0.716
2023-04-27 01:12:48,377 - INFO - Epoch 279 training loss = 0.705
2023-04-27 01:12:49,136 - INFO - Epoch 280 training loss = 0.6532
2023-04-27 01:12:49,197 - INFO - Validation loss = 1.52
2023-04-27 01:12:49,956 - INFO - Epoch 281 training loss = 0.6636
2023-04-27 01:12:50,712 - INFO - Epoch 282 training loss = 0.6476
2023-04-27 01:12:51,468 - INFO - Epoch 283 training loss = 0.638
2023-04-27 01:12:52,223 - INFO - Epoch 284 training loss = 0.6329
2023-04-27 01:12:52,979 - INFO - Epoch 285 training loss = 0.6256
2023-04-27 01:12:53,734 - INFO - Epoch 286 training loss = 0.6339
2023-04-27 01:12:54,489 - INFO - Epoch 287 training loss = 0.6372
2023-04-27 01:12:55,244 - INFO - Epoch 288 training loss = 0.6293
2023-04-27 01:12:55,999 - INFO - Epoch 289 training loss = 0.6214
2023-04-27 01:12:56,754 - INFO - Epoch 290 training loss = 0.6202
2023-04-27 01:12:56,817 - INFO - Validation loss = 1.387
2023-04-27 01:12:56,817 - INFO - best model
2023-04-27 01:12:57,595 - INFO - Epoch 291 training loss = 0.6155
2023-04-27 01:12:58,350 - INFO - Epoch 292 training loss = 0.5804
2023-04-27 01:12:59,105 - INFO - Epoch 293 training loss = 0.6137
2023-04-27 01:12:59,859 - INFO - Epoch 294 training loss = 0.6232
2023-04-27 01:13:00,614 - INFO - Epoch 295 training loss = 0.5749
2023-04-27 01:13:01,368 - INFO - Epoch 296 training loss = 0.5733
2023-04-27 01:13:02,124 - INFO - Epoch 297 training loss = 0.5868
2023-04-27 01:13:02,879 - INFO - Epoch 298 training loss = 0.5547
2023-04-27 01:13:03,633 - INFO - Epoch 299 training loss = 0.5709
2023-04-27 01:13:04,388 - INFO - Epoch 300 training loss = 0.5465
2023-04-27 01:13:04,449 - INFO - Validation loss = 1.33
2023-04-27 01:13:04,449 - INFO - best model
2023-04-27 01:13:05,214 - INFO - Epoch 301 training loss = 0.5237
2023-04-27 01:13:05,969 - INFO - Epoch 302 training loss = 0.5519
2023-04-27 01:13:06,724 - INFO - Epoch 303 training loss = 0.5337
2023-04-27 01:13:07,478 - INFO - Epoch 304 training loss = 0.5477
2023-04-27 01:13:08,233 - INFO - Epoch 305 training loss = 0.5252
2023-04-27 01:13:08,988 - INFO - Epoch 306 training loss = 0.5115
2023-04-27 01:13:09,743 - INFO - Epoch 307 training loss = 0.5243
2023-04-27 01:13:10,498 - INFO - Epoch 308 training loss = 0.5226
2023-04-27 01:13:11,253 - INFO - Epoch 309 training loss = 0.5175
2023-04-27 01:13:12,008 - INFO - Epoch 310 training loss = 0.5193
2023-04-27 01:13:12,069 - INFO - Validation loss = 1.686
2023-04-27 01:13:12,824 - INFO - Epoch 311 training loss = 0.5027
2023-04-27 01:13:13,580 - INFO - Epoch 312 training loss = 0.4995
2023-04-27 01:13:14,334 - INFO - Epoch 313 training loss = 0.4888
2023-04-27 01:13:15,089 - INFO - Epoch 314 training loss = 0.4725
2023-04-27 01:13:15,844 - INFO - Epoch 315 training loss =  0.5
2023-04-27 01:13:16,599 - INFO - Epoch 316 training loss = 0.4605
2023-04-27 01:13:17,355 - INFO - Epoch 317 training loss = 0.4828
2023-04-27 01:13:18,111 - INFO - Epoch 318 training loss = 0.4609
2023-04-27 01:13:18,866 - INFO - Epoch 319 training loss = 0.4816
2023-04-27 01:13:19,621 - INFO - Epoch 320 training loss = 0.4784
2023-04-27 01:13:19,682 - INFO - Validation loss = 1.403
2023-04-27 01:13:20,438 - INFO - Epoch 321 training loss = 0.4472
2023-04-27 01:13:21,194 - INFO - Epoch 322 training loss = 0.4538
2023-04-27 01:13:21,949 - INFO - Epoch 323 training loss = 0.449
2023-04-27 01:13:22,704 - INFO - Epoch 324 training loss = 0.4458
2023-04-27 01:13:23,460 - INFO - Epoch 325 training loss = 0.4231
2023-04-27 01:13:24,216 - INFO - Epoch 326 training loss = 0.4342
2023-04-27 01:13:24,971 - INFO - Epoch 327 training loss = 0.4076
2023-04-27 01:13:25,727 - INFO - Epoch 328 training loss = 0.4223
2023-04-27 01:13:26,482 - INFO - Epoch 329 training loss = 0.4314
2023-04-27 01:13:27,238 - INFO - Epoch 330 training loss = 0.3939
2023-04-27 01:13:27,299 - INFO - Validation loss = 1.31
2023-04-27 01:13:27,299 - INFO - best model
2023-04-27 01:13:28,064 - INFO - Epoch 331 training loss = 0.3999
2023-04-27 01:13:28,820 - INFO - Epoch 332 training loss = 0.4157
2023-04-27 01:13:29,575 - INFO - Epoch 333 training loss = 0.3981
2023-04-27 01:13:30,330 - INFO - Epoch 334 training loss = 0.397
2023-04-27 01:13:31,085 - INFO - Epoch 335 training loss = 0.3873
2023-04-27 01:13:31,840 - INFO - Epoch 336 training loss = 0.3834
2023-04-27 01:13:32,600 - INFO - Epoch 337 training loss = 0.3723
2023-04-27 01:13:33,358 - INFO - Epoch 338 training loss = 0.3805
2023-04-27 01:13:34,113 - INFO - Epoch 339 training loss = 0.3813
2023-04-27 01:13:34,869 - INFO - Epoch 340 training loss = 0.3711
2023-04-27 01:13:34,930 - INFO - Validation loss = 1.247
2023-04-27 01:13:34,930 - INFO - best model
2023-04-27 01:13:35,696 - INFO - Epoch 341 training loss = 0.3622
2023-04-27 01:13:36,451 - INFO - Epoch 342 training loss = 0.3591
2023-04-27 01:13:37,207 - INFO - Epoch 343 training loss = 0.3624
2023-04-27 01:13:37,962 - INFO - Epoch 344 training loss = 0.3675
2023-04-27 01:13:38,718 - INFO - Epoch 345 training loss = 0.3614
2023-04-27 01:13:39,473 - INFO - Epoch 346 training loss = 0.3467
2023-04-27 01:13:40,229 - INFO - Epoch 347 training loss = 0.343
2023-04-27 01:13:40,983 - INFO - Epoch 348 training loss = 0.3442
2023-04-27 01:13:41,738 - INFO - Epoch 349 training loss = 0.3409
2023-04-27 01:13:42,494 - INFO - Epoch 350 training loss = 0.3312
2023-04-27 01:13:42,555 - INFO - Validation loss = 1.203
2023-04-27 01:13:42,555 - INFO - best model
2023-04-27 01:13:43,322 - INFO - Epoch 351 training loss = 0.3299
2023-04-27 01:13:44,077 - INFO - Epoch 352 training loss = 0.3364
2023-04-27 01:13:44,832 - INFO - Epoch 353 training loss = 0.3236
2023-04-27 01:13:45,587 - INFO - Epoch 354 training loss = 0.3229
2023-04-27 01:13:46,343 - INFO - Epoch 355 training loss = 0.3262
2023-04-27 01:13:47,098 - INFO - Epoch 356 training loss = 0.3217
2023-04-27 01:13:47,853 - INFO - Epoch 357 training loss = 0.3146
2023-04-27 01:13:48,608 - INFO - Epoch 358 training loss = 0.3111
2023-04-27 01:13:49,363 - INFO - Epoch 359 training loss = 0.3098
2023-04-27 01:13:50,118 - INFO - Epoch 360 training loss = 0.2977
2023-04-27 01:13:50,179 - INFO - Validation loss = 1.171
2023-04-27 01:13:50,179 - INFO - best model
2023-04-27 01:13:50,944 - INFO - Epoch 361 training loss = 0.3052
2023-04-27 01:13:51,699 - INFO - Epoch 362 training loss = 0.3069
2023-04-27 01:13:52,453 - INFO - Epoch 363 training loss = 0.3023
2023-04-27 01:13:53,207 - INFO - Epoch 364 training loss = 0.296
2023-04-27 01:13:53,962 - INFO - Epoch 365 training loss = 0.2986
2023-04-27 01:13:54,716 - INFO - Epoch 366 training loss = 0.301
2023-04-27 01:13:55,471 - INFO - Epoch 367 training loss = 0.2942
2023-04-27 01:13:56,225 - INFO - Epoch 368 training loss = 0.2926
2023-04-27 01:13:56,980 - INFO - Epoch 369 training loss = 0.2852
2023-04-27 01:13:57,734 - INFO - Epoch 370 training loss = 0.2796
2023-04-27 01:13:57,795 - INFO - Validation loss = 1.172
2023-04-27 01:13:58,551 - INFO - Epoch 371 training loss = 0.2955
2023-04-27 01:13:59,305 - INFO - Epoch 372 training loss = 0.2767
2023-04-27 01:14:00,059 - INFO - Epoch 373 training loss = 0.2726
2023-04-27 01:14:00,814 - INFO - Epoch 374 training loss = 0.2778
2023-04-27 01:14:01,569 - INFO - Epoch 375 training loss = 0.2698
2023-04-27 01:14:02,325 - INFO - Epoch 376 training loss = 0.265
2023-04-27 01:14:03,081 - INFO - Epoch 377 training loss = 0.2701
2023-04-27 01:14:03,837 - INFO - Epoch 378 training loss = 0.2656
2023-04-27 01:14:04,592 - INFO - Epoch 379 training loss = 0.2663
2023-04-27 01:14:05,347 - INFO - Epoch 380 training loss = 0.2617
2023-04-27 01:14:05,408 - INFO - Validation loss = 1.134
2023-04-27 01:14:05,408 - INFO - best model
2023-04-27 01:14:06,173 - INFO - Epoch 381 training loss = 0.2578
2023-04-27 01:14:06,928 - INFO - Epoch 382 training loss = 0.2531
2023-04-27 01:14:07,683 - INFO - Epoch 383 training loss = 0.2559
2023-04-27 01:14:08,438 - INFO - Epoch 384 training loss = 0.259
2023-04-27 01:14:09,194 - INFO - Epoch 385 training loss = 0.2556
2023-04-27 01:14:09,949 - INFO - Epoch 386 training loss = 0.2495
2023-04-27 01:14:10,704 - INFO - Epoch 387 training loss = 0.2437
2023-04-27 01:14:11,459 - INFO - Epoch 388 training loss = 0.2448
2023-04-27 01:14:12,214 - INFO - Epoch 389 training loss = 0.2461
2023-04-27 01:14:12,970 - INFO - Epoch 390 training loss = 0.2403
2023-04-27 01:14:13,030 - INFO - Validation loss = 1.104
2023-04-27 01:14:13,031 - INFO - best model
2023-04-27 01:14:13,796 - INFO - Epoch 391 training loss = 0.2401
2023-04-27 01:14:14,550 - INFO - Epoch 392 training loss = 0.2358
2023-04-27 01:14:15,306 - INFO - Epoch 393 training loss = 0.2391
2023-04-27 01:14:16,061 - INFO - Epoch 394 training loss = 0.2329
2023-04-27 01:14:16,816 - INFO - Epoch 395 training loss = 0.2327
2023-04-27 01:14:17,571 - INFO - Epoch 396 training loss = 0.2356
2023-04-27 01:14:18,327 - INFO - Epoch 397 training loss = 0.2284
2023-04-27 01:14:19,081 - INFO - Epoch 398 training loss = 0.2312
2023-04-27 01:14:19,835 - INFO - Epoch 399 training loss = 0.2266
2023-04-27 01:14:20,590 - INFO - Epoch 400 training loss = 0.2283
2023-04-27 01:14:20,651 - INFO - Validation loss = 1.11
2023-04-27 01:14:21,406 - INFO - Epoch 401 training loss = 0.2229
2023-04-27 01:14:22,160 - INFO - Epoch 402 training loss = 0.2213
2023-04-27 01:14:22,915 - INFO - Epoch 403 training loss = 0.2201
2023-04-27 01:14:23,670 - INFO - Epoch 404 training loss = 0.2205
2023-04-27 01:14:24,425 - INFO - Epoch 405 training loss = 0.2195
2023-04-27 01:14:25,180 - INFO - Epoch 406 training loss = 0.2157
2023-04-27 01:14:25,934 - INFO - Epoch 407 training loss = 0.2177
2023-04-27 01:14:26,689 - INFO - Epoch 408 training loss = 0.2171
2023-04-27 01:14:27,444 - INFO - Epoch 409 training loss = 0.211
2023-04-27 01:14:28,199 - INFO - Epoch 410 training loss = 0.2118
2023-04-27 01:14:28,260 - INFO - Validation loss =  1.1
2023-04-27 01:14:28,260 - INFO - best model
2023-04-27 01:14:29,026 - INFO - Epoch 411 training loss = 0.2141
2023-04-27 01:14:29,780 - INFO - Epoch 412 training loss = 0.2095
2023-04-27 01:14:30,535 - INFO - Epoch 413 training loss = 0.2058
2023-04-27 01:14:31,290 - INFO - Epoch 414 training loss = 0.2069
2023-04-27 01:14:32,045 - INFO - Epoch 415 training loss = 0.2054
2023-04-27 01:14:32,800 - INFO - Epoch 416 training loss = 0.2033
2023-04-27 01:14:33,554 - INFO - Epoch 417 training loss = 0.2074
2023-04-27 01:14:34,309 - INFO - Epoch 418 training loss = 0.2015
2023-04-27 01:14:35,064 - INFO - Epoch 419 training loss = 0.2008
2023-04-27 01:14:35,819 - INFO - Epoch 420 training loss = 0.2006
2023-04-27 01:14:35,880 - INFO - Validation loss = 1.078
2023-04-27 01:14:35,880 - INFO - best model
2023-04-27 01:14:36,645 - INFO - Epoch 421 training loss = 0.1997
2023-04-27 01:14:37,400 - INFO - Epoch 422 training loss = 0.1981
2023-04-27 01:14:38,155 - INFO - Epoch 423 training loss = 0.1961
2023-04-27 01:14:38,910 - INFO - Epoch 424 training loss = 0.1962
2023-04-27 01:14:39,664 - INFO - Epoch 425 training loss = 0.1944
2023-04-27 01:14:40,420 - INFO - Epoch 426 training loss = 0.1939
2023-04-27 01:14:41,174 - INFO - Epoch 427 training loss = 0.1947
2023-04-27 01:14:41,930 - INFO - Epoch 428 training loss = 0.1921
2023-04-27 01:14:42,685 - INFO - Epoch 429 training loss = 0.1915
2023-04-27 01:14:43,441 - INFO - Epoch 430 training loss = 0.1915
2023-04-27 01:14:43,502 - INFO - Validation loss = 1.069
2023-04-27 01:14:43,502 - INFO - best model
2023-04-27 01:14:44,267 - INFO - Epoch 431 training loss = 0.1895
2023-04-27 01:14:45,023 - INFO - Epoch 432 training loss = 0.1888
2023-04-27 01:14:45,778 - INFO - Epoch 433 training loss = 0.1892
2023-04-27 01:14:46,534 - INFO - Epoch 434 training loss = 0.1868
2023-04-27 01:14:47,289 - INFO - Epoch 435 training loss = 0.1869
2023-04-27 01:14:48,044 - INFO - Epoch 436 training loss = 0.1871
2023-04-27 01:14:48,799 - INFO - Epoch 437 training loss = 0.1847
2023-04-27 01:14:49,554 - INFO - Epoch 438 training loss = 0.184
2023-04-27 01:14:50,309 - INFO - Epoch 439 training loss = 0.185
2023-04-27 01:14:51,063 - INFO - Epoch 440 training loss = 0.1832
2023-04-27 01:14:51,124 - INFO - Validation loss = 1.061
2023-04-27 01:14:51,125 - INFO - best model
2023-04-27 01:14:51,890 - INFO - Epoch 441 training loss = 0.183
2023-04-27 01:14:52,644 - INFO - Epoch 442 training loss = 0.1831
2023-04-27 01:14:53,399 - INFO - Epoch 443 training loss = 0.181
2023-04-27 01:14:54,154 - INFO - Epoch 444 training loss = 0.1826
2023-04-27 01:14:54,908 - INFO - Epoch 445 training loss = 0.1797
2023-04-27 01:14:55,663 - INFO - Epoch 446 training loss = 0.1803
2023-04-27 01:14:56,417 - INFO - Epoch 447 training loss = 0.1801
2023-04-27 01:14:57,172 - INFO - Epoch 448 training loss = 0.1794
2023-04-27 01:14:57,927 - INFO - Epoch 449 training loss = 0.1787
2023-04-27 01:14:58,682 - INFO - Epoch 450 training loss = 0.1781
2023-04-27 01:14:58,742 - INFO - Validation loss = 1.058
2023-04-27 01:14:58,743 - INFO - best model
2023-04-27 01:14:59,508 - INFO - Epoch 451 training loss = 0.1775
2023-04-27 01:15:00,263 - INFO - Epoch 452 training loss = 0.1763
2023-04-27 01:15:01,019 - INFO - Epoch 453 training loss = 0.1766
2023-04-27 01:15:01,773 - INFO - Epoch 454 training loss = 0.1763
2023-04-27 01:15:02,530 - INFO - Epoch 455 training loss = 0.1754
2023-04-27 01:15:03,285 - INFO - Epoch 456 training loss = 0.1753
2023-04-27 01:15:04,040 - INFO - Epoch 457 training loss = 0.1736
2023-04-27 01:15:04,794 - INFO - Epoch 458 training loss = 0.1736
2023-04-27 01:15:05,549 - INFO - Epoch 459 training loss = 0.1735
2023-04-27 01:15:06,304 - INFO - Epoch 460 training loss = 0.1726
2023-04-27 01:15:06,364 - INFO - Validation loss = 1.058
2023-04-27 01:15:07,119 - INFO - Epoch 461 training loss = 0.1728
2023-04-27 01:15:07,874 - INFO - Epoch 462 training loss = 0.1726
2023-04-27 01:15:08,629 - INFO - Epoch 463 training loss = 0.1719
2023-04-27 01:15:09,384 - INFO - Epoch 464 training loss = 0.171
2023-04-27 01:15:10,140 - INFO - Epoch 465 training loss = 0.1711
2023-04-27 01:15:10,895 - INFO - Epoch 466 training loss = 0.1707
2023-04-27 01:15:11,650 - INFO - Epoch 467 training loss = 0.1697
2023-04-27 01:15:12,406 - INFO - Epoch 468 training loss = 0.1703
2023-04-27 01:15:13,161 - INFO - Epoch 469 training loss = 0.1695
2023-04-27 01:15:13,916 - INFO - Epoch 470 training loss = 0.169
2023-04-27 01:15:13,977 - INFO - Validation loss = 1.052
2023-04-27 01:15:13,978 - INFO - best model
2023-04-27 01:15:14,743 - INFO - Epoch 471 training loss = 0.1692
2023-04-27 01:15:15,498 - INFO - Epoch 472 training loss = 0.1688
2023-04-27 01:15:16,253 - INFO - Epoch 473 training loss = 0.1683
2023-04-27 01:15:17,008 - INFO - Epoch 474 training loss = 0.1682
2023-04-27 01:15:17,761 - INFO - Epoch 475 training loss = 0.1681
2023-04-27 01:15:18,516 - INFO - Epoch 476 training loss = 0.1676
2023-04-27 01:15:19,270 - INFO - Epoch 477 training loss = 0.1674
2023-04-27 01:15:20,025 - INFO - Epoch 478 training loss = 0.167
2023-04-27 01:15:20,780 - INFO - Epoch 479 training loss = 0.1666
2023-04-27 01:15:21,535 - INFO - Epoch 480 training loss = 0.1667
2023-04-27 01:15:21,598 - INFO - Validation loss = 1.05
2023-04-27 01:15:21,598 - INFO - best model
2023-04-27 01:15:22,364 - INFO - Epoch 481 training loss = 0.1662
2023-04-27 01:15:23,118 - INFO - Epoch 482 training loss = 0.1662
2023-04-27 01:15:23,873 - INFO - Epoch 483 training loss = 0.166
2023-04-27 01:15:24,628 - INFO - Epoch 484 training loss = 0.166
2023-04-27 01:15:25,382 - INFO - Epoch 485 training loss = 0.1656
2023-04-27 01:15:26,137 - INFO - Epoch 486 training loss = 0.1654
2023-04-27 01:15:26,892 - INFO - Epoch 487 training loss = 0.1654
2023-04-27 01:15:27,646 - INFO - Epoch 488 training loss = 0.1651
2023-04-27 01:15:28,401 - INFO - Epoch 489 training loss = 0.165
2023-04-27 01:15:29,155 - INFO - Epoch 490 training loss = 0.1649
2023-04-27 01:15:29,216 - INFO - Validation loss = 1.049
2023-04-27 01:15:29,216 - INFO - best model
2023-04-27 01:15:29,981 - INFO - Epoch 491 training loss = 0.1647
2023-04-27 01:15:30,736 - INFO - Epoch 492 training loss = 0.1648
2023-04-27 01:15:31,491 - INFO - Epoch 493 training loss = 0.1646
2023-04-27 01:15:32,246 - INFO - Epoch 494 training loss = 0.1645
2023-04-27 01:15:33,001 - INFO - Epoch 495 training loss = 0.1645
2023-04-27 01:15:33,755 - INFO - Epoch 496 training loss = 0.1643
2023-04-27 01:15:34,509 - INFO - Epoch 497 training loss = 0.1643
2023-04-27 01:15:35,264 - INFO - Epoch 498 training loss = 0.1643
2023-04-27 01:15:36,018 - INFO - Epoch 499 training loss = 0.1642
2023-04-27 01:15:36,069 - INFO - Validation loss = 1.049
