2023-04-26 16:47:15,669 - INFO - Config:
Namespace(config='configs/implicit_mlp.yaml', dir='/user/delden/acoustics/spp_ai_acoustics/acousticNN/experiments/arch/no_encoding/implicitmlp', epochs=100, device='cuda', seed=0, parameter_list='configs/data.yaml', encoding='none', dir_name='arch/no_encoding/implicitmlp')
2023-04-26 16:47:15,669 - INFO - Config:
Munch({'optimizer': Munch({'type': 'AdamW', 'kwargs': Munch({'lr': 0.001, 'weight_decay': 0.005, 'betas': [0.9, 0.95]})}), 'scheduler': Munch({'type': 'CosLR', 'kwargs': Munch({'epochs': 500, 'initial_epochs': 25})}), 'model': Munch({'name': 'ImplicitMLP', 'input_encoding': False, 'encoding_dim': 16, 'embed_dim': 64, 'depth': 7, 'mlp_width': 256}), 'generation_frequency': 5001, 'validation_frequency': 10, 'batch_size': 16, 'epochs': 500, 'gradient_clip': 10})
2023-04-26 16:47:30,308 - INFO - ==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ImplicitMLP                              [16, 200, 4]              --
├─GroupwiseProjection: 1-1               [3200, 15, 64]            --
│    └─ModuleList: 2-1                   --                        --
│    │    └─Linear: 3-1                  [3200, 4, 64]             128
│    │    └─Linear: 3-2                  [3200, 5, 64]             128
│    │    └─Linear: 3-3                  [3200, 5, 64]             128
│    │    └─Linear: 3-4                  [3200, 1, 64]             128
├─MLP: 1-2                               [3200, 256]               --
│    └─Linear: 2-2                       [3200, 256]               246,016
│    └─ReLU: 2-3                         [3200, 256]               --
│    └─Dropout: 2-4                      [3200, 256]               --
│    └─Linear: 2-5                       [3200, 256]               65,792
│    └─ReLU: 2-6                         [3200, 256]               --
│    └─Dropout: 2-7                      [3200, 256]               --
│    └─Linear: 2-8                       [3200, 256]               65,792
│    └─ReLU: 2-9                         [3200, 256]               --
│    └─Dropout: 2-10                     [3200, 256]               --
│    └─Linear: 2-11                      [3200, 256]               65,792
│    └─ReLU: 2-12                        [3200, 256]               --
│    └─Dropout: 2-13                     [3200, 256]               --
│    └─Linear: 2-14                      [3200, 256]               65,792
│    └─ReLU: 2-15                        [3200, 256]               --
│    └─Dropout: 2-16                     [3200, 256]               --
│    └─Linear: 2-17                      [3200, 256]               65,792
│    └─ReLU: 2-18                        [3200, 256]               --
│    └─Dropout: 2-19                     [3200, 256]               --
│    └─Linear: 2-20                      [3200, 256]               65,792
│    └─Dropout: 2-21                     [3200, 256]               --
├─Linear: 1-3                            [3200, 4]                 1,028
==========================================================================================
Total params: 642,308
Trainable params: 642,308
Non-trainable params: 0
Total mult-adds (G): 2.06
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 70.55
Params size (MB): 2.57
Estimated Total Size (MB): 73.14
==========================================================================================
2023-04-26 16:47:31,113 - INFO - Epoch 0 training loss = 3.27e+03
2023-04-26 16:47:31,181 - INFO - Validation loss = 3.245e+03
2023-04-26 16:47:31,181 - INFO - best model
2023-04-26 16:47:31,970 - INFO - Epoch 1 training loss = 3.088e+03
2023-04-26 16:47:32,751 - INFO - Epoch 2 training loss = 763.7
2023-04-26 16:47:33,518 - INFO - Epoch 3 training loss = 124.3
2023-04-26 16:47:34,280 - INFO - Epoch 4 training loss = 99.48
2023-04-26 16:47:35,041 - INFO - Epoch 5 training loss = 85.62
2023-04-26 16:47:35,804 - INFO - Epoch 6 training loss = 72.27
2023-04-26 16:47:36,574 - INFO - Epoch 7 training loss = 63.32
2023-04-26 16:47:37,338 - INFO - Epoch 8 training loss = 52.79
2023-04-26 16:47:38,102 - INFO - Epoch 9 training loss = 43.34
2023-04-26 16:47:38,865 - INFO - Epoch 10 training loss = 37.99
2023-04-26 16:47:38,927 - INFO - Validation loss = 32.57
2023-04-26 16:47:38,927 - INFO - best model
2023-04-26 16:47:39,701 - INFO - Epoch 11 training loss = 32.77
2023-04-26 16:47:40,463 - INFO - Epoch 12 training loss = 28.7
2023-04-26 16:47:41,225 - INFO - Epoch 13 training loss = 27.94
2023-04-26 16:47:41,999 - INFO - Epoch 14 training loss = 26.86
2023-04-26 16:47:42,775 - INFO - Epoch 15 training loss = 24.39
2023-04-26 16:47:43,550 - INFO - Epoch 16 training loss = 22.84
2023-04-26 16:47:44,325 - INFO - Epoch 17 training loss = 20.56
2023-04-26 16:47:45,102 - INFO - Epoch 18 training loss = 21.15
2023-04-26 16:47:45,874 - INFO - Epoch 19 training loss = 19.73
2023-04-26 16:47:46,643 - INFO - Epoch 20 training loss = 18.09
2023-04-26 16:47:46,704 - INFO - Validation loss = 14.52
2023-04-26 16:47:46,705 - INFO - best model
2023-04-26 16:47:47,494 - INFO - Epoch 21 training loss = 17.18
2023-04-26 16:47:48,273 - INFO - Epoch 22 training loss = 16.64
2023-04-26 16:47:49,038 - INFO - Epoch 23 training loss = 16.24
2023-04-26 16:47:49,804 - INFO - Epoch 24 training loss = 15.04
2023-04-26 16:47:50,573 - INFO - Epoch 25 training loss = 15.84
2023-04-26 16:47:51,339 - INFO - Epoch 26 training loss = 15.2
2023-04-26 16:47:52,099 - INFO - Epoch 27 training loss = 13.14
2023-04-26 16:47:52,864 - INFO - Epoch 28 training loss = 13.38
2023-04-26 16:47:53,628 - INFO - Epoch 29 training loss = 11.14
2023-04-26 16:47:54,393 - INFO - Epoch 30 training loss = 11.23
2023-04-26 16:47:54,458 - INFO - Validation loss = 15.45
2023-04-26 16:47:55,222 - INFO - Epoch 31 training loss = 10.66
2023-04-26 16:47:55,999 - INFO - Epoch 32 training loss = 11.17
2023-04-26 16:47:56,776 - INFO - Epoch 33 training loss = 9.692
2023-04-26 16:47:57,552 - INFO - Epoch 34 training loss = 9.357
2023-04-26 16:47:58,321 - INFO - Epoch 35 training loss = 9.612
2023-04-26 16:47:59,087 - INFO - Epoch 36 training loss = 8.835
2023-04-26 16:47:59,861 - INFO - Epoch 37 training loss = 8.57
2023-04-26 16:48:00,636 - INFO - Epoch 38 training loss = 7.772
2023-04-26 16:48:01,409 - INFO - Epoch 39 training loss = 8.325
2023-04-26 16:48:02,182 - INFO - Epoch 40 training loss = 7.837
2023-04-26 16:48:02,245 - INFO - Validation loss = 6.703
2023-04-26 16:48:02,246 - INFO - best model
2023-04-26 16:48:03,036 - INFO - Epoch 41 training loss = 7.698
2023-04-26 16:48:03,806 - INFO - Epoch 42 training loss = 7.43
2023-04-26 16:48:04,568 - INFO - Epoch 43 training loss = 7.321
2023-04-26 16:48:05,336 - INFO - Epoch 44 training loss = 6.898
2023-04-26 16:48:06,098 - INFO - Epoch 45 training loss = 6.601
2023-04-26 16:48:06,867 - INFO - Epoch 46 training loss = 6.767
2023-04-26 16:48:07,632 - INFO - Epoch 47 training loss = 6.456
2023-04-26 16:48:08,401 - INFO - Epoch 48 training loss = 6.452
2023-04-26 16:48:09,173 - INFO - Epoch 49 training loss = 6.383
2023-04-26 16:48:09,950 - INFO - Epoch 50 training loss = 6.057
2023-04-26 16:48:10,013 - INFO - Validation loss = 5.99
2023-04-26 16:48:10,013 - INFO - best model
2023-04-26 16:48:10,793 - INFO - Epoch 51 training loss = 6.182
2023-04-26 16:48:11,564 - INFO - Epoch 52 training loss = 5.87
2023-04-26 16:48:12,336 - INFO - Epoch 53 training loss = 5.736
2023-04-26 16:48:13,111 - INFO - Epoch 54 training loss = 5.588
2023-04-26 16:48:13,882 - INFO - Epoch 55 training loss = 5.659
2023-04-26 16:48:14,648 - INFO - Epoch 56 training loss = 5.493
2023-04-26 16:48:15,413 - INFO - Epoch 57 training loss = 5.137
2023-04-26 16:48:16,177 - INFO - Epoch 58 training loss = 4.967
2023-04-26 16:48:16,948 - INFO - Epoch 59 training loss = 5.353
2023-04-26 16:48:17,730 - INFO - Epoch 60 training loss = 5.137
2023-04-26 16:48:17,793 - INFO - Validation loss = 7.463
2023-04-26 16:48:18,574 - INFO - Epoch 61 training loss = 4.568
2023-04-26 16:48:19,354 - INFO - Epoch 62 training loss = 4.842
2023-04-26 16:48:20,123 - INFO - Epoch 63 training loss = 4.732
2023-04-26 16:48:20,888 - INFO - Epoch 64 training loss = 4.498
2023-04-26 16:48:21,653 - INFO - Epoch 65 training loss = 4.757
2023-04-26 16:48:22,417 - INFO - Epoch 66 training loss = 4.716
2023-04-26 16:48:23,186 - INFO - Epoch 67 training loss = 4.581
2023-04-26 16:48:23,954 - INFO - Epoch 68 training loss = 4.338
2023-04-26 16:48:24,728 - INFO - Epoch 69 training loss = 4.614
2023-04-26 16:48:25,501 - INFO - Epoch 70 training loss = 4.472
2023-04-26 16:48:25,564 - INFO - Validation loss = 5.092
2023-04-26 16:48:25,564 - INFO - best model
2023-04-26 16:48:26,346 - INFO - Epoch 71 training loss = 4.302
2023-04-26 16:48:27,125 - INFO - Epoch 72 training loss = 4.274
2023-04-26 16:48:27,905 - INFO - Epoch 73 training loss = 4.235
2023-04-26 16:48:28,685 - INFO - Epoch 74 training loss = 4.209
2023-04-26 16:48:29,461 - INFO - Epoch 75 training loss = 4.135
2023-04-26 16:48:30,229 - INFO - Epoch 76 training loss = 4.317
2023-04-26 16:48:31,000 - INFO - Epoch 77 training loss = 4.093
2023-04-26 16:48:31,770 - INFO - Epoch 78 training loss = 3.944
2023-04-26 16:48:32,542 - INFO - Epoch 79 training loss = 4.201
2023-04-26 16:48:33,316 - INFO - Epoch 80 training loss = 3.838
2023-04-26 16:48:33,378 - INFO - Validation loss = 4.206
2023-04-26 16:48:33,379 - INFO - best model
2023-04-26 16:48:34,156 - INFO - Epoch 81 training loss = 4.177
2023-04-26 16:48:34,915 - INFO - Epoch 82 training loss = 3.758
2023-04-26 16:48:35,680 - INFO - Epoch 83 training loss = 3.872
2023-04-26 16:48:36,440 - INFO - Epoch 84 training loss = 3.887
2023-04-26 16:48:37,204 - INFO - Epoch 85 training loss = 3.665
2023-04-26 16:48:37,971 - INFO - Epoch 86 training loss = 3.81
2023-04-26 16:48:38,734 - INFO - Epoch 87 training loss = 3.511
2023-04-26 16:48:39,498 - INFO - Epoch 88 training loss = 3.639
2023-04-26 16:48:40,261 - INFO - Epoch 89 training loss = 3.672
2023-04-26 16:48:41,027 - INFO - Epoch 90 training loss = 3.354
2023-04-26 16:48:41,089 - INFO - Validation loss = 3.816
2023-04-26 16:48:41,089 - INFO - best model
2023-04-26 16:48:41,868 - INFO - Epoch 91 training loss = 3.289
2023-04-26 16:48:42,633 - INFO - Epoch 92 training loss = 3.42
2023-04-26 16:48:43,398 - INFO - Epoch 93 training loss = 3.473
2023-04-26 16:48:44,161 - INFO - Epoch 94 training loss = 3.317
2023-04-26 16:48:44,926 - INFO - Epoch 95 training loss = 3.219
2023-04-26 16:48:45,689 - INFO - Epoch 96 training loss = 3.174
2023-04-26 16:48:46,454 - INFO - Epoch 97 training loss = 3.329
2023-04-26 16:48:47,216 - INFO - Epoch 98 training loss = 3.549
2023-04-26 16:48:47,990 - INFO - Epoch 99 training loss = 3.021
2023-04-26 16:48:48,752 - INFO - Epoch 100 training loss = 3.083
2023-04-26 16:48:48,813 - INFO - Validation loss = 3.426
2023-04-26 16:48:48,814 - INFO - best model
2023-04-26 16:48:49,588 - INFO - Epoch 101 training loss = 3.153
2023-04-26 16:48:50,351 - INFO - Epoch 102 training loss = 3.353
2023-04-26 16:48:51,109 - INFO - Epoch 103 training loss = 3.123
2023-04-26 16:48:51,873 - INFO - Epoch 104 training loss = 3.148
2023-04-26 16:48:52,634 - INFO - Epoch 105 training loss = 3.043
2023-04-26 16:48:53,400 - INFO - Epoch 106 training loss = 2.911
2023-04-26 16:48:54,169 - INFO - Epoch 107 training loss = 3.19
2023-04-26 16:48:54,938 - INFO - Epoch 108 training loss = 2.725
2023-04-26 16:48:55,700 - INFO - Epoch 109 training loss = 2.997
2023-04-26 16:48:56,468 - INFO - Epoch 110 training loss = 2.941
2023-04-26 16:48:56,530 - INFO - Validation loss = 3.309
2023-04-26 16:48:56,530 - INFO - best model
2023-04-26 16:48:57,308 - INFO - Epoch 111 training loss = 2.804
2023-04-26 16:48:58,076 - INFO - Epoch 112 training loss = 2.552
2023-04-26 16:48:58,840 - INFO - Epoch 113 training loss = 2.823
2023-04-26 16:48:59,603 - INFO - Epoch 114 training loss = 2.893
2023-04-26 16:49:00,365 - INFO - Epoch 115 training loss = 2.885
2023-04-26 16:49:01,131 - INFO - Epoch 116 training loss = 2.804
2023-04-26 16:49:01,897 - INFO - Epoch 117 training loss = 2.972
2023-04-26 16:49:02,662 - INFO - Epoch 118 training loss = 2.883
2023-04-26 16:49:03,426 - INFO - Epoch 119 training loss = 2.765
2023-04-26 16:49:04,189 - INFO - Epoch 120 training loss = 2.592
2023-04-26 16:49:04,252 - INFO - Validation loss = 4.277
2023-04-26 16:49:05,022 - INFO - Epoch 121 training loss = 2.595
2023-04-26 16:49:05,789 - INFO - Epoch 122 training loss = 2.697
2023-04-26 16:49:06,556 - INFO - Epoch 123 training loss = 2.583
2023-04-26 16:49:07,322 - INFO - Epoch 124 training loss = 2.376
2023-04-26 16:49:08,094 - INFO - Epoch 125 training loss = 2.621
2023-04-26 16:49:08,856 - INFO - Epoch 126 training loss = 2.508
2023-04-26 16:49:09,621 - INFO - Epoch 127 training loss = 2.541
2023-04-26 16:49:10,392 - INFO - Epoch 128 training loss = 2.593
2023-04-26 16:49:11,154 - INFO - Epoch 129 training loss = 2.258
2023-04-26 16:49:11,918 - INFO - Epoch 130 training loss = 2.531
2023-04-26 16:49:11,980 - INFO - Validation loss = 3.154
2023-04-26 16:49:11,980 - INFO - best model
2023-04-26 16:49:12,757 - INFO - Epoch 131 training loss = 2.416
2023-04-26 16:49:13,526 - INFO - Epoch 132 training loss = 2.347
2023-04-26 16:49:14,300 - INFO - Epoch 133 training loss = 2.412
2023-04-26 16:49:15,063 - INFO - Epoch 134 training loss = 2.488
2023-04-26 16:49:15,839 - INFO - Epoch 135 training loss = 2.335
2023-04-26 16:49:16,604 - INFO - Epoch 136 training loss = 2.255
2023-04-26 16:49:17,365 - INFO - Epoch 137 training loss =  2.4
2023-04-26 16:49:18,140 - INFO - Epoch 138 training loss = 2.359
2023-04-26 16:49:18,907 - INFO - Epoch 139 training loss = 2.218
2023-04-26 16:49:19,675 - INFO - Epoch 140 training loss = 2.201
2023-04-26 16:49:19,737 - INFO - Validation loss = 3.115
2023-04-26 16:49:19,738 - INFO - best model
2023-04-26 16:49:20,517 - INFO - Epoch 141 training loss = 2.214
2023-04-26 16:49:21,289 - INFO - Epoch 142 training loss = 2.155
2023-04-26 16:49:22,058 - INFO - Epoch 143 training loss = 2.319
2023-04-26 16:49:22,826 - INFO - Epoch 144 training loss = 2.133
2023-04-26 16:49:23,591 - INFO - Epoch 145 training loss = 2.01
2023-04-26 16:49:24,355 - INFO - Epoch 146 training loss = 1.95
2023-04-26 16:49:25,120 - INFO - Epoch 147 training loss = 2.027
2023-04-26 16:49:25,883 - INFO - Epoch 148 training loss = 2.103
2023-04-26 16:49:26,646 - INFO - Epoch 149 training loss = 2.037
2023-04-26 16:49:27,409 - INFO - Epoch 150 training loss = 2.075
2023-04-26 16:49:27,473 - INFO - Validation loss = 3.052
2023-04-26 16:49:27,473 - INFO - best model
2023-04-26 16:49:28,249 - INFO - Epoch 151 training loss = 1.978
2023-04-26 16:49:29,014 - INFO - Epoch 152 training loss = 2.101
2023-04-26 16:49:29,783 - INFO - Epoch 153 training loss = 1.994
2023-04-26 16:49:30,550 - INFO - Epoch 154 training loss = 2.172
2023-04-26 16:49:31,317 - INFO - Epoch 155 training loss = 2.155
2023-04-26 16:49:32,085 - INFO - Epoch 156 training loss = 2.126
2023-04-26 16:49:32,853 - INFO - Epoch 157 training loss = 2.071
2023-04-26 16:49:33,623 - INFO - Epoch 158 training loss = 1.98
2023-04-26 16:49:34,388 - INFO - Epoch 159 training loss = 2.055
2023-04-26 16:49:35,149 - INFO - Epoch 160 training loss = 1.875
2023-04-26 16:49:35,212 - INFO - Validation loss = 3.014
2023-04-26 16:49:35,212 - INFO - best model
2023-04-26 16:49:35,987 - INFO - Epoch 161 training loss = 1.715
2023-04-26 16:49:36,749 - INFO - Epoch 162 training loss = 1.96
2023-04-26 16:49:37,517 - INFO - Epoch 163 training loss = 1.901
2023-04-26 16:49:38,281 - INFO - Epoch 164 training loss = 1.87
2023-04-26 16:49:39,045 - INFO - Epoch 165 training loss = 1.785
2023-04-26 16:49:39,810 - INFO - Epoch 166 training loss = 1.871
2023-04-26 16:49:40,579 - INFO - Epoch 167 training loss = 1.807
2023-04-26 16:49:41,342 - INFO - Epoch 168 training loss = 1.847
2023-04-26 16:49:42,109 - INFO - Epoch 169 training loss = 1.742
2023-04-26 16:49:42,873 - INFO - Epoch 170 training loss = 1.82
2023-04-26 16:49:42,935 - INFO - Validation loss = 2.385
2023-04-26 16:49:42,935 - INFO - best model
2023-04-26 16:49:43,708 - INFO - Epoch 171 training loss = 1.783
2023-04-26 16:49:44,476 - INFO - Epoch 172 training loss = 1.798
2023-04-26 16:49:45,245 - INFO - Epoch 173 training loss = 1.752
2023-04-26 16:49:46,012 - INFO - Epoch 174 training loss = 1.712
2023-04-26 16:49:46,779 - INFO - Epoch 175 training loss = 1.737
2023-04-26 16:49:47,550 - INFO - Epoch 176 training loss = 1.657
2023-04-26 16:49:48,324 - INFO - Epoch 177 training loss = 1.759
2023-04-26 16:49:49,094 - INFO - Epoch 178 training loss = 1.689
2023-04-26 16:49:49,856 - INFO - Epoch 179 training loss = 1.611
2023-04-26 16:49:50,620 - INFO - Epoch 180 training loss = 1.571
2023-04-26 16:49:50,682 - INFO - Validation loss = 2.217
2023-04-26 16:49:50,682 - INFO - best model
2023-04-26 16:49:51,457 - INFO - Epoch 181 training loss = 1.548
2023-04-26 16:49:52,217 - INFO - Epoch 182 training loss = 1.608
2023-04-26 16:49:52,982 - INFO - Epoch 183 training loss = 1.534
2023-04-26 16:49:53,744 - INFO - Epoch 184 training loss = 1.593
2023-04-26 16:49:54,510 - INFO - Epoch 185 training loss = 1.673
2023-04-26 16:49:55,270 - INFO - Epoch 186 training loss = 1.476
2023-04-26 16:49:56,071 - INFO - Epoch 187 training loss = 1.521
2023-04-26 16:49:56,870 - INFO - Epoch 188 training loss = 1.618
2023-04-26 16:49:57,635 - INFO - Epoch 189 training loss = 1.586
2023-04-26 16:49:58,400 - INFO - Epoch 190 training loss = 1.585
2023-04-26 16:49:58,462 - INFO - Validation loss = 2.527
2023-04-26 16:49:59,223 - INFO - Epoch 191 training loss = 1.507
2023-04-26 16:49:59,986 - INFO - Epoch 192 training loss = 1.589
2023-04-26 16:50:00,747 - INFO - Epoch 193 training loss = 1.424
2023-04-26 16:50:01,506 - INFO - Epoch 194 training loss = 1.468
2023-04-26 16:50:02,275 - INFO - Epoch 195 training loss = 1.418
2023-04-26 16:50:03,044 - INFO - Epoch 196 training loss = 1.453
2023-04-26 16:50:03,806 - INFO - Epoch 197 training loss = 1.389
2023-04-26 16:50:04,571 - INFO - Epoch 198 training loss = 1.406
2023-04-26 16:50:05,335 - INFO - Epoch 199 training loss = 1.437
2023-04-26 16:50:06,115 - INFO - Epoch 200 training loss = 1.418
2023-04-26 16:50:06,179 - INFO - Validation loss = 2.059
2023-04-26 16:50:06,180 - INFO - best model
2023-04-26 16:50:06,965 - INFO - Epoch 201 training loss = 1.319
2023-04-26 16:50:07,732 - INFO - Epoch 202 training loss = 1.337
2023-04-26 16:50:08,503 - INFO - Epoch 203 training loss = 1.277
2023-04-26 16:50:09,269 - INFO - Epoch 204 training loss = 1.42
2023-04-26 16:50:10,033 - INFO - Epoch 205 training loss = 1.374
2023-04-26 16:50:10,796 - INFO - Epoch 206 training loss = 1.406
2023-04-26 16:50:11,561 - INFO - Epoch 207 training loss = 1.333
2023-04-26 16:50:12,322 - INFO - Epoch 208 training loss = 1.316
2023-04-26 16:50:13,087 - INFO - Epoch 209 training loss = 1.354
2023-04-26 16:50:13,851 - INFO - Epoch 210 training loss = 1.393
2023-04-26 16:50:13,913 - INFO - Validation loss = 2.218
2023-04-26 16:50:14,675 - INFO - Epoch 211 training loss = 1.251
2023-04-26 16:50:15,440 - INFO - Epoch 212 training loss = 1.306
2023-04-26 16:50:16,203 - INFO - Epoch 213 training loss = 1.278
2023-04-26 16:50:16,966 - INFO - Epoch 214 training loss = 1.251
2023-04-26 16:50:17,731 - INFO - Epoch 215 training loss = 1.324
2023-04-26 16:50:18,497 - INFO - Epoch 216 training loss = 1.262
2023-04-26 16:50:19,261 - INFO - Epoch 217 training loss = 1.328
2023-04-26 16:50:20,022 - INFO - Epoch 218 training loss = 1.267
2023-04-26 16:50:20,785 - INFO - Epoch 219 training loss = 1.178
2023-04-26 16:50:21,546 - INFO - Epoch 220 training loss = 1.258
2023-04-26 16:50:21,607 - INFO - Validation loss = 1.75
2023-04-26 16:50:21,607 - INFO - best model
2023-04-26 16:50:22,381 - INFO - Epoch 221 training loss = 1.195
2023-04-26 16:50:23,142 - INFO - Epoch 222 training loss = 1.231
2023-04-26 16:50:23,905 - INFO - Epoch 223 training loss = 1.118
2023-04-26 16:50:24,668 - INFO - Epoch 224 training loss = 1.124
2023-04-26 16:50:25,429 - INFO - Epoch 225 training loss = 1.107
2023-04-26 16:50:26,191 - INFO - Epoch 226 training loss = 1.088
2023-04-26 16:50:26,952 - INFO - Epoch 227 training loss = 1.134
2023-04-26 16:50:27,717 - INFO - Epoch 228 training loss = 1.085
2023-04-26 16:50:28,479 - INFO - Epoch 229 training loss = 1.086
2023-04-26 16:50:29,241 - INFO - Epoch 230 training loss = 1.075
2023-04-26 16:50:29,303 - INFO - Validation loss = 2.676
2023-04-26 16:50:30,066 - INFO - Epoch 231 training loss = 1.06
2023-04-26 16:50:30,828 - INFO - Epoch 232 training loss = 1.068
2023-04-26 16:50:31,589 - INFO - Epoch 233 training loss = 1.105
2023-04-26 16:50:32,352 - INFO - Epoch 234 training loss = 1.165
2023-04-26 16:50:33,117 - INFO - Epoch 235 training loss = 1.03
2023-04-26 16:50:33,878 - INFO - Epoch 236 training loss = 0.9824
2023-04-26 16:50:34,641 - INFO - Epoch 237 training loss =  1.0
2023-04-26 16:50:35,400 - INFO - Epoch 238 training loss = 1.037
2023-04-26 16:50:36,160 - INFO - Epoch 239 training loss = 1.095
2023-04-26 16:50:36,920 - INFO - Epoch 240 training loss = 1.016
2023-04-26 16:50:36,981 - INFO - Validation loss = 1.824
2023-04-26 16:50:37,741 - INFO - Epoch 241 training loss = 0.9929
2023-04-26 16:50:38,499 - INFO - Epoch 242 training loss = 0.9922
2023-04-26 16:50:39,257 - INFO - Epoch 243 training loss = 0.9841
2023-04-26 16:50:40,017 - INFO - Epoch 244 training loss = 0.9821
2023-04-26 16:50:40,775 - INFO - Epoch 245 training loss = 0.9471
2023-04-26 16:50:41,534 - INFO - Epoch 246 training loss = 0.9587
2023-04-26 16:50:42,295 - INFO - Epoch 247 training loss = 0.9417
2023-04-26 16:50:43,058 - INFO - Epoch 248 training loss = 0.9223
2023-04-26 16:50:43,819 - INFO - Epoch 249 training loss = 0.9464
2023-04-26 16:50:44,582 - INFO - Epoch 250 training loss = 0.9333
2023-04-26 16:50:44,643 - INFO - Validation loss = 1.516
2023-04-26 16:50:44,643 - INFO - best model
2023-04-26 16:50:45,414 - INFO - Epoch 251 training loss = 0.9275
2023-04-26 16:50:46,174 - INFO - Epoch 252 training loss = 0.9142
2023-04-26 16:50:46,936 - INFO - Epoch 253 training loss = 0.9494
2023-04-26 16:50:47,697 - INFO - Epoch 254 training loss = 0.9067
2023-04-26 16:50:48,462 - INFO - Epoch 255 training loss = 0.8859
2023-04-26 16:50:49,222 - INFO - Epoch 256 training loss = 0.8659
2023-04-26 16:50:49,981 - INFO - Epoch 257 training loss = 0.8277
2023-04-26 16:50:50,739 - INFO - Epoch 258 training loss = 0.8688
2023-04-26 16:50:51,499 - INFO - Epoch 259 training loss = 0.8173
2023-04-26 16:50:52,257 - INFO - Epoch 260 training loss = 0.8563
2023-04-26 16:50:52,319 - INFO - Validation loss = 1.618
2023-04-26 16:50:53,080 - INFO - Epoch 261 training loss = 0.8128
2023-04-26 16:50:53,840 - INFO - Epoch 262 training loss = 0.8295
2023-04-26 16:50:54,599 - INFO - Epoch 263 training loss = 0.827
2023-04-26 16:50:55,359 - INFO - Epoch 264 training loss = 0.8032
2023-04-26 16:50:56,119 - INFO - Epoch 265 training loss = 0.7704
2023-04-26 16:50:56,882 - INFO - Epoch 266 training loss = 0.767
2023-04-26 16:50:57,643 - INFO - Epoch 267 training loss = 0.7755
2023-04-26 16:50:58,404 - INFO - Epoch 268 training loss = 0.7667
2023-04-26 16:50:59,165 - INFO - Epoch 269 training loss = 0.7335
2023-04-26 16:50:59,925 - INFO - Epoch 270 training loss = 0.7745
2023-04-26 16:50:59,986 - INFO - Validation loss = 1.448
2023-04-26 16:50:59,986 - INFO - best model
2023-04-26 16:51:00,756 - INFO - Epoch 271 training loss = 0.7691
2023-04-26 16:51:01,518 - INFO - Epoch 272 training loss = 0.7059
2023-04-26 16:51:02,280 - INFO - Epoch 273 training loss = 0.7337
2023-04-26 16:51:03,041 - INFO - Epoch 274 training loss = 0.7463
2023-04-26 16:51:03,801 - INFO - Epoch 275 training loss = 0.738
2023-04-26 16:51:04,562 - INFO - Epoch 276 training loss = 0.7095
2023-04-26 16:51:05,323 - INFO - Epoch 277 training loss = 0.6911
2023-04-26 16:51:06,088 - INFO - Epoch 278 training loss = 0.6796
2023-04-26 16:51:06,849 - INFO - Epoch 279 training loss = 0.6929
2023-04-26 16:51:07,611 - INFO - Epoch 280 training loss = 0.6693
2023-04-26 16:51:07,674 - INFO - Validation loss = 1.338
2023-04-26 16:51:07,674 - INFO - best model
2023-04-26 16:51:08,444 - INFO - Epoch 281 training loss = 0.6623
2023-04-26 16:51:09,201 - INFO - Epoch 282 training loss = 0.6672
2023-04-26 16:51:09,959 - INFO - Epoch 283 training loss = 0.6926
2023-04-26 16:51:10,721 - INFO - Epoch 284 training loss = 0.6923
2023-04-26 16:51:11,481 - INFO - Epoch 285 training loss = 0.6287
2023-04-26 16:51:12,243 - INFO - Epoch 286 training loss = 0.6607
2023-04-26 16:51:13,001 - INFO - Epoch 287 training loss = 0.6485
2023-04-26 16:51:13,761 - INFO - Epoch 288 training loss = 0.6211
2023-04-26 16:51:14,522 - INFO - Epoch 289 training loss = 0.6228
2023-04-26 16:51:15,285 - INFO - Epoch 290 training loss = 0.6424
2023-04-26 16:51:15,347 - INFO - Validation loss = 1.449
2023-04-26 16:51:16,121 - INFO - Epoch 291 training loss = 0.6514
2023-04-26 16:51:16,896 - INFO - Epoch 292 training loss = 0.6112
2023-04-26 16:51:17,675 - INFO - Epoch 293 training loss = 0.6286
2023-04-26 16:51:18,445 - INFO - Epoch 294 training loss = 0.6428
2023-04-26 16:51:19,203 - INFO - Epoch 295 training loss = 0.6107
2023-04-26 16:51:19,963 - INFO - Epoch 296 training loss = 0.5808
2023-04-26 16:51:20,725 - INFO - Epoch 297 training loss = 0.5783
2023-04-26 16:51:21,484 - INFO - Epoch 298 training loss = 0.5739
2023-04-26 16:51:22,244 - INFO - Epoch 299 training loss = 0.5532
2023-04-26 16:51:23,003 - INFO - Epoch 300 training loss = 0.5549
2023-04-26 16:51:23,064 - INFO - Validation loss = 1.304
2023-04-26 16:51:23,065 - INFO - best model
2023-04-26 16:51:23,836 - INFO - Epoch 301 training loss = 0.5539
2023-04-26 16:51:24,596 - INFO - Epoch 302 training loss = 0.5568
2023-04-26 16:51:25,359 - INFO - Epoch 303 training loss = 0.5691
2023-04-26 16:51:26,124 - INFO - Epoch 304 training loss = 0.535
2023-04-26 16:51:26,893 - INFO - Epoch 305 training loss = 0.5498
2023-04-26 16:51:27,664 - INFO - Epoch 306 training loss = 0.5153
2023-04-26 16:51:28,437 - INFO - Epoch 307 training loss = 0.5582
2023-04-26 16:51:29,201 - INFO - Epoch 308 training loss = 0.4967
2023-04-26 16:51:29,961 - INFO - Epoch 309 training loss = 0.5401
2023-04-26 16:51:30,719 - INFO - Epoch 310 training loss = 0.513
2023-04-26 16:51:30,780 - INFO - Validation loss = 1.303
2023-04-26 16:51:30,780 - INFO - best model
2023-04-26 16:51:31,550 - INFO - Epoch 311 training loss = 0.5118
2023-04-26 16:51:32,308 - INFO - Epoch 312 training loss = 0.4943
2023-04-26 16:51:33,068 - INFO - Epoch 313 training loss = 0.504
2023-04-26 16:51:33,828 - INFO - Epoch 314 training loss = 0.4935
2023-04-26 16:51:34,588 - INFO - Epoch 315 training loss = 0.5085
2023-04-26 16:51:35,348 - INFO - Epoch 316 training loss = 0.4863
2023-04-26 16:51:36,107 - INFO - Epoch 317 training loss = 0.4616
2023-04-26 16:51:36,870 - INFO - Epoch 318 training loss = 0.4616
2023-04-26 16:51:37,633 - INFO - Epoch 319 training loss = 0.4716
2023-04-26 16:51:38,398 - INFO - Epoch 320 training loss = 0.4787
2023-04-26 16:51:38,459 - INFO - Validation loss = 1.341
2023-04-26 16:51:39,222 - INFO - Epoch 321 training loss = 0.4412
2023-04-26 16:51:39,982 - INFO - Epoch 322 training loss = 0.4464
2023-04-26 16:51:40,740 - INFO - Epoch 323 training loss = 0.4351
2023-04-26 16:51:41,505 - INFO - Epoch 324 training loss = 0.4444
2023-04-26 16:51:42,265 - INFO - Epoch 325 training loss = 0.4464
2023-04-26 16:51:43,026 - INFO - Epoch 326 training loss = 0.4251
2023-04-26 16:51:43,786 - INFO - Epoch 327 training loss = 0.4304
2023-04-26 16:51:44,547 - INFO - Epoch 328 training loss = 0.4451
2023-04-26 16:51:45,310 - INFO - Epoch 329 training loss = 0.4382
2023-04-26 16:51:46,071 - INFO - Epoch 330 training loss = 0.414
2023-04-26 16:51:46,133 - INFO - Validation loss = 1.221
2023-04-26 16:51:46,133 - INFO - best model
2023-04-26 16:51:46,916 - INFO - Epoch 331 training loss = 0.4167
2023-04-26 16:51:47,686 - INFO - Epoch 332 training loss = 0.4109
2023-04-26 16:51:48,448 - INFO - Epoch 333 training loss = 0.4032
2023-04-26 16:51:49,208 - INFO - Epoch 334 training loss = 0.3958
2023-04-26 16:51:49,983 - INFO - Epoch 335 training loss = 0.4111
2023-04-26 16:51:50,754 - INFO - Epoch 336 training loss = 0.3872
2023-04-26 16:51:51,529 - INFO - Epoch 337 training loss = 0.3972
2023-04-26 16:51:52,299 - INFO - Epoch 338 training loss = 0.3936
2023-04-26 16:51:53,064 - INFO - Epoch 339 training loss = 0.3898
2023-04-26 16:51:53,828 - INFO - Epoch 340 training loss = 0.3714
2023-04-26 16:51:53,889 - INFO - Validation loss = 1.234
2023-04-26 16:51:54,653 - INFO - Epoch 341 training loss = 0.3818
2023-04-26 16:51:55,418 - INFO - Epoch 342 training loss = 0.3633
2023-04-26 16:51:56,182 - INFO - Epoch 343 training loss = 0.392
2023-04-26 16:51:56,947 - INFO - Epoch 344 training loss = 0.3648
2023-04-26 16:51:57,711 - INFO - Epoch 345 training loss = 0.3594
2023-04-26 16:51:58,472 - INFO - Epoch 346 training loss = 0.353
2023-04-26 16:51:59,234 - INFO - Epoch 347 training loss = 0.3477
2023-04-26 16:51:59,995 - INFO - Epoch 348 training loss = 0.3584
2023-04-26 16:52:00,757 - INFO - Epoch 349 training loss = 0.3471
2023-04-26 16:52:01,518 - INFO - Epoch 350 training loss = 0.3545
2023-04-26 16:52:01,580 - INFO - Validation loss = 1.167
2023-04-26 16:52:01,580 - INFO - best model
2023-04-26 16:52:02,365 - INFO - Epoch 351 training loss = 0.3534
2023-04-26 16:52:03,131 - INFO - Epoch 352 training loss = 0.3383
2023-04-26 16:52:03,892 - INFO - Epoch 353 training loss = 0.337
2023-04-26 16:52:04,650 - INFO - Epoch 354 training loss = 0.3275
2023-04-26 16:52:05,408 - INFO - Epoch 355 training loss = 0.325
2023-04-26 16:52:06,168 - INFO - Epoch 356 training loss = 0.3138
2023-04-26 16:52:06,927 - INFO - Epoch 357 training loss = 0.3339
2023-04-26 16:52:07,684 - INFO - Epoch 358 training loss = 0.3233
2023-04-26 16:52:08,444 - INFO - Epoch 359 training loss = 0.3104
2023-04-26 16:52:09,204 - INFO - Epoch 360 training loss = 0.3067
2023-04-26 16:52:09,265 - INFO - Validation loss = 1.162
2023-04-26 16:52:09,265 - INFO - best model
2023-04-26 16:52:10,039 - INFO - Epoch 361 training loss = 0.3145
2023-04-26 16:52:10,798 - INFO - Epoch 362 training loss = 0.3092
2023-04-26 16:52:11,557 - INFO - Epoch 363 training loss = 0.3002
2023-04-26 16:52:12,317 - INFO - Epoch 364 training loss = 0.297
2023-04-26 16:52:13,078 - INFO - Epoch 365 training loss = 0.3037
2023-04-26 16:52:13,839 - INFO - Epoch 366 training loss = 0.3023
2023-04-26 16:52:14,601 - INFO - Epoch 367 training loss = 0.2909
2023-04-26 16:52:15,360 - INFO - Epoch 368 training loss = 0.2864
2023-04-26 16:52:16,119 - INFO - Epoch 369 training loss = 0.2884
2023-04-26 16:52:16,879 - INFO - Epoch 370 training loss = 0.2928
2023-04-26 16:52:16,940 - INFO - Validation loss = 1.132
2023-04-26 16:52:16,941 - INFO - best model
2023-04-26 16:52:17,711 - INFO - Epoch 371 training loss = 0.2747
2023-04-26 16:52:18,471 - INFO - Epoch 372 training loss = 0.2817
2023-04-26 16:52:19,244 - INFO - Epoch 373 training loss = 0.2756
2023-04-26 16:52:20,011 - INFO - Epoch 374 training loss = 0.2778
2023-04-26 16:52:20,774 - INFO - Epoch 375 training loss = 0.2731
2023-04-26 16:52:21,539 - INFO - Epoch 376 training loss = 0.2716
2023-04-26 16:52:22,305 - INFO - Epoch 377 training loss = 0.2681
2023-04-26 16:52:23,072 - INFO - Epoch 378 training loss = 0.2711
2023-04-26 16:52:23,836 - INFO - Epoch 379 training loss = 0.2693
2023-04-26 16:52:24,610 - INFO - Epoch 380 training loss = 0.2665
2023-04-26 16:52:24,672 - INFO - Validation loss = 1.119
2023-04-26 16:52:24,673 - INFO - best model
2023-04-26 16:52:25,459 - INFO - Epoch 381 training loss = 0.2639
2023-04-26 16:52:26,236 - INFO - Epoch 382 training loss = 0.2576
2023-04-26 16:52:27,010 - INFO - Epoch 383 training loss = 0.259
2023-04-26 16:52:27,773 - INFO - Epoch 384 training loss = 0.2547
2023-04-26 16:52:28,536 - INFO - Epoch 385 training loss = 0.2533
2023-04-26 16:52:29,299 - INFO - Epoch 386 training loss = 0.2512
2023-04-26 16:52:30,069 - INFO - Epoch 387 training loss = 0.2486
2023-04-26 16:52:30,844 - INFO - Epoch 388 training loss = 0.2495
2023-04-26 16:52:31,619 - INFO - Epoch 389 training loss = 0.2465
2023-04-26 16:52:32,386 - INFO - Epoch 390 training loss = 0.2435
2023-04-26 16:52:32,447 - INFO - Validation loss = 1.097
2023-04-26 16:52:32,447 - INFO - best model
2023-04-26 16:52:33,218 - INFO - Epoch 391 training loss = 0.2404
2023-04-26 16:52:33,981 - INFO - Epoch 392 training loss = 0.2391
2023-04-26 16:52:34,746 - INFO - Epoch 393 training loss = 0.238
2023-04-26 16:52:35,512 - INFO - Epoch 394 training loss = 0.2364
2023-04-26 16:52:36,278 - INFO - Epoch 395 training loss = 0.2359
2023-04-26 16:52:37,045 - INFO - Epoch 396 training loss = 0.2345
2023-04-26 16:52:37,808 - INFO - Epoch 397 training loss = 0.2339
2023-04-26 16:52:38,574 - INFO - Epoch 398 training loss = 0.2279
2023-04-26 16:52:39,340 - INFO - Epoch 399 training loss = 0.2259
2023-04-26 16:52:40,106 - INFO - Epoch 400 training loss = 0.2298
2023-04-26 16:52:40,167 - INFO - Validation loss = 1.093
2023-04-26 16:52:40,167 - INFO - best model
2023-04-26 16:52:40,944 - INFO - Epoch 401 training loss = 0.2262
2023-04-26 16:52:41,711 - INFO - Epoch 402 training loss = 0.2254
2023-04-26 16:52:42,479 - INFO - Epoch 403 training loss = 0.2216
2023-04-26 16:52:43,246 - INFO - Epoch 404 training loss = 0.2226
2023-04-26 16:52:44,008 - INFO - Epoch 405 training loss = 0.22
2023-04-26 16:52:44,769 - INFO - Epoch 406 training loss = 0.2209
2023-04-26 16:52:45,528 - INFO - Epoch 407 training loss = 0.2172
2023-04-26 16:52:46,287 - INFO - Epoch 408 training loss = 0.214
2023-04-26 16:52:47,049 - INFO - Epoch 409 training loss = 0.2168
2023-04-26 16:52:47,818 - INFO - Epoch 410 training loss = 0.2126
2023-04-26 16:52:47,879 - INFO - Validation loss = 1.068
2023-04-26 16:52:47,880 - INFO - best model
2023-04-26 16:52:48,654 - INFO - Epoch 411 training loss = 0.2135
2023-04-26 16:52:49,415 - INFO - Epoch 412 training loss = 0.2118
2023-04-26 16:52:50,173 - INFO - Epoch 413 training loss = 0.21
2023-04-26 16:52:50,934 - INFO - Epoch 414 training loss = 0.2074
2023-04-26 16:52:51,697 - INFO - Epoch 415 training loss = 0.2072
2023-04-26 16:52:52,457 - INFO - Epoch 416 training loss = 0.2079
2023-04-26 16:52:53,217 - INFO - Epoch 417 training loss = 0.2037
2023-04-26 16:52:53,978 - INFO - Epoch 418 training loss = 0.2039
2023-04-26 16:52:54,738 - INFO - Epoch 419 training loss = 0.2042
2023-04-26 16:52:55,496 - INFO - Epoch 420 training loss = 0.203
2023-04-26 16:52:55,557 - INFO - Validation loss = 1.064
2023-04-26 16:52:55,557 - INFO - best model
2023-04-26 16:52:56,328 - INFO - Epoch 421 training loss = 0.203
2023-04-26 16:52:57,091 - INFO - Epoch 422 training loss = 0.2008
2023-04-26 16:52:57,851 - INFO - Epoch 423 training loss = 0.1992
2023-04-26 16:52:58,612 - INFO - Epoch 424 training loss = 0.1978
2023-04-26 16:52:59,373 - INFO - Epoch 425 training loss = 0.1973
2023-04-26 16:53:00,134 - INFO - Epoch 426 training loss = 0.1964
2023-04-26 16:53:00,894 - INFO - Epoch 427 training loss = 0.1956
2023-04-26 16:53:01,656 - INFO - Epoch 428 training loss = 0.1935
2023-04-26 16:53:02,419 - INFO - Epoch 429 training loss = 0.1935
2023-04-26 16:53:03,180 - INFO - Epoch 430 training loss = 0.1924
2023-04-26 16:53:03,241 - INFO - Validation loss = 1.06
2023-04-26 16:53:03,241 - INFO - best model
2023-04-26 16:53:04,008 - INFO - Epoch 431 training loss = 0.1932
2023-04-26 16:53:04,768 - INFO - Epoch 432 training loss = 0.1908
2023-04-26 16:53:05,529 - INFO - Epoch 433 training loss = 0.1896
2023-04-26 16:53:06,290 - INFO - Epoch 434 training loss = 0.1887
2023-04-26 16:53:07,051 - INFO - Epoch 435 training loss = 0.1887
2023-04-26 16:53:07,813 - INFO - Epoch 436 training loss = 0.1873
2023-04-26 16:53:08,578 - INFO - Epoch 437 training loss = 0.1872
2023-04-26 16:53:09,346 - INFO - Epoch 438 training loss = 0.1858
2023-04-26 16:53:10,105 - INFO - Epoch 439 training loss = 0.1866
2023-04-26 16:53:10,866 - INFO - Epoch 440 training loss = 0.1851
2023-04-26 16:53:10,928 - INFO - Validation loss = 1.054
2023-04-26 16:53:10,928 - INFO - best model
2023-04-26 16:53:11,702 - INFO - Epoch 441 training loss = 0.1845
2023-04-26 16:53:12,465 - INFO - Epoch 442 training loss = 0.1839
2023-04-26 16:53:13,227 - INFO - Epoch 443 training loss = 0.1831
2023-04-26 16:53:13,990 - INFO - Epoch 444 training loss = 0.1826
2023-04-26 16:53:14,754 - INFO - Epoch 445 training loss = 0.1815
2023-04-26 16:53:15,516 - INFO - Epoch 446 training loss = 0.1819
2023-04-26 16:53:16,283 - INFO - Epoch 447 training loss = 0.1807
2023-04-26 16:53:17,058 - INFO - Epoch 448 training loss = 0.1801
2023-04-26 16:53:17,827 - INFO - Epoch 449 training loss = 0.1788
2023-04-26 16:53:18,595 - INFO - Epoch 450 training loss = 0.1785
2023-04-26 16:53:18,656 - INFO - Validation loss = 1.056
2023-04-26 16:53:19,418 - INFO - Epoch 451 training loss = 0.179
2023-04-26 16:53:20,183 - INFO - Epoch 452 training loss = 0.1776
2023-04-26 16:53:20,945 - INFO - Epoch 453 training loss = 0.178
2023-04-26 16:53:21,707 - INFO - Epoch 454 training loss = 0.1778
2023-04-26 16:53:22,472 - INFO - Epoch 455 training loss = 0.1765
2023-04-26 16:53:23,238 - INFO - Epoch 456 training loss = 0.1764
2023-04-26 16:53:24,004 - INFO - Epoch 457 training loss = 0.176
2023-04-26 16:53:24,769 - INFO - Epoch 458 training loss = 0.1753
2023-04-26 16:53:25,534 - INFO - Epoch 459 training loss = 0.1746
2023-04-26 16:53:26,300 - INFO - Epoch 460 training loss = 0.1742
2023-04-26 16:53:26,361 - INFO - Validation loss = 1.052
2023-04-26 16:53:26,362 - INFO - best model
2023-04-26 16:53:27,136 - INFO - Epoch 461 training loss = 0.1742
2023-04-26 16:53:27,898 - INFO - Epoch 462 training loss = 0.1736
2023-04-26 16:53:28,662 - INFO - Epoch 463 training loss = 0.1727
2023-04-26 16:53:29,426 - INFO - Epoch 464 training loss = 0.1731
2023-04-26 16:53:30,204 - INFO - Epoch 465 training loss = 0.1732
2023-04-26 16:53:30,997 - INFO - Epoch 466 training loss = 0.172
2023-04-26 16:53:31,789 - INFO - Epoch 467 training loss = 0.1718
2023-04-26 16:53:32,554 - INFO - Epoch 468 training loss = 0.1716
2023-04-26 16:53:33,313 - INFO - Epoch 469 training loss = 0.171
2023-04-26 16:53:34,074 - INFO - Epoch 470 training loss = 0.1708
2023-04-26 16:53:34,135 - INFO - Validation loss = 1.046
2023-04-26 16:53:34,135 - INFO - best model
2023-04-26 16:53:34,906 - INFO - Epoch 471 training loss = 0.1704
2023-04-26 16:53:35,666 - INFO - Epoch 472 training loss = 0.1704
2023-04-26 16:53:36,426 - INFO - Epoch 473 training loss = 0.17
2023-04-26 16:53:37,186 - INFO - Epoch 474 training loss = 0.1701
2023-04-26 16:53:37,945 - INFO - Epoch 475 training loss = 0.1695
2023-04-26 16:53:38,704 - INFO - Epoch 476 training loss = 0.1695
2023-04-26 16:53:39,465 - INFO - Epoch 477 training loss = 0.1691
2023-04-26 16:53:40,226 - INFO - Epoch 478 training loss = 0.1686
2023-04-26 16:53:40,986 - INFO - Epoch 479 training loss = 0.1684
2023-04-26 16:53:41,747 - INFO - Epoch 480 training loss = 0.1683
2023-04-26 16:53:41,808 - INFO - Validation loss = 1.046
2023-04-26 16:53:42,569 - INFO - Epoch 481 training loss = 0.168
2023-04-26 16:53:43,330 - INFO - Epoch 482 training loss = 0.1679
2023-04-26 16:53:44,094 - INFO - Epoch 483 training loss = 0.1675
2023-04-26 16:53:44,855 - INFO - Epoch 484 training loss = 0.1677
2023-04-26 16:53:45,615 - INFO - Epoch 485 training loss = 0.1673
2023-04-26 16:53:46,373 - INFO - Epoch 486 training loss = 0.1673
2023-04-26 16:53:47,134 - INFO - Epoch 487 training loss = 0.167
2023-04-26 16:53:47,898 - INFO - Epoch 488 training loss = 0.1669
2023-04-26 16:53:48,660 - INFO - Epoch 489 training loss = 0.1668
2023-04-26 16:53:49,420 - INFO - Epoch 490 training loss = 0.1667
2023-04-26 16:53:49,481 - INFO - Validation loss = 1.044
2023-04-26 16:53:49,482 - INFO - best model
2023-04-26 16:53:50,253 - INFO - Epoch 491 training loss = 0.1665
2023-04-26 16:53:51,012 - INFO - Epoch 492 training loss = 0.1664
2023-04-26 16:53:51,772 - INFO - Epoch 493 training loss = 0.1663
2023-04-26 16:53:52,534 - INFO - Epoch 494 training loss = 0.1663
2023-04-26 16:53:53,294 - INFO - Epoch 495 training loss = 0.1662
2023-04-26 16:53:54,055 - INFO - Epoch 496 training loss = 0.1661
2023-04-26 16:53:54,815 - INFO - Epoch 497 training loss = 0.1661
2023-04-26 16:53:55,576 - INFO - Epoch 498 training loss = 0.1661
2023-04-26 16:53:56,336 - INFO - Epoch 499 training loss = 0.166
2023-04-26 16:53:56,386 - INFO - Validation loss = 1.044
