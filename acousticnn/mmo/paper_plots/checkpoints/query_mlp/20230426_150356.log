2023-04-26 15:03:56,264 - INFO - Config:
Namespace(config='configs/implicit_mlp.yaml', dir='/user/delden/acoustics/spp_ai_acoustics/acousticNN/experiments/arch/no_encoding/implicitmlp', epochs=100, device='cuda', seed=1, parameter_list='configs/data.yaml', encoding='none', dir_name='arch/no_encoding/implicitmlp')
2023-04-26 15:03:56,264 - INFO - Config:
Munch({'optimizer': Munch({'type': 'AdamW', 'kwargs': Munch({'lr': 0.001, 'weight_decay': 0.005, 'betas': [0.9, 0.95]})}), 'scheduler': Munch({'type': 'CosLR', 'kwargs': Munch({'epochs': 500, 'initial_epochs': 25})}), 'model': Munch({'name': 'ImplicitMLP', 'input_encoding': False, 'encoding_dim': 16, 'embed_dim': 64, 'depth': 7, 'mlp_width': 256}), 'generation_frequency': 5001, 'validation_frequency': 10, 'batch_size': 16, 'epochs': 500, 'gradient_clip': 10})
2023-04-26 15:04:11,213 - INFO - ==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ImplicitMLP                              [16, 200, 4]              --
├─GroupwiseProjection: 1-1               [3200, 15, 64]            --
│    └─ModuleList: 2-1                   --                        --
│    │    └─Linear: 3-1                  [3200, 4, 64]             128
│    │    └─Linear: 3-2                  [3200, 5, 64]             128
│    │    └─Linear: 3-3                  [3200, 5, 64]             128
│    │    └─Linear: 3-4                  [3200, 1, 64]             128
├─MLP: 1-2                               [3200, 256]               --
│    └─Linear: 2-2                       [3200, 256]               246,016
│    └─ReLU: 2-3                         [3200, 256]               --
│    └─Dropout: 2-4                      [3200, 256]               --
│    └─Linear: 2-5                       [3200, 256]               65,792
│    └─ReLU: 2-6                         [3200, 256]               --
│    └─Dropout: 2-7                      [3200, 256]               --
│    └─Linear: 2-8                       [3200, 256]               65,792
│    └─ReLU: 2-9                         [3200, 256]               --
│    └─Dropout: 2-10                     [3200, 256]               --
│    └─Linear: 2-11                      [3200, 256]               65,792
│    └─ReLU: 2-12                        [3200, 256]               --
│    └─Dropout: 2-13                     [3200, 256]               --
│    └─Linear: 2-14                      [3200, 256]               65,792
│    └─ReLU: 2-15                        [3200, 256]               --
│    └─Dropout: 2-16                     [3200, 256]               --
│    └─Linear: 2-17                      [3200, 256]               65,792
│    └─ReLU: 2-18                        [3200, 256]               --
│    └─Dropout: 2-19                     [3200, 256]               --
│    └─Linear: 2-20                      [3200, 256]               65,792
│    └─Dropout: 2-21                     [3200, 256]               --
├─Linear: 1-3                            [3200, 4]                 1,028
==========================================================================================
Total params: 642,308
Trainable params: 642,308
Non-trainable params: 0
Total mult-adds (G): 2.06
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 70.55
Params size (MB): 2.57
Estimated Total Size (MB): 73.14
==========================================================================================
2023-04-26 15:04:13,442 - INFO - Epoch 0 training loss = 3.266e+03
2023-04-26 15:04:13,559 - INFO - Validation loss = 3.143e+03
2023-04-26 15:04:13,560 - INFO - best model
2023-04-26 15:04:15,945 - INFO - Epoch 1 training loss = 3.028e+03
2023-04-26 15:04:18,329 - INFO - Epoch 2 training loss = 705.1
2023-04-26 15:04:20,696 - INFO - Epoch 3 training loss = 126.7
2023-04-26 15:04:23,042 - INFO - Epoch 4 training loss = 101.5
2023-04-26 15:04:25,297 - INFO - Epoch 5 training loss = 88.85
2023-04-26 15:04:27,404 - INFO - Epoch 6 training loss = 75.85
2023-04-26 15:04:29,693 - INFO - Epoch 7 training loss = 64.01
2023-04-26 15:04:32,085 - INFO - Epoch 8 training loss = 51.68
2023-04-26 15:04:34,294 - INFO - Epoch 9 training loss = 43.72
2023-04-26 15:04:36,440 - INFO - Epoch 10 training loss = 37.97
2023-04-26 15:04:36,557 - INFO - Validation loss = 34.09
2023-04-26 15:04:36,557 - INFO - best model
2023-04-26 15:04:38,933 - INFO - Epoch 11 training loss = 35.96
2023-04-26 15:04:41,229 - INFO - Epoch 12 training loss = 32.33
2023-04-26 15:04:43,547 - INFO - Epoch 13 training loss = 27.04
2023-04-26 15:04:45,818 - INFO - Epoch 14 training loss = 26.53
2023-04-26 15:04:47,974 - INFO - Epoch 15 training loss = 24.41
2023-04-26 15:04:50,078 - INFO - Epoch 16 training loss = 23.03
2023-04-26 15:04:52,091 - INFO - Epoch 17 training loss = 20.98
2023-04-26 15:04:54,135 - INFO - Epoch 18 training loss = 20.37
2023-04-26 15:04:56,195 - INFO - Epoch 19 training loss = 17.99
2023-04-26 15:04:58,253 - INFO - Epoch 20 training loss = 17.68
2023-04-26 15:04:58,359 - INFO - Validation loss = 17.36
2023-04-26 15:04:58,360 - INFO - best model
2023-04-26 15:05:00,482 - INFO - Epoch 21 training loss = 16.98
2023-04-26 15:05:02,599 - INFO - Epoch 22 training loss = 15.95
2023-04-26 15:05:04,874 - INFO - Epoch 23 training loss = 15.71
2023-04-26 15:05:07,131 - INFO - Epoch 24 training loss = 14.91
2023-04-26 15:05:09,340 - INFO - Epoch 25 training loss = 14.65
2023-04-26 15:05:11,428 - INFO - Epoch 26 training loss = 14.77
2023-04-26 15:05:13,571 - INFO - Epoch 27 training loss = 13.81
2023-04-26 15:05:15,647 - INFO - Epoch 28 training loss = 12.33
2023-04-26 15:05:17,733 - INFO - Epoch 29 training loss = 11.76
2023-04-26 15:05:19,921 - INFO - Epoch 30 training loss = 11.23
2023-04-26 15:05:20,028 - INFO - Validation loss = 9.836
2023-04-26 15:05:20,029 - INFO - best model
2023-04-26 15:05:22,094 - INFO - Epoch 31 training loss = 10.59
2023-04-26 15:05:24,298 - INFO - Epoch 32 training loss = 9.293
2023-04-26 15:05:26,450 - INFO - Epoch 33 training loss = 9.663
2023-04-26 15:05:28,513 - INFO - Epoch 34 training loss = 9.458
2023-04-26 15:05:30,656 - INFO - Epoch 35 training loss = 9.146
2023-04-26 15:05:32,745 - INFO - Epoch 36 training loss = 8.034
2023-04-26 15:05:34,779 - INFO - Epoch 37 training loss = 8.075
2023-04-26 15:05:36,858 - INFO - Epoch 38 training loss = 7.304
2023-04-26 15:05:39,018 - INFO - Epoch 39 training loss = 7.805
2023-04-26 15:05:41,119 - INFO - Epoch 40 training loss = 7.796
2023-04-26 15:05:41,226 - INFO - Validation loss = 7.737
2023-04-26 15:05:41,226 - INFO - best model
2023-04-26 15:05:43,397 - INFO - Epoch 41 training loss = 7.116
2023-04-26 15:05:45,556 - INFO - Epoch 42 training loss = 7.217
2023-04-26 15:05:47,771 - INFO - Epoch 43 training loss = 6.733
2023-04-26 15:05:50,001 - INFO - Epoch 44 training loss = 6.966
2023-04-26 15:05:52,172 - INFO - Epoch 45 training loss = 6.319
2023-04-26 15:05:54,307 - INFO - Epoch 46 training loss = 6.104
2023-04-26 15:05:56,416 - INFO - Epoch 47 training loss = 6.809
2023-04-26 15:05:58,467 - INFO - Epoch 48 training loss = 6.272
2023-04-26 15:06:00,438 - INFO - Epoch 49 training loss = 6.274
2023-04-26 15:06:02,449 - INFO - Epoch 50 training loss = 5.938
2023-04-26 15:06:02,556 - INFO - Validation loss = 4.917
2023-04-26 15:06:02,557 - INFO - best model
2023-04-26 15:06:04,544 - INFO - Epoch 51 training loss = 5.518
2023-04-26 15:06:06,569 - INFO - Epoch 52 training loss = 5.429
2023-04-26 15:06:08,676 - INFO - Epoch 53 training loss = 5.237
2023-04-26 15:06:10,775 - INFO - Epoch 54 training loss = 5.488
2023-04-26 15:06:12,955 - INFO - Epoch 55 training loss = 5.154
2023-04-26 15:06:15,152 - INFO - Epoch 56 training loss = 5.074
2023-04-26 15:06:17,299 - INFO - Epoch 57 training loss = 5.343
2023-04-26 15:06:19,410 - INFO - Epoch 58 training loss = 4.975
2023-04-26 15:06:21,435 - INFO - Epoch 59 training loss = 4.621
2023-04-26 15:06:23,604 - INFO - Epoch 60 training loss = 5.054
2023-04-26 15:06:23,722 - INFO - Validation loss = 5.748
2023-04-26 15:06:25,946 - INFO - Epoch 61 training loss = 5.223
2023-04-26 15:06:28,086 - INFO - Epoch 62 training loss = 4.665
2023-04-26 15:06:30,185 - INFO - Epoch 63 training loss = 4.748
2023-04-26 15:06:32,360 - INFO - Epoch 64 training loss = 4.701
2023-04-26 15:06:34,469 - INFO - Epoch 65 training loss = 4.757
2023-04-26 15:06:36,551 - INFO - Epoch 66 training loss = 4.248
2023-04-26 15:06:38,668 - INFO - Epoch 67 training loss = 4.704
2023-04-26 15:06:40,788 - INFO - Epoch 68 training loss = 4.431
2023-04-26 15:06:42,866 - INFO - Epoch 69 training loss = 4.397
2023-04-26 15:06:44,925 - INFO - Epoch 70 training loss = 4.184
2023-04-26 15:06:45,035 - INFO - Validation loss = 4.708
2023-04-26 15:06:45,035 - INFO - best model
2023-04-26 15:06:47,081 - INFO - Epoch 71 training loss = 4.548
2023-04-26 15:06:49,382 - INFO - Epoch 72 training loss = 3.803
2023-04-26 15:06:51,647 - INFO - Epoch 73 training loss = 4.066
2023-04-26 15:06:53,867 - INFO - Epoch 74 training loss = 3.992
2023-04-26 15:06:56,115 - INFO - Epoch 75 training loss = 4.031
2023-04-26 15:06:58,359 - INFO - Epoch 76 training loss = 3.904
2023-04-26 15:07:00,782 - INFO - Epoch 77 training loss = 3.87
2023-04-26 15:07:03,182 - INFO - Epoch 78 training loss = 3.975
2023-04-26 15:07:05,580 - INFO - Epoch 79 training loss = 3.841
2023-04-26 15:07:07,882 - INFO - Epoch 80 training loss = 3.576
2023-04-26 15:07:07,999 - INFO - Validation loss = 4.358
2023-04-26 15:07:07,999 - INFO - best model
2023-04-26 15:07:10,283 - INFO - Epoch 81 training loss = 3.725
2023-04-26 15:07:12,554 - INFO - Epoch 82 training loss = 3.65
2023-04-26 15:07:14,708 - INFO - Epoch 83 training loss = 3.727
2023-04-26 15:07:16,827 - INFO - Epoch 84 training loss = 3.837
2023-04-26 15:07:18,902 - INFO - Epoch 85 training loss = 3.583
2023-04-26 15:07:21,067 - INFO - Epoch 86 training loss = 3.283
2023-04-26 15:07:23,138 - INFO - Epoch 87 training loss = 3.586
2023-04-26 15:07:25,245 - INFO - Epoch 88 training loss = 3.508
2023-04-26 15:07:27,378 - INFO - Epoch 89 training loss = 3.46
2023-04-26 15:07:29,506 - INFO - Epoch 90 training loss = 3.435
2023-04-26 15:07:29,614 - INFO - Validation loss = 3.509
2023-04-26 15:07:29,614 - INFO - best model
2023-04-26 15:07:31,803 - INFO - Epoch 91 training loss = 3.158
2023-04-26 15:07:34,007 - INFO - Epoch 92 training loss = 3.37
2023-04-26 15:07:36,174 - INFO - Epoch 93 training loss = 3.106
2023-04-26 15:07:38,280 - INFO - Epoch 94 training loss = 3.248
2023-04-26 15:07:40,494 - INFO - Epoch 95 training loss = 3.21
2023-04-26 15:07:42,766 - INFO - Epoch 96 training loss = 3.203
2023-04-26 15:07:45,002 - INFO - Epoch 97 training loss = 3.191
2023-04-26 15:07:47,210 - INFO - Epoch 98 training loss = 2.953
2023-04-26 15:07:49,363 - INFO - Epoch 99 training loss = 3.244
2023-04-26 15:07:51,482 - INFO - Epoch 100 training loss = 3.124
2023-04-26 15:07:51,589 - INFO - Validation loss = 4.316
2023-04-26 15:07:53,761 - INFO - Epoch 101 training loss = 2.859
2023-04-26 15:07:55,919 - INFO - Epoch 102 training loss = 2.955
2023-04-26 15:07:58,311 - INFO - Epoch 103 training loss = 2.966
2023-04-26 15:08:00,626 - INFO - Epoch 104 training loss = 3.185
2023-04-26 15:08:03,093 - INFO - Epoch 105 training loss = 2.693
2023-04-26 15:08:05,420 - INFO - Epoch 106 training loss =  3.0
2023-04-26 15:08:07,486 - INFO - Epoch 107 training loss = 2.799
2023-04-26 15:08:09,725 - INFO - Epoch 108 training loss = 2.855
2023-04-26 15:08:11,930 - INFO - Epoch 109 training loss = 2.803
2023-04-26 15:08:14,063 - INFO - Epoch 110 training loss = 2.64
2023-04-26 15:08:14,172 - INFO - Validation loss = 2.798
2023-04-26 15:08:14,173 - INFO - best model
2023-04-26 15:08:16,412 - INFO - Epoch 111 training loss = 3.005
2023-04-26 15:08:18,608 - INFO - Epoch 112 training loss = 2.821
2023-04-26 15:08:20,810 - INFO - Epoch 113 training loss = 2.585
2023-04-26 15:08:22,889 - INFO - Epoch 114 training loss = 2.806
2023-04-26 15:08:24,945 - INFO - Epoch 115 training loss = 2.805
2023-04-26 15:08:26,986 - INFO - Epoch 116 training loss = 2.571
2023-04-26 15:08:29,037 - INFO - Epoch 117 training loss = 2.608
2023-04-26 15:08:31,219 - INFO - Epoch 118 training loss = 2.633
2023-04-26 15:08:33,515 - INFO - Epoch 119 training loss = 2.605
2023-04-26 15:08:35,679 - INFO - Epoch 120 training loss = 2.399
2023-04-26 15:08:35,788 - INFO - Validation loss = 3.817
2023-04-26 15:08:37,880 - INFO - Epoch 121 training loss = 2.638
2023-04-26 15:08:40,001 - INFO - Epoch 122 training loss = 2.431
2023-04-26 15:08:42,097 - INFO - Epoch 123 training loss = 2.688
2023-04-26 15:08:44,194 - INFO - Epoch 124 training loss = 2.46
2023-04-26 15:08:46,390 - INFO - Epoch 125 training loss = 2.32
2023-04-26 15:08:48,638 - INFO - Epoch 126 training loss = 2.267
2023-04-26 15:08:50,755 - INFO - Epoch 127 training loss = 2.529
2023-04-26 15:08:52,929 - INFO - Epoch 128 training loss = 2.425
2023-04-26 15:08:55,342 - INFO - Epoch 129 training loss = 2.44
2023-04-26 15:08:57,418 - INFO - Epoch 130 training loss = 2.309
2023-04-26 15:08:57,525 - INFO - Validation loss = 2.824
2023-04-26 15:08:59,810 - INFO - Epoch 131 training loss = 2.362
2023-04-26 15:09:02,131 - INFO - Epoch 132 training loss = 2.343
2023-04-26 15:09:04,427 - INFO - Epoch 133 training loss = 2.357
2023-04-26 15:09:06,575 - INFO - Epoch 134 training loss = 2.205
2023-04-26 15:09:08,789 - INFO - Epoch 135 training loss = 2.381
2023-04-26 15:09:11,100 - INFO - Epoch 136 training loss = 2.34
2023-04-26 15:09:13,481 - INFO - Epoch 137 training loss = 2.16
2023-04-26 15:09:15,600 - INFO - Epoch 138 training loss = 2.233
2023-04-26 15:09:17,708 - INFO - Epoch 139 training loss = 2.458
2023-04-26 15:09:19,992 - INFO - Epoch 140 training loss = 2.234
2023-04-26 15:09:20,098 - INFO - Validation loss = 3.341
2023-04-26 15:09:22,513 - INFO - Epoch 141 training loss = 2.251
2023-04-26 15:09:24,918 - INFO - Epoch 142 training loss = 2.081
2023-04-26 15:09:27,084 - INFO - Epoch 143 training loss = 2.133
2023-04-26 15:09:29,214 - INFO - Epoch 144 training loss = 2.105
2023-04-26 15:09:31,300 - INFO - Epoch 145 training loss = 2.135
2023-04-26 15:09:33,415 - INFO - Epoch 146 training loss = 2.02
2023-04-26 15:09:35,514 - INFO - Epoch 147 training loss = 2.105
2023-04-26 15:09:37,649 - INFO - Epoch 148 training loss = 2.12
2023-04-26 15:09:39,798 - INFO - Epoch 149 training loss = 2.048
2023-04-26 15:09:42,047 - INFO - Epoch 150 training loss = 1.966
2023-04-26 15:09:42,153 - INFO - Validation loss = 2.921
2023-04-26 15:09:44,395 - INFO - Epoch 151 training loss = 1.973
2023-04-26 15:09:46,604 - INFO - Epoch 152 training loss = 2.056
2023-04-26 15:09:48,896 - INFO - Epoch 153 training loss = 1.938
2023-04-26 15:09:51,151 - INFO - Epoch 154 training loss = 1.91
2023-04-26 15:09:53,282 - INFO - Epoch 155 training loss = 1.915
2023-04-26 15:09:55,455 - INFO - Epoch 156 training loss = 1.94
2023-04-26 15:09:57,610 - INFO - Epoch 157 training loss = 1.928
2023-04-26 15:09:59,745 - INFO - Epoch 158 training loss = 1.848
2023-04-26 15:10:01,938 - INFO - Epoch 159 training loss = 1.802
2023-04-26 15:10:04,063 - INFO - Epoch 160 training loss = 1.799
2023-04-26 15:10:04,169 - INFO - Validation loss = 2.338
2023-04-26 15:10:04,170 - INFO - best model
2023-04-26 15:10:06,561 - INFO - Epoch 161 training loss = 1.75
2023-04-26 15:10:08,802 - INFO - Epoch 162 training loss = 1.811
2023-04-26 15:10:11,020 - INFO - Epoch 163 training loss = 1.709
2023-04-26 15:10:13,336 - INFO - Epoch 164 training loss = 1.848
2023-04-26 15:10:15,712 - INFO - Epoch 165 training loss = 1.664
2023-04-26 15:10:17,861 - INFO - Epoch 166 training loss = 1.857
2023-04-26 15:10:20,112 - INFO - Epoch 167 training loss = 1.848
2023-04-26 15:10:22,377 - INFO - Epoch 168 training loss = 1.755
2023-04-26 15:10:24,539 - INFO - Epoch 169 training loss = 1.784
2023-04-26 15:10:26,740 - INFO - Epoch 170 training loss = 1.654
2023-04-26 15:10:26,848 - INFO - Validation loss = 2.186
2023-04-26 15:10:26,848 - INFO - best model
2023-04-26 15:10:29,103 - INFO - Epoch 171 training loss = 1.807
2023-04-26 15:10:31,282 - INFO - Epoch 172 training loss = 1.707
2023-04-26 15:10:33,358 - INFO - Epoch 173 training loss = 1.57
2023-04-26 15:10:35,640 - INFO - Epoch 174 training loss = 1.78
2023-04-26 15:10:37,876 - INFO - Epoch 175 training loss = 1.659
2023-04-26 15:10:40,268 - INFO - Epoch 176 training loss = 1.64
2023-04-26 15:10:42,653 - INFO - Epoch 177 training loss = 1.655
2023-04-26 15:10:44,893 - INFO - Epoch 178 training loss = 1.565
2023-04-26 15:10:47,098 - INFO - Epoch 179 training loss = 1.604
2023-04-26 15:10:49,249 - INFO - Epoch 180 training loss = 1.636
2023-04-26 15:10:49,358 - INFO - Validation loss = 3.063
2023-04-26 15:10:51,555 - INFO - Epoch 181 training loss = 1.555
2023-04-26 15:10:53,687 - INFO - Epoch 182 training loss = 1.556
2023-04-26 15:10:55,830 - INFO - Epoch 183 training loss = 1.525
2023-04-26 15:10:57,995 - INFO - Epoch 184 training loss = 1.493
2023-04-26 15:11:00,160 - INFO - Epoch 185 training loss = 1.54
2023-04-26 15:11:02,245 - INFO - Epoch 186 training loss = 1.539
2023-04-26 15:11:04,242 - INFO - Epoch 187 training loss = 1.653
2023-04-26 15:11:06,266 - INFO - Epoch 188 training loss = 1.385
2023-04-26 15:11:08,324 - INFO - Epoch 189 training loss = 1.438
2023-04-26 15:11:10,413 - INFO - Epoch 190 training loss = 1.444
2023-04-26 15:11:10,520 - INFO - Validation loss = 2.692
2023-04-26 15:11:12,694 - INFO - Epoch 191 training loss = 1.476
2023-04-26 15:11:14,814 - INFO - Epoch 192 training loss = 1.415
2023-04-26 15:11:17,002 - INFO - Epoch 193 training loss = 1.506
2023-04-26 15:11:19,153 - INFO - Epoch 194 training loss =  1.4
2023-04-26 15:11:21,473 - INFO - Epoch 195 training loss = 1.405
2023-04-26 15:11:23,564 - INFO - Epoch 196 training loss = 1.332
2023-04-26 15:11:25,683 - INFO - Epoch 197 training loss = 1.375
2023-04-26 15:11:27,800 - INFO - Epoch 198 training loss = 1.377
2023-04-26 15:11:29,993 - INFO - Epoch 199 training loss = 1.333
2023-04-26 15:11:32,077 - INFO - Epoch 200 training loss = 1.417
2023-04-26 15:11:32,194 - INFO - Validation loss = 2.092
2023-04-26 15:11:32,194 - INFO - best model
2023-04-26 15:11:34,403 - INFO - Epoch 201 training loss = 1.384
2023-04-26 15:11:36,553 - INFO - Epoch 202 training loss = 1.333
2023-04-26 15:11:38,624 - INFO - Epoch 203 training loss = 1.303
2023-04-26 15:11:40,790 - INFO - Epoch 204 training loss = 1.337
2023-04-26 15:11:42,896 - INFO - Epoch 205 training loss = 1.273
2023-04-26 15:11:45,081 - INFO - Epoch 206 training loss = 1.336
2023-04-26 15:11:47,425 - INFO - Epoch 207 training loss = 1.211
2023-04-26 15:11:49,686 - INFO - Epoch 208 training loss = 1.304
2023-04-26 15:11:51,904 - INFO - Epoch 209 training loss = 1.266
2023-04-26 15:11:54,249 - INFO - Epoch 210 training loss = 1.221
2023-04-26 15:11:54,359 - INFO - Validation loss = 2.112
2023-04-26 15:11:56,518 - INFO - Epoch 211 training loss = 1.17
2023-04-26 15:11:58,617 - INFO - Epoch 212 training loss = 1.162
2023-04-26 15:12:00,797 - INFO - Epoch 213 training loss = 1.226
2023-04-26 15:12:02,939 - INFO - Epoch 214 training loss = 1.282
2023-04-26 15:12:05,346 - INFO - Epoch 215 training loss = 1.183
2023-04-26 15:12:07,719 - INFO - Epoch 216 training loss = 1.131
2023-04-26 15:12:10,037 - INFO - Epoch 217 training loss = 1.198
2023-04-26 15:12:12,172 - INFO - Epoch 218 training loss = 1.163
2023-04-26 15:12:14,180 - INFO - Epoch 219 training loss = 1.126
2023-04-26 15:12:16,210 - INFO - Epoch 220 training loss = 1.144
2023-04-26 15:12:16,327 - INFO - Validation loss = 2.181
2023-04-26 15:12:18,417 - INFO - Epoch 221 training loss = 1.172
2023-04-26 15:12:20,454 - INFO - Epoch 222 training loss = 1.15
2023-04-26 15:12:22,523 - INFO - Epoch 223 training loss = 1.125
2023-04-26 15:12:24,664 - INFO - Epoch 224 training loss = 1.022
2023-04-26 15:12:26,818 - INFO - Epoch 225 training loss = 1.062
2023-04-26 15:12:29,003 - INFO - Epoch 226 training loss = 1.09
2023-04-26 15:12:31,198 - INFO - Epoch 227 training loss =  1.1
2023-04-26 15:12:33,281 - INFO - Epoch 228 training loss = 1.105
2023-04-26 15:12:35,314 - INFO - Epoch 229 training loss = 1.045
2023-04-26 15:12:37,554 - INFO - Epoch 230 training loss = 1.079
2023-04-26 15:12:37,660 - INFO - Validation loss = 1.884
2023-04-26 15:12:37,661 - INFO - best model
2023-04-26 15:12:40,034 - INFO - Epoch 231 training loss = 0.9887
2023-04-26 15:12:42,359 - INFO - Epoch 232 training loss = 1.039
2023-04-26 15:12:44,679 - INFO - Epoch 233 training loss = 1.034
2023-04-26 15:12:46,982 - INFO - Epoch 234 training loss = 0.9744
2023-04-26 15:12:49,214 - INFO - Epoch 235 training loss = 0.9397
2023-04-26 15:12:51,349 - INFO - Epoch 236 training loss = 0.9976
2023-04-26 15:12:53,359 - INFO - Epoch 237 training loss = 0.9242
2023-04-26 15:12:55,498 - INFO - Epoch 238 training loss = 0.9518
2023-04-26 15:12:57,626 - INFO - Epoch 239 training loss = 0.9557
2023-04-26 15:12:59,749 - INFO - Epoch 240 training loss = 1.046
2023-04-26 15:12:59,868 - INFO - Validation loss = 1.762
2023-04-26 15:12:59,868 - INFO - best model
2023-04-26 15:13:02,039 - INFO - Epoch 241 training loss = 0.9332
2023-04-26 15:13:04,184 - INFO - Epoch 242 training loss = 0.8452
2023-04-26 15:13:06,309 - INFO - Epoch 243 training loss = 0.9462
2023-04-26 15:13:08,657 - INFO - Epoch 244 training loss = 0.9705
2023-04-26 15:13:10,997 - INFO - Epoch 245 training loss = 0.9058
2023-04-26 15:13:13,290 - INFO - Epoch 246 training loss = 0.8847
2023-04-26 15:13:15,491 - INFO - Epoch 247 training loss = 0.8509
2023-04-26 15:13:17,726 - INFO - Epoch 248 training loss = 0.8963
2023-04-26 15:13:20,001 - INFO - Epoch 249 training loss = 0.8657
2023-04-26 15:13:22,163 - INFO - Epoch 250 training loss = 0.9012
2023-04-26 15:13:22,270 - INFO - Validation loss = 1.674
2023-04-26 15:13:22,270 - INFO - best model
2023-04-26 15:13:24,527 - INFO - Epoch 251 training loss = 0.8905
2023-04-26 15:13:26,611 - INFO - Epoch 252 training loss = 0.8296
2023-04-26 15:13:28,776 - INFO - Epoch 253 training loss = 0.8739
2023-04-26 15:13:31,064 - INFO - Epoch 254 training loss = 0.8889
2023-04-26 15:13:33,476 - INFO - Epoch 255 training loss = 0.8027
2023-04-26 15:13:35,776 - INFO - Epoch 256 training loss = 0.7846
2023-04-26 15:13:37,924 - INFO - Epoch 257 training loss = 0.7865
2023-04-26 15:13:40,156 - INFO - Epoch 258 training loss = 0.8281
2023-04-26 15:13:42,613 - INFO - Epoch 259 training loss = 0.7765
2023-04-26 15:13:44,785 - INFO - Epoch 260 training loss = 0.7679
2023-04-26 15:13:44,890 - INFO - Validation loss = 1.877
2023-04-26 15:13:47,008 - INFO - Epoch 261 training loss = 0.8256
2023-04-26 15:13:49,253 - INFO - Epoch 262 training loss = 0.8061
2023-04-26 15:13:51,355 - INFO - Epoch 263 training loss = 0.7591
2023-04-26 15:13:53,570 - INFO - Epoch 264 training loss = 0.7822
2023-04-26 15:13:56,009 - INFO - Epoch 265 training loss = 0.7401
2023-04-26 15:13:58,300 - INFO - Epoch 266 training loss = 0.7394
2023-04-26 15:14:00,499 - INFO - Epoch 267 training loss = 0.7237
2023-04-26 15:14:02,691 - INFO - Epoch 268 training loss = 0.745
2023-04-26 15:14:04,684 - INFO - Epoch 269 training loss = 0.71
2023-04-26 15:14:06,715 - INFO - Epoch 270 training loss = 0.708
2023-04-26 15:14:06,833 - INFO - Validation loss = 1.536
2023-04-26 15:14:06,834 - INFO - best model
2023-04-26 15:14:08,923 - INFO - Epoch 271 training loss = 0.7468
2023-04-26 15:14:11,051 - INFO - Epoch 272 training loss = 0.7165
2023-04-26 15:14:13,204 - INFO - Epoch 273 training loss = 0.7091
2023-04-26 15:14:15,368 - INFO - Epoch 274 training loss = 0.7004
2023-04-26 15:14:17,498 - INFO - Epoch 275 training loss = 0.6457
2023-04-26 15:14:19,737 - INFO - Epoch 276 training loss = 0.706
2023-04-26 15:14:21,822 - INFO - Epoch 277 training loss = 0.7024
2023-04-26 15:14:23,951 - INFO - Epoch 278 training loss = 0.6543
2023-04-26 15:14:25,996 - INFO - Epoch 279 training loss = 0.6601
2023-04-26 15:14:28,023 - INFO - Epoch 280 training loss = 0.6552
2023-04-26 15:14:28,130 - INFO - Validation loss = 1.471
2023-04-26 15:14:28,131 - INFO - best model
2023-04-26 15:14:30,536 - INFO - Epoch 281 training loss = 0.6353
2023-04-26 15:14:32,833 - INFO - Epoch 282 training loss = 0.6587
2023-04-26 15:14:34,664 - INFO - Epoch 283 training loss = 0.6355
2023-04-26 15:14:35,437 - INFO - Epoch 284 training loss = 0.6062
2023-04-26 15:14:36,207 - INFO - Epoch 285 training loss = 0.6046
2023-04-26 15:14:37,001 - INFO - Epoch 286 training loss = 0.6196
2023-04-26 15:14:37,781 - INFO - Epoch 287 training loss = 0.6024
2023-04-26 15:14:38,547 - INFO - Epoch 288 training loss = 0.6066
2023-04-26 15:14:39,317 - INFO - Epoch 289 training loss = 0.5816
2023-04-26 15:14:40,084 - INFO - Epoch 290 training loss = 0.5828
2023-04-26 15:14:40,146 - INFO - Validation loss = 1.475
2023-04-26 15:14:40,924 - INFO - Epoch 291 training loss = 0.5654
2023-04-26 15:14:41,701 - INFO - Epoch 292 training loss = 0.5616
2023-04-26 15:14:42,470 - INFO - Epoch 293 training loss = 0.5941
2023-04-26 15:14:43,272 - INFO - Epoch 294 training loss = 0.5678
2023-04-26 15:14:44,063 - INFO - Epoch 295 training loss = 0.5528
2023-04-26 15:14:44,836 - INFO - Epoch 296 training loss = 0.5633
2023-04-26 15:14:45,608 - INFO - Epoch 297 training loss = 0.5449
2023-04-26 15:14:46,384 - INFO - Epoch 298 training loss = 0.5385
2023-04-26 15:14:47,157 - INFO - Epoch 299 training loss = 0.5233
2023-04-26 15:14:47,926 - INFO - Epoch 300 training loss = 0.5064
2023-04-26 15:14:47,989 - INFO - Validation loss = 1.351
2023-04-26 15:14:47,990 - INFO - best model
2023-04-26 15:14:48,772 - INFO - Epoch 301 training loss = 0.5248
2023-04-26 15:14:49,552 - INFO - Epoch 302 training loss = 0.5257
2023-04-26 15:14:50,330 - INFO - Epoch 303 training loss = 0.5059
2023-04-26 15:14:51,100 - INFO - Epoch 304 training loss = 0.5059
2023-04-26 15:14:51,872 - INFO - Epoch 305 training loss = 0.4828
2023-04-26 15:14:52,644 - INFO - Epoch 306 training loss = 0.5095
2023-04-26 15:14:53,413 - INFO - Epoch 307 training loss = 0.5037
2023-04-26 15:14:54,184 - INFO - Epoch 308 training loss = 0.4827
2023-04-26 15:14:54,954 - INFO - Epoch 309 training loss = 0.5033
2023-04-26 15:14:55,723 - INFO - Epoch 310 training loss = 0.4913
2023-04-26 15:14:55,786 - INFO - Validation loss = 1.439
2023-04-26 15:14:56,557 - INFO - Epoch 311 training loss = 0.5116
2023-04-26 15:14:57,328 - INFO - Epoch 312 training loss = 0.482
2023-04-26 15:14:58,096 - INFO - Epoch 313 training loss = 0.485
2023-04-26 15:14:58,866 - INFO - Epoch 314 training loss = 0.4662
2023-04-26 15:14:59,637 - INFO - Epoch 315 training loss = 0.4672
2023-04-26 15:15:00,401 - INFO - Epoch 316 training loss = 0.4735
2023-04-26 15:15:01,164 - INFO - Epoch 317 training loss = 0.4437
2023-04-26 15:15:01,926 - INFO - Epoch 318 training loss = 0.4537
2023-04-26 15:15:02,692 - INFO - Epoch 319 training loss = 0.436
2023-04-26 15:15:03,477 - INFO - Epoch 320 training loss = 0.4473
2023-04-26 15:15:03,540 - INFO - Validation loss = 1.288
2023-04-26 15:15:03,540 - INFO - best model
2023-04-26 15:15:04,329 - INFO - Epoch 321 training loss = 0.4195
2023-04-26 15:15:05,094 - INFO - Epoch 322 training loss = 0.4507
2023-04-26 15:15:05,859 - INFO - Epoch 323 training loss = 0.4333
2023-04-26 15:15:06,624 - INFO - Epoch 324 training loss = 0.4258
2023-04-26 15:15:07,390 - INFO - Epoch 325 training loss = 0.4172
2023-04-26 15:15:08,156 - INFO - Epoch 326 training loss = 0.4112
2023-04-26 15:15:08,925 - INFO - Epoch 327 training loss = 0.4216
2023-04-26 15:15:09,696 - INFO - Epoch 328 training loss = 0.3876
2023-04-26 15:15:10,462 - INFO - Epoch 329 training loss = 0.4036
2023-04-26 15:15:11,230 - INFO - Epoch 330 training loss = 0.432
2023-04-26 15:15:11,293 - INFO - Validation loss =  1.4
2023-04-26 15:15:12,065 - INFO - Epoch 331 training loss = 0.3825
2023-04-26 15:15:12,841 - INFO - Epoch 332 training loss = 0.3901
2023-04-26 15:15:13,612 - INFO - Epoch 333 training loss = 0.3836
2023-04-26 15:15:14,381 - INFO - Epoch 334 training loss = 0.3857
2023-04-26 15:15:15,148 - INFO - Epoch 335 training loss = 0.3903
2023-04-26 15:15:15,911 - INFO - Epoch 336 training loss = 0.3817
2023-04-26 15:15:16,677 - INFO - Epoch 337 training loss = 0.3715
2023-04-26 15:15:17,445 - INFO - Epoch 338 training loss = 0.3572
2023-04-26 15:15:18,216 - INFO - Epoch 339 training loss = 0.3646
2023-04-26 15:15:18,985 - INFO - Epoch 340 training loss = 0.3668
2023-04-26 15:15:19,047 - INFO - Validation loss = 1.253
2023-04-26 15:15:19,047 - INFO - best model
2023-04-26 15:15:19,830 - INFO - Epoch 341 training loss = 0.3512
2023-04-26 15:15:20,607 - INFO - Epoch 342 training loss = 0.3488
2023-04-26 15:15:21,388 - INFO - Epoch 343 training loss = 0.3494
2023-04-26 15:15:22,160 - INFO - Epoch 344 training loss = 0.3596
2023-04-26 15:15:22,927 - INFO - Epoch 345 training loss = 0.3338
2023-04-26 15:15:23,696 - INFO - Epoch 346 training loss = 0.3523
2023-04-26 15:15:24,465 - INFO - Epoch 347 training loss = 0.3344
2023-04-26 15:15:25,228 - INFO - Epoch 348 training loss = 0.328
2023-04-26 15:15:25,991 - INFO - Epoch 349 training loss = 0.3289
2023-04-26 15:15:26,759 - INFO - Epoch 350 training loss = 0.3364
2023-04-26 15:15:26,822 - INFO - Validation loss = 1.235
2023-04-26 15:15:26,822 - INFO - best model
2023-04-26 15:15:27,603 - INFO - Epoch 351 training loss = 0.3281
2023-04-26 15:15:28,380 - INFO - Epoch 352 training loss = 0.3165
2023-04-26 15:15:29,154 - INFO - Epoch 353 training loss = 0.3268
2023-04-26 15:15:29,926 - INFO - Epoch 354 training loss = 0.3165
2023-04-26 15:15:30,697 - INFO - Epoch 355 training loss = 0.3195
2023-04-26 15:15:31,477 - INFO - Epoch 356 training loss = 0.3137
2023-04-26 15:15:32,258 - INFO - Epoch 357 training loss = 0.3133
2023-04-26 15:15:33,039 - INFO - Epoch 358 training loss = 0.313
2023-04-26 15:15:33,823 - INFO - Epoch 359 training loss = 0.3035
2023-04-26 15:15:34,599 - INFO - Epoch 360 training loss = 0.299
2023-04-26 15:15:34,661 - INFO - Validation loss = 1.184
2023-04-26 15:15:34,661 - INFO - best model
2023-04-26 15:15:35,441 - INFO - Epoch 361 training loss = 0.3005
2023-04-26 15:15:36,212 - INFO - Epoch 362 training loss = 0.2937
2023-04-26 15:15:36,981 - INFO - Epoch 363 training loss = 0.2933
2023-04-26 15:15:37,744 - INFO - Epoch 364 training loss = 0.2786
2023-04-26 15:15:38,512 - INFO - Epoch 365 training loss = 0.286
2023-04-26 15:15:39,282 - INFO - Epoch 366 training loss = 0.2876
2023-04-26 15:15:40,051 - INFO - Epoch 367 training loss = 0.2932
2023-04-26 15:15:40,819 - INFO - Epoch 368 training loss = 0.2741
2023-04-26 15:15:41,586 - INFO - Epoch 369 training loss = 0.2804
2023-04-26 15:15:42,369 - INFO - Epoch 370 training loss = 0.2722
2023-04-26 15:15:42,433 - INFO - Validation loss = 1.204
2023-04-26 15:15:43,241 - INFO - Epoch 371 training loss = 0.276
2023-04-26 15:15:44,050 - INFO - Epoch 372 training loss = 0.2649
2023-04-26 15:15:44,850 - INFO - Epoch 373 training loss = 0.2638
2023-04-26 15:15:45,639 - INFO - Epoch 374 training loss = 0.2748
2023-04-26 15:15:46,425 - INFO - Epoch 375 training loss = 0.2581
2023-04-26 15:15:47,214 - INFO - Epoch 376 training loss = 0.2629
2023-04-26 15:15:47,999 - INFO - Epoch 377 training loss = 0.2565
2023-04-26 15:15:48,784 - INFO - Epoch 378 training loss = 0.2596
2023-04-26 15:15:49,565 - INFO - Epoch 379 training loss = 0.2581
2023-04-26 15:15:50,344 - INFO - Epoch 380 training loss = 0.2558
2023-04-26 15:15:50,408 - INFO - Validation loss = 1.173
2023-04-26 15:15:50,408 - INFO - best model
2023-04-26 15:15:51,200 - INFO - Epoch 381 training loss = 0.2482
2023-04-26 15:15:51,991 - INFO - Epoch 382 training loss = 0.2486
2023-04-26 15:15:52,779 - INFO - Epoch 383 training loss = 0.2464
2023-04-26 15:15:53,566 - INFO - Epoch 384 training loss = 0.2431
2023-04-26 15:15:54,355 - INFO - Epoch 385 training loss = 0.2435
2023-04-26 15:15:55,144 - INFO - Epoch 386 training loss = 0.2413
2023-04-26 15:15:55,927 - INFO - Epoch 387 training loss = 0.2386
2023-04-26 15:15:56,708 - INFO - Epoch 388 training loss = 0.2472
2023-04-26 15:15:57,534 - INFO - Epoch 389 training loss = 0.2357
2023-04-26 15:15:58,407 - INFO - Epoch 390 training loss = 0.2343
2023-04-26 15:15:58,478 - INFO - Validation loss = 1.168
2023-04-26 15:15:58,478 - INFO - best model
2023-04-26 15:16:00,615 - INFO - Epoch 391 training loss = 0.2381
2023-04-26 15:16:02,963 - INFO - Epoch 392 training loss = 0.2275
2023-04-26 15:16:05,319 - INFO - Epoch 393 training loss = 0.2255
2023-04-26 15:16:07,629 - INFO - Epoch 394 training loss = 0.23
2023-04-26 15:16:09,746 - INFO - Epoch 395 training loss = 0.2232
2023-04-26 15:16:11,844 - INFO - Epoch 396 training loss = 0.2231
2023-04-26 15:16:14,095 - INFO - Epoch 397 training loss = 0.2203
2023-04-26 15:16:16,238 - INFO - Epoch 398 training loss = 0.2226
2023-04-26 15:16:18,321 - INFO - Epoch 399 training loss = 0.2177
2023-04-26 15:16:20,523 - INFO - Epoch 400 training loss = 0.2173
2023-04-26 15:16:20,629 - INFO - Validation loss = 1.155
2023-04-26 15:16:20,630 - INFO - best model
2023-04-26 15:16:22,925 - INFO - Epoch 401 training loss = 0.2182
2023-04-26 15:16:25,300 - INFO - Epoch 402 training loss = 0.2145
2023-04-26 15:16:27,479 - INFO - Epoch 403 training loss = 0.2141
2023-04-26 15:16:29,827 - INFO - Epoch 404 training loss = 0.2138
2023-04-26 15:16:32,157 - INFO - Epoch 405 training loss = 0.2122
2023-04-26 15:16:34,419 - INFO - Epoch 406 training loss = 0.2104
2023-04-26 15:16:36,554 - INFO - Epoch 407 training loss = 0.2075
2023-04-26 15:16:38,655 - INFO - Epoch 408 training loss = 0.2069
2023-04-26 15:16:40,842 - INFO - Epoch 409 training loss = 0.2089
2023-04-26 15:16:43,074 - INFO - Epoch 410 training loss = 0.2059
2023-04-26 15:16:43,193 - INFO - Validation loss = 1.14
2023-04-26 15:16:43,193 - INFO - best model
2023-04-26 15:16:45,351 - INFO - Epoch 411 training loss = 0.2048
2023-04-26 15:16:47,722 - INFO - Epoch 412 training loss = 0.2048
2023-04-26 15:16:50,057 - INFO - Epoch 413 training loss = 0.2033
2023-04-26 15:16:52,342 - INFO - Epoch 414 training loss = 0.204
2023-04-26 15:16:54,727 - INFO - Epoch 415 training loss = 0.2002
2023-04-26 15:16:57,080 - INFO - Epoch 416 training loss = 0.1993
2023-04-26 15:16:59,375 - INFO - Epoch 417 training loss = 0.1978
2023-04-26 15:17:01,534 - INFO - Epoch 418 training loss = 0.1975
2023-04-26 15:17:03,679 - INFO - Epoch 419 training loss = 0.1949
2023-04-26 15:17:05,782 - INFO - Epoch 420 training loss = 0.1962
2023-04-26 15:17:05,891 - INFO - Validation loss = 1.147
2023-04-26 15:17:08,099 - INFO - Epoch 421 training loss = 0.1947
2023-04-26 15:17:10,294 - INFO - Epoch 422 training loss = 0.1942
2023-04-26 15:17:12,485 - INFO - Epoch 423 training loss = 0.194
2023-04-26 15:17:14,750 - INFO - Epoch 424 training loss = 0.1921
2023-04-26 15:17:16,900 - INFO - Epoch 425 training loss = 0.1906
2023-04-26 15:17:19,123 - INFO - Epoch 426 training loss = 0.1893
2023-04-26 15:17:21,326 - INFO - Epoch 427 training loss = 0.1884
2023-04-26 15:17:23,709 - INFO - Epoch 428 training loss = 0.1887
2023-04-26 15:17:26,032 - INFO - Epoch 429 training loss = 0.1874
2023-04-26 15:17:28,341 - INFO - Epoch 430 training loss = 0.1871
2023-04-26 15:17:28,448 - INFO - Validation loss = 1.128
2023-04-26 15:17:28,449 - INFO - best model
2023-04-26 15:17:30,732 - INFO - Epoch 431 training loss = 0.1854
2023-04-26 15:17:33,108 - INFO - Epoch 432 training loss = 0.183
2023-04-26 15:17:35,447 - INFO - Epoch 433 training loss = 0.1851
2023-04-26 15:17:37,765 - INFO - Epoch 434 training loss = 0.1831
2023-04-26 15:17:40,033 - INFO - Epoch 435 training loss = 0.1822
2023-04-26 15:17:42,415 - INFO - Epoch 436 training loss = 0.1824
2023-04-26 15:17:44,612 - INFO - Epoch 437 training loss = 0.1815
2023-04-26 15:17:46,910 - INFO - Epoch 438 training loss = 0.1814
2023-04-26 15:17:49,285 - INFO - Epoch 439 training loss = 0.1799
2023-04-26 15:17:51,640 - INFO - Epoch 440 training loss = 0.1786
2023-04-26 15:17:51,749 - INFO - Validation loss = 1.131
2023-04-26 15:17:54,143 - INFO - Epoch 441 training loss = 0.1789
2023-04-26 15:17:56,548 - INFO - Epoch 442 training loss = 0.1781
2023-04-26 15:17:58,924 - INFO - Epoch 443 training loss = 0.1771
2023-04-26 15:18:01,090 - INFO - Epoch 444 training loss = 0.1764
2023-04-26 15:18:03,162 - INFO - Epoch 445 training loss = 0.1769
2023-04-26 15:18:05,343 - INFO - Epoch 446 training loss = 0.1765
2023-04-26 15:18:07,516 - INFO - Epoch 447 training loss = 0.175
2023-04-26 15:18:09,679 - INFO - Epoch 448 training loss = 0.1748
2023-04-26 15:18:12,019 - INFO - Epoch 449 training loss = 0.1743
2023-04-26 15:18:14,419 - INFO - Epoch 450 training loss = 0.1737
2023-04-26 15:18:14,526 - INFO - Validation loss = 1.112
2023-04-26 15:18:14,527 - INFO - best model
2023-04-26 15:18:16,886 - INFO - Epoch 451 training loss = 0.1726
2023-04-26 15:18:19,261 - INFO - Epoch 452 training loss = 0.173
2023-04-26 15:18:21,665 - INFO - Epoch 453 training loss = 0.1722
2023-04-26 15:18:24,066 - INFO - Epoch 454 training loss = 0.1707
2023-04-26 15:18:26,403 - INFO - Epoch 455 training loss = 0.1709
2023-04-26 15:18:28,632 - INFO - Epoch 456 training loss = 0.1705
2023-04-26 15:18:30,891 - INFO - Epoch 457 training loss = 0.1698
2023-04-26 15:18:33,197 - INFO - Epoch 458 training loss = 0.1701
2023-04-26 15:18:35,554 - INFO - Epoch 459 training loss = 0.1691
2023-04-26 15:18:37,890 - INFO - Epoch 460 training loss = 0.1687
2023-04-26 15:18:37,998 - INFO - Validation loss = 1.117
2023-04-26 15:18:40,423 - INFO - Epoch 461 training loss = 0.1687
2023-04-26 15:18:42,574 - INFO - Epoch 462 training loss = 0.1685
2023-04-26 15:18:44,793 - INFO - Epoch 463 training loss = 0.1679
2023-04-26 15:18:47,128 - INFO - Epoch 464 training loss = 0.1673
2023-04-26 15:18:49,400 - INFO - Epoch 465 training loss = 0.1667
2023-04-26 15:18:51,524 - INFO - Epoch 466 training loss = 0.167
2023-04-26 15:18:53,785 - INFO - Epoch 467 training loss = 0.1663
2023-04-26 15:18:56,107 - INFO - Epoch 468 training loss = 0.166
2023-04-26 15:18:58,384 - INFO - Epoch 469 training loss = 0.1657
2023-04-26 15:19:00,714 - INFO - Epoch 470 training loss = 0.1654
2023-04-26 15:19:00,832 - INFO - Validation loss = 1.11
2023-04-26 15:19:00,833 - INFO - best model
2023-04-26 15:19:03,216 - INFO - Epoch 471 training loss = 0.1653
2023-04-26 15:19:05,589 - INFO - Epoch 472 training loss = 0.1647
2023-04-26 15:19:07,962 - INFO - Epoch 473 training loss = 0.1647
2023-04-26 15:19:10,372 - INFO - Epoch 474 training loss = 0.1645
2023-04-26 15:19:12,698 - INFO - Epoch 475 training loss = 0.1642
2023-04-26 15:19:14,917 - INFO - Epoch 476 training loss = 0.1639
2023-04-26 15:19:17,144 - INFO - Epoch 477 training loss = 0.1636
2023-04-26 15:19:19,371 - INFO - Epoch 478 training loss = 0.1635
2023-04-26 15:19:20,139 - INFO - Epoch 479 training loss = 0.1636
2023-04-26 15:19:20,913 - INFO - Epoch 480 training loss = 0.1628
2023-04-26 15:19:20,977 - INFO - Validation loss = 1.11
2023-04-26 15:19:21,766 - INFO - Epoch 481 training loss = 0.1627
2023-04-26 15:19:22,572 - INFO - Epoch 482 training loss = 0.1626
2023-04-26 15:19:23,349 - INFO - Epoch 483 training loss = 0.1625
2023-04-26 15:19:24,128 - INFO - Epoch 484 training loss = 0.1623
2023-04-26 15:19:24,904 - INFO - Epoch 485 training loss = 0.1622
2023-04-26 15:19:25,674 - INFO - Epoch 486 training loss = 0.162
2023-04-26 15:19:26,445 - INFO - Epoch 487 training loss = 0.1618
2023-04-26 15:19:27,222 - INFO - Epoch 488 training loss = 0.1618
2023-04-26 15:19:27,998 - INFO - Epoch 489 training loss = 0.1618
2023-04-26 15:19:28,766 - INFO - Epoch 490 training loss = 0.1615
2023-04-26 15:19:28,829 - INFO - Validation loss = 1.106
2023-04-26 15:19:28,829 - INFO - best model
2023-04-26 15:19:29,609 - INFO - Epoch 491 training loss = 0.1615
2023-04-26 15:19:30,380 - INFO - Epoch 492 training loss = 0.1613
2023-04-26 15:19:31,143 - INFO - Epoch 493 training loss = 0.1614
2023-04-26 15:19:31,912 - INFO - Epoch 494 training loss = 0.1611
2023-04-26 15:19:32,686 - INFO - Epoch 495 training loss = 0.1611
2023-04-26 15:19:33,450 - INFO - Epoch 496 training loss = 0.161
2023-04-26 15:19:34,227 - INFO - Epoch 497 training loss = 0.161
2023-04-26 15:19:34,998 - INFO - Epoch 498 training loss = 0.1609
2023-04-26 15:19:35,765 - INFO - Epoch 499 training loss = 0.1609
2023-04-26 15:19:35,816 - INFO - Validation loss = 1.105
