2023-04-27 04:45:49,681 - INFO - Config:
Namespace(config='configs/explicit_mlp.yaml', dir='/user/delden/acoustics/spp_ai_acoustics/acousticNN/experiments/arch/no_encoding/explicitmlp', epochs=100, device='cuda', seed=2, parameter_list='configs/data.yaml', encoding='none', dir_name='arch/no_encoding/explicitmlp')
2023-04-27 04:45:49,681 - INFO - Config:
Munch({'optimizer': Munch({'type': 'AdamW', 'kwargs': Munch({'lr': 0.001, 'weight_decay': 0.005, 'betas': [0.9, 0.95]})}), 'scheduler': Munch({'type': 'CosLR', 'kwargs': Munch({'epochs': 1000, 'initial_epochs': 101})}), 'model': Munch({'name': 'ExplicitMLP', 'input_encoding': True, 'encoding_dim': 16, 'embed_dim': 64, 'depth': 6, 'mlp_width': 256, 'num_frequencies': 200}), 'generation_frequency': 5001, 'validation_frequency': 10, 'batch_size': 128, 'epochs': 1000, 'gradient_clip': 10})
2023-04-27 04:46:03,989 - INFO - ==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ExplicitMLP                              [128, 200, 4]             --
├─GroupwiseProjection: 1-1               [128, 14, 64]             --
│    └─ModuleList: 2-1                   --                        --
│    │    └─Linear: 3-1                  [128, 4, 64]              128
│    │    └─Linear: 3-2                  [128, 5, 64]              128
│    │    └─Linear: 3-3                  [128, 5, 64]              128
├─MLP: 1-2                               [128, 256]                --
│    └─Linear: 2-2                       [128, 256]                229,632
│    └─ReLU: 2-3                         [128, 256]                --
│    └─Dropout: 2-4                      [128, 256]                --
│    └─Linear: 2-5                       [128, 256]                65,792
│    └─ReLU: 2-6                         [128, 256]                --
│    └─Dropout: 2-7                      [128, 256]                --
│    └─Linear: 2-8                       [128, 256]                65,792
│    └─ReLU: 2-9                         [128, 256]                --
│    └─Dropout: 2-10                     [128, 256]                --
│    └─Linear: 2-11                      [128, 256]                65,792
│    └─ReLU: 2-12                        [128, 256]                --
│    └─Dropout: 2-13                     [128, 256]                --
│    └─Linear: 2-14                      [128, 256]                65,792
│    └─ReLU: 2-15                        [128, 256]                --
│    └─Dropout: 2-16                     [128, 256]                --
│    └─Linear: 2-17                      [128, 256]                65,792
│    └─Dropout: 2-18                     [128, 256]                --
├─Linear: 1-3                            [128, 800]                205,600
==========================================================================================
Total params: 764,576
Trainable params: 764,576
Non-trainable params: 0
Total mult-adds (M): 97.87
==========================================================================================
Input size (MB): 0.11
Forward/backward pass size (MB): 3.31
Params size (MB): 3.06
Estimated Total Size (MB): 6.48
==========================================================================================
2023-04-27 04:46:04,090 - INFO - Epoch 0 training loss = 3.381e+03
2023-04-27 04:46:04,123 - INFO - Validation loss = 3.409e+03
2023-04-27 04:46:04,123 - INFO - best model
2023-04-27 04:46:04,238 - INFO - Epoch 1 training loss = 3.376e+03
2023-04-27 04:46:04,337 - INFO - Epoch 2 training loss = 3.358e+03
2023-04-27 04:46:04,436 - INFO - Epoch 3 training loss = 3.303e+03
2023-04-27 04:46:04,536 - INFO - Epoch 4 training loss = 3.108e+03
2023-04-27 04:46:04,635 - INFO - Epoch 5 training loss = 2.524e+03
2023-04-27 04:46:04,734 - INFO - Epoch 6 training loss = 1.379e+03
2023-04-27 04:46:04,833 - INFO - Epoch 7 training loss = 497.2
2023-04-27 04:46:04,932 - INFO - Epoch 8 training loss = 245.2
2023-04-27 04:46:05,031 - INFO - Epoch 9 training loss = 217.5
2023-04-27 04:46:05,130 - INFO - Epoch 10 training loss = 207.4
2023-04-27 04:46:05,158 - INFO - Validation loss = 208.2
2023-04-27 04:46:05,158 - INFO - best model
2023-04-27 04:46:05,269 - INFO - Epoch 11 training loss = 195.6
2023-04-27 04:46:05,368 - INFO - Epoch 12 training loss = 181.4
2023-04-27 04:46:05,467 - INFO - Epoch 13 training loss = 166.0
2023-04-27 04:46:05,566 - INFO - Epoch 14 training loss = 152.9
2023-04-27 04:46:05,666 - INFO - Epoch 15 training loss = 141.3
2023-04-27 04:46:05,765 - INFO - Epoch 16 training loss = 132.3
2023-04-27 04:46:05,865 - INFO - Epoch 17 training loss = 123.0
2023-04-27 04:46:05,964 - INFO - Epoch 18 training loss = 114.4
2023-04-27 04:46:06,064 - INFO - Epoch 19 training loss = 105.4
2023-04-27 04:46:06,163 - INFO - Epoch 20 training loss = 97.44
2023-04-27 04:46:06,191 - INFO - Validation loss = 103.8
2023-04-27 04:46:06,191 - INFO - best model
2023-04-27 04:46:06,303 - INFO - Epoch 21 training loss = 90.17
2023-04-27 04:46:06,403 - INFO - Epoch 22 training loss = 82.72
2023-04-27 04:46:06,502 - INFO - Epoch 23 training loss = 75.47
2023-04-27 04:46:06,602 - INFO - Epoch 24 training loss = 70.68
2023-04-27 04:46:06,702 - INFO - Epoch 25 training loss = 66.49
2023-04-27 04:46:06,801 - INFO - Epoch 26 training loss = 63.14
2023-04-27 04:46:06,900 - INFO - Epoch 27 training loss = 60.4
2023-04-27 04:46:07,000 - INFO - Epoch 28 training loss = 57.39
2023-04-27 04:46:07,159 - INFO - Epoch 29 training loss = 55.64
2023-04-27 04:46:07,259 - INFO - Epoch 30 training loss = 53.8
2023-04-27 04:46:07,286 - INFO - Validation loss = 57.36
2023-04-27 04:46:07,286 - INFO - best model
2023-04-27 04:46:07,398 - INFO - Epoch 31 training loss = 51.49
2023-04-27 04:46:07,498 - INFO - Epoch 32 training loss = 49.9
2023-04-27 04:46:07,597 - INFO - Epoch 33 training loss = 48.69
2023-04-27 04:46:07,696 - INFO - Epoch 34 training loss = 47.44
2023-04-27 04:46:07,796 - INFO - Epoch 35 training loss = 46.79
2023-04-27 04:46:07,895 - INFO - Epoch 36 training loss = 44.69
2023-04-27 04:46:07,995 - INFO - Epoch 37 training loss = 43.49
2023-04-27 04:46:08,094 - INFO - Epoch 38 training loss = 43.26
2023-04-27 04:46:08,193 - INFO - Epoch 39 training loss = 41.26
2023-04-27 04:46:08,293 - INFO - Epoch 40 training loss = 40.65
2023-04-27 04:46:08,321 - INFO - Validation loss = 42.88
2023-04-27 04:46:08,321 - INFO - best model
2023-04-27 04:46:08,431 - INFO - Epoch 41 training loss = 39.28
2023-04-27 04:46:08,531 - INFO - Epoch 42 training loss = 38.47
2023-04-27 04:46:08,630 - INFO - Epoch 43 training loss = 37.66
2023-04-27 04:46:08,729 - INFO - Epoch 44 training loss = 36.29
2023-04-27 04:46:08,828 - INFO - Epoch 45 training loss = 35.39
2023-04-27 04:46:08,928 - INFO - Epoch 46 training loss = 34.05
2023-04-27 04:46:09,027 - INFO - Epoch 47 training loss = 33.94
2023-04-27 04:46:09,127 - INFO - Epoch 48 training loss = 32.88
2023-04-27 04:46:09,227 - INFO - Epoch 49 training loss = 31.7
2023-04-27 04:46:09,327 - INFO - Epoch 50 training loss = 31.65
2023-04-27 04:46:09,354 - INFO - Validation loss = 34.74
2023-04-27 04:46:09,355 - INFO - best model
2023-04-27 04:46:09,465 - INFO - Epoch 51 training loss = 30.7
2023-04-27 04:46:09,564 - INFO - Epoch 52 training loss = 29.97
2023-04-27 04:46:09,664 - INFO - Epoch 53 training loss = 29.15
2023-04-27 04:46:09,763 - INFO - Epoch 54 training loss = 29.06
2023-04-27 04:46:09,862 - INFO - Epoch 55 training loss = 28.33
2023-04-27 04:46:09,962 - INFO - Epoch 56 training loss = 27.59
2023-04-27 04:46:10,062 - INFO - Epoch 57 training loss = 27.15
2023-04-27 04:46:10,161 - INFO - Epoch 58 training loss = 27.41
2023-04-27 04:46:10,260 - INFO - Epoch 59 training loss = 26.66
2023-04-27 04:46:10,359 - INFO - Epoch 60 training loss = 25.49
2023-04-27 04:46:10,386 - INFO - Validation loss = 30.1
2023-04-27 04:46:10,386 - INFO - best model
2023-04-27 04:46:10,556 - INFO - Epoch 61 training loss = 25.89
2023-04-27 04:46:10,655 - INFO - Epoch 62 training loss = 25.34
2023-04-27 04:46:10,754 - INFO - Epoch 63 training loss = 24.55
2023-04-27 04:46:10,852 - INFO - Epoch 64 training loss = 24.11
2023-04-27 04:46:10,951 - INFO - Epoch 65 training loss = 24.29
2023-04-27 04:46:11,050 - INFO - Epoch 66 training loss = 23.5
2023-04-27 04:46:11,150 - INFO - Epoch 67 training loss = 23.72
2023-04-27 04:46:11,250 - INFO - Epoch 68 training loss = 23.13
2023-04-27 04:46:11,350 - INFO - Epoch 69 training loss = 22.63
2023-04-27 04:46:11,448 - INFO - Epoch 70 training loss = 21.82
2023-04-27 04:46:11,476 - INFO - Validation loss = 24.67
2023-04-27 04:46:11,476 - INFO - best model
2023-04-27 04:46:11,587 - INFO - Epoch 71 training loss = 21.87
2023-04-27 04:46:11,686 - INFO - Epoch 72 training loss = 21.36
2023-04-27 04:46:11,785 - INFO - Epoch 73 training loss = 21.72
2023-04-27 04:46:11,884 - INFO - Epoch 74 training loss = 20.92
2023-04-27 04:46:11,983 - INFO - Epoch 75 training loss = 20.97
2023-04-27 04:46:12,083 - INFO - Epoch 76 training loss = 20.46
2023-04-27 04:46:12,182 - INFO - Epoch 77 training loss = 20.69
2023-04-27 04:46:12,282 - INFO - Epoch 78 training loss = 20.01
2023-04-27 04:46:12,382 - INFO - Epoch 79 training loss = 20.21
2023-04-27 04:46:12,481 - INFO - Epoch 80 training loss = 20.37
2023-04-27 04:46:12,508 - INFO - Validation loss = 25.88
2023-04-27 04:46:12,607 - INFO - Epoch 81 training loss = 18.75
2023-04-27 04:46:12,706 - INFO - Epoch 82 training loss = 20.61
2023-04-27 04:46:12,805 - INFO - Epoch 83 training loss = 19.04
2023-04-27 04:46:12,905 - INFO - Epoch 84 training loss = 19.67
2023-04-27 04:46:13,004 - INFO - Epoch 85 training loss = 19.32
2023-04-27 04:46:13,103 - INFO - Epoch 86 training loss = 17.82
2023-04-27 04:46:13,202 - INFO - Epoch 87 training loss = 19.75
2023-04-27 04:46:13,303 - INFO - Epoch 88 training loss = 17.54
2023-04-27 04:46:13,402 - INFO - Epoch 89 training loss = 17.98
2023-04-27 04:46:13,502 - INFO - Epoch 90 training loss = 18.31
2023-04-27 04:46:13,529 - INFO - Validation loss = 20.64
2023-04-27 04:46:13,529 - INFO - best model
2023-04-27 04:46:13,639 - INFO - Epoch 91 training loss = 18.74
2023-04-27 04:46:13,797 - INFO - Epoch 92 training loss = 17.83
2023-04-27 04:46:13,897 - INFO - Epoch 93 training loss = 17.02
2023-04-27 04:46:13,996 - INFO - Epoch 94 training loss = 17.36
2023-04-27 04:46:14,095 - INFO - Epoch 95 training loss = 17.82
2023-04-27 04:46:14,194 - INFO - Epoch 96 training loss = 16.27
2023-04-27 04:46:14,295 - INFO - Epoch 97 training loss = 16.49
2023-04-27 04:46:14,394 - INFO - Epoch 98 training loss = 16.09
2023-04-27 04:46:14,493 - INFO - Epoch 99 training loss = 16.9
2023-04-27 04:46:14,592 - INFO - Epoch 100 training loss = 16.16
2023-04-27 04:46:14,620 - INFO - Validation loss = 21.24
2023-04-27 04:46:14,719 - INFO - Epoch 101 training loss = 16.39
2023-04-27 04:46:14,818 - INFO - Epoch 102 training loss = 15.63
2023-04-27 04:46:14,917 - INFO - Epoch 103 training loss = 15.92
2023-04-27 04:46:15,016 - INFO - Epoch 104 training loss = 15.28
2023-04-27 04:46:15,115 - INFO - Epoch 105 training loss = 15.67
2023-04-27 04:46:15,214 - INFO - Epoch 106 training loss = 15.43
2023-04-27 04:46:15,315 - INFO - Epoch 107 training loss = 14.4
2023-04-27 04:46:15,414 - INFO - Epoch 108 training loss = 15.12
2023-04-27 04:46:15,513 - INFO - Epoch 109 training loss = 14.68
2023-04-27 04:46:15,611 - INFO - Epoch 110 training loss = 15.56
2023-04-27 04:46:15,639 - INFO - Validation loss = 18.21
2023-04-27 04:46:15,639 - INFO - best model
2023-04-27 04:46:15,749 - INFO - Epoch 111 training loss = 14.15
2023-04-27 04:46:15,848 - INFO - Epoch 112 training loss = 13.89
2023-04-27 04:46:15,947 - INFO - Epoch 113 training loss = 14.53
2023-04-27 04:46:16,046 - INFO - Epoch 114 training loss = 14.33
2023-04-27 04:46:16,145 - INFO - Epoch 115 training loss = 13.32
2023-04-27 04:46:16,245 - INFO - Epoch 116 training loss = 13.6
2023-04-27 04:46:16,346 - INFO - Epoch 117 training loss = 13.83
2023-04-27 04:46:16,445 - INFO - Epoch 118 training loss = 13.08
2023-04-27 04:46:16,545 - INFO - Epoch 119 training loss = 13.46
2023-04-27 04:46:16,644 - INFO - Epoch 120 training loss = 12.99
2023-04-27 04:46:16,671 - INFO - Validation loss = 16.13
2023-04-27 04:46:16,672 - INFO - best model
2023-04-27 04:46:16,782 - INFO - Epoch 121 training loss = 13.51
2023-04-27 04:46:16,881 - INFO - Epoch 122 training loss = 12.81
2023-04-27 04:46:16,981 - INFO - Epoch 123 training loss = 12.55
2023-04-27 04:46:17,139 - INFO - Epoch 124 training loss = 12.25
2023-04-27 04:46:17,239 - INFO - Epoch 125 training loss = 12.06
2023-04-27 04:46:17,339 - INFO - Epoch 126 training loss = 12.95
2023-04-27 04:46:17,439 - INFO - Epoch 127 training loss = 11.47
2023-04-27 04:46:17,539 - INFO - Epoch 128 training loss = 12.21
2023-04-27 04:46:17,638 - INFO - Epoch 129 training loss = 12.06
2023-04-27 04:46:17,738 - INFO - Epoch 130 training loss = 11.9
2023-04-27 04:46:17,765 - INFO - Validation loss = 16.05
2023-04-27 04:46:17,765 - INFO - best model
2023-04-27 04:46:17,876 - INFO - Epoch 131 training loss = 11.8
2023-04-27 04:46:17,976 - INFO - Epoch 132 training loss = 11.35
2023-04-27 04:46:18,076 - INFO - Epoch 133 training loss = 11.58
2023-04-27 04:46:18,175 - INFO - Epoch 134 training loss = 10.99
2023-04-27 04:46:18,274 - INFO - Epoch 135 training loss = 11.3
2023-04-27 04:46:18,373 - INFO - Epoch 136 training loss = 11.84
2023-04-27 04:46:18,472 - INFO - Epoch 137 training loss = 10.98
2023-04-27 04:46:18,571 - INFO - Epoch 138 training loss = 11.57
2023-04-27 04:46:18,670 - INFO - Epoch 139 training loss = 11.05
2023-04-27 04:46:18,770 - INFO - Epoch 140 training loss = 10.63
2023-04-27 04:46:18,797 - INFO - Validation loss = 15.41
2023-04-27 04:46:18,797 - INFO - best model
2023-04-27 04:46:18,908 - INFO - Epoch 141 training loss = 10.96
2023-04-27 04:46:19,007 - INFO - Epoch 142 training loss = 10.35
2023-04-27 04:46:19,107 - INFO - Epoch 143 training loss = 10.95
2023-04-27 04:46:19,207 - INFO - Epoch 144 training loss = 10.44
2023-04-27 04:46:19,306 - INFO - Epoch 145 training loss = 10.27
2023-04-27 04:46:19,406 - INFO - Epoch 146 training loss = 10.52
2023-04-27 04:46:19,505 - INFO - Epoch 147 training loss = 10.46
2023-04-27 04:46:19,605 - INFO - Epoch 148 training loss = 10.24
2023-04-27 04:46:19,704 - INFO - Epoch 149 training loss = 10.09
2023-04-27 04:46:19,803 - INFO - Epoch 150 training loss = 10.36
2023-04-27 04:46:19,831 - INFO - Validation loss = 20.15
2023-04-27 04:46:19,930 - INFO - Epoch 151 training loss = 9.981
2023-04-27 04:46:20,030 - INFO - Epoch 152 training loss = 9.845
2023-04-27 04:46:20,130 - INFO - Epoch 153 training loss = 9.717
2023-04-27 04:46:20,229 - INFO - Epoch 154 training loss = 10.23
2023-04-27 04:46:20,388 - INFO - Epoch 155 training loss = 9.486
2023-04-27 04:46:20,487 - INFO - Epoch 156 training loss = 9.46
2023-04-27 04:46:20,587 - INFO - Epoch 157 training loss = 10.14
2023-04-27 04:46:20,686 - INFO - Epoch 158 training loss = 9.694
2023-04-27 04:46:20,786 - INFO - Epoch 159 training loss = 9.37
2023-04-27 04:46:20,885 - INFO - Epoch 160 training loss = 9.268
2023-04-27 04:46:20,912 - INFO - Validation loss = 13.95
2023-04-27 04:46:20,913 - INFO - best model
2023-04-27 04:46:21,023 - INFO - Epoch 161 training loss = 9.301
2023-04-27 04:46:21,123 - INFO - Epoch 162 training loss = 8.959
2023-04-27 04:46:21,222 - INFO - Epoch 163 training loss = 8.992
2023-04-27 04:46:21,324 - INFO - Epoch 164 training loss = 9.16
2023-04-27 04:46:21,424 - INFO - Epoch 165 training loss = 8.98
2023-04-27 04:46:21,523 - INFO - Epoch 166 training loss = 9.367
2023-04-27 04:46:21,623 - INFO - Epoch 167 training loss = 8.802
2023-04-27 04:46:21,722 - INFO - Epoch 168 training loss = 8.734
2023-04-27 04:46:21,822 - INFO - Epoch 169 training loss = 8.656
2023-04-27 04:46:21,921 - INFO - Epoch 170 training loss = 8.577
2023-04-27 04:46:21,949 - INFO - Validation loss = 13.68
2023-04-27 04:46:21,949 - INFO - best model
2023-04-27 04:46:22,060 - INFO - Epoch 171 training loss = 8.563
2023-04-27 04:46:22,160 - INFO - Epoch 172 training loss = 8.838
2023-04-27 04:46:22,259 - INFO - Epoch 173 training loss = 8.512
2023-04-27 04:46:22,359 - INFO - Epoch 174 training loss = 8.175
2023-04-27 04:46:22,458 - INFO - Epoch 175 training loss = 8.468
2023-04-27 04:46:22,558 - INFO - Epoch 176 training loss = 8.236
2023-04-27 04:46:22,657 - INFO - Epoch 177 training loss = 8.779
2023-04-27 04:46:22,757 - INFO - Epoch 178 training loss = 8.626
2023-04-27 04:46:22,856 - INFO - Epoch 179 training loss = 7.936
2023-04-27 04:46:22,956 - INFO - Epoch 180 training loss = 8.255
2023-04-27 04:46:22,983 - INFO - Validation loss = 13.38
2023-04-27 04:46:22,983 - INFO - best model
2023-04-27 04:46:23,094 - INFO - Epoch 181 training loss = 8.165
2023-04-27 04:46:23,193 - INFO - Epoch 182 training loss = 7.881
2023-04-27 04:46:23,294 - INFO - Epoch 183 training loss = 7.995
2023-04-27 04:46:23,393 - INFO - Epoch 184 training loss = 7.987
2023-04-27 04:46:23,493 - INFO - Epoch 185 training loss = 7.921
2023-04-27 04:46:23,651 - INFO - Epoch 186 training loss = 7.658
2023-04-27 04:46:23,751 - INFO - Epoch 187 training loss = 7.709
2023-04-27 04:46:23,850 - INFO - Epoch 188 training loss = 8.294
2023-04-27 04:46:23,949 - INFO - Epoch 189 training loss = 7.38
2023-04-27 04:46:24,049 - INFO - Epoch 190 training loss = 8.493
2023-04-27 04:46:24,076 - INFO - Validation loss = 12.84
2023-04-27 04:46:24,077 - INFO - best model
2023-04-27 04:46:24,188 - INFO - Epoch 191 training loss = 7.296
2023-04-27 04:46:24,287 - INFO - Epoch 192 training loss = 7.892
2023-04-27 04:46:24,386 - INFO - Epoch 193 training loss = 7.616
2023-04-27 04:46:24,486 - INFO - Epoch 194 training loss = 7.521
2023-04-27 04:46:24,585 - INFO - Epoch 195 training loss = 7.344
2023-04-27 04:46:24,685 - INFO - Epoch 196 training loss = 7.788
2023-04-27 04:46:24,784 - INFO - Epoch 197 training loss = 7.862
2023-04-27 04:46:24,883 - INFO - Epoch 198 training loss = 7.422
2023-04-27 04:46:24,983 - INFO - Epoch 199 training loss = 7.288
2023-04-27 04:46:25,083 - INFO - Epoch 200 training loss = 7.686
2023-04-27 04:46:25,110 - INFO - Validation loss = 12.52
2023-04-27 04:46:25,110 - INFO - best model
2023-04-27 04:46:25,221 - INFO - Epoch 201 training loss = 7.386
2023-04-27 04:46:25,321 - INFO - Epoch 202 training loss = 7.011
2023-04-27 04:46:25,420 - INFO - Epoch 203 training loss = 7.074
2023-04-27 04:46:25,519 - INFO - Epoch 204 training loss = 7.789
2023-04-27 04:46:25,619 - INFO - Epoch 205 training loss = 7.153
2023-04-27 04:46:25,718 - INFO - Epoch 206 training loss = 7.105
2023-04-27 04:46:25,818 - INFO - Epoch 207 training loss = 7.384
2023-04-27 04:46:25,917 - INFO - Epoch 208 training loss = 6.791
2023-04-27 04:46:26,016 - INFO - Epoch 209 training loss = 6.896
2023-04-27 04:46:26,116 - INFO - Epoch 210 training loss = 7.379
2023-04-27 04:46:26,144 - INFO - Validation loss = 13.1
2023-04-27 04:46:26,243 - INFO - Epoch 211 training loss = 6.901
2023-04-27 04:46:26,343 - INFO - Epoch 212 training loss = 6.996
2023-04-27 04:46:26,442 - INFO - Epoch 213 training loss = 6.489
2023-04-27 04:46:26,541 - INFO - Epoch 214 training loss = 7.099
2023-04-27 04:46:26,641 - INFO - Epoch 215 training loss = 7.191
2023-04-27 04:46:26,741 - INFO - Epoch 216 training loss = 6.613
2023-04-27 04:46:26,840 - INFO - Epoch 217 training loss = 6.751
2023-04-27 04:46:26,998 - INFO - Epoch 218 training loss = 6.72
2023-04-27 04:46:27,098 - INFO - Epoch 219 training loss = 7.034
2023-04-27 04:46:27,197 - INFO - Epoch 220 training loss = 6.443
2023-04-27 04:46:27,225 - INFO - Validation loss = 12.1
2023-04-27 04:46:27,225 - INFO - best model
2023-04-27 04:46:27,336 - INFO - Epoch 221 training loss = 7.121
2023-04-27 04:46:27,435 - INFO - Epoch 222 training loss = 6.418
2023-04-27 04:46:27,535 - INFO - Epoch 223 training loss = 6.666
2023-04-27 04:46:27,634 - INFO - Epoch 224 training loss = 6.55
2023-04-27 04:46:27,734 - INFO - Epoch 225 training loss = 6.374
2023-04-27 04:46:27,833 - INFO - Epoch 226 training loss = 6.484
2023-04-27 04:46:27,933 - INFO - Epoch 227 training loss = 6.416
2023-04-27 04:46:28,032 - INFO - Epoch 228 training loss = 6.525
2023-04-27 04:46:28,132 - INFO - Epoch 229 training loss = 6.478
2023-04-27 04:46:28,232 - INFO - Epoch 230 training loss = 6.504
2023-04-27 04:46:28,259 - INFO - Validation loss = 12.42
2023-04-27 04:46:28,358 - INFO - Epoch 231 training loss = 6.621
2023-04-27 04:46:28,458 - INFO - Epoch 232 training loss = 6.302
2023-04-27 04:46:28,557 - INFO - Epoch 233 training loss =  6.3
2023-04-27 04:46:28,657 - INFO - Epoch 234 training loss = 6.297
2023-04-27 04:46:28,756 - INFO - Epoch 235 training loss = 6.245
2023-04-27 04:46:28,856 - INFO - Epoch 236 training loss = 6.409
2023-04-27 04:46:28,955 - INFO - Epoch 237 training loss = 5.873
2023-04-27 04:46:29,054 - INFO - Epoch 238 training loss = 6.247
2023-04-27 04:46:29,153 - INFO - Epoch 239 training loss = 6.092
2023-04-27 04:46:29,251 - INFO - Epoch 240 training loss = 6.037
2023-04-27 04:46:29,279 - INFO - Validation loss = 12.09
2023-04-27 04:46:29,279 - INFO - best model
2023-04-27 04:46:29,389 - INFO - Epoch 241 training loss = 6.015
2023-04-27 04:46:29,487 - INFO - Epoch 242 training loss = 5.905
2023-04-27 04:46:29,586 - INFO - Epoch 243 training loss = 5.933
2023-04-27 04:46:29,685 - INFO - Epoch 244 training loss = 6.124
2023-04-27 04:46:29,783 - INFO - Epoch 245 training loss = 5.763
2023-04-27 04:46:29,882 - INFO - Epoch 246 training loss = 6.11
2023-04-27 04:46:29,980 - INFO - Epoch 247 training loss = 6.257
2023-04-27 04:46:30,080 - INFO - Epoch 248 training loss = 5.918
2023-04-27 04:46:30,178 - INFO - Epoch 249 training loss = 5.77
2023-04-27 04:46:30,335 - INFO - Epoch 250 training loss = 6.151
2023-04-27 04:46:30,362 - INFO - Validation loss = 12.31
2023-04-27 04:46:30,461 - INFO - Epoch 251 training loss = 5.773
2023-04-27 04:46:30,559 - INFO - Epoch 252 training loss = 5.938
2023-04-27 04:46:30,658 - INFO - Epoch 253 training loss = 5.634
2023-04-27 04:46:30,757 - INFO - Epoch 254 training loss = 6.018
2023-04-27 04:46:30,855 - INFO - Epoch 255 training loss = 5.677
2023-04-27 04:46:30,954 - INFO - Epoch 256 training loss = 5.479
2023-04-27 04:46:31,053 - INFO - Epoch 257 training loss = 6.28
2023-04-27 04:46:31,152 - INFO - Epoch 258 training loss = 5.423
2023-04-27 04:46:31,251 - INFO - Epoch 259 training loss = 5.605
2023-04-27 04:46:31,349 - INFO - Epoch 260 training loss = 5.645
2023-04-27 04:46:31,377 - INFO - Validation loss = 11.54
2023-04-27 04:46:31,377 - INFO - best model
2023-04-27 04:46:31,487 - INFO - Epoch 261 training loss = 5.618
2023-04-27 04:46:31,585 - INFO - Epoch 262 training loss = 5.732
2023-04-27 04:46:31,684 - INFO - Epoch 263 training loss = 5.603
2023-04-27 04:46:31,783 - INFO - Epoch 264 training loss = 5.728
2023-04-27 04:46:31,881 - INFO - Epoch 265 training loss = 5.779
2023-04-27 04:46:31,980 - INFO - Epoch 266 training loss = 5.37
2023-04-27 04:46:32,079 - INFO - Epoch 267 training loss = 5.465
2023-04-27 04:46:32,178 - INFO - Epoch 268 training loss = 5.443
2023-04-27 04:46:32,277 - INFO - Epoch 269 training loss = 5.484
2023-04-27 04:46:32,375 - INFO - Epoch 270 training loss = 6.067
2023-04-27 04:46:32,402 - INFO - Validation loss = 11.46
2023-04-27 04:46:32,403 - INFO - best model
2023-04-27 04:46:32,512 - INFO - Epoch 271 training loss = 5.12
2023-04-27 04:46:32,611 - INFO - Epoch 272 training loss = 5.581
2023-04-27 04:46:32,709 - INFO - Epoch 273 training loss = 5.09
2023-04-27 04:46:32,808 - INFO - Epoch 274 training loss = 5.412
2023-04-27 04:46:32,907 - INFO - Epoch 275 training loss = 5.405
2023-04-27 04:46:33,005 - INFO - Epoch 276 training loss = 5.245
2023-04-27 04:46:33,104 - INFO - Epoch 277 training loss = 5.288
2023-04-27 04:46:33,203 - INFO - Epoch 278 training loss = 5.287
2023-04-27 04:46:33,302 - INFO - Epoch 279 training loss = 5.056
2023-04-27 04:46:33,401 - INFO - Epoch 280 training loss = 5.169
2023-04-27 04:46:33,428 - INFO - Validation loss = 11.05
2023-04-27 04:46:33,428 - INFO - best model
2023-04-27 04:46:33,595 - INFO - Epoch 281 training loss = 5.489
2023-04-27 04:46:33,694 - INFO - Epoch 282 training loss = 5.151
2023-04-27 04:46:33,793 - INFO - Epoch 283 training loss = 5.054
2023-04-27 04:46:33,891 - INFO - Epoch 284 training loss = 5.206
2023-04-27 04:46:33,990 - INFO - Epoch 285 training loss = 4.963
2023-04-27 04:46:34,089 - INFO - Epoch 286 training loss = 5.15
2023-04-27 04:46:34,188 - INFO - Epoch 287 training loss = 5.139
2023-04-27 04:46:34,287 - INFO - Epoch 288 training loss = 5.069
2023-04-27 04:46:34,385 - INFO - Epoch 289 training loss = 4.994
2023-04-27 04:46:34,484 - INFO - Epoch 290 training loss = 5.054
2023-04-27 04:46:34,511 - INFO - Validation loss = 11.21
2023-04-27 04:46:34,610 - INFO - Epoch 291 training loss = 5.09
2023-04-27 04:46:34,709 - INFO - Epoch 292 training loss = 4.994
2023-04-27 04:46:34,807 - INFO - Epoch 293 training loss = 4.958
2023-04-27 04:46:34,906 - INFO - Epoch 294 training loss = 4.952
2023-04-27 04:46:35,006 - INFO - Epoch 295 training loss = 4.928
2023-04-27 04:46:35,105 - INFO - Epoch 296 training loss = 5.248
2023-04-27 04:46:35,205 - INFO - Epoch 297 training loss = 4.877
2023-04-27 04:46:35,304 - INFO - Epoch 298 training loss = 4.837
2023-04-27 04:46:35,403 - INFO - Epoch 299 training loss = 4.859
2023-04-27 04:46:35,502 - INFO - Epoch 300 training loss =  4.9
2023-04-27 04:46:35,529 - INFO - Validation loss = 10.92
2023-04-27 04:46:35,529 - INFO - best model
2023-04-27 04:46:35,639 - INFO - Epoch 301 training loss = 4.867
2023-04-27 04:46:35,738 - INFO - Epoch 302 training loss = 4.817
2023-04-27 04:46:35,837 - INFO - Epoch 303 training loss = 4.857
2023-04-27 04:46:35,935 - INFO - Epoch 304 training loss = 4.908
2023-04-27 04:46:36,034 - INFO - Epoch 305 training loss = 4.965
2023-04-27 04:46:36,133 - INFO - Epoch 306 training loss = 5.023
2023-04-27 04:46:36,232 - INFO - Epoch 307 training loss = 4.673
2023-04-27 04:46:36,332 - INFO - Epoch 308 training loss = 4.698
2023-04-27 04:46:36,432 - INFO - Epoch 309 training loss = 4.777
2023-04-27 04:46:36,532 - INFO - Epoch 310 training loss = 4.872
2023-04-27 04:46:36,559 - INFO - Validation loss = 11.01
2023-04-27 04:46:36,659 - INFO - Epoch 311 training loss = 4.64
2023-04-27 04:46:36,817 - INFO - Epoch 312 training loss = 4.606
2023-04-27 04:46:36,917 - INFO - Epoch 313 training loss = 4.709
2023-04-27 04:46:37,016 - INFO - Epoch 314 training loss = 4.629
2023-04-27 04:46:37,116 - INFO - Epoch 315 training loss = 4.571
2023-04-27 04:46:37,216 - INFO - Epoch 316 training loss = 4.505
2023-04-27 04:46:37,316 - INFO - Epoch 317 training loss = 4.577
2023-04-27 04:46:37,415 - INFO - Epoch 318 training loss = 4.559
2023-04-27 04:46:37,515 - INFO - Epoch 319 training loss = 4.947
2023-04-27 04:46:37,615 - INFO - Epoch 320 training loss = 4.681
2023-04-27 04:46:37,642 - INFO - Validation loss = 10.87
2023-04-27 04:46:37,642 - INFO - best model
2023-04-27 04:46:37,753 - INFO - Epoch 321 training loss = 4.482
2023-04-27 04:46:37,852 - INFO - Epoch 322 training loss = 4.461
2023-04-27 04:46:37,952 - INFO - Epoch 323 training loss =  4.5
2023-04-27 04:46:38,051 - INFO - Epoch 324 training loss = 4.767
2023-04-27 04:46:38,151 - INFO - Epoch 325 training loss = 4.708
2023-04-27 04:46:38,249 - INFO - Epoch 326 training loss = 4.366
2023-04-27 04:46:38,347 - INFO - Epoch 327 training loss = 4.332
2023-04-27 04:46:38,445 - INFO - Epoch 328 training loss = 4.784
2023-04-27 04:46:38,543 - INFO - Epoch 329 training loss = 4.451
2023-04-27 04:46:38,641 - INFO - Epoch 330 training loss = 4.433
2023-04-27 04:46:38,668 - INFO - Validation loss = 11.13
2023-04-27 04:46:38,767 - INFO - Epoch 331 training loss = 4.375
2023-04-27 04:46:38,866 - INFO - Epoch 332 training loss = 4.322
2023-04-27 04:46:38,964 - INFO - Epoch 333 training loss = 4.448
2023-04-27 04:46:39,062 - INFO - Epoch 334 training loss = 4.388
2023-04-27 04:46:39,160 - INFO - Epoch 335 training loss = 4.234
2023-04-27 04:46:39,259 - INFO - Epoch 336 training loss = 4.263
2023-04-27 04:46:39,357 - INFO - Epoch 337 training loss = 4.348
2023-04-27 04:46:39,455 - INFO - Epoch 338 training loss = 4.285
2023-04-27 04:46:39,553 - INFO - Epoch 339 training loss = 4.349
2023-04-27 04:46:39,651 - INFO - Epoch 340 training loss = 4.295
2023-04-27 04:46:39,678 - INFO - Validation loss = 10.85
2023-04-27 04:46:39,679 - INFO - best model
2023-04-27 04:46:39,788 - INFO - Epoch 341 training loss = 4.231
2023-04-27 04:46:39,886 - INFO - Epoch 342 training loss = 4.152
2023-04-27 04:46:39,984 - INFO - Epoch 343 training loss = 4.189
2023-04-27 04:46:40,140 - INFO - Epoch 344 training loss = 4.157
2023-04-27 04:46:40,238 - INFO - Epoch 345 training loss = 4.062
2023-04-27 04:46:40,336 - INFO - Epoch 346 training loss = 4.366
2023-04-27 04:46:40,434 - INFO - Epoch 347 training loss = 4.161
2023-04-27 04:46:40,532 - INFO - Epoch 348 training loss = 4.275
2023-04-27 04:46:40,630 - INFO - Epoch 349 training loss = 4.027
2023-04-27 04:46:40,729 - INFO - Epoch 350 training loss = 4.284
2023-04-27 04:46:40,756 - INFO - Validation loss = 10.69
2023-04-27 04:46:40,756 - INFO - best model
2023-04-27 04:46:40,865 - INFO - Epoch 351 training loss = 4.149
2023-04-27 04:46:40,963 - INFO - Epoch 352 training loss = 4.03
2023-04-27 04:46:41,061 - INFO - Epoch 353 training loss = 4.082
2023-04-27 04:46:41,160 - INFO - Epoch 354 training loss = 4.119
2023-04-27 04:46:41,259 - INFO - Epoch 355 training loss = 3.954
2023-04-27 04:46:41,358 - INFO - Epoch 356 training loss = 4.042
2023-04-27 04:46:41,457 - INFO - Epoch 357 training loss = 4.11
2023-04-27 04:46:41,555 - INFO - Epoch 358 training loss = 3.923
2023-04-27 04:46:41,655 - INFO - Epoch 359 training loss = 4.244
2023-04-27 04:46:41,756 - INFO - Epoch 360 training loss = 4.109
2023-04-27 04:46:41,783 - INFO - Validation loss = 10.9
2023-04-27 04:46:41,883 - INFO - Epoch 361 training loss = 3.873
2023-04-27 04:46:41,982 - INFO - Epoch 362 training loss = 4.136
2023-04-27 04:46:42,082 - INFO - Epoch 363 training loss = 3.93
2023-04-27 04:46:42,182 - INFO - Epoch 364 training loss = 4.04
2023-04-27 04:46:42,281 - INFO - Epoch 365 training loss = 3.923
2023-04-27 04:46:42,380 - INFO - Epoch 366 training loss = 3.88
2023-04-27 04:46:42,478 - INFO - Epoch 367 training loss = 3.844
2023-04-27 04:46:42,577 - INFO - Epoch 368 training loss = 3.792
2023-04-27 04:46:42,676 - INFO - Epoch 369 training loss = 4.238
2023-04-27 04:46:42,776 - INFO - Epoch 370 training loss = 3.823
2023-04-27 04:46:42,803 - INFO - Validation loss = 10.52
2023-04-27 04:46:42,803 - INFO - best model
2023-04-27 04:46:42,913 - INFO - Epoch 371 training loss = 3.972
2023-04-27 04:46:43,012 - INFO - Epoch 372 training loss = 3.878
2023-04-27 04:46:43,111 - INFO - Epoch 373 training loss = 4.166
2023-04-27 04:46:43,210 - INFO - Epoch 374 training loss = 3.778
2023-04-27 04:46:43,367 - INFO - Epoch 375 training loss = 3.913
2023-04-27 04:46:43,465 - INFO - Epoch 376 training loss = 3.676
2023-04-27 04:46:43,564 - INFO - Epoch 377 training loss = 3.789
2023-04-27 04:46:43,663 - INFO - Epoch 378 training loss = 3.738
2023-04-27 04:46:43,763 - INFO - Epoch 379 training loss = 3.827
2023-04-27 04:46:43,861 - INFO - Epoch 380 training loss = 3.737
2023-04-27 04:46:43,889 - INFO - Validation loss = 10.77
2023-04-27 04:46:43,987 - INFO - Epoch 381 training loss = 3.698
2023-04-27 04:46:44,086 - INFO - Epoch 382 training loss = 3.911
2023-04-27 04:46:44,185 - INFO - Epoch 383 training loss = 3.676
2023-04-27 04:46:44,284 - INFO - Epoch 384 training loss = 3.677
2023-04-27 04:46:44,382 - INFO - Epoch 385 training loss = 3.838
2023-04-27 04:46:44,481 - INFO - Epoch 386 training loss = 3.555
2023-04-27 04:46:44,579 - INFO - Epoch 387 training loss = 3.739
2023-04-27 04:46:44,678 - INFO - Epoch 388 training loss = 3.797
2023-04-27 04:46:44,778 - INFO - Epoch 389 training loss = 3.69
2023-04-27 04:46:44,877 - INFO - Epoch 390 training loss = 3.679
2023-04-27 04:46:44,904 - INFO - Validation loss = 10.66
2023-04-27 04:46:45,003 - INFO - Epoch 391 training loss = 3.647
2023-04-27 04:46:45,102 - INFO - Epoch 392 training loss = 3.579
2023-04-27 04:46:45,200 - INFO - Epoch 393 training loss = 3.714
2023-04-27 04:46:45,299 - INFO - Epoch 394 training loss = 3.628
2023-04-27 04:46:45,398 - INFO - Epoch 395 training loss = 3.538
2023-04-27 04:46:45,496 - INFO - Epoch 396 training loss = 3.667
2023-04-27 04:46:45,594 - INFO - Epoch 397 training loss = 3.651
2023-04-27 04:46:45,692 - INFO - Epoch 398 training loss = 3.521
2023-04-27 04:46:45,792 - INFO - Epoch 399 training loss = 3.682
2023-04-27 04:46:45,890 - INFO - Epoch 400 training loss = 3.556
2023-04-27 04:46:45,917 - INFO - Validation loss = 10.75
2023-04-27 04:46:46,015 - INFO - Epoch 401 training loss = 3.529
2023-04-27 04:46:46,113 - INFO - Epoch 402 training loss = 3.558
2023-04-27 04:46:46,211 - INFO - Epoch 403 training loss = 3.552
2023-04-27 04:46:46,310 - INFO - Epoch 404 training loss = 3.518
2023-04-27 04:46:46,408 - INFO - Epoch 405 training loss = 3.603
2023-04-27 04:46:46,563 - INFO - Epoch 406 training loss = 3.552
2023-04-27 04:46:46,661 - INFO - Epoch 407 training loss =  3.5
2023-04-27 04:46:46,760 - INFO - Epoch 408 training loss = 3.383
2023-04-27 04:46:46,859 - INFO - Epoch 409 training loss = 3.671
2023-04-27 04:46:46,957 - INFO - Epoch 410 training loss = 3.448
2023-04-27 04:46:46,984 - INFO - Validation loss = 10.57
2023-04-27 04:46:47,083 - INFO - Epoch 411 training loss = 3.402
2023-04-27 04:46:47,181 - INFO - Epoch 412 training loss = 3.382
2023-04-27 04:46:47,280 - INFO - Epoch 413 training loss = 3.491
2023-04-27 04:46:47,378 - INFO - Epoch 414 training loss = 3.45
2023-04-27 04:46:47,476 - INFO - Epoch 415 training loss = 3.493
2023-04-27 04:46:47,574 - INFO - Epoch 416 training loss = 3.414
2023-04-27 04:46:47,672 - INFO - Epoch 417 training loss = 3.416
2023-04-27 04:46:47,770 - INFO - Epoch 418 training loss = 3.429
2023-04-27 04:46:47,870 - INFO - Epoch 419 training loss = 3.343
2023-04-27 04:46:47,969 - INFO - Epoch 420 training loss = 3.473
2023-04-27 04:46:47,996 - INFO - Validation loss = 10.53
2023-04-27 04:46:48,095 - INFO - Epoch 421 training loss = 3.337
2023-04-27 04:46:48,194 - INFO - Epoch 422 training loss = 3.339
2023-04-27 04:46:48,293 - INFO - Epoch 423 training loss = 3.329
2023-04-27 04:46:48,392 - INFO - Epoch 424 training loss = 3.369
2023-04-27 04:46:48,491 - INFO - Epoch 425 training loss = 3.317
2023-04-27 04:46:48,589 - INFO - Epoch 426 training loss = 3.416
2023-04-27 04:46:48,688 - INFO - Epoch 427 training loss = 3.42
2023-04-27 04:46:48,787 - INFO - Epoch 428 training loss = 3.291
2023-04-27 04:46:48,886 - INFO - Epoch 429 training loss = 3.329
2023-04-27 04:46:48,984 - INFO - Epoch 430 training loss = 3.258
2023-04-27 04:46:49,012 - INFO - Validation loss = 10.64
2023-04-27 04:46:49,111 - INFO - Epoch 431 training loss = 3.376
2023-04-27 04:46:49,210 - INFO - Epoch 432 training loss = 3.206
2023-04-27 04:46:49,309 - INFO - Epoch 433 training loss = 3.191
2023-04-27 04:46:49,408 - INFO - Epoch 434 training loss = 3.28
2023-04-27 04:46:49,506 - INFO - Epoch 435 training loss = 3.22
2023-04-27 04:46:49,605 - INFO - Epoch 436 training loss = 3.234
2023-04-27 04:46:49,704 - INFO - Epoch 437 training loss = 3.362
2023-04-27 04:46:49,860 - INFO - Epoch 438 training loss = 3.235
2023-04-27 04:46:49,959 - INFO - Epoch 439 training loss = 3.317
2023-04-27 04:46:50,058 - INFO - Epoch 440 training loss = 3.208
2023-04-27 04:46:50,085 - INFO - Validation loss = 10.59
2023-04-27 04:46:50,184 - INFO - Epoch 441 training loss = 3.192
2023-04-27 04:46:50,283 - INFO - Epoch 442 training loss = 3.168
2023-04-27 04:46:50,381 - INFO - Epoch 443 training loss = 3.117
2023-04-27 04:46:50,480 - INFO - Epoch 444 training loss = 3.168
2023-04-27 04:46:50,578 - INFO - Epoch 445 training loss = 3.192
2023-04-27 04:46:50,676 - INFO - Epoch 446 training loss = 3.126
2023-04-27 04:46:50,774 - INFO - Epoch 447 training loss = 3.223
2023-04-27 04:46:50,872 - INFO - Epoch 448 training loss = 3.137
2023-04-27 04:46:50,970 - INFO - Epoch 449 training loss = 3.237
2023-04-27 04:46:51,068 - INFO - Epoch 450 training loss = 3.083
2023-04-27 04:46:51,095 - INFO - Validation loss = 10.39
2023-04-27 04:46:51,096 - INFO - best model
2023-04-27 04:46:51,205 - INFO - Epoch 451 training loss = 3.119
2023-04-27 04:46:51,304 - INFO - Epoch 452 training loss = 3.091
2023-04-27 04:46:51,404 - INFO - Epoch 453 training loss = 3.09
2023-04-27 04:46:51,502 - INFO - Epoch 454 training loss = 3.096
2023-04-27 04:46:51,600 - INFO - Epoch 455 training loss = 3.195
2023-04-27 04:46:51,698 - INFO - Epoch 456 training loss = 3.057
2023-04-27 04:46:51,797 - INFO - Epoch 457 training loss = 3.029
2023-04-27 04:46:51,895 - INFO - Epoch 458 training loss = 3.096
2023-04-27 04:46:51,993 - INFO - Epoch 459 training loss = 3.023
2023-04-27 04:46:52,092 - INFO - Epoch 460 training loss = 3.041
2023-04-27 04:46:52,119 - INFO - Validation loss = 10.49
2023-04-27 04:46:52,217 - INFO - Epoch 461 training loss = 2.969
2023-04-27 04:46:52,315 - INFO - Epoch 462 training loss = 3.082
2023-04-27 04:46:52,413 - INFO - Epoch 463 training loss = 3.05
2023-04-27 04:46:52,511 - INFO - Epoch 464 training loss = 2.994
2023-04-27 04:46:52,609 - INFO - Epoch 465 training loss = 3.015
2023-04-27 04:46:52,707 - INFO - Epoch 466 training loss = 3.033
2023-04-27 04:46:52,805 - INFO - Epoch 467 training loss = 3.068
2023-04-27 04:46:52,904 - INFO - Epoch 468 training loss = 3.001
2023-04-27 04:46:53,060 - INFO - Epoch 469 training loss = 3.041
2023-04-27 04:46:53,158 - INFO - Epoch 470 training loss = 2.952
2023-04-27 04:46:53,185 - INFO - Validation loss = 10.19
2023-04-27 04:46:53,185 - INFO - best model
2023-04-27 04:46:53,294 - INFO - Epoch 471 training loss = 3.075
2023-04-27 04:46:53,393 - INFO - Epoch 472 training loss = 2.986
2023-04-27 04:46:53,491 - INFO - Epoch 473 training loss = 2.984
2023-04-27 04:46:53,589 - INFO - Epoch 474 training loss = 2.885
2023-04-27 04:46:53,687 - INFO - Epoch 475 training loss = 2.894
2023-04-27 04:46:53,785 - INFO - Epoch 476 training loss = 2.925
2023-04-27 04:46:53,883 - INFO - Epoch 477 training loss = 2.89
2023-04-27 04:46:53,982 - INFO - Epoch 478 training loss = 2.971
2023-04-27 04:46:54,080 - INFO - Epoch 479 training loss = 2.873
2023-04-27 04:46:54,178 - INFO - Epoch 480 training loss = 2.891
2023-04-27 04:46:54,205 - INFO - Validation loss = 10.54
2023-04-27 04:46:54,303 - INFO - Epoch 481 training loss = 2.963
2023-04-27 04:46:54,401 - INFO - Epoch 482 training loss = 2.812
2023-04-27 04:46:54,500 - INFO - Epoch 483 training loss = 2.844
2023-04-27 04:46:54,598 - INFO - Epoch 484 training loss = 2.881
2023-04-27 04:46:54,696 - INFO - Epoch 485 training loss = 2.893
2023-04-27 04:46:54,794 - INFO - Epoch 486 training loss = 2.831
2023-04-27 04:46:54,892 - INFO - Epoch 487 training loss = 2.903
2023-04-27 04:46:54,990 - INFO - Epoch 488 training loss = 2.833
2023-04-27 04:46:55,088 - INFO - Epoch 489 training loss = 2.78
2023-04-27 04:46:55,186 - INFO - Epoch 490 training loss = 2.874
2023-04-27 04:46:55,213 - INFO - Validation loss = 10.61
2023-04-27 04:46:55,312 - INFO - Epoch 491 training loss = 2.809
2023-04-27 04:46:55,410 - INFO - Epoch 492 training loss = 2.838
2023-04-27 04:46:55,508 - INFO - Epoch 493 training loss = 2.84
2023-04-27 04:46:55,606 - INFO - Epoch 494 training loss = 2.758
2023-04-27 04:46:55,704 - INFO - Epoch 495 training loss = 2.818
2023-04-27 04:46:55,802 - INFO - Epoch 496 training loss = 2.877
2023-04-27 04:46:55,900 - INFO - Epoch 497 training loss = 2.825
2023-04-27 04:46:55,998 - INFO - Epoch 498 training loss = 2.72
2023-04-27 04:46:56,096 - INFO - Epoch 499 training loss = 2.807
2023-04-27 04:46:56,252 - INFO - Epoch 500 training loss = 2.885
2023-04-27 04:46:56,279 - INFO - Validation loss = 10.46
2023-04-27 04:46:56,377 - INFO - Epoch 501 training loss = 2.728
2023-04-27 04:46:56,476 - INFO - Epoch 502 training loss = 2.748
2023-04-27 04:46:56,574 - INFO - Epoch 503 training loss = 2.771
2023-04-27 04:46:56,672 - INFO - Epoch 504 training loss = 2.778
2023-04-27 04:46:56,770 - INFO - Epoch 505 training loss = 2.699
2023-04-27 04:46:56,868 - INFO - Epoch 506 training loss = 2.835
2023-04-27 04:46:56,966 - INFO - Epoch 507 training loss = 2.829
2023-04-27 04:46:57,064 - INFO - Epoch 508 training loss = 2.674
2023-04-27 04:46:57,162 - INFO - Epoch 509 training loss = 2.708
2023-04-27 04:46:57,261 - INFO - Epoch 510 training loss = 2.714
2023-04-27 04:46:57,288 - INFO - Validation loss = 10.38
2023-04-27 04:46:57,387 - INFO - Epoch 511 training loss = 2.691
2023-04-27 04:46:57,485 - INFO - Epoch 512 training loss = 2.76
2023-04-27 04:46:57,584 - INFO - Epoch 513 training loss = 2.667
2023-04-27 04:46:57,682 - INFO - Epoch 514 training loss = 2.702
2023-04-27 04:46:57,781 - INFO - Epoch 515 training loss = 2.678
2023-04-27 04:46:57,880 - INFO - Epoch 516 training loss = 2.636
2023-04-27 04:46:57,978 - INFO - Epoch 517 training loss = 2.656
2023-04-27 04:46:58,077 - INFO - Epoch 518 training loss = 2.707
2023-04-27 04:46:58,176 - INFO - Epoch 519 training loss = 2.593
2023-04-27 04:46:58,275 - INFO - Epoch 520 training loss = 2.667
2023-04-27 04:46:58,302 - INFO - Validation loss = 10.48
2023-04-27 04:46:58,400 - INFO - Epoch 521 training loss = 2.62
2023-04-27 04:46:58,499 - INFO - Epoch 522 training loss = 2.66
2023-04-27 04:46:58,598 - INFO - Epoch 523 training loss = 2.649
2023-04-27 04:46:58,696 - INFO - Epoch 524 training loss = 2.592
2023-04-27 04:46:58,795 - INFO - Epoch 525 training loss = 2.583
2023-04-27 04:46:58,893 - INFO - Epoch 526 training loss = 2.622
2023-04-27 04:46:58,992 - INFO - Epoch 527 training loss = 2.635
2023-04-27 04:46:59,091 - INFO - Epoch 528 training loss = 2.649
2023-04-27 04:46:59,190 - INFO - Epoch 529 training loss = 2.564
2023-04-27 04:46:59,289 - INFO - Epoch 530 training loss = 2.642
2023-04-27 04:46:59,316 - INFO - Validation loss = 10.31
2023-04-27 04:46:59,414 - INFO - Epoch 531 training loss = 2.593
2023-04-27 04:46:59,571 - INFO - Epoch 532 training loss = 2.588
2023-04-27 04:46:59,669 - INFO - Epoch 533 training loss = 2.549
2023-04-27 04:46:59,768 - INFO - Epoch 534 training loss = 2.541
2023-04-27 04:46:59,867 - INFO - Epoch 535 training loss = 2.609
2023-04-27 04:46:59,966 - INFO - Epoch 536 training loss = 2.559
2023-04-27 04:47:00,065 - INFO - Epoch 537 training loss = 2.614
2023-04-27 04:47:00,163 - INFO - Epoch 538 training loss = 2.556
2023-04-27 04:47:00,261 - INFO - Epoch 539 training loss = 2.559
2023-04-27 04:47:00,359 - INFO - Epoch 540 training loss = 2.539
2023-04-27 04:47:00,386 - INFO - Validation loss = 10.29
2023-04-27 04:47:00,485 - INFO - Epoch 541 training loss = 2.514
2023-04-27 04:47:00,583 - INFO - Epoch 542 training loss = 2.539
2023-04-27 04:47:00,681 - INFO - Epoch 543 training loss = 2.502
2023-04-27 04:47:00,779 - INFO - Epoch 544 training loss = 2.476
2023-04-27 04:47:00,877 - INFO - Epoch 545 training loss = 2.595
2023-04-27 04:47:00,976 - INFO - Epoch 546 training loss = 2.469
2023-04-27 04:47:01,075 - INFO - Epoch 547 training loss = 2.494
2023-04-27 04:47:01,174 - INFO - Epoch 548 training loss = 2.471
2023-04-27 04:47:01,274 - INFO - Epoch 549 training loss = 2.467
2023-04-27 04:47:01,373 - INFO - Epoch 550 training loss = 2.522
2023-04-27 04:47:01,400 - INFO - Validation loss = 10.24
2023-04-27 04:47:01,499 - INFO - Epoch 551 training loss = 2.424
2023-04-27 04:47:01,598 - INFO - Epoch 552 training loss = 2.483
2023-04-27 04:47:01,697 - INFO - Epoch 553 training loss = 2.412
2023-04-27 04:47:01,796 - INFO - Epoch 554 training loss = 2.471
2023-04-27 04:47:01,895 - INFO - Epoch 555 training loss = 2.425
2023-04-27 04:47:01,994 - INFO - Epoch 556 training loss = 2.477
2023-04-27 04:47:02,093 - INFO - Epoch 557 training loss = 2.442
2023-04-27 04:47:02,193 - INFO - Epoch 558 training loss = 2.465
2023-04-27 04:47:02,292 - INFO - Epoch 559 training loss = 2.485
2023-04-27 04:47:02,391 - INFO - Epoch 560 training loss = 2.433
2023-04-27 04:47:02,418 - INFO - Validation loss = 10.22
2023-04-27 04:47:02,517 - INFO - Epoch 561 training loss = 2.372
2023-04-27 04:47:02,616 - INFO - Epoch 562 training loss = 2.424
2023-04-27 04:47:02,773 - INFO - Epoch 563 training loss = 2.437
2023-04-27 04:47:02,872 - INFO - Epoch 564 training loss = 2.379
2023-04-27 04:47:02,971 - INFO - Epoch 565 training loss = 2.383
2023-04-27 04:47:03,072 - INFO - Epoch 566 training loss = 2.431
2023-04-27 04:47:03,171 - INFO - Epoch 567 training loss = 2.43
2023-04-27 04:47:03,270 - INFO - Epoch 568 training loss = 2.361
2023-04-27 04:47:03,369 - INFO - Epoch 569 training loss = 2.419
2023-04-27 04:47:03,468 - INFO - Epoch 570 training loss = 2.346
2023-04-27 04:47:03,495 - INFO - Validation loss = 10.29
2023-04-27 04:47:03,594 - INFO - Epoch 571 training loss =  2.4
2023-04-27 04:47:03,693 - INFO - Epoch 572 training loss = 2.383
2023-04-27 04:47:03,792 - INFO - Epoch 573 training loss = 2.436
2023-04-27 04:47:03,891 - INFO - Epoch 574 training loss = 2.299
2023-04-27 04:47:03,990 - INFO - Epoch 575 training loss = 2.329
2023-04-27 04:47:04,089 - INFO - Epoch 576 training loss = 2.376
2023-04-27 04:47:04,188 - INFO - Epoch 577 training loss = 2.321
2023-04-27 04:47:04,287 - INFO - Epoch 578 training loss = 2.348
2023-04-27 04:47:04,386 - INFO - Epoch 579 training loss = 2.336
2023-04-27 04:47:04,485 - INFO - Epoch 580 training loss = 2.305
2023-04-27 04:47:04,512 - INFO - Validation loss = 10.31
2023-04-27 04:47:04,611 - INFO - Epoch 581 training loss = 2.34
2023-04-27 04:47:04,710 - INFO - Epoch 582 training loss = 2.327
2023-04-27 04:47:04,809 - INFO - Epoch 583 training loss = 2.32
2023-04-27 04:47:04,907 - INFO - Epoch 584 training loss = 2.315
2023-04-27 04:47:05,006 - INFO - Epoch 585 training loss = 2.276
2023-04-27 04:47:05,106 - INFO - Epoch 586 training loss = 2.279
2023-04-27 04:47:05,205 - INFO - Epoch 587 training loss = 2.338
2023-04-27 04:47:05,304 - INFO - Epoch 588 training loss = 2.279
2023-04-27 04:47:05,403 - INFO - Epoch 589 training loss = 2.331
2023-04-27 04:47:05,502 - INFO - Epoch 590 training loss = 2.267
2023-04-27 04:47:05,529 - INFO - Validation loss = 10.25
2023-04-27 04:47:05,628 - INFO - Epoch 591 training loss = 2.309
2023-04-27 04:47:05,728 - INFO - Epoch 592 training loss = 2.278
2023-04-27 04:47:05,827 - INFO - Epoch 593 training loss = 2.273
2023-04-27 04:47:05,986 - INFO - Epoch 594 training loss = 2.284
2023-04-27 04:47:06,086 - INFO - Epoch 595 training loss = 2.262
2023-04-27 04:47:06,186 - INFO - Epoch 596 training loss = 2.273
2023-04-27 04:47:06,286 - INFO - Epoch 597 training loss = 2.217
2023-04-27 04:47:06,386 - INFO - Epoch 598 training loss = 2.28
2023-04-27 04:47:06,486 - INFO - Epoch 599 training loss = 2.228
2023-04-27 04:47:06,585 - INFO - Epoch 600 training loss = 2.245
2023-04-27 04:47:06,612 - INFO - Validation loss = 10.35
2023-04-27 04:47:06,712 - INFO - Epoch 601 training loss = 2.254
2023-04-27 04:47:06,811 - INFO - Epoch 602 training loss = 2.236
2023-04-27 04:47:06,911 - INFO - Epoch 603 training loss = 2.262
2023-04-27 04:47:07,010 - INFO - Epoch 604 training loss = 2.235
2023-04-27 04:47:07,109 - INFO - Epoch 605 training loss = 2.23
2023-04-27 04:47:07,208 - INFO - Epoch 606 training loss = 2.227
2023-04-27 04:47:07,308 - INFO - Epoch 607 training loss = 2.188
2023-04-27 04:47:07,407 - INFO - Epoch 608 training loss = 2.165
2023-04-27 04:47:07,505 - INFO - Epoch 609 training loss = 2.209
2023-04-27 04:47:07,604 - INFO - Epoch 610 training loss = 2.203
2023-04-27 04:47:07,631 - INFO - Validation loss = 10.45
2023-04-27 04:47:07,730 - INFO - Epoch 611 training loss = 2.265
2023-04-27 04:47:07,829 - INFO - Epoch 612 training loss = 2.17
2023-04-27 04:47:07,928 - INFO - Epoch 613 training loss = 2.189
2023-04-27 04:47:08,027 - INFO - Epoch 614 training loss = 2.184
2023-04-27 04:47:08,126 - INFO - Epoch 615 training loss = 2.183
2023-04-27 04:47:08,225 - INFO - Epoch 616 training loss = 2.15
2023-04-27 04:47:08,324 - INFO - Epoch 617 training loss = 2.195
2023-04-27 04:47:08,423 - INFO - Epoch 618 training loss = 2.164
2023-04-27 04:47:08,522 - INFO - Epoch 619 training loss = 2.165
2023-04-27 04:47:08,621 - INFO - Epoch 620 training loss = 2.166
2023-04-27 04:47:08,648 - INFO - Validation loss = 10.29
2023-04-27 04:47:08,747 - INFO - Epoch 621 training loss = 2.17
2023-04-27 04:47:08,846 - INFO - Epoch 622 training loss = 2.137
2023-04-27 04:47:08,945 - INFO - Epoch 623 training loss = 2.144
2023-04-27 04:47:09,044 - INFO - Epoch 624 training loss = 2.166
2023-04-27 04:47:09,143 - INFO - Epoch 625 training loss = 2.146
2023-04-27 04:47:09,302 - INFO - Epoch 626 training loss = 2.128
2023-04-27 04:47:09,401 - INFO - Epoch 627 training loss = 2.132
2023-04-27 04:47:09,500 - INFO - Epoch 628 training loss = 2.161
2023-04-27 04:47:09,599 - INFO - Epoch 629 training loss = 2.111
2023-04-27 04:47:09,698 - INFO - Epoch 630 training loss = 2.124
2023-04-27 04:47:09,726 - INFO - Validation loss = 10.68
2023-04-27 04:47:09,825 - INFO - Epoch 631 training loss = 2.181
2023-04-27 04:47:09,924 - INFO - Epoch 632 training loss = 2.107
2023-04-27 04:47:10,023 - INFO - Epoch 633 training loss = 2.092
2023-04-27 04:47:10,123 - INFO - Epoch 634 training loss = 2.097
2023-04-27 04:47:10,222 - INFO - Epoch 635 training loss = 2.094
2023-04-27 04:47:10,321 - INFO - Epoch 636 training loss = 2.111
2023-04-27 04:47:10,419 - INFO - Epoch 637 training loss = 2.101
2023-04-27 04:47:10,518 - INFO - Epoch 638 training loss = 2.082
2023-04-27 04:47:10,617 - INFO - Epoch 639 training loss = 2.101
2023-04-27 04:47:10,716 - INFO - Epoch 640 training loss = 2.097
2023-04-27 04:47:10,743 - INFO - Validation loss = 10.29
2023-04-27 04:47:10,842 - INFO - Epoch 641 training loss = 2.06
2023-04-27 04:47:10,941 - INFO - Epoch 642 training loss = 2.084
2023-04-27 04:47:11,041 - INFO - Epoch 643 training loss = 2.097
2023-04-27 04:47:11,140 - INFO - Epoch 644 training loss = 2.066
2023-04-27 04:47:11,239 - INFO - Epoch 645 training loss = 2.065
2023-04-27 04:47:11,338 - INFO - Epoch 646 training loss = 2.061
2023-04-27 04:47:11,437 - INFO - Epoch 647 training loss = 2.066
2023-04-27 04:47:11,535 - INFO - Epoch 648 training loss = 2.056
2023-04-27 04:47:11,634 - INFO - Epoch 649 training loss = 2.094
2023-04-27 04:47:11,733 - INFO - Epoch 650 training loss = 2.035
2023-04-27 04:47:11,760 - INFO - Validation loss = 10.28
2023-04-27 04:47:11,859 - INFO - Epoch 651 training loss = 2.052
2023-04-27 04:47:11,958 - INFO - Epoch 652 training loss = 2.059
2023-04-27 04:47:12,058 - INFO - Epoch 653 training loss = 2.039
2023-04-27 04:47:12,157 - INFO - Epoch 654 training loss = 2.036
2023-04-27 04:47:12,256 - INFO - Epoch 655 training loss = 2.069
2023-04-27 04:47:12,355 - INFO - Epoch 656 training loss = 2.017
2023-04-27 04:47:12,513 - INFO - Epoch 657 training loss = 2.035
2023-04-27 04:47:12,611 - INFO - Epoch 658 training loss = 2.01
2023-04-27 04:47:12,710 - INFO - Epoch 659 training loss = 2.009
2023-04-27 04:47:12,809 - INFO - Epoch 660 training loss = 2.007
2023-04-27 04:47:12,836 - INFO - Validation loss = 10.34
2023-04-27 04:47:12,935 - INFO - Epoch 661 training loss = 2.028
2023-04-27 04:47:13,034 - INFO - Epoch 662 training loss = 2.024
2023-04-27 04:47:13,135 - INFO - Epoch 663 training loss = 1.992
2023-04-27 04:47:13,234 - INFO - Epoch 664 training loss = 2.015
2023-04-27 04:47:13,333 - INFO - Epoch 665 training loss = 1.991
2023-04-27 04:47:13,432 - INFO - Epoch 666 training loss = 1.986
2023-04-27 04:47:13,531 - INFO - Epoch 667 training loss = 2.016
2023-04-27 04:47:13,630 - INFO - Epoch 668 training loss = 1.991
2023-04-27 04:47:13,729 - INFO - Epoch 669 training loss = 1.985
2023-04-27 04:47:13,828 - INFO - Epoch 670 training loss = 1.977
2023-04-27 04:47:13,855 - INFO - Validation loss = 10.36
2023-04-27 04:47:13,954 - INFO - Epoch 671 training loss = 1.971
2023-04-27 04:47:14,053 - INFO - Epoch 672 training loss = 2.006
2023-04-27 04:47:14,153 - INFO - Epoch 673 training loss = 1.963
2023-04-27 04:47:14,252 - INFO - Epoch 674 training loss = 1.975
2023-04-27 04:47:14,351 - INFO - Epoch 675 training loss = 1.973
2023-04-27 04:47:14,450 - INFO - Epoch 676 training loss = 1.967
2023-04-27 04:47:14,548 - INFO - Epoch 677 training loss = 1.959
2023-04-27 04:47:14,647 - INFO - Epoch 678 training loss = 1.967
2023-04-27 04:47:14,746 - INFO - Epoch 679 training loss = 1.962
2023-04-27 04:47:14,845 - INFO - Epoch 680 training loss = 1.97
2023-04-27 04:47:14,872 - INFO - Validation loss = 10.33
2023-04-27 04:47:14,971 - INFO - Epoch 681 training loss = 1.943
2023-04-27 04:47:15,070 - INFO - Epoch 682 training loss = 1.948
2023-04-27 04:47:15,170 - INFO - Epoch 683 training loss = 1.965
2023-04-27 04:47:15,270 - INFO - Epoch 684 training loss = 1.961
2023-04-27 04:47:15,369 - INFO - Epoch 685 training loss = 1.932
2023-04-27 04:47:15,468 - INFO - Epoch 686 training loss = 1.933
2023-04-27 04:47:15,567 - INFO - Epoch 687 training loss = 1.938
2023-04-27 04:47:15,665 - INFO - Epoch 688 training loss = 1.925
2023-04-27 04:47:15,823 - INFO - Epoch 689 training loss = 1.944
2023-04-27 04:47:15,922 - INFO - Epoch 690 training loss = 1.935
2023-04-27 04:47:15,949 - INFO - Validation loss = 10.36
2023-04-27 04:47:16,048 - INFO - Epoch 691 training loss = 1.925
2023-04-27 04:47:16,148 - INFO - Epoch 692 training loss = 1.922
2023-04-27 04:47:16,247 - INFO - Epoch 693 training loss = 1.91
2023-04-27 04:47:16,346 - INFO - Epoch 694 training loss = 1.939
2023-04-27 04:47:16,444 - INFO - Epoch 695 training loss = 1.908
2023-04-27 04:47:16,543 - INFO - Epoch 696 training loss = 1.903
2023-04-27 04:47:16,642 - INFO - Epoch 697 training loss = 1.904
2023-04-27 04:47:16,741 - INFO - Epoch 698 training loss = 1.911
2023-04-27 04:47:16,840 - INFO - Epoch 699 training loss = 1.896
2023-04-27 04:47:16,938 - INFO - Epoch 700 training loss =  1.9
2023-04-27 04:47:16,966 - INFO - Validation loss = 10.48
2023-04-27 04:47:17,065 - INFO - Epoch 701 training loss = 1.902
2023-04-27 04:47:17,165 - INFO - Epoch 702 training loss = 1.902
2023-04-27 04:47:17,264 - INFO - Epoch 703 training loss = 1.886
2023-04-27 04:47:17,363 - INFO - Epoch 704 training loss = 1.899
2023-04-27 04:47:17,462 - INFO - Epoch 705 training loss = 1.882
2023-04-27 04:47:17,561 - INFO - Epoch 706 training loss = 1.887
2023-04-27 04:47:17,660 - INFO - Epoch 707 training loss = 1.874
2023-04-27 04:47:17,759 - INFO - Epoch 708 training loss = 1.881
2023-04-27 04:47:17,858 - INFO - Epoch 709 training loss = 1.874
2023-04-27 04:47:17,956 - INFO - Epoch 710 training loss = 1.883
2023-04-27 04:47:17,984 - INFO - Validation loss = 10.43
2023-04-27 04:47:18,083 - INFO - Epoch 711 training loss = 1.875
2023-04-27 04:47:18,182 - INFO - Epoch 712 training loss = 1.877
2023-04-27 04:47:18,281 - INFO - Epoch 713 training loss = 1.859
2023-04-27 04:47:18,380 - INFO - Epoch 714 training loss = 1.864
2023-04-27 04:47:18,478 - INFO - Epoch 715 training loss = 1.868
2023-04-27 04:47:18,577 - INFO - Epoch 716 training loss = 1.856
2023-04-27 04:47:18,676 - INFO - Epoch 717 training loss = 1.864
2023-04-27 04:47:18,775 - INFO - Epoch 718 training loss = 1.866
2023-04-27 04:47:18,874 - INFO - Epoch 719 training loss = 1.841
2023-04-27 04:47:19,032 - INFO - Epoch 720 training loss = 1.846
2023-04-27 04:47:19,059 - INFO - Validation loss = 10.42
2023-04-27 04:47:19,158 - INFO - Epoch 721 training loss = 1.854
2023-04-27 04:47:19,258 - INFO - Epoch 722 training loss = 1.832
2023-04-27 04:47:19,357 - INFO - Epoch 723 training loss = 1.835
2023-04-27 04:47:19,456 - INFO - Epoch 724 training loss = 1.839
2023-04-27 04:47:19,554 - INFO - Epoch 725 training loss = 1.842
2023-04-27 04:47:19,653 - INFO - Epoch 726 training loss = 1.833
2023-04-27 04:47:19,752 - INFO - Epoch 727 training loss = 1.84
2023-04-27 04:47:19,851 - INFO - Epoch 728 training loss = 1.845
2023-04-27 04:47:19,950 - INFO - Epoch 729 training loss = 1.821
2023-04-27 04:47:20,049 - INFO - Epoch 730 training loss = 1.825
2023-04-27 04:47:20,076 - INFO - Validation loss = 10.43
2023-04-27 04:47:20,175 - INFO - Epoch 731 training loss = 1.835
2023-04-27 04:47:20,274 - INFO - Epoch 732 training loss = 1.819
2023-04-27 04:47:20,373 - INFO - Epoch 733 training loss = 1.823
2023-04-27 04:47:20,472 - INFO - Epoch 734 training loss = 1.826
2023-04-27 04:47:20,571 - INFO - Epoch 735 training loss = 1.817
2023-04-27 04:47:20,670 - INFO - Epoch 736 training loss = 1.814
2023-04-27 04:47:20,769 - INFO - Epoch 737 training loss = 1.809
2023-04-27 04:47:20,868 - INFO - Epoch 738 training loss = 1.804
2023-04-27 04:47:20,966 - INFO - Epoch 739 training loss = 1.805
2023-04-27 04:47:21,065 - INFO - Epoch 740 training loss = 1.808
2023-04-27 04:47:21,093 - INFO - Validation loss = 10.44
2023-04-27 04:47:21,192 - INFO - Epoch 741 training loss =  1.8
2023-04-27 04:47:21,292 - INFO - Epoch 742 training loss = 1.802
2023-04-27 04:47:21,392 - INFO - Epoch 743 training loss = 1.792
2023-04-27 04:47:21,492 - INFO - Epoch 744 training loss = 1.811
2023-04-27 04:47:21,591 - INFO - Epoch 745 training loss = 1.789
2023-04-27 04:47:21,690 - INFO - Epoch 746 training loss = 1.792
2023-04-27 04:47:21,788 - INFO - Epoch 747 training loss = 1.795
2023-04-27 04:47:21,887 - INFO - Epoch 748 training loss = 1.789
2023-04-27 04:47:21,986 - INFO - Epoch 749 training loss = 1.786
2023-04-27 04:47:22,085 - INFO - Epoch 750 training loss = 1.798
2023-04-27 04:47:22,112 - INFO - Validation loss = 10.44
2023-04-27 04:47:22,270 - INFO - Epoch 751 training loss = 1.778
2023-04-27 04:47:22,369 - INFO - Epoch 752 training loss = 1.791
2023-04-27 04:47:22,468 - INFO - Epoch 753 training loss = 1.786
2023-04-27 04:47:22,567 - INFO - Epoch 754 training loss = 1.774
2023-04-27 04:47:22,666 - INFO - Epoch 755 training loss = 1.772
2023-04-27 04:47:22,764 - INFO - Epoch 756 training loss = 1.778
2023-04-27 04:47:22,863 - INFO - Epoch 757 training loss = 1.775
2023-04-27 04:47:22,962 - INFO - Epoch 758 training loss = 1.771
2023-04-27 04:47:23,061 - INFO - Epoch 759 training loss = 1.774
2023-04-27 04:47:23,160 - INFO - Epoch 760 training loss = 1.768
2023-04-27 04:47:23,187 - INFO - Validation loss = 10.47
2023-04-27 04:47:23,286 - INFO - Epoch 761 training loss = 1.757
2023-04-27 04:47:23,385 - INFO - Epoch 762 training loss = 1.761
2023-04-27 04:47:23,485 - INFO - Epoch 763 training loss = 1.767
2023-04-27 04:47:23,583 - INFO - Epoch 764 training loss = 1.766
2023-04-27 04:47:23,682 - INFO - Epoch 765 training loss = 1.749
2023-04-27 04:47:23,781 - INFO - Epoch 766 training loss = 1.758
2023-04-27 04:47:23,880 - INFO - Epoch 767 training loss = 1.754
2023-04-27 04:47:23,979 - INFO - Epoch 768 training loss = 1.751
2023-04-27 04:47:24,078 - INFO - Epoch 769 training loss = 1.744
2023-04-27 04:47:24,177 - INFO - Epoch 770 training loss = 1.749
2023-04-27 04:47:24,204 - INFO - Validation loss = 10.49
2023-04-27 04:47:24,303 - INFO - Epoch 771 training loss = 1.746
2023-04-27 04:47:24,402 - INFO - Epoch 772 training loss = 1.741
2023-04-27 04:47:24,501 - INFO - Epoch 773 training loss = 1.747
2023-04-27 04:47:24,599 - INFO - Epoch 774 training loss = 1.739
2023-04-27 04:47:24,698 - INFO - Epoch 775 training loss = 1.735
2023-04-27 04:47:24,797 - INFO - Epoch 776 training loss = 1.735
2023-04-27 04:47:24,896 - INFO - Epoch 777 training loss = 1.737
2023-04-27 04:47:24,995 - INFO - Epoch 778 training loss = 1.738
2023-04-27 04:47:25,094 - INFO - Epoch 779 training loss = 1.725
2023-04-27 04:47:25,192 - INFO - Epoch 780 training loss = 1.732
2023-04-27 04:47:25,220 - INFO - Validation loss = 10.49
2023-04-27 04:47:25,319 - INFO - Epoch 781 training loss = 1.738
2023-04-27 04:47:25,418 - INFO - Epoch 782 training loss = 1.727
2023-04-27 04:47:25,575 - INFO - Epoch 783 training loss = 1.726
2023-04-27 04:47:25,674 - INFO - Epoch 784 training loss = 1.729
2023-04-27 04:47:25,773 - INFO - Epoch 785 training loss = 1.729
2023-04-27 04:47:25,871 - INFO - Epoch 786 training loss = 1.719
2023-04-27 04:47:25,970 - INFO - Epoch 787 training loss = 1.721
2023-04-27 04:47:26,069 - INFO - Epoch 788 training loss = 1.715
2023-04-27 04:47:26,168 - INFO - Epoch 789 training loss = 1.711
2023-04-27 04:47:26,268 - INFO - Epoch 790 training loss = 1.72
2023-04-27 04:47:26,295 - INFO - Validation loss = 10.53
2023-04-27 04:47:26,394 - INFO - Epoch 791 training loss = 1.718
2023-04-27 04:47:26,492 - INFO - Epoch 792 training loss = 1.713
2023-04-27 04:47:26,591 - INFO - Epoch 793 training loss = 1.712
2023-04-27 04:47:26,690 - INFO - Epoch 794 training loss = 1.704
2023-04-27 04:47:26,792 - INFO - Epoch 795 training loss = 1.704
2023-04-27 04:47:26,890 - INFO - Epoch 796 training loss = 1.708
2023-04-27 04:47:26,989 - INFO - Epoch 797 training loss = 1.709
2023-04-27 04:47:27,088 - INFO - Epoch 798 training loss =  1.7
2023-04-27 04:47:27,187 - INFO - Epoch 799 training loss = 1.702
2023-04-27 04:47:27,286 - INFO - Epoch 800 training loss = 1.698
2023-04-27 04:47:27,314 - INFO - Validation loss = 10.5
2023-04-27 04:47:27,412 - INFO - Epoch 801 training loss = 1.696
2023-04-27 04:47:27,511 - INFO - Epoch 802 training loss = 1.695
2023-04-27 04:47:27,610 - INFO - Epoch 803 training loss = 1.698
2023-04-27 04:47:27,709 - INFO - Epoch 804 training loss = 1.695
2023-04-27 04:47:27,808 - INFO - Epoch 805 training loss = 1.695
2023-04-27 04:47:27,906 - INFO - Epoch 806 training loss = 1.688
2023-04-27 04:47:28,005 - INFO - Epoch 807 training loss = 1.689
2023-04-27 04:47:28,104 - INFO - Epoch 808 training loss = 1.689
2023-04-27 04:47:28,203 - INFO - Epoch 809 training loss = 1.686
2023-04-27 04:47:28,302 - INFO - Epoch 810 training loss = 1.688
2023-04-27 04:47:28,329 - INFO - Validation loss = 10.49
2023-04-27 04:47:28,429 - INFO - Epoch 811 training loss = 1.689
2023-04-27 04:47:28,527 - INFO - Epoch 812 training loss = 1.682
2023-04-27 04:47:28,627 - INFO - Epoch 813 training loss = 1.677
2023-04-27 04:47:28,784 - INFO - Epoch 814 training loss = 1.681
2023-04-27 04:47:28,883 - INFO - Epoch 815 training loss = 1.678
2023-04-27 04:47:28,982 - INFO - Epoch 816 training loss = 1.674
2023-04-27 04:47:29,081 - INFO - Epoch 817 training loss = 1.677
2023-04-27 04:47:29,180 - INFO - Epoch 818 training loss = 1.676
2023-04-27 04:47:29,279 - INFO - Epoch 819 training loss = 1.67
2023-04-27 04:47:29,378 - INFO - Epoch 820 training loss = 1.671
2023-04-27 04:47:29,405 - INFO - Validation loss = 10.52
2023-04-27 04:47:29,504 - INFO - Epoch 821 training loss = 1.668
2023-04-27 04:47:29,603 - INFO - Epoch 822 training loss = 1.671
2023-04-27 04:47:29,702 - INFO - Epoch 823 training loss = 1.669
2023-04-27 04:47:29,801 - INFO - Epoch 824 training loss = 1.668
2023-04-27 04:47:29,899 - INFO - Epoch 825 training loss = 1.667
2023-04-27 04:47:29,998 - INFO - Epoch 826 training loss = 1.665
2023-04-27 04:47:30,097 - INFO - Epoch 827 training loss = 1.663
2023-04-27 04:47:30,197 - INFO - Epoch 828 training loss = 1.664
2023-04-27 04:47:30,295 - INFO - Epoch 829 training loss = 1.659
2023-04-27 04:47:30,394 - INFO - Epoch 830 training loss = 1.662
2023-04-27 04:47:30,454 - INFO - Validation loss = 10.55
2023-04-27 04:47:30,552 - INFO - Epoch 831 training loss = 1.664
2023-04-27 04:47:30,651 - INFO - Epoch 832 training loss = 1.66
2023-04-27 04:47:30,750 - INFO - Epoch 833 training loss = 1.657
2023-04-27 04:47:30,849 - INFO - Epoch 834 training loss = 1.656
2023-04-27 04:47:30,948 - INFO - Epoch 835 training loss = 1.657
2023-04-27 04:47:31,046 - INFO - Epoch 836 training loss = 1.656
2023-04-27 04:47:31,145 - INFO - Epoch 837 training loss = 1.652
2023-04-27 04:47:31,245 - INFO - Epoch 838 training loss = 1.652
2023-04-27 04:47:31,344 - INFO - Epoch 839 training loss = 1.651
2023-04-27 04:47:31,443 - INFO - Epoch 840 training loss = 1.648
2023-04-27 04:47:31,470 - INFO - Validation loss = 10.56
2023-04-27 04:47:31,569 - INFO - Epoch 841 training loss = 1.647
2023-04-27 04:47:31,668 - INFO - Epoch 842 training loss = 1.648
2023-04-27 04:47:31,766 - INFO - Epoch 843 training loss = 1.648
2023-04-27 04:47:31,865 - INFO - Epoch 844 training loss = 1.646
2023-04-27 04:47:31,964 - INFO - Epoch 845 training loss = 1.647
2023-04-27 04:47:32,122 - INFO - Epoch 846 training loss = 1.642
2023-04-27 04:47:32,221 - INFO - Epoch 847 training loss = 1.643
2023-04-27 04:47:32,319 - INFO - Epoch 848 training loss = 1.64
2023-04-27 04:47:32,418 - INFO - Epoch 849 training loss = 1.642
2023-04-27 04:47:32,517 - INFO - Epoch 850 training loss = 1.641
2023-04-27 04:47:32,544 - INFO - Validation loss = 10.57
2023-04-27 04:47:32,643 - INFO - Epoch 851 training loss = 1.636
2023-04-27 04:47:32,742 - INFO - Epoch 852 training loss = 1.637
2023-04-27 04:47:32,840 - INFO - Epoch 853 training loss = 1.633
2023-04-27 04:47:32,939 - INFO - Epoch 854 training loss = 1.637
2023-04-27 04:47:33,038 - INFO - Epoch 855 training loss = 1.632
2023-04-27 04:47:33,137 - INFO - Epoch 856 training loss = 1.634
2023-04-27 04:47:33,236 - INFO - Epoch 857 training loss = 1.63
2023-04-27 04:47:33,335 - INFO - Epoch 858 training loss = 1.63
2023-04-27 04:47:33,434 - INFO - Epoch 859 training loss = 1.628
2023-04-27 04:47:33,532 - INFO - Epoch 860 training loss = 1.63
2023-04-27 04:47:33,560 - INFO - Validation loss = 10.57
2023-04-27 04:47:33,659 - INFO - Epoch 861 training loss = 1.631
2023-04-27 04:47:33,758 - INFO - Epoch 862 training loss = 1.628
2023-04-27 04:47:33,856 - INFO - Epoch 863 training loss = 1.624
2023-04-27 04:47:33,955 - INFO - Epoch 864 training loss = 1.624
2023-04-27 04:47:34,054 - INFO - Epoch 865 training loss = 1.625
2023-04-27 04:47:34,153 - INFO - Epoch 866 training loss = 1.624
2023-04-27 04:47:34,252 - INFO - Epoch 867 training loss = 1.624
2023-04-27 04:47:34,351 - INFO - Epoch 868 training loss = 1.623
2023-04-27 04:47:34,450 - INFO - Epoch 869 training loss = 1.621
2023-04-27 04:47:34,549 - INFO - Epoch 870 training loss = 1.621
2023-04-27 04:47:34,576 - INFO - Validation loss = 10.58
2023-04-27 04:47:34,675 - INFO - Epoch 871 training loss = 1.62
2023-04-27 04:47:34,774 - INFO - Epoch 872 training loss = 1.618
2023-04-27 04:47:34,872 - INFO - Epoch 873 training loss = 1.617
2023-04-27 04:47:34,971 - INFO - Epoch 874 training loss = 1.619
2023-04-27 04:47:35,070 - INFO - Epoch 875 training loss = 1.613
2023-04-27 04:47:35,169 - INFO - Epoch 876 training loss = 1.616
2023-04-27 04:47:35,327 - INFO - Epoch 877 training loss = 1.618
2023-04-27 04:47:35,426 - INFO - Epoch 878 training loss = 1.613
2023-04-27 04:47:35,524 - INFO - Epoch 879 training loss = 1.613
2023-04-27 04:47:35,623 - INFO - Epoch 880 training loss = 1.613
2023-04-27 04:47:35,651 - INFO - Validation loss = 10.58
2023-04-27 04:47:35,749 - INFO - Epoch 881 training loss = 1.61
2023-04-27 04:47:35,848 - INFO - Epoch 882 training loss = 1.613
2023-04-27 04:47:35,947 - INFO - Epoch 883 training loss = 1.61
2023-04-27 04:47:36,046 - INFO - Epoch 884 training loss = 1.609
2023-04-27 04:47:36,145 - INFO - Epoch 885 training loss = 1.611
2023-04-27 04:47:36,245 - INFO - Epoch 886 training loss = 1.608
2023-04-27 04:47:36,344 - INFO - Epoch 887 training loss = 1.606
2023-04-27 04:47:36,444 - INFO - Epoch 888 training loss = 1.607
2023-04-27 04:47:36,543 - INFO - Epoch 889 training loss = 1.606
2023-04-27 04:47:36,642 - INFO - Epoch 890 training loss = 1.606
2023-04-27 04:47:36,669 - INFO - Validation loss = 10.6
2023-04-27 04:47:36,768 - INFO - Epoch 891 training loss = 1.602
2023-04-27 04:47:36,867 - INFO - Epoch 892 training loss = 1.606
2023-04-27 04:47:36,966 - INFO - Epoch 893 training loss = 1.603
2023-04-27 04:47:37,065 - INFO - Epoch 894 training loss = 1.602
2023-04-27 04:47:37,164 - INFO - Epoch 895 training loss = 1.599
2023-04-27 04:47:37,264 - INFO - Epoch 896 training loss = 1.598
2023-04-27 04:47:37,362 - INFO - Epoch 897 training loss = 1.601
2023-04-27 04:47:37,461 - INFO - Epoch 898 training loss = 1.601
2023-04-27 04:47:37,560 - INFO - Epoch 899 training loss =  1.6
2023-04-27 04:47:37,659 - INFO - Epoch 900 training loss = 1.599
2023-04-27 04:47:37,686 - INFO - Validation loss = 10.6
2023-04-27 04:47:37,785 - INFO - Epoch 901 training loss = 1.597
2023-04-27 04:47:37,884 - INFO - Epoch 902 training loss = 1.597
2023-04-27 04:47:37,983 - INFO - Epoch 903 training loss = 1.597
2023-04-27 04:47:38,082 - INFO - Epoch 904 training loss = 1.596
2023-04-27 04:47:38,181 - INFO - Epoch 905 training loss = 1.597
2023-04-27 04:47:38,280 - INFO - Epoch 906 training loss = 1.595
2023-04-27 04:47:38,379 - INFO - Epoch 907 training loss = 1.594
2023-04-27 04:47:38,537 - INFO - Epoch 908 training loss = 1.59
2023-04-27 04:47:38,636 - INFO - Epoch 909 training loss = 1.595
2023-04-27 04:47:38,735 - INFO - Epoch 910 training loss = 1.593
2023-04-27 04:47:38,762 - INFO - Validation loss = 10.61
2023-04-27 04:47:38,861 - INFO - Epoch 911 training loss = 1.593
2023-04-27 04:47:38,960 - INFO - Epoch 912 training loss = 1.592
2023-04-27 04:47:39,059 - INFO - Epoch 913 training loss = 1.591
2023-04-27 04:47:39,158 - INFO - Epoch 914 training loss = 1.59
2023-04-27 04:47:39,258 - INFO - Epoch 915 training loss = 1.59
2023-04-27 04:47:39,357 - INFO - Epoch 916 training loss = 1.588
2023-04-27 04:47:39,455 - INFO - Epoch 917 training loss = 1.586
2023-04-27 04:47:39,553 - INFO - Epoch 918 training loss = 1.589
2023-04-27 04:47:39,651 - INFO - Epoch 919 training loss = 1.588
2023-04-27 04:47:39,750 - INFO - Epoch 920 training loss = 1.587
2023-04-27 04:47:39,776 - INFO - Validation loss = 10.61
2023-04-27 04:47:39,875 - INFO - Epoch 921 training loss = 1.587
2023-04-27 04:47:39,973 - INFO - Epoch 922 training loss = 1.586
2023-04-27 04:47:40,071 - INFO - Epoch 923 training loss = 1.588
2023-04-27 04:47:40,169 - INFO - Epoch 924 training loss = 1.585
2023-04-27 04:47:40,268 - INFO - Epoch 925 training loss = 1.585
2023-04-27 04:47:40,366 - INFO - Epoch 926 training loss = 1.584
2023-04-27 04:47:40,464 - INFO - Epoch 927 training loss = 1.585
2023-04-27 04:47:40,563 - INFO - Epoch 928 training loss = 1.584
2023-04-27 04:47:40,661 - INFO - Epoch 929 training loss = 1.582
2023-04-27 04:47:40,759 - INFO - Epoch 930 training loss = 1.583
2023-04-27 04:47:40,786 - INFO - Validation loss = 10.61
2023-04-27 04:47:40,884 - INFO - Epoch 931 training loss = 1.583
2023-04-27 04:47:40,982 - INFO - Epoch 932 training loss = 1.583
2023-04-27 04:47:41,080 - INFO - Epoch 933 training loss = 1.58
2023-04-27 04:47:41,179 - INFO - Epoch 934 training loss = 1.582
2023-04-27 04:47:41,277 - INFO - Epoch 935 training loss = 1.582
2023-04-27 04:47:41,375 - INFO - Epoch 936 training loss = 1.579
2023-04-27 04:47:41,473 - INFO - Epoch 937 training loss = 1.58
2023-04-27 04:47:41,571 - INFO - Epoch 938 training loss = 1.58
2023-04-27 04:47:41,671 - INFO - Epoch 939 training loss = 1.577
2023-04-27 04:47:41,826 - INFO - Epoch 940 training loss = 1.579
2023-04-27 04:47:41,853 - INFO - Validation loss = 10.61
2023-04-27 04:47:41,951 - INFO - Epoch 941 training loss = 1.578
2023-04-27 04:47:42,050 - INFO - Epoch 942 training loss = 1.576
2023-04-27 04:47:42,148 - INFO - Epoch 943 training loss = 1.577
2023-04-27 04:47:42,247 - INFO - Epoch 944 training loss = 1.577
2023-04-27 04:47:42,345 - INFO - Epoch 945 training loss = 1.575
2023-04-27 04:47:42,443 - INFO - Epoch 946 training loss = 1.573
2023-04-27 04:47:42,541 - INFO - Epoch 947 training loss = 1.576
2023-04-27 04:47:42,640 - INFO - Epoch 948 training loss = 1.576
2023-04-27 04:47:42,739 - INFO - Epoch 949 training loss = 1.577
2023-04-27 04:47:42,837 - INFO - Epoch 950 training loss = 1.576
2023-04-27 04:47:42,864 - INFO - Validation loss = 10.62
2023-04-27 04:47:42,962 - INFO - Epoch 951 training loss = 1.574
2023-04-27 04:47:43,060 - INFO - Epoch 952 training loss = 1.575
2023-04-27 04:47:43,159 - INFO - Epoch 953 training loss = 1.575
2023-04-27 04:47:43,257 - INFO - Epoch 954 training loss = 1.575
2023-04-27 04:47:43,355 - INFO - Epoch 955 training loss = 1.573
2023-04-27 04:47:43,453 - INFO - Epoch 956 training loss = 1.574
2023-04-27 04:47:43,551 - INFO - Epoch 957 training loss = 1.571
2023-04-27 04:47:43,651 - INFO - Epoch 958 training loss = 1.572
2023-04-27 04:47:43,749 - INFO - Epoch 959 training loss = 1.574
2023-04-27 04:47:43,847 - INFO - Epoch 960 training loss = 1.571
2023-04-27 04:47:43,874 - INFO - Validation loss = 10.61
2023-04-27 04:47:43,972 - INFO - Epoch 961 training loss = 1.571
2023-04-27 04:47:44,070 - INFO - Epoch 962 training loss = 1.571
2023-04-27 04:47:44,168 - INFO - Epoch 963 training loss = 1.571
2023-04-27 04:47:44,266 - INFO - Epoch 964 training loss = 1.571
2023-04-27 04:47:44,364 - INFO - Epoch 965 training loss = 1.572
2023-04-27 04:47:44,462 - INFO - Epoch 966 training loss = 1.571
2023-04-27 04:47:44,561 - INFO - Epoch 967 training loss = 1.572
2023-04-27 04:47:44,660 - INFO - Epoch 968 training loss = 1.568
2023-04-27 04:47:44,758 - INFO - Epoch 969 training loss = 1.57
2023-04-27 04:47:44,856 - INFO - Epoch 970 training loss = 1.573
2023-04-27 04:47:44,883 - INFO - Validation loss = 10.62
2023-04-27 04:47:45,038 - INFO - Epoch 971 training loss = 1.57
2023-04-27 04:47:45,137 - INFO - Epoch 972 training loss = 1.57
2023-04-27 04:47:45,235 - INFO - Epoch 973 training loss = 1.57
2023-04-27 04:47:45,334 - INFO - Epoch 974 training loss = 1.57
2023-04-27 04:47:45,432 - INFO - Epoch 975 training loss = 1.569
2023-04-27 04:47:45,530 - INFO - Epoch 976 training loss = 1.57
2023-04-27 04:47:45,629 - INFO - Epoch 977 training loss = 1.569
2023-04-27 04:47:45,727 - INFO - Epoch 978 training loss = 1.57
2023-04-27 04:47:45,824 - INFO - Epoch 979 training loss = 1.569
2023-04-27 04:47:45,921 - INFO - Epoch 980 training loss = 1.568
2023-04-27 04:47:45,948 - INFO - Validation loss = 10.62
2023-04-27 04:47:46,046 - INFO - Epoch 981 training loss = 1.57
2023-04-27 04:47:46,143 - INFO - Epoch 982 training loss = 1.567
2023-04-27 04:47:46,241 - INFO - Epoch 983 training loss = 1.569
2023-04-27 04:47:46,339 - INFO - Epoch 984 training loss = 1.566
2023-04-27 04:47:46,437 - INFO - Epoch 985 training loss = 1.567
2023-04-27 04:47:46,535 - INFO - Epoch 986 training loss = 1.568
2023-04-27 04:47:46,633 - INFO - Epoch 987 training loss = 1.569
2023-04-27 04:47:46,733 - INFO - Epoch 988 training loss = 1.567
2023-04-27 04:47:46,831 - INFO - Epoch 989 training loss = 1.569
2023-04-27 04:47:46,929 - INFO - Epoch 990 training loss = 1.567
2023-04-27 04:47:46,956 - INFO - Validation loss = 10.62
2023-04-27 04:47:47,055 - INFO - Epoch 991 training loss = 1.567
2023-04-27 04:47:47,153 - INFO - Epoch 992 training loss = 1.567
2023-04-27 04:47:47,251 - INFO - Epoch 993 training loss = 1.567
2023-04-27 04:47:47,349 - INFO - Epoch 994 training loss = 1.568
2023-04-27 04:47:47,447 - INFO - Epoch 995 training loss = 1.565
2023-04-27 04:47:47,544 - INFO - Epoch 996 training loss = 1.569
2023-04-27 04:47:47,642 - INFO - Epoch 997 training loss = 1.566
2023-04-27 04:47:47,741 - INFO - Epoch 998 training loss = 1.568
2023-04-27 04:47:47,838 - INFO - Epoch 999 training loss = 1.567
2023-04-27 04:47:47,854 - INFO - Validation loss = 10.62
