2023-04-24 18:22:52,479 - INFO - Config:
Namespace(config='configs/explicit_mlp.yaml', dir='/user/delden/acoustics/spp_ai_acoustics/acousticNN/experiments/arch/no_encoding/explicitmlp', epochs=100, device='cuda', seed=0, parameter_list='configs/data.yaml')
2023-04-24 18:22:52,479 - INFO - Config:
Munch({'optimizer': Munch({'type': 'AdamW', 'kwargs': Munch({'lr': 0.001, 'weight_decay': 0.005, 'betas': [0.9, 0.95]})}), 'scheduler': Munch({'type': 'CosLR', 'kwargs': Munch({'epochs': 1000, 'initial_epochs': 101})}), 'model': Munch({'name': 'ExplicitMLP', 'input_encoding': True, 'encoding_dim': 16, 'embed_dim': 64, 'depth': 6, 'mlp_width': 256, 'num_frequencies': 200}), 'generation_frequency': 5001, 'validation_frequency': 10, 'batch_size': 128, 'epochs': 1000, 'gradient_clip': 10})
2023-04-24 18:23:05,066 - INFO - ==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ExplicitMLP                              [128, 200, 4]             --
├─GroupwiseProjection: 1-1               [128, 14, 64]             --
│    └─ModuleList: 2-1                   --                        --
│    │    └─Linear: 3-1                  [128, 4, 64]              128
│    │    └─Linear: 3-2                  [128, 5, 64]              128
│    │    └─Linear: 3-3                  [128, 5, 64]              128
├─MLP: 1-2                               [128, 256]                --
│    └─Linear: 2-2                       [128, 256]                229,632
│    └─ReLU: 2-3                         [128, 256]                --
│    └─Dropout: 2-4                      [128, 256]                --
│    └─Linear: 2-5                       [128, 256]                65,792
│    └─ReLU: 2-6                         [128, 256]                --
│    └─Dropout: 2-7                      [128, 256]                --
│    └─Linear: 2-8                       [128, 256]                65,792
│    └─ReLU: 2-9                         [128, 256]                --
│    └─Dropout: 2-10                     [128, 256]                --
│    └─Linear: 2-11                      [128, 256]                65,792
│    └─ReLU: 2-12                        [128, 256]                --
│    └─Dropout: 2-13                     [128, 256]                --
│    └─Linear: 2-14                      [128, 256]                65,792
│    └─ReLU: 2-15                        [128, 256]                --
│    └─Dropout: 2-16                     [128, 256]                --
│    └─Linear: 2-17                      [128, 256]                65,792
│    └─Dropout: 2-18                     [128, 256]                --
├─Linear: 1-3                            [128, 800]                205,600
==========================================================================================
Total params: 764,576
Trainable params: 764,576
Non-trainable params: 0
Total mult-adds (M): 97.87
==========================================================================================
Input size (MB): 0.11
Forward/backward pass size (MB): 3.31
Params size (MB): 3.06
Estimated Total Size (MB): 6.48
==========================================================================================
2023-04-24 18:23:05,165 - INFO - Epoch 0 training loss = 3.407e+03
2023-04-24 18:23:05,197 - INFO - Validation loss = 3.408e+03
2023-04-24 18:23:05,197 - INFO - best model
2023-04-24 18:23:05,308 - INFO - Epoch 1 training loss = 3.403e+03
2023-04-24 18:23:05,405 - INFO - Epoch 2 training loss = 3.39e+03
2023-04-24 18:23:05,501 - INFO - Epoch 3 training loss = 3.325e+03
2023-04-24 18:23:05,598 - INFO - Epoch 4 training loss = 3.132e+03
2023-04-24 18:23:05,694 - INFO - Epoch 5 training loss = 2.555e+03
2023-04-24 18:23:05,791 - INFO - Epoch 6 training loss = 1.442e+03
2023-04-24 18:23:05,887 - INFO - Epoch 7 training loss = 571.9
2023-04-24 18:23:05,984 - INFO - Epoch 8 training loss = 264.1
2023-04-24 18:23:06,080 - INFO - Epoch 9 training loss = 222.2
2023-04-24 18:23:06,177 - INFO - Epoch 10 training loss = 213.0
2023-04-24 18:23:06,208 - INFO - Validation loss = 204.3
2023-04-24 18:23:06,208 - INFO - best model
2023-04-24 18:23:06,320 - INFO - Epoch 11 training loss = 202.2
2023-04-24 18:23:06,417 - INFO - Epoch 12 training loss = 187.5
2023-04-24 18:23:06,513 - INFO - Epoch 13 training loss = 170.3
2023-04-24 18:23:06,610 - INFO - Epoch 14 training loss = 156.2
2023-04-24 18:23:06,706 - INFO - Epoch 15 training loss = 141.5
2023-04-24 18:23:06,853 - INFO - Epoch 16 training loss = 130.9
2023-04-24 18:23:06,949 - INFO - Epoch 17 training loss = 120.9
2023-04-24 18:23:07,046 - INFO - Epoch 18 training loss = 112.6
2023-04-24 18:23:07,142 - INFO - Epoch 19 training loss = 104.8
2023-04-24 18:23:07,239 - INFO - Epoch 20 training loss = 96.48
2023-04-24 18:23:07,270 - INFO - Validation loss = 98.59
2023-04-24 18:23:07,270 - INFO - best model
2023-04-24 18:23:07,382 - INFO - Epoch 21 training loss = 89.8
2023-04-24 18:23:07,479 - INFO - Epoch 22 training loss = 82.74
2023-04-24 18:23:07,575 - INFO - Epoch 23 training loss = 75.98
2023-04-24 18:23:07,672 - INFO - Epoch 24 training loss = 70.51
2023-04-24 18:23:07,768 - INFO - Epoch 25 training loss = 66.93
2023-04-24 18:23:07,865 - INFO - Epoch 26 training loss = 63.64
2023-04-24 18:23:07,962 - INFO - Epoch 27 training loss = 60.58
2023-04-24 18:23:08,058 - INFO - Epoch 28 training loss = 57.88
2023-04-24 18:23:08,154 - INFO - Epoch 29 training loss = 55.39
2023-04-24 18:23:08,251 - INFO - Epoch 30 training loss = 54.01
2023-04-24 18:23:08,282 - INFO - Validation loss = 54.86
2023-04-24 18:23:08,282 - INFO - best model
2023-04-24 18:23:08,394 - INFO - Epoch 31 training loss = 51.36
2023-04-24 18:23:08,490 - INFO - Epoch 32 training loss = 50.31
2023-04-24 18:23:08,586 - INFO - Epoch 33 training loss = 48.17
2023-04-24 18:23:08,683 - INFO - Epoch 34 training loss = 46.82
2023-04-24 18:23:08,779 - INFO - Epoch 35 training loss = 45.31
2023-04-24 18:23:08,875 - INFO - Epoch 36 training loss = 44.26
2023-04-24 18:23:08,971 - INFO - Epoch 37 training loss = 43.14
2023-04-24 18:23:09,067 - INFO - Epoch 38 training loss = 42.23
2023-04-24 18:23:09,163 - INFO - Epoch 39 training loss = 39.76
2023-04-24 18:23:09,259 - INFO - Epoch 40 training loss = 39.16
2023-04-24 18:23:09,291 - INFO - Validation loss = 41.24
2023-04-24 18:23:09,291 - INFO - best model
2023-04-24 18:23:09,414 - INFO - Epoch 41 training loss = 38.03
2023-04-24 18:23:09,510 - INFO - Epoch 42 training loss = 37.15
2023-04-24 18:23:09,656 - INFO - Epoch 43 training loss = 36.15
2023-04-24 18:23:09,752 - INFO - Epoch 44 training loss = 35.58
2023-04-24 18:23:09,848 - INFO - Epoch 45 training loss = 34.24
2023-04-24 18:23:09,944 - INFO - Epoch 46 training loss = 33.83
2023-04-24 18:23:10,040 - INFO - Epoch 47 training loss = 32.78
2023-04-24 18:23:10,137 - INFO - Epoch 48 training loss = 32.89
2023-04-24 18:23:10,233 - INFO - Epoch 49 training loss = 31.17
2023-04-24 18:23:10,330 - INFO - Epoch 50 training loss = 30.36
2023-04-24 18:23:10,360 - INFO - Validation loss = 32.38
2023-04-24 18:23:10,360 - INFO - best model
2023-04-24 18:23:10,472 - INFO - Epoch 51 training loss = 30.02
2023-04-24 18:23:10,568 - INFO - Epoch 52 training loss = 29.8
2023-04-24 18:23:10,664 - INFO - Epoch 53 training loss = 28.9
2023-04-24 18:23:10,761 - INFO - Epoch 54 training loss = 28.3
2023-04-24 18:23:10,857 - INFO - Epoch 55 training loss = 28.68
2023-04-24 18:23:10,953 - INFO - Epoch 56 training loss = 27.72
2023-04-24 18:23:11,050 - INFO - Epoch 57 training loss = 27.18
2023-04-24 18:23:11,146 - INFO - Epoch 58 training loss = 27.19
2023-04-24 18:23:11,243 - INFO - Epoch 59 training loss = 25.41
2023-04-24 18:23:11,341 - INFO - Epoch 60 training loss = 25.48
2023-04-24 18:23:11,373 - INFO - Validation loss = 27.87
2023-04-24 18:23:11,373 - INFO - best model
2023-04-24 18:23:11,486 - INFO - Epoch 61 training loss = 24.83
2023-04-24 18:23:11,582 - INFO - Epoch 62 training loss = 24.88
2023-04-24 18:23:11,679 - INFO - Epoch 63 training loss = 24.91
2023-04-24 18:23:11,775 - INFO - Epoch 64 training loss = 24.16
2023-04-24 18:23:11,871 - INFO - Epoch 65 training loss = 23.66
2023-04-24 18:23:11,968 - INFO - Epoch 66 training loss = 23.96
2023-04-24 18:23:12,064 - INFO - Epoch 67 training loss = 22.48
2023-04-24 18:23:12,160 - INFO - Epoch 68 training loss = 23.58
2023-04-24 18:23:12,256 - INFO - Epoch 69 training loss = 22.16
2023-04-24 18:23:12,402 - INFO - Epoch 70 training loss = 22.27
2023-04-24 18:23:12,432 - INFO - Validation loss = 24.95
2023-04-24 18:23:12,432 - INFO - best model
2023-04-24 18:23:12,539 - INFO - Epoch 71 training loss = 21.65
2023-04-24 18:23:12,635 - INFO - Epoch 72 training loss = 21.65
2023-04-24 18:23:12,730 - INFO - Epoch 73 training loss = 22.21
2023-04-24 18:23:12,826 - INFO - Epoch 74 training loss = 20.94
2023-04-24 18:23:12,922 - INFO - Epoch 75 training loss = 20.86
2023-04-24 18:23:13,017 - INFO - Epoch 76 training loss = 20.61
2023-04-24 18:23:13,113 - INFO - Epoch 77 training loss = 20.22
2023-04-24 18:23:13,209 - INFO - Epoch 78 training loss = 20.28
2023-04-24 18:23:13,305 - INFO - Epoch 79 training loss = 19.97
2023-04-24 18:23:13,401 - INFO - Epoch 80 training loss = 19.8
2023-04-24 18:23:13,432 - INFO - Validation loss = 22.24
2023-04-24 18:23:13,432 - INFO - best model
2023-04-24 18:23:13,542 - INFO - Epoch 81 training loss = 19.41
2023-04-24 18:23:13,638 - INFO - Epoch 82 training loss = 19.82
2023-04-24 18:23:13,734 - INFO - Epoch 83 training loss = 19.54
2023-04-24 18:23:13,829 - INFO - Epoch 84 training loss = 19.74
2023-04-24 18:23:13,925 - INFO - Epoch 85 training loss = 18.66
2023-04-24 18:23:14,020 - INFO - Epoch 86 training loss = 18.6
2023-04-24 18:23:14,116 - INFO - Epoch 87 training loss = 18.32
2023-04-24 18:23:14,212 - INFO - Epoch 88 training loss = 18.05
2023-04-24 18:23:14,309 - INFO - Epoch 89 training loss = 18.08
2023-04-24 18:23:14,404 - INFO - Epoch 90 training loss = 17.93
2023-04-24 18:23:14,434 - INFO - Validation loss = 19.76
2023-04-24 18:23:14,435 - INFO - best model
2023-04-24 18:23:14,545 - INFO - Epoch 91 training loss = 18.2
2023-04-24 18:23:14,641 - INFO - Epoch 92 training loss = 18.39
2023-04-24 18:23:14,736 - INFO - Epoch 93 training loss = 17.35
2023-04-24 18:23:14,832 - INFO - Epoch 94 training loss = 17.03
2023-04-24 18:23:14,928 - INFO - Epoch 95 training loss = 17.07
2023-04-24 18:23:15,023 - INFO - Epoch 96 training loss = 17.53
2023-04-24 18:23:15,119 - INFO - Epoch 97 training loss = 15.9
2023-04-24 18:23:15,264 - INFO - Epoch 98 training loss = 17.06
2023-04-24 18:23:15,361 - INFO - Epoch 99 training loss = 16.31
2023-04-24 18:23:15,456 - INFO - Epoch 100 training loss = 16.58
2023-04-24 18:23:15,483 - INFO - Validation loss = 18.91
2023-04-24 18:23:15,483 - INFO - best model
2023-04-24 18:23:15,590 - INFO - Epoch 101 training loss = 16.41
2023-04-24 18:23:15,685 - INFO - Epoch 102 training loss = 18.54
2023-04-24 18:23:15,781 - INFO - Epoch 103 training loss = 14.96
2023-04-24 18:23:15,876 - INFO - Epoch 104 training loss = 15.71
2023-04-24 18:23:15,972 - INFO - Epoch 105 training loss = 15.12
2023-04-24 18:23:16,068 - INFO - Epoch 106 training loss = 16.36
2023-04-24 18:23:16,164 - INFO - Epoch 107 training loss = 14.25
2023-04-24 18:23:16,259 - INFO - Epoch 108 training loss = 14.99
2023-04-24 18:23:16,356 - INFO - Epoch 109 training loss = 14.55
2023-04-24 18:23:16,452 - INFO - Epoch 110 training loss = 14.26
2023-04-24 18:23:16,482 - INFO - Validation loss = 17.77
2023-04-24 18:23:16,482 - INFO - best model
2023-04-24 18:23:16,590 - INFO - Epoch 111 training loss = 14.61
2023-04-24 18:23:16,686 - INFO - Epoch 112 training loss = 14.37
2023-04-24 18:23:16,781 - INFO - Epoch 113 training loss = 14.4
2023-04-24 18:23:16,877 - INFO - Epoch 114 training loss = 14.53
2023-04-24 18:23:16,973 - INFO - Epoch 115 training loss = 14.76
2023-04-24 18:23:17,069 - INFO - Epoch 116 training loss = 13.42
2023-04-24 18:23:17,164 - INFO - Epoch 117 training loss = 14.03
2023-04-24 18:23:17,260 - INFO - Epoch 118 training loss = 12.98
2023-04-24 18:23:17,357 - INFO - Epoch 119 training loss = 13.49
2023-04-24 18:23:17,452 - INFO - Epoch 120 training loss = 13.25
2023-04-24 18:23:17,482 - INFO - Validation loss = 15.69
2023-04-24 18:23:17,482 - INFO - best model
2023-04-24 18:23:17,593 - INFO - Epoch 121 training loss = 12.75
2023-04-24 18:23:17,688 - INFO - Epoch 122 training loss = 12.98
2023-04-24 18:23:17,783 - INFO - Epoch 123 training loss = 13.0
2023-04-24 18:23:17,878 - INFO - Epoch 124 training loss = 12.92
2023-04-24 18:23:18,022 - INFO - Epoch 125 training loss = 12.51
2023-04-24 18:23:18,118 - INFO - Epoch 126 training loss = 12.54
2023-04-24 18:23:18,214 - INFO - Epoch 127 training loss = 12.25
2023-04-24 18:23:18,310 - INFO - Epoch 128 training loss = 11.3
2023-04-24 18:23:18,405 - INFO - Epoch 129 training loss = 12.44
2023-04-24 18:23:18,501 - INFO - Epoch 130 training loss = 12.42
2023-04-24 18:23:18,531 - INFO - Validation loss = 18.45
2023-04-24 18:23:18,627 - INFO - Epoch 131 training loss = 11.9
2023-04-24 18:23:18,723 - INFO - Epoch 132 training loss = 11.76
2023-04-24 18:23:18,819 - INFO - Epoch 133 training loss = 12.42
2023-04-24 18:23:18,914 - INFO - Epoch 134 training loss = 10.97
2023-04-24 18:23:19,010 - INFO - Epoch 135 training loss = 12.05
2023-04-24 18:23:19,106 - INFO - Epoch 136 training loss = 11.89
2023-04-24 18:23:19,202 - INFO - Epoch 137 training loss = 11.33
2023-04-24 18:23:19,297 - INFO - Epoch 138 training loss = 10.92
2023-04-24 18:23:19,393 - INFO - Epoch 139 training loss = 10.88
2023-04-24 18:23:19,488 - INFO - Epoch 140 training loss = 10.85
2023-04-24 18:23:19,519 - INFO - Validation loss = 15.77
2023-04-24 18:23:19,615 - INFO - Epoch 141 training loss = 10.74
2023-04-24 18:23:19,711 - INFO - Epoch 142 training loss = 10.85
2023-04-24 18:23:19,806 - INFO - Epoch 143 training loss = 10.75
2023-04-24 18:23:19,902 - INFO - Epoch 144 training loss = 10.71
2023-04-24 18:23:19,997 - INFO - Epoch 145 training loss = 10.58
2023-04-24 18:23:20,093 - INFO - Epoch 146 training loss = 11.47
2023-04-24 18:23:20,189 - INFO - Epoch 147 training loss = 10.03
2023-04-24 18:23:20,285 - INFO - Epoch 148 training loss = 11.59
2023-04-24 18:23:20,381 - INFO - Epoch 149 training loss = 10.29
2023-04-24 18:23:20,476 - INFO - Epoch 150 training loss = 9.894
2023-04-24 18:23:20,503 - INFO - Validation loss = 13.74
2023-04-24 18:23:20,503 - INFO - best model
2023-04-24 18:23:20,610 - INFO - Epoch 151 training loss = 10.24
2023-04-24 18:23:20,754 - INFO - Epoch 152 training loss = 10.05
2023-04-24 18:23:20,850 - INFO - Epoch 153 training loss = 10.68
2023-04-24 18:23:20,945 - INFO - Epoch 154 training loss = 9.467
2023-04-24 18:23:21,041 - INFO - Epoch 155 training loss = 10.53
2023-04-24 18:23:21,137 - INFO - Epoch 156 training loss = 9.289
2023-04-24 18:23:21,233 - INFO - Epoch 157 training loss = 10.21
2023-04-24 18:23:21,328 - INFO - Epoch 158 training loss = 9.539
2023-04-24 18:23:21,424 - INFO - Epoch 159 training loss = 9.66
2023-04-24 18:23:21,520 - INFO - Epoch 160 training loss = 9.453
2023-04-24 18:23:21,550 - INFO - Validation loss = 13.24
2023-04-24 18:23:21,550 - INFO - best model
2023-04-24 18:23:21,662 - INFO - Epoch 161 training loss = 9.236
2023-04-24 18:23:21,758 - INFO - Epoch 162 training loss = 9.399
2023-04-24 18:23:21,854 - INFO - Epoch 163 training loss = 9.87
2023-04-24 18:23:21,950 - INFO - Epoch 164 training loss = 9.196
2023-04-24 18:23:22,045 - INFO - Epoch 165 training loss = 9.072
2023-04-24 18:23:22,141 - INFO - Epoch 166 training loss = 9.05
2023-04-24 18:23:22,237 - INFO - Epoch 167 training loss = 8.972
2023-04-24 18:23:22,332 - INFO - Epoch 168 training loss = 8.799
2023-04-24 18:23:22,428 - INFO - Epoch 169 training loss = 8.834
2023-04-24 18:23:22,524 - INFO - Epoch 170 training loss = 9.014
2023-04-24 18:23:22,554 - INFO - Validation loss = 13.62
2023-04-24 18:23:22,650 - INFO - Epoch 171 training loss = 8.962
2023-04-24 18:23:22,746 - INFO - Epoch 172 training loss = 8.833
2023-04-24 18:23:22,841 - INFO - Epoch 173 training loss = 8.519
2023-04-24 18:23:22,937 - INFO - Epoch 174 training loss = 8.535
2023-04-24 18:23:23,033 - INFO - Epoch 175 training loss = 8.498
2023-04-24 18:23:23,129 - INFO - Epoch 176 training loss = 8.789
2023-04-24 18:23:23,224 - INFO - Epoch 177 training loss = 8.61
2023-04-24 18:23:23,320 - INFO - Epoch 178 training loss = 8.449
2023-04-24 18:23:23,464 - INFO - Epoch 179 training loss = 8.256
2023-04-24 18:23:23,560 - INFO - Epoch 180 training loss = 8.454
2023-04-24 18:23:23,591 - INFO - Validation loss = 14.03
2023-04-24 18:23:23,686 - INFO - Epoch 181 training loss = 8.46
2023-04-24 18:23:23,782 - INFO - Epoch 182 training loss = 7.937
2023-04-24 18:23:23,878 - INFO - Epoch 183 training loss = 8.102
2023-04-24 18:23:23,973 - INFO - Epoch 184 training loss = 8.908
2023-04-24 18:23:24,069 - INFO - Epoch 185 training loss = 7.797
2023-04-24 18:23:24,165 - INFO - Epoch 186 training loss = 8.368
2023-04-24 18:23:24,261 - INFO - Epoch 187 training loss = 8.142
2023-04-24 18:23:24,356 - INFO - Epoch 188 training loss = 8.147
2023-04-24 18:23:24,452 - INFO - Epoch 189 training loss = 7.812
2023-04-24 18:23:24,548 - INFO - Epoch 190 training loss = 8.665
2023-04-24 18:23:24,578 - INFO - Validation loss = 12.41
2023-04-24 18:23:24,578 - INFO - best model
2023-04-24 18:23:24,686 - INFO - Epoch 191 training loss = 7.579
2023-04-24 18:23:24,782 - INFO - Epoch 192 training loss = 7.831
2023-04-24 18:23:24,877 - INFO - Epoch 193 training loss = 7.63
2023-04-24 18:23:24,973 - INFO - Epoch 194 training loss = 8.243
2023-04-24 18:23:25,069 - INFO - Epoch 195 training loss = 7.464
2023-04-24 18:23:25,164 - INFO - Epoch 196 training loss = 7.82
2023-04-24 18:23:25,260 - INFO - Epoch 197 training loss = 7.537
2023-04-24 18:23:25,356 - INFO - Epoch 198 training loss = 7.546
2023-04-24 18:23:25,451 - INFO - Epoch 199 training loss = 7.638
2023-04-24 18:23:25,547 - INFO - Epoch 200 training loss = 7.414
2023-04-24 18:23:25,577 - INFO - Validation loss = 13.09
2023-04-24 18:23:25,673 - INFO - Epoch 201 training loss = 7.603
2023-04-24 18:23:25,769 - INFO - Epoch 202 training loss = 7.387
2023-04-24 18:23:25,864 - INFO - Epoch 203 training loss = 7.322
2023-04-24 18:23:25,960 - INFO - Epoch 204 training loss = 7.302
2023-04-24 18:23:26,056 - INFO - Epoch 205 training loss = 7.133
2023-04-24 18:23:26,200 - INFO - Epoch 206 training loss = 7.194
2023-04-24 18:23:26,296 - INFO - Epoch 207 training loss = 7.226
2023-04-24 18:23:26,392 - INFO - Epoch 208 training loss = 7.169
2023-04-24 18:23:26,487 - INFO - Epoch 209 training loss = 6.929
2023-04-24 18:23:26,583 - INFO - Epoch 210 training loss = 7.355
2023-04-24 18:23:26,614 - INFO - Validation loss = 12.05
2023-04-24 18:23:26,614 - INFO - best model
2023-04-24 18:23:26,725 - INFO - Epoch 211 training loss =  7.0
2023-04-24 18:23:26,820 - INFO - Epoch 212 training loss = 6.901
2023-04-24 18:23:26,916 - INFO - Epoch 213 training loss = 7.084
2023-04-24 18:23:27,011 - INFO - Epoch 214 training loss = 7.384
2023-04-24 18:23:27,107 - INFO - Epoch 215 training loss = 6.774
2023-04-24 18:23:27,203 - INFO - Epoch 216 training loss = 7.635
2023-04-24 18:23:27,299 - INFO - Epoch 217 training loss = 6.612
2023-04-24 18:23:27,394 - INFO - Epoch 218 training loss = 7.122
2023-04-24 18:23:27,490 - INFO - Epoch 219 training loss = 6.661
2023-04-24 18:23:27,586 - INFO - Epoch 220 training loss = 6.787
2023-04-24 18:23:27,616 - INFO - Validation loss = 11.6
2023-04-24 18:23:27,616 - INFO - best model
2023-04-24 18:23:27,723 - INFO - Epoch 221 training loss = 6.505
2023-04-24 18:23:27,819 - INFO - Epoch 222 training loss = 7.086
2023-04-24 18:23:27,914 - INFO - Epoch 223 training loss = 6.453
2023-04-24 18:23:28,010 - INFO - Epoch 224 training loss = 6.814
2023-04-24 18:23:28,106 - INFO - Epoch 225 training loss = 6.626
2023-04-24 18:23:28,201 - INFO - Epoch 226 training loss = 6.421
2023-04-24 18:23:28,297 - INFO - Epoch 227 training loss = 6.586
2023-04-24 18:23:28,393 - INFO - Epoch 228 training loss = 6.548
2023-04-24 18:23:28,488 - INFO - Epoch 229 training loss = 6.792
2023-04-24 18:23:28,584 - INFO - Epoch 230 training loss = 6.702
2023-04-24 18:23:28,614 - INFO - Validation loss = 11.4
2023-04-24 18:23:28,614 - INFO - best model
2023-04-24 18:23:28,725 - INFO - Epoch 231 training loss = 6.166
2023-04-24 18:23:28,821 - INFO - Epoch 232 training loss = 6.883
2023-04-24 18:23:28,965 - INFO - Epoch 233 training loss = 6.499
2023-04-24 18:23:29,061 - INFO - Epoch 234 training loss = 6.268
2023-04-24 18:23:29,156 - INFO - Epoch 235 training loss = 6.202
2023-04-24 18:23:29,252 - INFO - Epoch 236 training loss = 6.363
2023-04-24 18:23:29,348 - INFO - Epoch 237 training loss = 6.373
2023-04-24 18:23:29,444 - INFO - Epoch 238 training loss = 6.104
2023-04-24 18:23:29,539 - INFO - Epoch 239 training loss = 6.418
2023-04-24 18:23:29,635 - INFO - Epoch 240 training loss = 6.302
2023-04-24 18:23:29,666 - INFO - Validation loss = 11.62
2023-04-24 18:23:29,762 - INFO - Epoch 241 training loss = 6.247
2023-04-24 18:23:29,857 - INFO - Epoch 242 training loss = 6.118
2023-04-24 18:23:29,953 - INFO - Epoch 243 training loss = 6.148
2023-04-24 18:23:30,048 - INFO - Epoch 244 training loss = 5.921
2023-04-24 18:23:30,144 - INFO - Epoch 245 training loss = 6.065
2023-04-24 18:23:30,240 - INFO - Epoch 246 training loss = 6.101
2023-04-24 18:23:30,336 - INFO - Epoch 247 training loss = 6.227
2023-04-24 18:23:30,431 - INFO - Epoch 248 training loss = 5.859
2023-04-24 18:23:30,527 - INFO - Epoch 249 training loss = 6.022
2023-04-24 18:23:30,622 - INFO - Epoch 250 training loss = 5.958
2023-04-24 18:23:30,653 - INFO - Validation loss = 11.04
2023-04-24 18:23:30,653 - INFO - best model
2023-04-24 18:23:30,765 - INFO - Epoch 251 training loss = 5.915
2023-04-24 18:23:30,861 - INFO - Epoch 252 training loss = 6.057
2023-04-24 18:23:30,957 - INFO - Epoch 253 training loss = 6.435
2023-04-24 18:23:31,053 - INFO - Epoch 254 training loss = 5.862
2023-04-24 18:23:31,149 - INFO - Epoch 255 training loss = 6.08
2023-04-24 18:23:31,245 - INFO - Epoch 256 training loss = 5.863
2023-04-24 18:23:31,341 - INFO - Epoch 257 training loss = 5.615
2023-04-24 18:23:31,437 - INFO - Epoch 258 training loss = 5.67
2023-04-24 18:23:31,533 - INFO - Epoch 259 training loss = 5.739
2023-04-24 18:23:31,679 - INFO - Epoch 260 training loss = 5.605
2023-04-24 18:23:31,710 - INFO - Validation loss = 11.93
2023-04-24 18:23:31,806 - INFO - Epoch 261 training loss = 5.722
2023-04-24 18:23:31,902 - INFO - Epoch 262 training loss = 5.65
2023-04-24 18:23:31,998 - INFO - Epoch 263 training loss = 5.597
2023-04-24 18:23:32,094 - INFO - Epoch 264 training loss = 5.64
2023-04-24 18:23:32,191 - INFO - Epoch 265 training loss = 5.525
2023-04-24 18:23:32,287 - INFO - Epoch 266 training loss = 5.522
2023-04-24 18:23:32,383 - INFO - Epoch 267 training loss = 5.906
2023-04-24 18:23:32,479 - INFO - Epoch 268 training loss = 5.703
2023-04-24 18:23:32,575 - INFO - Epoch 269 training loss = 5.625
2023-04-24 18:23:32,671 - INFO - Epoch 270 training loss = 5.16
2023-04-24 18:23:32,702 - INFO - Validation loss = 12.13
2023-04-24 18:23:32,798 - INFO - Epoch 271 training loss = 5.552
2023-04-24 18:23:32,894 - INFO - Epoch 272 training loss = 5.332
2023-04-24 18:23:32,990 - INFO - Epoch 273 training loss = 5.469
2023-04-24 18:23:33,086 - INFO - Epoch 274 training loss = 5.561
2023-04-24 18:23:33,182 - INFO - Epoch 275 training loss = 5.368
2023-04-24 18:23:33,278 - INFO - Epoch 276 training loss = 5.459
2023-04-24 18:23:33,374 - INFO - Epoch 277 training loss = 5.197
2023-04-24 18:23:33,471 - INFO - Epoch 278 training loss = 5.704
2023-04-24 18:23:33,566 - INFO - Epoch 279 training loss = 5.312
2023-04-24 18:23:33,663 - INFO - Epoch 280 training loss = 5.149
2023-04-24 18:23:33,693 - INFO - Validation loss = 11.18
2023-04-24 18:23:33,790 - INFO - Epoch 281 training loss = 5.314
2023-04-24 18:23:33,886 - INFO - Epoch 282 training loss = 5.416
2023-04-24 18:23:33,982 - INFO - Epoch 283 training loss = 5.045
2023-04-24 18:23:34,078 - INFO - Epoch 284 training loss = 5.17
2023-04-24 18:23:34,174 - INFO - Epoch 285 training loss = 5.52
2023-04-24 18:23:34,270 - INFO - Epoch 286 training loss = 5.099
2023-04-24 18:23:34,416 - INFO - Epoch 287 training loss = 5.51
2023-04-24 18:23:34,512 - INFO - Epoch 288 training loss = 5.046
2023-04-24 18:23:34,609 - INFO - Epoch 289 training loss = 5.115
2023-04-24 18:23:34,705 - INFO - Epoch 290 training loss = 5.276
2023-04-24 18:23:34,736 - INFO - Validation loss = 10.7
2023-04-24 18:23:34,736 - INFO - best model
2023-04-24 18:23:34,848 - INFO - Epoch 291 training loss = 4.888
2023-04-24 18:23:34,944 - INFO - Epoch 292 training loss = 5.287
2023-04-24 18:23:35,040 - INFO - Epoch 293 training loss = 4.966
2023-04-24 18:23:35,136 - INFO - Epoch 294 training loss = 4.956
2023-04-24 18:23:35,232 - INFO - Epoch 295 training loss = 4.919
2023-04-24 18:23:35,328 - INFO - Epoch 296 training loss = 4.914
2023-04-24 18:23:35,424 - INFO - Epoch 297 training loss = 4.95
2023-04-24 18:23:35,521 - INFO - Epoch 298 training loss = 4.757
2023-04-24 18:23:35,617 - INFO - Epoch 299 training loss = 5.394
2023-04-24 18:23:35,713 - INFO - Epoch 300 training loss = 4.828
2023-04-24 18:23:35,739 - INFO - Validation loss = 10.9
2023-04-24 18:23:35,836 - INFO - Epoch 301 training loss = 4.928
2023-04-24 18:23:35,932 - INFO - Epoch 302 training loss = 5.02
2023-04-24 18:23:36,028 - INFO - Epoch 303 training loss = 4.645
2023-04-24 18:23:36,124 - INFO - Epoch 304 training loss = 4.952
2023-04-24 18:23:36,220 - INFO - Epoch 305 training loss = 4.82
2023-04-24 18:23:36,316 - INFO - Epoch 306 training loss = 4.932
2023-04-24 18:23:36,412 - INFO - Epoch 307 training loss = 4.794
2023-04-24 18:23:36,508 - INFO - Epoch 308 training loss = 4.617
2023-04-24 18:23:36,605 - INFO - Epoch 309 training loss = 4.885
2023-04-24 18:23:36,701 - INFO - Epoch 310 training loss = 4.722
2023-04-24 18:23:36,732 - INFO - Validation loss = 10.82
2023-04-24 18:23:36,828 - INFO - Epoch 311 training loss = 4.77
2023-04-24 18:23:36,924 - INFO - Epoch 312 training loss = 4.727
2023-04-24 18:23:37,021 - INFO - Epoch 313 training loss = 4.886
2023-04-24 18:23:37,166 - INFO - Epoch 314 training loss = 4.668
2023-04-24 18:23:37,262 - INFO - Epoch 315 training loss = 4.571
2023-04-24 18:23:37,359 - INFO - Epoch 316 training loss = 4.692
2023-04-24 18:23:37,455 - INFO - Epoch 317 training loss = 4.768
2023-04-24 18:23:37,551 - INFO - Epoch 318 training loss = 4.727
2023-04-24 18:23:37,647 - INFO - Epoch 319 training loss = 4.584
2023-04-24 18:23:37,743 - INFO - Epoch 320 training loss = 4.506
2023-04-24 18:23:37,773 - INFO - Validation loss = 11.48
2023-04-24 18:23:37,869 - INFO - Epoch 321 training loss = 4.68
2023-04-24 18:23:37,965 - INFO - Epoch 322 training loss =  4.7
2023-04-24 18:23:38,062 - INFO - Epoch 323 training loss = 4.786
2023-04-24 18:23:38,158 - INFO - Epoch 324 training loss = 4.615
2023-04-24 18:23:38,254 - INFO - Epoch 325 training loss = 4.31
2023-04-24 18:23:38,350 - INFO - Epoch 326 training loss = 4.51
2023-04-24 18:23:38,446 - INFO - Epoch 327 training loss = 4.468
2023-04-24 18:23:38,542 - INFO - Epoch 328 training loss = 4.463
2023-04-24 18:23:38,639 - INFO - Epoch 329 training loss = 4.529
2023-04-24 18:23:38,735 - INFO - Epoch 330 training loss = 4.429
2023-04-24 18:23:38,765 - INFO - Validation loss = 10.66
2023-04-24 18:23:38,766 - INFO - best model
2023-04-24 18:23:38,877 - INFO - Epoch 331 training loss = 4.434
2023-04-24 18:23:38,973 - INFO - Epoch 332 training loss = 4.544
2023-04-24 18:23:39,069 - INFO - Epoch 333 training loss = 4.524
2023-04-24 18:23:39,165 - INFO - Epoch 334 training loss = 4.491
2023-04-24 18:23:39,261 - INFO - Epoch 335 training loss = 4.44
2023-04-24 18:23:39,357 - INFO - Epoch 336 training loss = 4.307
2023-04-24 18:23:39,453 - INFO - Epoch 337 training loss = 4.445
2023-04-24 18:23:39,549 - INFO - Epoch 338 training loss = 4.18
2023-04-24 18:23:39,645 - INFO - Epoch 339 training loss = 4.492
2023-04-24 18:23:39,741 - INFO - Epoch 340 training loss = 4.236
2023-04-24 18:23:39,772 - INFO - Validation loss = 10.49
2023-04-24 18:23:39,772 - INFO - best model
2023-04-24 18:23:39,929 - INFO - Epoch 341 training loss = 4.158
2023-04-24 18:23:40,025 - INFO - Epoch 342 training loss = 4.483
2023-04-24 18:23:40,121 - INFO - Epoch 343 training loss = 4.293
2023-04-24 18:23:40,217 - INFO - Epoch 344 training loss = 4.44
2023-04-24 18:23:40,313 - INFO - Epoch 345 training loss = 4.121
2023-04-24 18:23:40,409 - INFO - Epoch 346 training loss = 4.27
2023-04-24 18:23:40,506 - INFO - Epoch 347 training loss = 4.141
2023-04-24 18:23:40,602 - INFO - Epoch 348 training loss = 4.279
2023-04-24 18:23:40,698 - INFO - Epoch 349 training loss = 4.196
2023-04-24 18:23:40,795 - INFO - Epoch 350 training loss = 4.187
2023-04-24 18:23:40,825 - INFO - Validation loss = 11.15
2023-04-24 18:23:40,921 - INFO - Epoch 351 training loss = 4.216
2023-04-24 18:23:41,017 - INFO - Epoch 352 training loss = 4.131
2023-04-24 18:23:41,114 - INFO - Epoch 353 training loss = 4.252
2023-04-24 18:23:41,210 - INFO - Epoch 354 training loss = 4.097
2023-04-24 18:23:41,306 - INFO - Epoch 355 training loss = 4.19
2023-04-24 18:23:41,402 - INFO - Epoch 356 training loss = 4.109
2023-04-24 18:23:41,498 - INFO - Epoch 357 training loss = 4.197
2023-04-24 18:23:41,595 - INFO - Epoch 358 training loss = 4.14
2023-04-24 18:23:41,692 - INFO - Epoch 359 training loss = 4.257
2023-04-24 18:23:41,788 - INFO - Epoch 360 training loss = 3.991
2023-04-24 18:23:41,818 - INFO - Validation loss = 10.3
2023-04-24 18:23:41,819 - INFO - best model
2023-04-24 18:23:41,930 - INFO - Epoch 361 training loss = 3.99
2023-04-24 18:23:42,026 - INFO - Epoch 362 training loss = 3.986
2023-04-24 18:23:42,122 - INFO - Epoch 363 training loss = 3.998
2023-04-24 18:23:42,219 - INFO - Epoch 364 training loss = 3.971
2023-04-24 18:23:42,315 - INFO - Epoch 365 training loss = 3.985
2023-04-24 18:23:42,411 - INFO - Epoch 366 training loss = 3.937
2023-04-24 18:23:42,508 - INFO - Epoch 367 training loss = 3.965
2023-04-24 18:23:42,605 - INFO - Epoch 368 training loss = 4.002
2023-04-24 18:23:42,751 - INFO - Epoch 369 training loss = 3.899
2023-04-24 18:23:42,847 - INFO - Epoch 370 training loss = 4.111
2023-04-24 18:23:42,878 - INFO - Validation loss = 10.38
2023-04-24 18:23:42,974 - INFO - Epoch 371 training loss = 3.951
2023-04-24 18:23:43,070 - INFO - Epoch 372 training loss = 4.044
2023-04-24 18:23:43,167 - INFO - Epoch 373 training loss = 3.891
2023-04-24 18:23:43,263 - INFO - Epoch 374 training loss = 4.028
2023-04-24 18:23:43,359 - INFO - Epoch 375 training loss = 3.788
2023-04-24 18:23:43,455 - INFO - Epoch 376 training loss = 3.948
2023-04-24 18:23:43,551 - INFO - Epoch 377 training loss = 3.92
2023-04-24 18:23:43,649 - INFO - Epoch 378 training loss = 3.963
2023-04-24 18:23:43,745 - INFO - Epoch 379 training loss = 3.794
2023-04-24 18:23:43,842 - INFO - Epoch 380 training loss = 3.791
2023-04-24 18:23:43,872 - INFO - Validation loss = 10.44
2023-04-24 18:23:43,968 - INFO - Epoch 381 training loss = 3.727
2023-04-24 18:23:44,065 - INFO - Epoch 382 training loss = 3.965
2023-04-24 18:23:44,161 - INFO - Epoch 383 training loss = 3.784
2023-04-24 18:23:44,257 - INFO - Epoch 384 training loss = 3.764
2023-04-24 18:23:44,353 - INFO - Epoch 385 training loss = 3.951
2023-04-24 18:23:44,449 - INFO - Epoch 386 training loss = 3.736
2023-04-24 18:23:44,545 - INFO - Epoch 387 training loss = 3.945
2023-04-24 18:23:44,643 - INFO - Epoch 388 training loss = 3.784
2023-04-24 18:23:44,739 - INFO - Epoch 389 training loss = 3.673
2023-04-24 18:23:44,836 - INFO - Epoch 390 training loss = 3.675
2023-04-24 18:23:44,866 - INFO - Validation loss = 10.58
2023-04-24 18:23:44,962 - INFO - Epoch 391 training loss = 3.989
2023-04-24 18:23:45,058 - INFO - Epoch 392 training loss = 3.864
2023-04-24 18:23:45,155 - INFO - Epoch 393 training loss = 3.646
2023-04-24 18:23:45,251 - INFO - Epoch 394 training loss = 3.811
2023-04-24 18:23:45,347 - INFO - Epoch 395 training loss = 3.555
2023-04-24 18:23:45,493 - INFO - Epoch 396 training loss = 3.614
2023-04-24 18:23:45,589 - INFO - Epoch 397 training loss = 3.636
2023-04-24 18:23:45,687 - INFO - Epoch 398 training loss = 3.677
2023-04-24 18:23:45,783 - INFO - Epoch 399 training loss = 3.574
2023-04-24 18:23:45,879 - INFO - Epoch 400 training loss = 3.761
2023-04-24 18:23:45,910 - INFO - Validation loss = 10.75
2023-04-24 18:23:46,006 - INFO - Epoch 401 training loss = 3.581
2023-04-24 18:23:46,102 - INFO - Epoch 402 training loss = 3.511
2023-04-24 18:23:46,198 - INFO - Epoch 403 training loss = 3.542
2023-04-24 18:23:46,294 - INFO - Epoch 404 training loss = 3.719
2023-04-24 18:23:46,390 - INFO - Epoch 405 training loss = 3.657
2023-04-24 18:23:46,486 - INFO - Epoch 406 training loss = 3.556
2023-04-24 18:23:46,583 - INFO - Epoch 407 training loss = 3.544
2023-04-24 18:23:46,680 - INFO - Epoch 408 training loss = 3.572
2023-04-24 18:23:46,777 - INFO - Epoch 409 training loss = 3.49
2023-04-24 18:23:46,873 - INFO - Epoch 410 training loss = 3.475
2023-04-24 18:23:46,904 - INFO - Validation loss = 10.77
2023-04-24 18:23:47,000 - INFO - Epoch 411 training loss = 3.448
2023-04-24 18:23:47,096 - INFO - Epoch 412 training loss =  3.7
2023-04-24 18:23:47,192 - INFO - Epoch 413 training loss = 3.48
2023-04-24 18:23:47,288 - INFO - Epoch 414 training loss = 3.47
2023-04-24 18:23:47,384 - INFO - Epoch 415 training loss = 3.536
2023-04-24 18:23:47,480 - INFO - Epoch 416 training loss = 3.484
2023-04-24 18:23:47,577 - INFO - Epoch 417 training loss = 3.527
2023-04-24 18:23:47,674 - INFO - Epoch 418 training loss = 3.607
2023-04-24 18:23:47,770 - INFO - Epoch 419 training loss = 3.499
2023-04-24 18:23:47,866 - INFO - Epoch 420 training loss = 3.381
2023-04-24 18:23:47,897 - INFO - Validation loss = 10.4
2023-04-24 18:23:47,993 - INFO - Epoch 421 training loss = 3.35
2023-04-24 18:23:48,089 - INFO - Epoch 422 training loss = 3.452
2023-04-24 18:23:48,235 - INFO - Epoch 423 training loss = 3.579
2023-04-24 18:23:48,331 - INFO - Epoch 424 training loss = 3.336
2023-04-24 18:23:48,428 - INFO - Epoch 425 training loss = 3.305
2023-04-24 18:23:48,524 - INFO - Epoch 426 training loss = 3.399
2023-04-24 18:23:48,620 - INFO - Epoch 427 training loss = 3.472
2023-04-24 18:23:48,717 - INFO - Epoch 428 training loss = 3.323
2023-04-24 18:23:48,813 - INFO - Epoch 429 training loss = 3.357
2023-04-24 18:23:48,909 - INFO - Epoch 430 training loss = 3.451
2023-04-24 18:23:48,936 - INFO - Validation loss = 10.37
2023-04-24 18:23:49,032 - INFO - Epoch 431 training loss = 3.263
2023-04-24 18:23:49,128 - INFO - Epoch 432 training loss = 3.392
2023-04-24 18:23:49,224 - INFO - Epoch 433 training loss = 3.271
2023-04-24 18:23:49,321 - INFO - Epoch 434 training loss = 3.317
2023-04-24 18:23:49,417 - INFO - Epoch 435 training loss = 3.276
2023-04-24 18:23:49,513 - INFO - Epoch 436 training loss = 3.282
2023-04-24 18:23:49,609 - INFO - Epoch 437 training loss = 3.284
2023-04-24 18:23:49,705 - INFO - Epoch 438 training loss = 3.235
2023-04-24 18:23:49,801 - INFO - Epoch 439 training loss = 3.271
2023-04-24 18:23:49,898 - INFO - Epoch 440 training loss = 3.329
2023-04-24 18:23:49,928 - INFO - Validation loss = 10.25
2023-04-24 18:23:49,928 - INFO - best model
2023-04-24 18:23:50,040 - INFO - Epoch 441 training loss = 3.225
2023-04-24 18:23:50,136 - INFO - Epoch 442 training loss = 3.248
2023-04-24 18:23:50,232 - INFO - Epoch 443 training loss = 3.173
2023-04-24 18:23:50,328 - INFO - Epoch 444 training loss = 3.154
2023-04-24 18:23:50,424 - INFO - Epoch 445 training loss = 3.33
2023-04-24 18:23:50,520 - INFO - Epoch 446 training loss = 3.217
2023-04-24 18:23:50,616 - INFO - Epoch 447 training loss = 3.179
2023-04-24 18:23:50,713 - INFO - Epoch 448 training loss = 3.241
2023-04-24 18:23:50,809 - INFO - Epoch 449 training loss = 3.126
2023-04-24 18:23:50,955 - INFO - Epoch 450 training loss = 3.237
2023-04-24 18:23:50,985 - INFO - Validation loss = 10.46
2023-04-24 18:23:51,082 - INFO - Epoch 451 training loss = 3.126
2023-04-24 18:23:51,178 - INFO - Epoch 452 training loss = 3.109
2023-04-24 18:23:51,274 - INFO - Epoch 453 training loss = 3.291
2023-04-24 18:23:51,370 - INFO - Epoch 454 training loss = 3.128
2023-04-24 18:23:51,466 - INFO - Epoch 455 training loss = 3.126
2023-04-24 18:23:51,562 - INFO - Epoch 456 training loss = 3.239
2023-04-24 18:23:51,658 - INFO - Epoch 457 training loss = 3.21
2023-04-24 18:23:51,755 - INFO - Epoch 458 training loss = 3.17
2023-04-24 18:23:51,851 - INFO - Epoch 459 training loss = 3.064
2023-04-24 18:23:51,947 - INFO - Epoch 460 training loss = 3.114
2023-04-24 18:23:51,977 - INFO - Validation loss = 10.16
2023-04-24 18:23:51,977 - INFO - best model
2023-04-24 18:23:52,089 - INFO - Epoch 461 training loss = 3.034
2023-04-24 18:23:52,185 - INFO - Epoch 462 training loss = 3.272
2023-04-24 18:23:52,281 - INFO - Epoch 463 training loss = 3.123
2023-04-24 18:23:52,377 - INFO - Epoch 464 training loss = 3.089
2023-04-24 18:23:52,473 - INFO - Epoch 465 training loss = 3.093
2023-04-24 18:23:52,570 - INFO - Epoch 466 training loss = 3.106
2023-04-24 18:23:52,666 - INFO - Epoch 467 training loss = 3.022
2023-04-24 18:23:52,762 - INFO - Epoch 468 training loss = 3.019
2023-04-24 18:23:52,858 - INFO - Epoch 469 training loss = 3.049
2023-04-24 18:23:52,954 - INFO - Epoch 470 training loss = 3.072
2023-04-24 18:23:52,985 - INFO - Validation loss = 10.29
2023-04-24 18:23:53,081 - INFO - Epoch 471 training loss = 3.012
2023-04-24 18:23:53,177 - INFO - Epoch 472 training loss = 3.011
2023-04-24 18:23:53,274 - INFO - Epoch 473 training loss = 2.957
2023-04-24 18:23:53,370 - INFO - Epoch 474 training loss = 3.018
2023-04-24 18:23:53,466 - INFO - Epoch 475 training loss = 3.012
2023-04-24 18:23:53,562 - INFO - Epoch 476 training loss = 3.031
2023-04-24 18:23:53,707 - INFO - Epoch 477 training loss = 3.068
2023-04-24 18:23:53,804 - INFO - Epoch 478 training loss = 3.038
2023-04-24 18:23:53,900 - INFO - Epoch 479 training loss = 2.899
2023-04-24 18:23:53,996 - INFO - Epoch 480 training loss = 3.065
2023-04-24 18:23:54,026 - INFO - Validation loss = 10.34
2023-04-24 18:23:54,123 - INFO - Epoch 481 training loss = 2.934
2023-04-24 18:23:54,219 - INFO - Epoch 482 training loss = 3.012
2023-04-24 18:23:54,315 - INFO - Epoch 483 training loss = 2.987
2023-04-24 18:23:54,411 - INFO - Epoch 484 training loss = 2.92
2023-04-24 18:23:54,507 - INFO - Epoch 485 training loss = 2.956
2023-04-24 18:23:54,603 - INFO - Epoch 486 training loss = 2.957
2023-04-24 18:23:54,700 - INFO - Epoch 487 training loss = 2.896
2023-04-24 18:23:54,796 - INFO - Epoch 488 training loss = 2.821
2023-04-24 18:23:54,892 - INFO - Epoch 489 training loss = 3.003
2023-04-24 18:23:54,988 - INFO - Epoch 490 training loss = 2.844
2023-04-24 18:23:55,019 - INFO - Validation loss = 10.31
2023-04-24 18:23:55,115 - INFO - Epoch 491 training loss = 2.939
2023-04-24 18:23:55,212 - INFO - Epoch 492 training loss = 2.939
2023-04-24 18:23:55,308 - INFO - Epoch 493 training loss = 2.874
2023-04-24 18:23:55,404 - INFO - Epoch 494 training loss = 2.859
2023-04-24 18:23:55,500 - INFO - Epoch 495 training loss = 2.902
2023-04-24 18:23:55,596 - INFO - Epoch 496 training loss = 2.828
2023-04-24 18:23:55,692 - INFO - Epoch 497 training loss = 2.85
2023-04-24 18:23:55,789 - INFO - Epoch 498 training loss = 2.844
2023-04-24 18:23:55,885 - INFO - Epoch 499 training loss = 2.961
2023-04-24 18:23:55,981 - INFO - Epoch 500 training loss = 2.799
2023-04-24 18:23:56,011 - INFO - Validation loss = 10.21
2023-04-24 18:23:56,108 - INFO - Epoch 501 training loss = 2.835
2023-04-24 18:23:56,204 - INFO - Epoch 502 training loss = 2.802
2023-04-24 18:23:56,300 - INFO - Epoch 503 training loss = 2.795
2023-04-24 18:23:56,446 - INFO - Epoch 504 training loss = 2.813
2023-04-24 18:23:56,542 - INFO - Epoch 505 training loss = 2.822
2023-04-24 18:23:56,639 - INFO - Epoch 506 training loss = 2.802
2023-04-24 18:23:56,735 - INFO - Epoch 507 training loss = 2.871
2023-04-24 18:23:56,831 - INFO - Epoch 508 training loss = 2.752
2023-04-24 18:23:56,928 - INFO - Epoch 509 training loss = 2.802
2023-04-24 18:23:57,024 - INFO - Epoch 510 training loss = 2.765
2023-04-24 18:23:57,053 - INFO - Validation loss = 10.08
2023-04-24 18:23:57,053 - INFO - best model
2023-04-24 18:23:57,161 - INFO - Epoch 511 training loss = 2.731
2023-04-24 18:23:57,257 - INFO - Epoch 512 training loss = 2.759
2023-04-24 18:23:57,353 - INFO - Epoch 513 training loss = 2.758
2023-04-24 18:23:57,449 - INFO - Epoch 514 training loss = 2.681
2023-04-24 18:23:57,545 - INFO - Epoch 515 training loss = 2.713
2023-04-24 18:23:57,641 - INFO - Epoch 516 training loss = 2.695
2023-04-24 18:23:57,738 - INFO - Epoch 517 training loss = 2.71
2023-04-24 18:23:57,834 - INFO - Epoch 518 training loss = 2.716
2023-04-24 18:23:57,930 - INFO - Epoch 519 training loss = 2.731
2023-04-24 18:23:58,026 - INFO - Epoch 520 training loss = 2.723
2023-04-24 18:23:58,057 - INFO - Validation loss = 10.17
2023-04-24 18:23:58,153 - INFO - Epoch 521 training loss = 2.686
2023-04-24 18:23:58,249 - INFO - Epoch 522 training loss = 2.736
2023-04-24 18:23:58,345 - INFO - Epoch 523 training loss = 2.65
2023-04-24 18:23:58,441 - INFO - Epoch 524 training loss = 2.661
2023-04-24 18:23:58,538 - INFO - Epoch 525 training loss = 2.644
2023-04-24 18:23:58,634 - INFO - Epoch 526 training loss = 2.679
2023-04-24 18:23:58,730 - INFO - Epoch 527 training loss = 2.641
2023-04-24 18:23:58,826 - INFO - Epoch 528 training loss = 2.655
2023-04-24 18:23:58,923 - INFO - Epoch 529 training loss = 2.708
2023-04-24 18:23:59,019 - INFO - Epoch 530 training loss = 2.608
2023-04-24 18:23:59,049 - INFO - Validation loss = 10.17
2023-04-24 18:23:59,195 - INFO - Epoch 531 training loss = 2.687
2023-04-24 18:23:59,292 - INFO - Epoch 532 training loss = 2.605
2023-04-24 18:23:59,388 - INFO - Epoch 533 training loss = 2.615
2023-04-24 18:23:59,484 - INFO - Epoch 534 training loss = 2.643
2023-04-24 18:23:59,580 - INFO - Epoch 535 training loss = 2.578
2023-04-24 18:23:59,676 - INFO - Epoch 536 training loss = 2.639
2023-04-24 18:23:59,772 - INFO - Epoch 537 training loss = 2.613
2023-04-24 18:23:59,868 - INFO - Epoch 538 training loss = 2.601
2023-04-24 18:23:59,964 - INFO - Epoch 539 training loss = 2.586
2023-04-24 18:24:00,061 - INFO - Epoch 540 training loss = 2.555
2023-04-24 18:24:00,091 - INFO - Validation loss = 10.23
2023-04-24 18:24:00,187 - INFO - Epoch 541 training loss = 2.627
2023-04-24 18:24:00,284 - INFO - Epoch 542 training loss = 2.561
2023-04-24 18:24:00,380 - INFO - Epoch 543 training loss = 2.594
2023-04-24 18:24:00,476 - INFO - Epoch 544 training loss = 2.525
2023-04-24 18:24:00,572 - INFO - Epoch 545 training loss = 2.564
2023-04-24 18:24:00,668 - INFO - Epoch 546 training loss = 2.579
2023-04-24 18:24:00,765 - INFO - Epoch 547 training loss = 2.531
2023-04-24 18:24:00,861 - INFO - Epoch 548 training loss = 2.632
2023-04-24 18:24:00,957 - INFO - Epoch 549 training loss = 2.537
2023-04-24 18:24:01,054 - INFO - Epoch 550 training loss = 2.583
2023-04-24 18:24:01,084 - INFO - Validation loss = 10.26
2023-04-24 18:24:01,181 - INFO - Epoch 551 training loss = 2.485
2023-04-24 18:24:01,277 - INFO - Epoch 552 training loss = 2.497
2023-04-24 18:24:01,373 - INFO - Epoch 553 training loss = 2.52
2023-04-24 18:24:01,469 - INFO - Epoch 554 training loss = 2.522
2023-04-24 18:24:01,565 - INFO - Epoch 555 training loss = 2.571
2023-04-24 18:24:01,661 - INFO - Epoch 556 training loss = 2.498
2023-04-24 18:24:01,758 - INFO - Epoch 557 training loss = 2.504
2023-04-24 18:24:01,904 - INFO - Epoch 558 training loss = 2.481
2023-04-24 18:24:02,000 - INFO - Epoch 559 training loss = 2.525
2023-04-24 18:24:02,096 - INFO - Epoch 560 training loss = 2.48
2023-04-24 18:24:02,127 - INFO - Validation loss = 10.21
2023-04-24 18:24:02,223 - INFO - Epoch 561 training loss = 2.454
2023-04-24 18:24:02,319 - INFO - Epoch 562 training loss = 2.461
2023-04-24 18:24:02,415 - INFO - Epoch 563 training loss = 2.429
2023-04-24 18:24:02,512 - INFO - Epoch 564 training loss = 2.512
2023-04-24 18:24:02,608 - INFO - Epoch 565 training loss = 2.416
2023-04-24 18:24:02,704 - INFO - Epoch 566 training loss = 2.418
2023-04-24 18:24:02,800 - INFO - Epoch 567 training loss = 2.429
2023-04-24 18:24:02,897 - INFO - Epoch 568 training loss = 2.441
2023-04-24 18:24:02,993 - INFO - Epoch 569 training loss = 2.421
2023-04-24 18:24:03,089 - INFO - Epoch 570 training loss = 2.506
2023-04-24 18:24:03,119 - INFO - Validation loss = 10.24
2023-04-24 18:24:03,216 - INFO - Epoch 571 training loss = 2.382
2023-04-24 18:24:03,312 - INFO - Epoch 572 training loss = 2.415
2023-04-24 18:24:03,408 - INFO - Epoch 573 training loss = 2.408
2023-04-24 18:24:03,505 - INFO - Epoch 574 training loss = 2.422
2023-04-24 18:24:03,601 - INFO - Epoch 575 training loss = 2.407
2023-04-24 18:24:03,697 - INFO - Epoch 576 training loss = 2.395
2023-04-24 18:24:03,793 - INFO - Epoch 577 training loss =  2.4
2023-04-24 18:24:03,890 - INFO - Epoch 578 training loss = 2.406
2023-04-24 18:24:03,986 - INFO - Epoch 579 training loss = 2.373
2023-04-24 18:24:04,082 - INFO - Epoch 580 training loss = 2.376
2023-04-24 18:24:04,113 - INFO - Validation loss = 10.24
2023-04-24 18:24:04,209 - INFO - Epoch 581 training loss = 2.384
2023-04-24 18:24:04,305 - INFO - Epoch 582 training loss = 2.399
2023-04-24 18:24:04,401 - INFO - Epoch 583 training loss = 2.388
2023-04-24 18:24:04,498 - INFO - Epoch 584 training loss = 2.392
2023-04-24 18:24:04,644 - INFO - Epoch 585 training loss = 2.348
2023-04-24 18:24:04,740 - INFO - Epoch 586 training loss = 2.371
2023-04-24 18:24:04,836 - INFO - Epoch 587 training loss = 2.32
2023-04-24 18:24:04,932 - INFO - Epoch 588 training loss = 2.318
2023-04-24 18:24:05,028 - INFO - Epoch 589 training loss = 2.365
2023-04-24 18:24:05,125 - INFO - Epoch 590 training loss = 2.327
2023-04-24 18:24:05,155 - INFO - Validation loss = 10.21
2023-04-24 18:24:05,252 - INFO - Epoch 591 training loss = 2.332
2023-04-24 18:24:05,348 - INFO - Epoch 592 training loss = 2.311
2023-04-24 18:24:05,444 - INFO - Epoch 593 training loss = 2.311
2023-04-24 18:24:05,540 - INFO - Epoch 594 training loss = 2.346
2023-04-24 18:24:05,637 - INFO - Epoch 595 training loss = 2.33
2023-04-24 18:24:05,733 - INFO - Epoch 596 training loss = 2.298
2023-04-24 18:24:05,829 - INFO - Epoch 597 training loss = 2.329
2023-04-24 18:24:05,925 - INFO - Epoch 598 training loss =  2.3
2023-04-24 18:24:06,021 - INFO - Epoch 599 training loss = 2.294
2023-04-24 18:24:06,118 - INFO - Epoch 600 training loss = 2.302
2023-04-24 18:24:06,148 - INFO - Validation loss = 10.22
2023-04-24 18:24:06,245 - INFO - Epoch 601 training loss = 2.274
2023-04-24 18:24:06,341 - INFO - Epoch 602 training loss = 2.302
2023-04-24 18:24:06,437 - INFO - Epoch 603 training loss = 2.283
2023-04-24 18:24:06,533 - INFO - Epoch 604 training loss = 2.267
2023-04-24 18:24:06,629 - INFO - Epoch 605 training loss = 2.278
2023-04-24 18:24:06,725 - INFO - Epoch 606 training loss = 2.284
2023-04-24 18:24:06,821 - INFO - Epoch 607 training loss = 2.265
2023-04-24 18:24:06,918 - INFO - Epoch 608 training loss = 2.287
2023-04-24 18:24:07,014 - INFO - Epoch 609 training loss = 2.22
2023-04-24 18:24:07,110 - INFO - Epoch 610 training loss = 2.254
2023-04-24 18:24:07,140 - INFO - Validation loss = 10.26
2023-04-24 18:24:07,237 - INFO - Epoch 611 training loss = 2.251
2023-04-24 18:24:07,383 - INFO - Epoch 612 training loss = 2.263
2023-04-24 18:24:07,479 - INFO - Epoch 613 training loss = 2.244
2023-04-24 18:24:07,575 - INFO - Epoch 614 training loss = 2.232
2023-04-24 18:24:07,671 - INFO - Epoch 615 training loss = 2.248
2023-04-24 18:24:07,767 - INFO - Epoch 616 training loss = 2.211
2023-04-24 18:24:07,864 - INFO - Epoch 617 training loss = 2.209
2023-04-24 18:24:07,960 - INFO - Epoch 618 training loss = 2.225
2023-04-24 18:24:08,056 - INFO - Epoch 619 training loss = 2.228
2023-04-24 18:24:08,152 - INFO - Epoch 620 training loss = 2.21
2023-04-24 18:24:08,181 - INFO - Validation loss = 10.23
2023-04-24 18:24:08,277 - INFO - Epoch 621 training loss = 2.202
2023-04-24 18:24:08,374 - INFO - Epoch 622 training loss = 2.211
2023-04-24 18:24:08,470 - INFO - Epoch 623 training loss = 2.176
2023-04-24 18:24:08,566 - INFO - Epoch 624 training loss = 2.218
2023-04-24 18:24:08,662 - INFO - Epoch 625 training loss = 2.221
2023-04-24 18:24:08,758 - INFO - Epoch 626 training loss = 2.156
2023-04-24 18:24:08,855 - INFO - Epoch 627 training loss = 2.187
2023-04-24 18:24:08,951 - INFO - Epoch 628 training loss = 2.185
2023-04-24 18:24:09,047 - INFO - Epoch 629 training loss = 2.16
2023-04-24 18:24:09,143 - INFO - Epoch 630 training loss = 2.181
2023-04-24 18:24:09,174 - INFO - Validation loss = 10.22
2023-04-24 18:24:09,270 - INFO - Epoch 631 training loss = 2.167
2023-04-24 18:24:09,366 - INFO - Epoch 632 training loss = 2.147
2023-04-24 18:24:09,463 - INFO - Epoch 633 training loss = 2.152
2023-04-24 18:24:09,559 - INFO - Epoch 634 training loss = 2.211
2023-04-24 18:24:09,655 - INFO - Epoch 635 training loss = 2.114
2023-04-24 18:24:09,751 - INFO - Epoch 636 training loss = 2.14
2023-04-24 18:24:09,847 - INFO - Epoch 637 training loss = 2.169
2023-04-24 18:24:09,945 - INFO - Epoch 638 training loss = 2.155
2023-04-24 18:24:10,041 - INFO - Epoch 639 training loss = 2.117
2023-04-24 18:24:10,187 - INFO - Epoch 640 training loss = 2.133
2023-04-24 18:24:10,218 - INFO - Validation loss = 10.29
2023-04-24 18:24:10,314 - INFO - Epoch 641 training loss = 2.146
2023-04-24 18:24:10,410 - INFO - Epoch 642 training loss = 2.128
2023-04-24 18:24:10,506 - INFO - Epoch 643 training loss = 2.113
2023-04-24 18:24:10,602 - INFO - Epoch 644 training loss = 2.137
2023-04-24 18:24:10,698 - INFO - Epoch 645 training loss = 2.151
2023-04-24 18:24:10,795 - INFO - Epoch 646 training loss = 2.097
2023-04-24 18:24:10,891 - INFO - Epoch 647 training loss = 2.113
2023-04-24 18:24:10,987 - INFO - Epoch 648 training loss = 2.112
2023-04-24 18:24:11,083 - INFO - Epoch 649 training loss = 2.098
2023-04-24 18:24:11,179 - INFO - Epoch 650 training loss = 2.097
2023-04-24 18:24:11,206 - INFO - Validation loss = 10.3
2023-04-24 18:24:11,302 - INFO - Epoch 651 training loss = 2.094
2023-04-24 18:24:11,398 - INFO - Epoch 652 training loss = 2.102
2023-04-24 18:24:11,494 - INFO - Epoch 653 training loss = 2.085
2023-04-24 18:24:11,591 - INFO - Epoch 654 training loss = 2.083
2023-04-24 18:24:11,687 - INFO - Epoch 655 training loss = 2.101
2023-04-24 18:24:11,783 - INFO - Epoch 656 training loss = 2.113
2023-04-24 18:24:11,879 - INFO - Epoch 657 training loss = 2.067
2023-04-24 18:24:11,977 - INFO - Epoch 658 training loss = 2.068
2023-04-24 18:24:12,073 - INFO - Epoch 659 training loss = 2.08
2023-04-24 18:24:12,169 - INFO - Epoch 660 training loss = 2.065
2023-04-24 18:24:12,260 - INFO - Validation loss = 10.34
2023-04-24 18:24:12,360 - INFO - Epoch 661 training loss = 2.055
2023-04-24 18:24:12,456 - INFO - Epoch 662 training loss = 2.057
2023-04-24 18:24:12,552 - INFO - Epoch 663 training loss = 2.055
2023-04-24 18:24:12,648 - INFO - Epoch 664 training loss = 2.055
2023-04-24 18:24:12,744 - INFO - Epoch 665 training loss = 2.052
2023-04-24 18:24:12,841 - INFO - Epoch 666 training loss = 2.036
2023-04-24 18:24:12,988 - INFO - Epoch 667 training loss = 2.034
2023-04-24 18:24:13,084 - INFO - Epoch 668 training loss = 2.068
2023-04-24 18:24:13,181 - INFO - Epoch 669 training loss = 2.038
2023-04-24 18:24:13,277 - INFO - Epoch 670 training loss = 2.037
2023-04-24 18:24:13,308 - INFO - Validation loss = 10.32
2023-04-24 18:24:13,405 - INFO - Epoch 671 training loss = 2.024
2023-04-24 18:24:13,502 - INFO - Epoch 672 training loss = 2.026
2023-04-24 18:24:13,598 - INFO - Epoch 673 training loss = 2.056
2023-04-24 18:24:13,694 - INFO - Epoch 674 training loss = 2.009
2023-04-24 18:24:13,791 - INFO - Epoch 675 training loss = 2.005
2023-04-24 18:24:13,887 - INFO - Epoch 676 training loss = 2.025
2023-04-24 18:24:13,985 - INFO - Epoch 677 training loss = 2.026
2023-04-24 18:24:14,082 - INFO - Epoch 678 training loss = 2.021
2023-04-24 18:24:14,178 - INFO - Epoch 679 training loss = 2.003
2023-04-24 18:24:14,275 - INFO - Epoch 680 training loss = 2.01
2023-04-24 18:24:14,305 - INFO - Validation loss = 10.3
2023-04-24 18:24:14,402 - INFO - Epoch 681 training loss = 1.991
2023-04-24 18:24:14,499 - INFO - Epoch 682 training loss = 1.984
2023-04-24 18:24:14,595 - INFO - Epoch 683 training loss = 2.007
2023-04-24 18:24:14,692 - INFO - Epoch 684 training loss = 2.012
2023-04-24 18:24:14,788 - INFO - Epoch 685 training loss = 1.995
2023-04-24 18:24:14,885 - INFO - Epoch 686 training loss = 1.993
2023-04-24 18:24:14,982 - INFO - Epoch 687 training loss = 1.994
2023-04-24 18:24:15,078 - INFO - Epoch 688 training loss = 1.979
2023-04-24 18:24:15,174 - INFO - Epoch 689 training loss = 1.979
2023-04-24 18:24:15,270 - INFO - Epoch 690 training loss = 1.987
2023-04-24 18:24:15,300 - INFO - Validation loss = 10.36
2023-04-24 18:24:15,396 - INFO - Epoch 691 training loss = 1.979
2023-04-24 18:24:15,492 - INFO - Epoch 692 training loss = 1.963
2023-04-24 18:24:15,588 - INFO - Epoch 693 training loss = 1.982
2023-04-24 18:24:15,732 - INFO - Epoch 694 training loss = 1.975
2023-04-24 18:24:15,828 - INFO - Epoch 695 training loss = 1.978
2023-04-24 18:24:15,924 - INFO - Epoch 696 training loss = 1.98
2023-04-24 18:24:16,021 - INFO - Epoch 697 training loss = 1.95
2023-04-24 18:24:16,116 - INFO - Epoch 698 training loss = 1.944
2023-04-24 18:24:16,212 - INFO - Epoch 699 training loss = 1.965
2023-04-24 18:24:16,308 - INFO - Epoch 700 training loss = 1.962
2023-04-24 18:24:16,338 - INFO - Validation loss = 10.39
2023-04-24 18:24:16,434 - INFO - Epoch 701 training loss = 1.931
2023-04-24 18:24:16,530 - INFO - Epoch 702 training loss = 1.951
2023-04-24 18:24:16,626 - INFO - Epoch 703 training loss = 1.955
2023-04-24 18:24:16,721 - INFO - Epoch 704 training loss = 1.949
2023-04-24 18:24:16,817 - INFO - Epoch 705 training loss = 1.93
2023-04-24 18:24:16,913 - INFO - Epoch 706 training loss = 1.947
2023-04-24 18:24:17,010 - INFO - Epoch 707 training loss = 1.924
2023-04-24 18:24:17,105 - INFO - Epoch 708 training loss = 1.93
2023-04-24 18:24:17,201 - INFO - Epoch 709 training loss = 1.934
2023-04-24 18:24:17,297 - INFO - Epoch 710 training loss = 1.927
2023-04-24 18:24:17,327 - INFO - Validation loss = 10.37
2023-04-24 18:24:17,423 - INFO - Epoch 711 training loss = 1.922
2023-04-24 18:24:17,519 - INFO - Epoch 712 training loss = 1.917
2023-04-24 18:24:17,614 - INFO - Epoch 713 training loss = 1.912
2023-04-24 18:24:17,710 - INFO - Epoch 714 training loss = 1.925
2023-04-24 18:24:17,806 - INFO - Epoch 715 training loss = 1.91
2023-04-24 18:24:17,903 - INFO - Epoch 716 training loss = 1.899
2023-04-24 18:24:18,000 - INFO - Epoch 717 training loss = 1.912
2023-04-24 18:24:18,096 - INFO - Epoch 718 training loss = 1.903
2023-04-24 18:24:18,192 - INFO - Epoch 719 training loss = 1.911
2023-04-24 18:24:18,287 - INFO - Epoch 720 training loss = 1.898
2023-04-24 18:24:18,318 - INFO - Validation loss = 10.39
2023-04-24 18:24:18,462 - INFO - Epoch 721 training loss = 1.902
2023-04-24 18:24:18,558 - INFO - Epoch 722 training loss = 1.885
2023-04-24 18:24:18,654 - INFO - Epoch 723 training loss = 1.905
2023-04-24 18:24:18,750 - INFO - Epoch 724 training loss = 1.889
2023-04-24 18:24:18,845 - INFO - Epoch 725 training loss = 1.891
2023-04-24 18:24:18,941 - INFO - Epoch 726 training loss = 1.883
2023-04-24 18:24:19,037 - INFO - Epoch 727 training loss = 1.889
2023-04-24 18:24:19,132 - INFO - Epoch 728 training loss = 1.88
2023-04-24 18:24:19,228 - INFO - Epoch 729 training loss = 1.891
2023-04-24 18:24:19,324 - INFO - Epoch 730 training loss = 1.873
2023-04-24 18:24:19,354 - INFO - Validation loss = 10.44
2023-04-24 18:24:19,450 - INFO - Epoch 731 training loss = 1.874
2023-04-24 18:24:19,545 - INFO - Epoch 732 training loss = 1.877
2023-04-24 18:24:19,640 - INFO - Epoch 733 training loss = 1.868
2023-04-24 18:24:19,735 - INFO - Epoch 734 training loss = 1.871
2023-04-24 18:24:19,830 - INFO - Epoch 735 training loss = 1.867
2023-04-24 18:24:19,925 - INFO - Epoch 736 training loss = 1.864
2023-04-24 18:24:20,020 - INFO - Epoch 737 training loss = 1.863
2023-04-24 18:24:20,115 - INFO - Epoch 738 training loss = 1.859
2023-04-24 18:24:20,211 - INFO - Epoch 739 training loss = 1.856
2023-04-24 18:24:20,306 - INFO - Epoch 740 training loss = 1.856
2023-04-24 18:24:20,336 - INFO - Validation loss = 10.43
2023-04-24 18:24:20,431 - INFO - Epoch 741 training loss = 1.856
2023-04-24 18:24:20,526 - INFO - Epoch 742 training loss = 1.849
2023-04-24 18:24:20,621 - INFO - Epoch 743 training loss = 1.849
2023-04-24 18:24:20,716 - INFO - Epoch 744 training loss = 1.845
2023-04-24 18:24:20,812 - INFO - Epoch 745 training loss = 1.853
2023-04-24 18:24:20,907 - INFO - Epoch 746 training loss = 1.844
2023-04-24 18:24:21,002 - INFO - Epoch 747 training loss = 1.839
2023-04-24 18:24:21,146 - INFO - Epoch 748 training loss = 1.846
2023-04-24 18:24:21,241 - INFO - Epoch 749 training loss = 1.835
2023-04-24 18:24:21,336 - INFO - Epoch 750 training loss = 1.841
2023-04-24 18:24:21,366 - INFO - Validation loss = 10.44
2023-04-24 18:24:21,462 - INFO - Epoch 751 training loss = 1.826
2023-04-24 18:24:21,557 - INFO - Epoch 752 training loss = 1.827
2023-04-24 18:24:21,652 - INFO - Epoch 753 training loss = 1.828
2023-04-24 18:24:21,747 - INFO - Epoch 754 training loss = 1.834
2023-04-24 18:24:21,842 - INFO - Epoch 755 training loss = 1.821
2023-04-24 18:24:21,937 - INFO - Epoch 756 training loss = 1.821
2023-04-24 18:24:22,032 - INFO - Epoch 757 training loss = 1.824
2023-04-24 18:24:22,127 - INFO - Epoch 758 training loss = 1.825
2023-04-24 18:24:22,222 - INFO - Epoch 759 training loss = 1.815
2023-04-24 18:24:22,317 - INFO - Epoch 760 training loss = 1.812
2023-04-24 18:24:22,348 - INFO - Validation loss = 10.48
2023-04-24 18:24:22,443 - INFO - Epoch 761 training loss = 1.815
2023-04-24 18:24:22,538 - INFO - Epoch 762 training loss = 1.811
2023-04-24 18:24:22,633 - INFO - Epoch 763 training loss = 1.811
2023-04-24 18:24:22,728 - INFO - Epoch 764 training loss = 1.807
2023-04-24 18:24:22,823 - INFO - Epoch 765 training loss = 1.808
2023-04-24 18:24:22,918 - INFO - Epoch 766 training loss = 1.81
2023-04-24 18:24:23,013 - INFO - Epoch 767 training loss = 1.799
2023-04-24 18:24:23,109 - INFO - Epoch 768 training loss = 1.805
2023-04-24 18:24:23,204 - INFO - Epoch 769 training loss = 1.797
2023-04-24 18:24:23,299 - INFO - Epoch 770 training loss = 1.801
2023-04-24 18:24:23,329 - INFO - Validation loss = 10.45
2023-04-24 18:24:23,425 - INFO - Epoch 771 training loss = 1.794
2023-04-24 18:24:23,520 - INFO - Epoch 772 training loss = 1.798
2023-04-24 18:24:23,615 - INFO - Epoch 773 training loss = 1.794
2023-04-24 18:24:23,710 - INFO - Epoch 774 training loss = 1.798
2023-04-24 18:24:23,854 - INFO - Epoch 775 training loss = 1.79
2023-04-24 18:24:23,949 - INFO - Epoch 776 training loss = 1.785
2023-04-24 18:24:24,044 - INFO - Epoch 777 training loss = 1.786
2023-04-24 18:24:24,139 - INFO - Epoch 778 training loss = 1.782
2023-04-24 18:24:24,234 - INFO - Epoch 779 training loss = 1.791
2023-04-24 18:24:24,329 - INFO - Epoch 780 training loss = 1.783
2023-04-24 18:24:24,360 - INFO - Validation loss = 10.48
2023-04-24 18:24:24,455 - INFO - Epoch 781 training loss = 1.773
2023-04-24 18:24:24,551 - INFO - Epoch 782 training loss = 1.777
2023-04-24 18:24:24,647 - INFO - Epoch 783 training loss = 1.771
2023-04-24 18:24:24,742 - INFO - Epoch 784 training loss = 1.773
2023-04-24 18:24:24,838 - INFO - Epoch 785 training loss = 1.776
2023-04-24 18:24:24,934 - INFO - Epoch 786 training loss = 1.774
2023-04-24 18:24:25,030 - INFO - Epoch 787 training loss = 1.767
2023-04-24 18:24:25,125 - INFO - Epoch 788 training loss = 1.77
2023-04-24 18:24:25,221 - INFO - Epoch 789 training loss = 1.769
2023-04-24 18:24:25,317 - INFO - Epoch 790 training loss = 1.763
2023-04-24 18:24:25,347 - INFO - Validation loss = 10.49
2023-04-24 18:24:25,443 - INFO - Epoch 791 training loss = 1.76
2023-04-24 18:24:25,539 - INFO - Epoch 792 training loss = 1.758
2023-04-24 18:24:25,634 - INFO - Epoch 793 training loss = 1.762
2023-04-24 18:24:25,730 - INFO - Epoch 794 training loss = 1.763
2023-04-24 18:24:25,826 - INFO - Epoch 795 training loss = 1.758
2023-04-24 18:24:25,921 - INFO - Epoch 796 training loss = 1.755
2023-04-24 18:24:26,017 - INFO - Epoch 797 training loss = 1.758
2023-04-24 18:24:26,113 - INFO - Epoch 798 training loss = 1.753
2023-04-24 18:24:26,209 - INFO - Epoch 799 training loss = 1.751
2023-04-24 18:24:26,304 - INFO - Epoch 800 training loss = 1.751
2023-04-24 18:24:26,334 - INFO - Validation loss = 10.5
2023-04-24 18:24:26,430 - INFO - Epoch 801 training loss = 1.748
2023-04-24 18:24:26,575 - INFO - Epoch 802 training loss = 1.742
2023-04-24 18:24:26,671 - INFO - Epoch 803 training loss = 1.745
2023-04-24 18:24:26,766 - INFO - Epoch 804 training loss = 1.744
2023-04-24 18:24:26,862 - INFO - Epoch 805 training loss = 1.746
2023-04-24 18:24:26,958 - INFO - Epoch 806 training loss = 1.735
2023-04-24 18:24:27,054 - INFO - Epoch 807 training loss = 1.74
2023-04-24 18:24:27,150 - INFO - Epoch 808 training loss = 1.739
2023-04-24 18:24:27,245 - INFO - Epoch 809 training loss = 1.737
2023-04-24 18:24:27,341 - INFO - Epoch 810 training loss = 1.734
2023-04-24 18:24:27,371 - INFO - Validation loss = 10.56
2023-04-24 18:24:27,467 - INFO - Epoch 811 training loss = 1.732
2023-04-24 18:24:27,563 - INFO - Epoch 812 training loss = 1.735
2023-04-24 18:24:27,659 - INFO - Epoch 813 training loss = 1.732
2023-04-24 18:24:27,755 - INFO - Epoch 814 training loss = 1.728
2023-04-24 18:24:27,851 - INFO - Epoch 815 training loss = 1.727
2023-04-24 18:24:27,948 - INFO - Epoch 816 training loss = 1.728
2023-04-24 18:24:28,044 - INFO - Epoch 817 training loss = 1.725
2023-04-24 18:24:28,141 - INFO - Epoch 818 training loss = 1.725
2023-04-24 18:24:28,237 - INFO - Epoch 819 training loss = 1.727
2023-04-24 18:24:28,334 - INFO - Epoch 820 training loss = 1.721
2023-04-24 18:24:28,364 - INFO - Validation loss = 10.54
2023-04-24 18:24:28,461 - INFO - Epoch 821 training loss = 1.721
2023-04-24 18:24:28,558 - INFO - Epoch 822 training loss = 1.722
2023-04-24 18:24:28,654 - INFO - Epoch 823 training loss = 1.72
2023-04-24 18:24:28,751 - INFO - Epoch 824 training loss = 1.718
2023-04-24 18:24:28,848 - INFO - Epoch 825 training loss = 1.715
2023-04-24 18:24:28,944 - INFO - Epoch 826 training loss = 1.714
2023-04-24 18:24:29,041 - INFO - Epoch 827 training loss = 1.712
2023-04-24 18:24:29,137 - INFO - Epoch 828 training loss = 1.715
2023-04-24 18:24:29,283 - INFO - Epoch 829 training loss = 1.71
2023-04-24 18:24:29,380 - INFO - Epoch 830 training loss = 1.712
2023-04-24 18:24:29,410 - INFO - Validation loss = 10.56
2023-04-24 18:24:29,507 - INFO - Epoch 831 training loss = 1.708
2023-04-24 18:24:29,604 - INFO - Epoch 832 training loss = 1.706
2023-04-24 18:24:29,700 - INFO - Epoch 833 training loss = 1.707
2023-04-24 18:24:29,797 - INFO - Epoch 834 training loss = 1.704
2023-04-24 18:24:29,893 - INFO - Epoch 835 training loss = 1.704
2023-04-24 18:24:29,989 - INFO - Epoch 836 training loss = 1.705
2023-04-24 18:24:30,086 - INFO - Epoch 837 training loss = 1.702
2023-04-24 18:24:30,182 - INFO - Epoch 838 training loss = 1.701
2023-04-24 18:24:30,279 - INFO - Epoch 839 training loss = 1.702
2023-04-24 18:24:30,375 - INFO - Epoch 840 training loss = 1.701
2023-04-24 18:24:30,405 - INFO - Validation loss = 10.56
2023-04-24 18:24:30,501 - INFO - Epoch 841 training loss = 1.699
2023-04-24 18:24:30,596 - INFO - Epoch 842 training loss = 1.697
2023-04-24 18:24:30,691 - INFO - Epoch 843 training loss = 1.697
2023-04-24 18:24:30,786 - INFO - Epoch 844 training loss = 1.693
2023-04-24 18:24:30,881 - INFO - Epoch 845 training loss = 1.694
2023-04-24 18:24:30,976 - INFO - Epoch 846 training loss = 1.692
2023-04-24 18:24:31,071 - INFO - Epoch 847 training loss = 1.69
2023-04-24 18:24:31,166 - INFO - Epoch 848 training loss = 1.689
2023-04-24 18:24:31,261 - INFO - Epoch 849 training loss = 1.688
2023-04-24 18:24:31,356 - INFO - Epoch 850 training loss = 1.687
2023-04-24 18:24:31,386 - INFO - Validation loss = 10.56
2023-04-24 18:24:31,482 - INFO - Epoch 851 training loss = 1.688
2023-04-24 18:24:31,577 - INFO - Epoch 852 training loss = 1.683
2023-04-24 18:24:31,672 - INFO - Epoch 853 training loss = 1.687
2023-04-24 18:24:31,767 - INFO - Epoch 854 training loss = 1.683
2023-04-24 18:24:31,862 - INFO - Epoch 855 training loss = 1.684
2023-04-24 18:24:32,006 - INFO - Epoch 856 training loss = 1.681
2023-04-24 18:24:32,101 - INFO - Epoch 857 training loss = 1.682
2023-04-24 18:24:32,196 - INFO - Epoch 858 training loss = 1.678
2023-04-24 18:24:32,291 - INFO - Epoch 859 training loss = 1.681
2023-04-24 18:24:32,386 - INFO - Epoch 860 training loss = 1.682
2023-04-24 18:24:32,413 - INFO - Validation loss = 10.59
2023-04-24 18:24:32,509 - INFO - Epoch 861 training loss = 1.677
2023-04-24 18:24:32,604 - INFO - Epoch 862 training loss = 1.676
2023-04-24 18:24:32,699 - INFO - Epoch 863 training loss = 1.678
2023-04-24 18:24:32,794 - INFO - Epoch 864 training loss = 1.675
2023-04-24 18:24:32,889 - INFO - Epoch 865 training loss = 1.676
2023-04-24 18:24:32,984 - INFO - Epoch 866 training loss = 1.676
2023-04-24 18:24:33,079 - INFO - Epoch 867 training loss = 1.67
2023-04-24 18:24:33,174 - INFO - Epoch 868 training loss = 1.673
2023-04-24 18:24:33,269 - INFO - Epoch 869 training loss = 1.673
2023-04-24 18:24:33,364 - INFO - Epoch 870 training loss = 1.671
2023-04-24 18:24:33,406 - INFO - Validation loss = 10.59
2023-04-24 18:24:33,501 - INFO - Epoch 871 training loss = 1.668
2023-04-24 18:24:33,596 - INFO - Epoch 872 training loss = 1.67
2023-04-24 18:24:33,691 - INFO - Epoch 873 training loss = 1.665
2023-04-24 18:24:33,787 - INFO - Epoch 874 training loss = 1.664
2023-04-24 18:24:33,882 - INFO - Epoch 875 training loss = 1.665
2023-04-24 18:24:33,977 - INFO - Epoch 876 training loss = 1.667
2023-04-24 18:24:34,072 - INFO - Epoch 877 training loss = 1.663
2023-04-24 18:24:34,167 - INFO - Epoch 878 training loss = 1.664
2023-04-24 18:24:34,262 - INFO - Epoch 879 training loss = 1.662
2023-04-24 18:24:34,357 - INFO - Epoch 880 training loss = 1.662
2023-04-24 18:24:34,388 - INFO - Validation loss = 10.6
2023-04-24 18:24:34,483 - INFO - Epoch 881 training loss = 1.663
2023-04-24 18:24:34,578 - INFO - Epoch 882 training loss = 1.661
2023-04-24 18:24:34,722 - INFO - Epoch 883 training loss = 1.661
2023-04-24 18:24:34,817 - INFO - Epoch 884 training loss = 1.659
2023-04-24 18:24:34,912 - INFO - Epoch 885 training loss = 1.657
2023-04-24 18:24:35,007 - INFO - Epoch 886 training loss = 1.657
2023-04-24 18:24:35,103 - INFO - Epoch 887 training loss = 1.658
2023-04-24 18:24:35,198 - INFO - Epoch 888 training loss = 1.655
2023-04-24 18:24:35,293 - INFO - Epoch 889 training loss = 1.655
2023-04-24 18:24:35,388 - INFO - Epoch 890 training loss = 1.653
2023-04-24 18:24:35,419 - INFO - Validation loss = 10.62
2023-04-24 18:24:35,514 - INFO - Epoch 891 training loss = 1.654
2023-04-24 18:24:35,610 - INFO - Epoch 892 training loss = 1.652
2023-04-24 18:24:35,705 - INFO - Epoch 893 training loss = 1.653
2023-04-24 18:24:35,801 - INFO - Epoch 894 training loss = 1.651
2023-04-24 18:24:35,897 - INFO - Epoch 895 training loss = 1.651
2023-04-24 18:24:35,993 - INFO - Epoch 896 training loss = 1.651
2023-04-24 18:24:36,088 - INFO - Epoch 897 training loss = 1.651
2023-04-24 18:24:36,184 - INFO - Epoch 898 training loss = 1.648
2023-04-24 18:24:36,280 - INFO - Epoch 899 training loss = 1.648
2023-04-24 18:24:36,376 - INFO - Epoch 900 training loss = 1.648
2023-04-24 18:24:36,406 - INFO - Validation loss = 10.62
2023-04-24 18:24:36,502 - INFO - Epoch 901 training loss = 1.647
2023-04-24 18:24:36,598 - INFO - Epoch 902 training loss = 1.645
2023-04-24 18:24:36,694 - INFO - Epoch 903 training loss = 1.645
2023-04-24 18:24:36,789 - INFO - Epoch 904 training loss = 1.646
2023-04-24 18:24:36,885 - INFO - Epoch 905 training loss = 1.645
2023-04-24 18:24:36,981 - INFO - Epoch 906 training loss = 1.642
2023-04-24 18:24:37,077 - INFO - Epoch 907 training loss = 1.645
2023-04-24 18:24:37,172 - INFO - Epoch 908 training loss = 1.643
2023-04-24 18:24:37,268 - INFO - Epoch 909 training loss = 1.64
2023-04-24 18:24:37,412 - INFO - Epoch 910 training loss = 1.642
2023-04-24 18:24:37,443 - INFO - Validation loss = 10.63
2023-04-24 18:24:37,538 - INFO - Epoch 911 training loss = 1.64
2023-04-24 18:24:37,634 - INFO - Epoch 912 training loss = 1.643
2023-04-24 18:24:37,730 - INFO - Epoch 913 training loss = 1.641
2023-04-24 18:24:37,825 - INFO - Epoch 914 training loss = 1.638
2023-04-24 18:24:37,921 - INFO - Epoch 915 training loss = 1.64
2023-04-24 18:24:38,017 - INFO - Epoch 916 training loss = 1.638
2023-04-24 18:24:38,112 - INFO - Epoch 917 training loss = 1.639
2023-04-24 18:24:38,208 - INFO - Epoch 918 training loss = 1.638
2023-04-24 18:24:38,304 - INFO - Epoch 919 training loss = 1.638
2023-04-24 18:24:38,399 - INFO - Epoch 920 training loss = 1.636
2023-04-24 18:24:38,430 - INFO - Validation loss = 10.63
2023-04-24 18:24:38,525 - INFO - Epoch 921 training loss = 1.637
2023-04-24 18:24:38,621 - INFO - Epoch 922 training loss = 1.634
2023-04-24 18:24:38,717 - INFO - Epoch 923 training loss = 1.634
2023-04-24 18:24:38,813 - INFO - Epoch 924 training loss = 1.635
2023-04-24 18:24:38,908 - INFO - Epoch 925 training loss = 1.634
2023-04-24 18:24:39,004 - INFO - Epoch 926 training loss = 1.634
2023-04-24 18:24:39,100 - INFO - Epoch 927 training loss = 1.633
2023-04-24 18:24:39,196 - INFO - Epoch 928 training loss = 1.632
2023-04-24 18:24:39,291 - INFO - Epoch 929 training loss = 1.632
2023-04-24 18:24:39,387 - INFO - Epoch 930 training loss = 1.632
2023-04-24 18:24:39,417 - INFO - Validation loss = 10.63
2023-04-24 18:24:39,513 - INFO - Epoch 931 training loss = 1.63
2023-04-24 18:24:39,609 - INFO - Epoch 932 training loss = 1.632
2023-04-24 18:24:39,705 - INFO - Epoch 933 training loss = 1.631
2023-04-24 18:24:39,800 - INFO - Epoch 934 training loss = 1.631
2023-04-24 18:24:39,896 - INFO - Epoch 935 training loss = 1.629
2023-04-24 18:24:39,992 - INFO - Epoch 936 training loss = 1.629
2023-04-24 18:24:40,087 - INFO - Epoch 937 training loss = 1.627
2023-04-24 18:24:40,232 - INFO - Epoch 938 training loss = 1.628
2023-04-24 18:24:40,328 - INFO - Epoch 939 training loss = 1.628
2023-04-24 18:24:40,423 - INFO - Epoch 940 training loss = 1.627
2023-04-24 18:24:40,454 - INFO - Validation loss = 10.64
2023-04-24 18:24:40,550 - INFO - Epoch 941 training loss = 1.627
2023-04-24 18:24:40,645 - INFO - Epoch 942 training loss = 1.628
2023-04-24 18:24:40,741 - INFO - Epoch 943 training loss = 1.627
2023-04-24 18:24:40,836 - INFO - Epoch 944 training loss = 1.624
2023-04-24 18:24:40,932 - INFO - Epoch 945 training loss = 1.624
2023-04-24 18:24:41,028 - INFO - Epoch 946 training loss = 1.625
2023-04-24 18:24:41,123 - INFO - Epoch 947 training loss = 1.626
2023-04-24 18:24:41,220 - INFO - Epoch 948 training loss = 1.624
2023-04-24 18:24:41,316 - INFO - Epoch 949 training loss = 1.622
2023-04-24 18:24:41,412 - INFO - Epoch 950 training loss = 1.624
2023-04-24 18:24:41,442 - INFO - Validation loss = 10.64
2023-04-24 18:24:41,538 - INFO - Epoch 951 training loss = 1.625
2023-04-24 18:24:41,634 - INFO - Epoch 952 training loss = 1.625
2023-04-24 18:24:41,730 - INFO - Epoch 953 training loss = 1.62
2023-04-24 18:24:41,825 - INFO - Epoch 954 training loss = 1.623
2023-04-24 18:24:41,921 - INFO - Epoch 955 training loss = 1.622
2023-04-24 18:24:42,017 - INFO - Epoch 956 training loss = 1.623
2023-04-24 18:24:42,112 - INFO - Epoch 957 training loss = 1.622
2023-04-24 18:24:42,209 - INFO - Epoch 958 training loss = 1.62
2023-04-24 18:24:42,305 - INFO - Epoch 959 training loss = 1.621
2023-04-24 18:24:42,401 - INFO - Epoch 960 training loss = 1.623
2023-04-24 18:24:42,431 - INFO - Validation loss = 10.64
2023-04-24 18:24:42,527 - INFO - Epoch 961 training loss = 1.622
2023-04-24 18:24:42,623 - INFO - Epoch 962 training loss = 1.622
2023-04-24 18:24:42,718 - INFO - Epoch 963 training loss = 1.621
2023-04-24 18:24:42,814 - INFO - Epoch 964 training loss = 1.621
2023-04-24 18:24:42,959 - INFO - Epoch 965 training loss = 1.62
2023-04-24 18:24:43,055 - INFO - Epoch 966 training loss = 1.619
2023-04-24 18:24:43,150 - INFO - Epoch 967 training loss = 1.621
2023-04-24 18:24:43,248 - INFO - Epoch 968 training loss = 1.619
2023-04-24 18:24:43,344 - INFO - Epoch 969 training loss = 1.619
2023-04-24 18:24:43,441 - INFO - Epoch 970 training loss = 1.618
2023-04-24 18:24:43,471 - INFO - Validation loss = 10.64
2023-04-24 18:24:43,568 - INFO - Epoch 971 training loss = 1.619
2023-04-24 18:24:43,664 - INFO - Epoch 972 training loss = 1.619
2023-04-24 18:24:43,759 - INFO - Epoch 973 training loss = 1.62
2023-04-24 18:24:43,855 - INFO - Epoch 974 training loss = 1.62
2023-04-24 18:24:43,951 - INFO - Epoch 975 training loss = 1.62
2023-04-24 18:24:44,047 - INFO - Epoch 976 training loss = 1.616
2023-04-24 18:24:44,143 - INFO - Epoch 977 training loss = 1.618
2023-04-24 18:24:44,240 - INFO - Epoch 978 training loss = 1.619
2023-04-24 18:24:44,336 - INFO - Epoch 979 training loss = 1.62
2023-04-24 18:24:44,432 - INFO - Epoch 980 training loss = 1.617
2023-04-24 18:24:44,462 - INFO - Validation loss = 10.64
2023-04-24 18:24:44,558 - INFO - Epoch 981 training loss = 1.618
2023-04-24 18:24:44,654 - INFO - Epoch 982 training loss = 1.616
2023-04-24 18:24:44,750 - INFO - Epoch 983 training loss = 1.617
2023-04-24 18:24:44,846 - INFO - Epoch 984 training loss = 1.616
2023-04-24 18:24:44,941 - INFO - Epoch 985 training loss = 1.618
2023-04-24 18:24:45,037 - INFO - Epoch 986 training loss = 1.62
2023-04-24 18:24:45,133 - INFO - Epoch 987 training loss = 1.618
2023-04-24 18:24:45,230 - INFO - Epoch 988 training loss = 1.616
2023-04-24 18:24:45,326 - INFO - Epoch 989 training loss = 1.616
2023-04-24 18:24:45,422 - INFO - Epoch 990 training loss = 1.616
2023-04-24 18:24:45,452 - INFO - Validation loss = 10.64
2023-04-24 18:24:45,549 - INFO - Epoch 991 training loss = 1.616
2023-04-24 18:24:45,694 - INFO - Epoch 992 training loss = 1.617
2023-04-24 18:24:45,790 - INFO - Epoch 993 training loss = 1.618
2023-04-24 18:24:45,885 - INFO - Epoch 994 training loss = 1.616
2023-04-24 18:24:45,981 - INFO - Epoch 995 training loss = 1.618
2023-04-24 18:24:46,077 - INFO - Epoch 996 training loss = 1.616
2023-04-24 18:24:46,173 - INFO - Epoch 997 training loss = 1.618
2023-04-24 18:24:46,270 - INFO - Epoch 998 training loss = 1.617
2023-04-24 18:24:46,366 - INFO - Epoch 999 training loss = 1.617
2023-04-24 18:24:46,382 - INFO - Validation loss = 10.64
