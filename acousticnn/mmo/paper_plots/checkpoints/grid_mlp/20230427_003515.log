2023-04-27 00:35:15,281 - INFO - Config:
Namespace(config='configs/explicit_mlp.yaml', dir='/user/delden/acoustics/spp_ai_acoustics/acousticNN/experiments/arch/no_encoding/explicitmlp', epochs=100, device='cuda', seed=1, parameter_list='configs/data.yaml', encoding='none', dir_name='arch/no_encoding/explicitmlp')
2023-04-27 00:35:15,281 - INFO - Config:
Munch({'optimizer': Munch({'type': 'AdamW', 'kwargs': Munch({'lr': 0.001, 'weight_decay': 0.005, 'betas': [0.9, 0.95]})}), 'scheduler': Munch({'type': 'CosLR', 'kwargs': Munch({'epochs': 1000, 'initial_epochs': 101})}), 'model': Munch({'name': 'ExplicitMLP', 'input_encoding': True, 'encoding_dim': 16, 'embed_dim': 64, 'depth': 6, 'mlp_width': 256, 'num_frequencies': 200}), 'generation_frequency': 5001, 'validation_frequency': 10, 'batch_size': 128, 'epochs': 1000, 'gradient_clip': 10})
2023-04-27 00:35:28,995 - INFO - ==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ExplicitMLP                              [128, 200, 4]             --
├─GroupwiseProjection: 1-1               [128, 14, 64]             --
│    └─ModuleList: 2-1                   --                        --
│    │    └─Linear: 3-1                  [128, 4, 64]              128
│    │    └─Linear: 3-2                  [128, 5, 64]              128
│    │    └─Linear: 3-3                  [128, 5, 64]              128
├─MLP: 1-2                               [128, 256]                --
│    └─Linear: 2-2                       [128, 256]                229,632
│    └─ReLU: 2-3                         [128, 256]                --
│    └─Dropout: 2-4                      [128, 256]                --
│    └─Linear: 2-5                       [128, 256]                65,792
│    └─ReLU: 2-6                         [128, 256]                --
│    └─Dropout: 2-7                      [128, 256]                --
│    └─Linear: 2-8                       [128, 256]                65,792
│    └─ReLU: 2-9                         [128, 256]                --
│    └─Dropout: 2-10                     [128, 256]                --
│    └─Linear: 2-11                      [128, 256]                65,792
│    └─ReLU: 2-12                        [128, 256]                --
│    └─Dropout: 2-13                     [128, 256]                --
│    └─Linear: 2-14                      [128, 256]                65,792
│    └─ReLU: 2-15                        [128, 256]                --
│    └─Dropout: 2-16                     [128, 256]                --
│    └─Linear: 2-17                      [128, 256]                65,792
│    └─Dropout: 2-18                     [128, 256]                --
├─Linear: 1-3                            [128, 800]                205,600
==========================================================================================
Total params: 764,576
Trainable params: 764,576
Non-trainable params: 0
Total mult-adds (M): 97.87
==========================================================================================
Input size (MB): 0.11
Forward/backward pass size (MB): 3.31
Params size (MB): 3.06
Estimated Total Size (MB): 6.48
==========================================================================================
2023-04-27 00:35:29,096 - INFO - Epoch 0 training loss = 3.418e+03
2023-04-27 00:35:29,129 - INFO - Validation loss = 3.412e+03
2023-04-27 00:35:29,129 - INFO - best model
2023-04-27 00:35:29,244 - INFO - Epoch 1 training loss = 3.414e+03
2023-04-27 00:35:29,343 - INFO - Epoch 2 training loss = 3.396e+03
2023-04-27 00:35:29,442 - INFO - Epoch 3 training loss = 3.343e+03
2023-04-27 00:35:29,541 - INFO - Epoch 4 training loss = 3.18e+03
2023-04-27 00:35:29,640 - INFO - Epoch 5 training loss = 2.656e+03
2023-04-27 00:35:29,739 - INFO - Epoch 6 training loss = 1.565e+03
2023-04-27 00:35:29,838 - INFO - Epoch 7 training loss = 675.2
2023-04-27 00:35:29,937 - INFO - Epoch 8 training loss = 292.7
2023-04-27 00:35:30,037 - INFO - Epoch 9 training loss = 222.5
2023-04-27 00:35:30,136 - INFO - Epoch 10 training loss = 212.9
2023-04-27 00:35:30,163 - INFO - Validation loss = 217.3
2023-04-27 00:35:30,163 - INFO - best model
2023-04-27 00:35:30,273 - INFO - Epoch 11 training loss = 203.9
2023-04-27 00:35:30,372 - INFO - Epoch 12 training loss = 191.4
2023-04-27 00:35:30,471 - INFO - Epoch 13 training loss = 176.3
2023-04-27 00:35:30,570 - INFO - Epoch 14 training loss = 159.8
2023-04-27 00:35:30,669 - INFO - Epoch 15 training loss = 146.5
2023-04-27 00:35:30,768 - INFO - Epoch 16 training loss = 135.1
2023-04-27 00:35:30,867 - INFO - Epoch 17 training loss = 125.4
2023-04-27 00:35:30,966 - INFO - Epoch 18 training loss = 115.8
2023-04-27 00:35:31,065 - INFO - Epoch 19 training loss = 107.3
2023-04-27 00:35:31,164 - INFO - Epoch 20 training loss = 98.33
2023-04-27 00:35:31,191 - INFO - Validation loss = 101.3
2023-04-27 00:35:31,191 - INFO - best model
2023-04-27 00:35:31,302 - INFO - Epoch 21 training loss = 90.14
2023-04-27 00:35:31,401 - INFO - Epoch 22 training loss = 81.99
2023-04-27 00:35:31,500 - INFO - Epoch 23 training loss = 76.24
2023-04-27 00:35:31,599 - INFO - Epoch 24 training loss = 70.5
2023-04-27 00:35:31,698 - INFO - Epoch 25 training loss = 66.5
2023-04-27 00:35:31,797 - INFO - Epoch 26 training loss = 63.33
2023-04-27 00:35:31,897 - INFO - Epoch 27 training loss = 60.59
2023-04-27 00:35:31,996 - INFO - Epoch 28 training loss = 58.16
2023-04-27 00:35:32,154 - INFO - Epoch 29 training loss = 55.75
2023-04-27 00:35:32,253 - INFO - Epoch 30 training loss = 53.85
2023-04-27 00:35:32,281 - INFO - Validation loss = 58.0
2023-04-27 00:35:32,281 - INFO - best model
2023-04-27 00:35:32,391 - INFO - Epoch 31 training loss = 51.54
2023-04-27 00:35:32,490 - INFO - Epoch 32 training loss = 49.94
2023-04-27 00:35:32,589 - INFO - Epoch 33 training loss = 48.25
2023-04-27 00:35:32,688 - INFO - Epoch 34 training loss = 47.48
2023-04-27 00:35:32,788 - INFO - Epoch 35 training loss = 45.41
2023-04-27 00:35:32,887 - INFO - Epoch 36 training loss = 44.54
2023-04-27 00:35:32,986 - INFO - Epoch 37 training loss = 43.12
2023-04-27 00:35:33,085 - INFO - Epoch 38 training loss = 41.56
2023-04-27 00:35:33,184 - INFO - Epoch 39 training loss = 40.96
2023-04-27 00:35:33,283 - INFO - Epoch 40 training loss = 39.53
2023-04-27 00:35:33,311 - INFO - Validation loss = 43.18
2023-04-27 00:35:33,311 - INFO - best model
2023-04-27 00:35:33,421 - INFO - Epoch 41 training loss = 38.51
2023-04-27 00:35:33,520 - INFO - Epoch 42 training loss = 37.38
2023-04-27 00:35:33,619 - INFO - Epoch 43 training loss = 36.91
2023-04-27 00:35:33,718 - INFO - Epoch 44 training loss = 35.12
2023-04-27 00:35:33,817 - INFO - Epoch 45 training loss = 35.09
2023-04-27 00:35:33,916 - INFO - Epoch 46 training loss = 33.47
2023-04-27 00:35:34,015 - INFO - Epoch 47 training loss = 33.78
2023-04-27 00:35:34,115 - INFO - Epoch 48 training loss = 32.7
2023-04-27 00:35:34,214 - INFO - Epoch 49 training loss = 31.48
2023-04-27 00:35:34,313 - INFO - Epoch 50 training loss = 31.1
2023-04-27 00:35:34,340 - INFO - Validation loss = 34.87
2023-04-27 00:35:34,340 - INFO - best model
2023-04-27 00:35:34,451 - INFO - Epoch 51 training loss = 30.26
2023-04-27 00:35:34,550 - INFO - Epoch 52 training loss = 29.81
2023-04-27 00:35:34,649 - INFO - Epoch 53 training loss = 29.08
2023-04-27 00:35:34,748 - INFO - Epoch 54 training loss = 28.09
2023-04-27 00:35:34,848 - INFO - Epoch 55 training loss = 27.98
2023-04-27 00:35:34,947 - INFO - Epoch 56 training loss = 27.96
2023-04-27 00:35:35,048 - INFO - Epoch 57 training loss = 26.95
2023-04-27 00:35:35,147 - INFO - Epoch 58 training loss = 26.94
2023-04-27 00:35:35,246 - INFO - Epoch 59 training loss = 26.01
2023-04-27 00:35:35,345 - INFO - Epoch 60 training loss = 25.69
2023-04-27 00:35:35,372 - INFO - Validation loss = 30.15
2023-04-27 00:35:35,373 - INFO - best model
2023-04-27 00:35:35,541 - INFO - Epoch 61 training loss = 25.57
2023-04-27 00:35:35,640 - INFO - Epoch 62 training loss = 25.31
2023-04-27 00:35:35,739 - INFO - Epoch 63 training loss = 24.74
2023-04-27 00:35:35,838 - INFO - Epoch 64 training loss = 23.99
2023-04-27 00:35:35,937 - INFO - Epoch 65 training loss = 23.58
2023-04-27 00:35:36,037 - INFO - Epoch 66 training loss = 23.85
2023-04-27 00:35:36,136 - INFO - Epoch 67 training loss = 24.01
2023-04-27 00:35:36,235 - INFO - Epoch 68 training loss = 22.69
2023-04-27 00:35:36,334 - INFO - Epoch 69 training loss = 22.73
2023-04-27 00:35:36,433 - INFO - Epoch 70 training loss = 22.27
2023-04-27 00:35:36,461 - INFO - Validation loss = 26.83
2023-04-27 00:35:36,461 - INFO - best model
2023-04-27 00:35:36,571 - INFO - Epoch 71 training loss = 21.94
2023-04-27 00:35:36,671 - INFO - Epoch 72 training loss = 21.48
2023-04-27 00:35:36,770 - INFO - Epoch 73 training loss = 22.01
2023-04-27 00:35:36,870 - INFO - Epoch 74 training loss = 21.41
2023-04-27 00:35:36,969 - INFO - Epoch 75 training loss = 20.69
2023-04-27 00:35:37,068 - INFO - Epoch 76 training loss = 21.53
2023-04-27 00:35:37,168 - INFO - Epoch 77 training loss = 19.91
2023-04-27 00:35:37,267 - INFO - Epoch 78 training loss = 20.15
2023-04-27 00:35:37,366 - INFO - Epoch 79 training loss = 19.95
2023-04-27 00:35:37,465 - INFO - Epoch 80 training loss = 19.62
2023-04-27 00:35:37,492 - INFO - Validation loss = 28.41
2023-04-27 00:35:37,592 - INFO - Epoch 81 training loss = 20.23
2023-04-27 00:35:37,691 - INFO - Epoch 82 training loss = 19.23
2023-04-27 00:35:37,790 - INFO - Epoch 83 training loss = 20.0
2023-04-27 00:35:37,889 - INFO - Epoch 84 training loss = 18.93
2023-04-27 00:35:37,988 - INFO - Epoch 85 training loss = 18.65
2023-04-27 00:35:38,088 - INFO - Epoch 86 training loss = 18.87
2023-04-27 00:35:38,187 - INFO - Epoch 87 training loss = 18.94
2023-04-27 00:35:38,286 - INFO - Epoch 88 training loss = 17.8
2023-04-27 00:35:38,385 - INFO - Epoch 89 training loss = 18.5
2023-04-27 00:35:38,484 - INFO - Epoch 90 training loss = 18.75
2023-04-27 00:35:38,511 - INFO - Validation loss = 21.88
2023-04-27 00:35:38,511 - INFO - best model
2023-04-27 00:35:38,621 - INFO - Epoch 91 training loss = 17.37
2023-04-27 00:35:38,779 - INFO - Epoch 92 training loss = 17.17
2023-04-27 00:35:38,879 - INFO - Epoch 93 training loss = 17.9
2023-04-27 00:35:38,978 - INFO - Epoch 94 training loss = 17.11
2023-04-27 00:35:39,078 - INFO - Epoch 95 training loss = 16.9
2023-04-27 00:35:39,177 - INFO - Epoch 96 training loss = 17.45
2023-04-27 00:35:39,276 - INFO - Epoch 97 training loss = 16.71
2023-04-27 00:35:39,376 - INFO - Epoch 98 training loss = 17.39
2023-04-27 00:35:39,475 - INFO - Epoch 99 training loss = 15.79
2023-04-27 00:35:39,574 - INFO - Epoch 100 training loss = 16.19
2023-04-27 00:35:39,602 - INFO - Validation loss = 21.06
2023-04-27 00:35:39,602 - INFO - best model
2023-04-27 00:35:39,712 - INFO - Epoch 101 training loss = 16.16
2023-04-27 00:35:39,812 - INFO - Epoch 102 training loss = 16.24
2023-04-27 00:35:39,911 - INFO - Epoch 103 training loss = 16.92
2023-04-27 00:35:40,010 - INFO - Epoch 104 training loss = 16.03
2023-04-27 00:35:40,109 - INFO - Epoch 105 training loss = 18.1
2023-04-27 00:35:40,209 - INFO - Epoch 106 training loss = 14.22
2023-04-27 00:35:40,308 - INFO - Epoch 107 training loss = 14.72
2023-04-27 00:35:40,407 - INFO - Epoch 108 training loss = 16.43
2023-04-27 00:35:40,506 - INFO - Epoch 109 training loss = 14.07
2023-04-27 00:35:40,605 - INFO - Epoch 110 training loss = 14.74
2023-04-27 00:35:40,633 - INFO - Validation loss = 21.62
2023-04-27 00:35:40,732 - INFO - Epoch 111 training loss = 14.33
2023-04-27 00:35:40,832 - INFO - Epoch 112 training loss = 14.97
2023-04-27 00:35:40,931 - INFO - Epoch 113 training loss = 13.58
2023-04-27 00:35:41,030 - INFO - Epoch 114 training loss = 14.05
2023-04-27 00:35:41,129 - INFO - Epoch 115 training loss = 13.54
2023-04-27 00:35:41,228 - INFO - Epoch 116 training loss = 12.94
2023-04-27 00:35:41,327 - INFO - Epoch 117 training loss = 13.7
2023-04-27 00:35:41,426 - INFO - Epoch 118 training loss = 13.61
2023-04-27 00:35:41,525 - INFO - Epoch 119 training loss = 12.94
2023-04-27 00:35:41,625 - INFO - Epoch 120 training loss = 12.91
2023-04-27 00:35:41,652 - INFO - Validation loss = 17.07
2023-04-27 00:35:41,653 - INFO - best model
2023-04-27 00:35:41,763 - INFO - Epoch 121 training loss = 12.74
2023-04-27 00:35:41,864 - INFO - Epoch 122 training loss = 13.72
2023-04-27 00:35:41,963 - INFO - Epoch 123 training loss = 12.4
2023-04-27 00:35:42,121 - INFO - Epoch 124 training loss = 12.1
2023-04-27 00:35:42,220 - INFO - Epoch 125 training loss = 12.32
2023-04-27 00:35:42,319 - INFO - Epoch 126 training loss = 12.47
2023-04-27 00:35:42,418 - INFO - Epoch 127 training loss = 12.2
2023-04-27 00:35:42,517 - INFO - Epoch 128 training loss = 12.46
2023-04-27 00:35:42,616 - INFO - Epoch 129 training loss = 11.58
2023-04-27 00:35:42,715 - INFO - Epoch 130 training loss = 11.87
2023-04-27 00:35:42,742 - INFO - Validation loss = 16.82
2023-04-27 00:35:42,742 - INFO - best model
2023-04-27 00:35:42,853 - INFO - Epoch 131 training loss = 11.48
2023-04-27 00:35:42,953 - INFO - Epoch 132 training loss = 12.51
2023-04-27 00:35:43,052 - INFO - Epoch 133 training loss = 11.21
2023-04-27 00:35:43,152 - INFO - Epoch 134 training loss = 11.25
2023-04-27 00:35:43,251 - INFO - Epoch 135 training loss = 11.07
2023-04-27 00:35:43,350 - INFO - Epoch 136 training loss = 11.11
2023-04-27 00:35:43,449 - INFO - Epoch 137 training loss = 11.28
2023-04-27 00:35:43,549 - INFO - Epoch 138 training loss = 10.84
2023-04-27 00:35:43,648 - INFO - Epoch 139 training loss = 10.51
2023-04-27 00:35:43,748 - INFO - Epoch 140 training loss = 10.66
2023-04-27 00:35:43,775 - INFO - Validation loss = 15.77
2023-04-27 00:35:43,775 - INFO - best model
2023-04-27 00:35:43,886 - INFO - Epoch 141 training loss = 11.92
2023-04-27 00:35:43,986 - INFO - Epoch 142 training loss = 10.49
2023-04-27 00:35:44,085 - INFO - Epoch 143 training loss = 10.64
2023-04-27 00:35:44,185 - INFO - Epoch 144 training loss = 11.19
2023-04-27 00:35:44,284 - INFO - Epoch 145 training loss = 9.936
2023-04-27 00:35:44,383 - INFO - Epoch 146 training loss = 11.81
2023-04-27 00:35:44,482 - INFO - Epoch 147 training loss = 9.792
2023-04-27 00:35:44,581 - INFO - Epoch 148 training loss = 10.02
2023-04-27 00:35:44,680 - INFO - Epoch 149 training loss = 9.98
2023-04-27 00:35:44,779 - INFO - Epoch 150 training loss = 9.831
2023-04-27 00:35:44,807 - INFO - Validation loss = 15.08
2023-04-27 00:35:44,807 - INFO - best model
2023-04-27 00:35:44,919 - INFO - Epoch 151 training loss = 9.808
2023-04-27 00:35:45,018 - INFO - Epoch 152 training loss = 10.48
2023-04-27 00:35:45,117 - INFO - Epoch 153 training loss = 9.27
2023-04-27 00:35:45,217 - INFO - Epoch 154 training loss = 12.18
2023-04-27 00:35:45,375 - INFO - Epoch 155 training loss = 9.059
2023-04-27 00:35:45,474 - INFO - Epoch 156 training loss = 9.978
2023-04-27 00:35:45,574 - INFO - Epoch 157 training loss = 9.548
2023-04-27 00:35:45,673 - INFO - Epoch 158 training loss = 9.208
2023-04-27 00:35:45,772 - INFO - Epoch 159 training loss = 10.21
2023-04-27 00:35:45,872 - INFO - Epoch 160 training loss = 9.05
2023-04-27 00:35:45,899 - INFO - Validation loss = 15.44
2023-04-27 00:35:46,000 - INFO - Epoch 161 training loss = 9.511
2023-04-27 00:35:46,099 - INFO - Epoch 162 training loss = 9.209
2023-04-27 00:35:46,198 - INFO - Epoch 163 training loss = 8.863
2023-04-27 00:35:46,297 - INFO - Epoch 164 training loss = 9.08
2023-04-27 00:35:46,396 - INFO - Epoch 165 training loss = 8.902
2023-04-27 00:35:46,495 - INFO - Epoch 166 training loss = 9.048
2023-04-27 00:35:46,594 - INFO - Epoch 167 training loss = 9.058
2023-04-27 00:35:46,693 - INFO - Epoch 168 training loss = 8.801
2023-04-27 00:35:46,792 - INFO - Epoch 169 training loss = 9.01
2023-04-27 00:35:46,893 - INFO - Epoch 170 training loss = 9.268
2023-04-27 00:35:46,920 - INFO - Validation loss = 14.03
2023-04-27 00:35:46,921 - INFO - best model
2023-04-27 00:35:47,033 - INFO - Epoch 171 training loss = 8.783
2023-04-27 00:35:47,133 - INFO - Epoch 172 training loss = 8.231
2023-04-27 00:35:47,232 - INFO - Epoch 173 training loss = 9.245
2023-04-27 00:35:47,331 - INFO - Epoch 174 training loss = 8.081
2023-04-27 00:35:47,430 - INFO - Epoch 175 training loss = 9.436
2023-04-27 00:35:47,529 - INFO - Epoch 176 training loss = 7.996
2023-04-27 00:35:47,629 - INFO - Epoch 177 training loss = 8.758
2023-04-27 00:35:47,728 - INFO - Epoch 178 training loss = 8.119
2023-04-27 00:35:47,827 - INFO - Epoch 179 training loss = 8.679
2023-04-27 00:35:47,926 - INFO - Epoch 180 training loss = 8.101
2023-04-27 00:35:47,954 - INFO - Validation loss = 14.15
2023-04-27 00:35:48,055 - INFO - Epoch 181 training loss = 8.11
2023-04-27 00:35:48,153 - INFO - Epoch 182 training loss = 7.92
2023-04-27 00:35:48,252 - INFO - Epoch 183 training loss = 8.281
2023-04-27 00:35:48,351 - INFO - Epoch 184 training loss = 8.777
2023-04-27 00:35:48,450 - INFO - Epoch 185 training loss = 9.311
2023-04-27 00:35:48,608 - INFO - Epoch 186 training loss = 7.711
2023-04-27 00:35:48,707 - INFO - Epoch 187 training loss = 8.156
2023-04-27 00:35:48,806 - INFO - Epoch 188 training loss = 7.949
2023-04-27 00:35:48,905 - INFO - Epoch 189 training loss = 8.178
2023-04-27 00:35:49,004 - INFO - Epoch 190 training loss = 7.789
2023-04-27 00:35:49,031 - INFO - Validation loss = 13.41
2023-04-27 00:35:49,032 - INFO - best model
2023-04-27 00:35:49,142 - INFO - Epoch 191 training loss = 8.055
2023-04-27 00:35:49,241 - INFO - Epoch 192 training loss = 7.682
2023-04-27 00:35:49,340 - INFO - Epoch 193 training loss = 7.458
2023-04-27 00:35:49,439 - INFO - Epoch 194 training loss = 7.77
2023-04-27 00:35:49,538 - INFO - Epoch 195 training loss = 7.678
2023-04-27 00:35:49,637 - INFO - Epoch 196 training loss = 8.123
2023-04-27 00:35:49,736 - INFO - Epoch 197 training loss = 7.232
2023-04-27 00:35:49,835 - INFO - Epoch 198 training loss = 7.799
2023-04-27 00:35:49,934 - INFO - Epoch 199 training loss = 7.15
2023-04-27 00:35:50,033 - INFO - Epoch 200 training loss = 7.395
2023-04-27 00:35:50,061 - INFO - Validation loss = 13.09
2023-04-27 00:35:50,061 - INFO - best model
2023-04-27 00:35:50,172 - INFO - Epoch 201 training loss = 7.553
2023-04-27 00:35:50,271 - INFO - Epoch 202 training loss = 7.605
2023-04-27 00:35:50,369 - INFO - Epoch 203 training loss = 7.177
2023-04-27 00:35:50,468 - INFO - Epoch 204 training loss = 7.249
2023-04-27 00:35:50,568 - INFO - Epoch 205 training loss = 7.036
2023-04-27 00:35:50,666 - INFO - Epoch 206 training loss = 7.006
2023-04-27 00:35:50,766 - INFO - Epoch 207 training loss = 7.121
2023-04-27 00:35:50,865 - INFO - Epoch 208 training loss = 7.024
2023-04-27 00:35:50,964 - INFO - Epoch 209 training loss = 7.181
2023-04-27 00:35:51,063 - INFO - Epoch 210 training loss = 6.933
2023-04-27 00:35:51,091 - INFO - Validation loss = 12.79
2023-04-27 00:35:51,091 - INFO - best model
2023-04-27 00:35:51,201 - INFO - Epoch 211 training loss = 7.144
2023-04-27 00:35:51,301 - INFO - Epoch 212 training loss = 7.155
2023-04-27 00:35:51,400 - INFO - Epoch 213 training loss = 7.28
2023-04-27 00:35:51,498 - INFO - Epoch 214 training loss = 7.264
2023-04-27 00:35:51,597 - INFO - Epoch 215 training loss = 6.751
2023-04-27 00:35:51,697 - INFO - Epoch 216 training loss = 7.101
2023-04-27 00:35:51,796 - INFO - Epoch 217 training loss = 7.153
2023-04-27 00:35:51,954 - INFO - Epoch 218 training loss = 6.754
2023-04-27 00:35:52,053 - INFO - Epoch 219 training loss = 7.178
2023-04-27 00:35:52,152 - INFO - Epoch 220 training loss = 6.558
2023-04-27 00:35:52,179 - INFO - Validation loss = 13.07
2023-04-27 00:35:52,278 - INFO - Epoch 221 training loss = 6.509
2023-04-27 00:35:52,377 - INFO - Epoch 222 training loss = 6.656
2023-04-27 00:35:52,476 - INFO - Epoch 223 training loss = 6.506
2023-04-27 00:35:52,575 - INFO - Epoch 224 training loss = 7.233
2023-04-27 00:35:52,674 - INFO - Epoch 225 training loss = 6.343
2023-04-27 00:35:52,773 - INFO - Epoch 226 training loss = 6.686
2023-04-27 00:35:52,873 - INFO - Epoch 227 training loss = 6.233
2023-04-27 00:35:52,972 - INFO - Epoch 228 training loss = 6.546
2023-04-27 00:35:53,071 - INFO - Epoch 229 training loss = 7.013
2023-04-27 00:35:53,170 - INFO - Epoch 230 training loss = 6.429
2023-04-27 00:35:53,197 - INFO - Validation loss = 12.75
2023-04-27 00:35:53,198 - INFO - best model
2023-04-27 00:35:53,308 - INFO - Epoch 231 training loss = 6.381
2023-04-27 00:35:53,407 - INFO - Epoch 232 training loss = 6.901
2023-04-27 00:35:53,506 - INFO - Epoch 233 training loss = 6.298
2023-04-27 00:35:53,605 - INFO - Epoch 234 training loss = 6.343
2023-04-27 00:35:53,704 - INFO - Epoch 235 training loss = 6.302
2023-04-27 00:35:53,803 - INFO - Epoch 236 training loss = 6.267
2023-04-27 00:35:53,902 - INFO - Epoch 237 training loss = 6.194
2023-04-27 00:35:54,001 - INFO - Epoch 238 training loss = 6.218
2023-04-27 00:35:54,100 - INFO - Epoch 239 training loss = 6.193
2023-04-27 00:35:54,199 - INFO - Epoch 240 training loss = 6.361
2023-04-27 00:35:54,227 - INFO - Validation loss = 13.05
2023-04-27 00:35:54,326 - INFO - Epoch 241 training loss = 5.872
2023-04-27 00:35:54,425 - INFO - Epoch 242 training loss = 6.625
2023-04-27 00:35:54,524 - INFO - Epoch 243 training loss = 5.814
2023-04-27 00:35:54,623 - INFO - Epoch 244 training loss = 6.099
2023-04-27 00:35:54,722 - INFO - Epoch 245 training loss = 6.093
2023-04-27 00:35:54,821 - INFO - Epoch 246 training loss = 5.893
2023-04-27 00:35:54,920 - INFO - Epoch 247 training loss = 5.904
2023-04-27 00:35:55,019 - INFO - Epoch 248 training loss = 5.885
2023-04-27 00:35:55,118 - INFO - Epoch 249 training loss = 6.103
2023-04-27 00:35:55,276 - INFO - Epoch 250 training loss = 6.044
2023-04-27 00:35:55,303 - INFO - Validation loss = 12.46
2023-04-27 00:35:55,304 - INFO - best model
2023-04-27 00:35:55,414 - INFO - Epoch 251 training loss = 5.751
2023-04-27 00:35:55,513 - INFO - Epoch 252 training loss = 5.985
2023-04-27 00:35:55,612 - INFO - Epoch 253 training loss = 5.832
2023-04-27 00:35:55,711 - INFO - Epoch 254 training loss = 6.017
2023-04-27 00:35:55,810 - INFO - Epoch 255 training loss = 5.905
2023-04-27 00:35:55,909 - INFO - Epoch 256 training loss = 5.59
2023-04-27 00:35:56,008 - INFO - Epoch 257 training loss = 5.637
2023-04-27 00:35:56,107 - INFO - Epoch 258 training loss = 6.056
2023-04-27 00:35:56,207 - INFO - Epoch 259 training loss = 5.638
2023-04-27 00:35:56,306 - INFO - Epoch 260 training loss = 5.827
2023-04-27 00:35:56,333 - INFO - Validation loss = 12.37
2023-04-27 00:35:56,333 - INFO - best model
2023-04-27 00:35:56,443 - INFO - Epoch 261 training loss = 5.58
2023-04-27 00:35:56,542 - INFO - Epoch 262 training loss = 5.745
2023-04-27 00:35:56,641 - INFO - Epoch 263 training loss = 5.648
2023-04-27 00:35:56,740 - INFO - Epoch 264 training loss = 5.509
2023-04-27 00:35:56,840 - INFO - Epoch 265 training loss = 5.681
2023-04-27 00:35:56,939 - INFO - Epoch 266 training loss = 5.558
2023-04-27 00:35:57,039 - INFO - Epoch 267 training loss = 5.629
2023-04-27 00:35:57,138 - INFO - Epoch 268 training loss = 5.653
2023-04-27 00:35:57,238 - INFO - Epoch 269 training loss = 5.429
2023-04-27 00:35:57,337 - INFO - Epoch 270 training loss = 5.436
2023-04-27 00:35:57,364 - INFO - Validation loss = 14.01
2023-04-27 00:35:57,463 - INFO - Epoch 271 training loss = 6.212
2023-04-27 00:35:57,563 - INFO - Epoch 272 training loss = 5.234
2023-04-27 00:35:57,662 - INFO - Epoch 273 training loss = 6.346
2023-04-27 00:35:57,762 - INFO - Epoch 274 training loss = 5.102
2023-04-27 00:35:57,861 - INFO - Epoch 275 training loss = 5.287
2023-04-27 00:35:57,960 - INFO - Epoch 276 training loss = 5.721
2023-04-27 00:35:58,060 - INFO - Epoch 277 training loss = 5.308
2023-04-27 00:35:58,160 - INFO - Epoch 278 training loss = 5.29
2023-04-27 00:35:58,259 - INFO - Epoch 279 training loss = 5.496
2023-04-27 00:35:58,358 - INFO - Epoch 280 training loss = 5.212
2023-04-27 00:35:58,386 - INFO - Validation loss = 12.25
2023-04-27 00:35:58,386 - INFO - best model
2023-04-27 00:35:58,555 - INFO - Epoch 281 training loss = 5.332
2023-04-27 00:35:58,654 - INFO - Epoch 282 training loss = 5.24
2023-04-27 00:35:58,753 - INFO - Epoch 283 training loss = 5.185
2023-04-27 00:35:58,853 - INFO - Epoch 284 training loss = 5.282
2023-04-27 00:35:58,952 - INFO - Epoch 285 training loss = 5.078
2023-04-27 00:35:59,051 - INFO - Epoch 286 training loss = 5.278
2023-04-27 00:35:59,150 - INFO - Epoch 287 training loss = 5.046
2023-04-27 00:35:59,249 - INFO - Epoch 288 training loss = 5.503
2023-04-27 00:35:59,348 - INFO - Epoch 289 training loss = 5.157
2023-04-27 00:35:59,447 - INFO - Epoch 290 training loss = 5.32
2023-04-27 00:35:59,474 - INFO - Validation loss = 11.85
2023-04-27 00:35:59,474 - INFO - best model
2023-04-27 00:35:59,584 - INFO - Epoch 291 training loss = 5.046
2023-04-27 00:35:59,683 - INFO - Epoch 292 training loss = 4.981
2023-04-27 00:35:59,782 - INFO - Epoch 293 training loss = 5.02
2023-04-27 00:35:59,881 - INFO - Epoch 294 training loss = 5.063
2023-04-27 00:35:59,980 - INFO - Epoch 295 training loss = 4.924
2023-04-27 00:36:00,080 - INFO - Epoch 296 training loss = 5.181
2023-04-27 00:36:00,179 - INFO - Epoch 297 training loss = 5.201
2023-04-27 00:36:00,278 - INFO - Epoch 298 training loss = 4.835
2023-04-27 00:36:00,377 - INFO - Epoch 299 training loss = 5.075
2023-04-27 00:36:00,476 - INFO - Epoch 300 training loss = 4.934
2023-04-27 00:36:00,504 - INFO - Validation loss = 11.4
2023-04-27 00:36:00,504 - INFO - best model
2023-04-27 00:36:00,614 - INFO - Epoch 301 training loss = 4.832
2023-04-27 00:36:00,713 - INFO - Epoch 302 training loss = 4.838
2023-04-27 00:36:00,812 - INFO - Epoch 303 training loss = 4.809
2023-04-27 00:36:00,911 - INFO - Epoch 304 training loss = 5.095
2023-04-27 00:36:01,010 - INFO - Epoch 305 training loss = 4.882
2023-04-27 00:36:01,109 - INFO - Epoch 306 training loss = 5.169
2023-04-27 00:36:01,208 - INFO - Epoch 307 training loss = 4.846
2023-04-27 00:36:01,307 - INFO - Epoch 308 training loss = 4.725
2023-04-27 00:36:01,406 - INFO - Epoch 309 training loss = 4.903
2023-04-27 00:36:01,505 - INFO - Epoch 310 training loss = 4.806
2023-04-27 00:36:01,532 - INFO - Validation loss = 11.55
2023-04-27 00:36:01,632 - INFO - Epoch 311 training loss = 4.822
2023-04-27 00:36:01,789 - INFO - Epoch 312 training loss = 4.839
2023-04-27 00:36:01,889 - INFO - Epoch 313 training loss = 4.786
2023-04-27 00:36:01,989 - INFO - Epoch 314 training loss = 4.751
2023-04-27 00:36:02,088 - INFO - Epoch 315 training loss = 5.018
2023-04-27 00:36:02,187 - INFO - Epoch 316 training loss = 4.586
2023-04-27 00:36:02,286 - INFO - Epoch 317 training loss = 4.779
2023-04-27 00:36:02,385 - INFO - Epoch 318 training loss = 4.671
2023-04-27 00:36:02,484 - INFO - Epoch 319 training loss = 4.46
2023-04-27 00:36:02,583 - INFO - Epoch 320 training loss = 5.026
2023-04-27 00:36:02,610 - INFO - Validation loss = 12.11
2023-04-27 00:36:02,709 - INFO - Epoch 321 training loss = 4.515
2023-04-27 00:36:02,808 - INFO - Epoch 322 training loss = 4.62
2023-04-27 00:36:02,907 - INFO - Epoch 323 training loss = 4.508
2023-04-27 00:36:03,006 - INFO - Epoch 324 training loss = 4.751
2023-04-27 00:36:03,105 - INFO - Epoch 325 training loss = 4.479
2023-04-27 00:36:03,205 - INFO - Epoch 326 training loss = 4.417
2023-04-27 00:36:03,304 - INFO - Epoch 327 training loss = 5.116
2023-04-27 00:36:03,403 - INFO - Epoch 328 training loss = 4.267
2023-04-27 00:36:03,502 - INFO - Epoch 329 training loss = 4.535
2023-04-27 00:36:03,601 - INFO - Epoch 330 training loss = 4.42
2023-04-27 00:36:03,628 - INFO - Validation loss = 11.41
2023-04-27 00:36:03,727 - INFO - Epoch 331 training loss = 4.422
2023-04-27 00:36:03,826 - INFO - Epoch 332 training loss = 4.535
2023-04-27 00:36:03,925 - INFO - Epoch 333 training loss = 4.424
2023-04-27 00:36:04,024 - INFO - Epoch 334 training loss = 4.666
2023-04-27 00:36:04,123 - INFO - Epoch 335 training loss = 4.438
2023-04-27 00:36:04,222 - INFO - Epoch 336 training loss = 4.336
2023-04-27 00:36:04,321 - INFO - Epoch 337 training loss = 4.296
2023-04-27 00:36:04,420 - INFO - Epoch 338 training loss = 4.454
2023-04-27 00:36:04,519 - INFO - Epoch 339 training loss = 4.28
2023-04-27 00:36:04,618 - INFO - Epoch 340 training loss = 4.452
2023-04-27 00:36:04,645 - INFO - Validation loss = 11.39
2023-04-27 00:36:04,645 - INFO - best model
2023-04-27 00:36:04,755 - INFO - Epoch 341 training loss = 4.213
2023-04-27 00:36:04,855 - INFO - Epoch 342 training loss = 4.334
2023-04-27 00:36:04,953 - INFO - Epoch 343 training loss = 4.389
2023-04-27 00:36:05,112 - INFO - Epoch 344 training loss = 4.204
2023-04-27 00:36:05,211 - INFO - Epoch 345 training loss = 4.373
2023-04-27 00:36:05,310 - INFO - Epoch 346 training loss = 4.335
2023-04-27 00:36:05,409 - INFO - Epoch 347 training loss = 4.394
2023-04-27 00:36:05,508 - INFO - Epoch 348 training loss = 4.102
2023-04-27 00:36:05,607 - INFO - Epoch 349 training loss = 4.208
2023-04-27 00:36:05,706 - INFO - Epoch 350 training loss =  4.2
2023-04-27 00:36:05,734 - INFO - Validation loss = 11.54
2023-04-27 00:36:05,833 - INFO - Epoch 351 training loss = 4.237
2023-04-27 00:36:05,932 - INFO - Epoch 352 training loss = 4.255
2023-04-27 00:36:06,031 - INFO - Epoch 353 training loss = 4.285
2023-04-27 00:36:06,130 - INFO - Epoch 354 training loss = 4.002
2023-04-27 00:36:06,229 - INFO - Epoch 355 training loss = 4.053
2023-04-27 00:36:06,328 - INFO - Epoch 356 training loss = 4.172
2023-04-27 00:36:06,427 - INFO - Epoch 357 training loss = 4.318
2023-04-27 00:36:06,526 - INFO - Epoch 358 training loss = 4.077
2023-04-27 00:36:06,625 - INFO - Epoch 359 training loss = 4.161
2023-04-27 00:36:06,724 - INFO - Epoch 360 training loss = 3.986
2023-04-27 00:36:06,751 - INFO - Validation loss = 11.67
2023-04-27 00:36:06,850 - INFO - Epoch 361 training loss = 4.419
2023-04-27 00:36:06,949 - INFO - Epoch 362 training loss = 3.949
2023-04-27 00:36:07,048 - INFO - Epoch 363 training loss = 4.037
2023-04-27 00:36:07,148 - INFO - Epoch 364 training loss = 4.084
2023-04-27 00:36:07,247 - INFO - Epoch 365 training loss = 4.255
2023-04-27 00:36:07,346 - INFO - Epoch 366 training loss = 4.107
2023-04-27 00:36:07,444 - INFO - Epoch 367 training loss = 3.906
2023-04-27 00:36:07,543 - INFO - Epoch 368 training loss = 3.965
2023-04-27 00:36:07,643 - INFO - Epoch 369 training loss = 3.799
2023-04-27 00:36:07,742 - INFO - Epoch 370 training loss = 4.141
2023-04-27 00:36:07,769 - INFO - Validation loss = 11.34
2023-04-27 00:36:07,769 - INFO - best model
2023-04-27 00:36:07,879 - INFO - Epoch 371 training loss = 3.935
2023-04-27 00:36:07,978 - INFO - Epoch 372 training loss = 4.011
2023-04-27 00:36:08,077 - INFO - Epoch 373 training loss = 3.809
2023-04-27 00:36:08,177 - INFO - Epoch 374 training loss = 4.221
2023-04-27 00:36:08,334 - INFO - Epoch 375 training loss = 3.857
2023-04-27 00:36:08,433 - INFO - Epoch 376 training loss = 3.944
2023-04-27 00:36:08,532 - INFO - Epoch 377 training loss = 3.892
2023-04-27 00:36:08,631 - INFO - Epoch 378 training loss = 3.821
2023-04-27 00:36:08,731 - INFO - Epoch 379 training loss =  3.8
2023-04-27 00:36:08,830 - INFO - Epoch 380 training loss = 3.84
2023-04-27 00:36:08,857 - INFO - Validation loss = 11.07
2023-04-27 00:36:08,858 - INFO - best model
2023-04-27 00:36:08,969 - INFO - Epoch 381 training loss = 3.909
2023-04-27 00:36:09,069 - INFO - Epoch 382 training loss = 3.84
2023-04-27 00:36:09,169 - INFO - Epoch 383 training loss = 3.859
2023-04-27 00:36:09,268 - INFO - Epoch 384 training loss = 3.817
2023-04-27 00:36:09,367 - INFO - Epoch 385 training loss = 3.775
2023-04-27 00:36:09,467 - INFO - Epoch 386 training loss = 3.749
2023-04-27 00:36:09,566 - INFO - Epoch 387 training loss = 3.796
2023-04-27 00:36:09,666 - INFO - Epoch 388 training loss = 3.757
2023-04-27 00:36:09,765 - INFO - Epoch 389 training loss = 3.637
2023-04-27 00:36:09,865 - INFO - Epoch 390 training loss = 3.893
2023-04-27 00:36:09,892 - INFO - Validation loss = 11.25
2023-04-27 00:36:09,991 - INFO - Epoch 391 training loss = 3.728
2023-04-27 00:36:10,092 - INFO - Epoch 392 training loss = 3.801
2023-04-27 00:36:10,191 - INFO - Epoch 393 training loss = 3.682
2023-04-27 00:36:10,291 - INFO - Epoch 394 training loss = 3.722
2023-04-27 00:36:10,390 - INFO - Epoch 395 training loss = 3.675
2023-04-27 00:36:10,489 - INFO - Epoch 396 training loss = 3.762
2023-04-27 00:36:10,588 - INFO - Epoch 397 training loss = 3.677
2023-04-27 00:36:10,688 - INFO - Epoch 398 training loss = 3.823
2023-04-27 00:36:10,787 - INFO - Epoch 399 training loss = 3.535
2023-04-27 00:36:10,887 - INFO - Epoch 400 training loss = 3.839
2023-04-27 00:36:10,914 - INFO - Validation loss = 11.6
2023-04-27 00:36:11,014 - INFO - Epoch 401 training loss = 3.658
2023-04-27 00:36:11,113 - INFO - Epoch 402 training loss = 3.57
2023-04-27 00:36:11,213 - INFO - Epoch 403 training loss = 3.636
2023-04-27 00:36:11,312 - INFO - Epoch 404 training loss = 3.578
2023-04-27 00:36:11,411 - INFO - Epoch 405 training loss = 3.538
2023-04-27 00:36:11,571 - INFO - Epoch 406 training loss = 3.565
2023-04-27 00:36:11,670 - INFO - Epoch 407 training loss = 3.616
2023-04-27 00:36:11,769 - INFO - Epoch 408 training loss = 3.588
2023-04-27 00:36:11,868 - INFO - Epoch 409 training loss = 3.623
2023-04-27 00:36:11,967 - INFO - Epoch 410 training loss = 3.552
2023-04-27 00:36:11,994 - INFO - Validation loss = 10.99
2023-04-27 00:36:11,994 - INFO - best model
2023-04-27 00:36:12,105 - INFO - Epoch 411 training loss = 3.451
2023-04-27 00:36:12,204 - INFO - Epoch 412 training loss = 3.557
2023-04-27 00:36:12,303 - INFO - Epoch 413 training loss = 3.48
2023-04-27 00:36:12,402 - INFO - Epoch 414 training loss = 3.464
2023-04-27 00:36:12,501 - INFO - Epoch 415 training loss = 3.557
2023-04-27 00:36:12,601 - INFO - Epoch 416 training loss = 3.506
2023-04-27 00:36:12,701 - INFO - Epoch 417 training loss = 3.499
2023-04-27 00:36:12,800 - INFO - Epoch 418 training loss = 3.287
2023-04-27 00:36:12,899 - INFO - Epoch 419 training loss = 3.544
2023-04-27 00:36:12,998 - INFO - Epoch 420 training loss = 3.516
2023-04-27 00:36:13,025 - INFO - Validation loss = 11.11
2023-04-27 00:36:13,125 - INFO - Epoch 421 training loss = 3.537
2023-04-27 00:36:13,224 - INFO - Epoch 422 training loss = 3.379
2023-04-27 00:36:13,323 - INFO - Epoch 423 training loss = 3.412
2023-04-27 00:36:13,422 - INFO - Epoch 424 training loss = 3.374
2023-04-27 00:36:13,522 - INFO - Epoch 425 training loss = 3.371
2023-04-27 00:36:13,622 - INFO - Epoch 426 training loss = 3.432
2023-04-27 00:36:13,720 - INFO - Epoch 427 training loss = 3.352
2023-04-27 00:36:13,820 - INFO - Epoch 428 training loss = 3.314
2023-04-27 00:36:13,918 - INFO - Epoch 429 training loss = 3.354
2023-04-27 00:36:14,017 - INFO - Epoch 430 training loss = 3.344
2023-04-27 00:36:14,045 - INFO - Validation loss = 10.84
2023-04-27 00:36:14,045 - INFO - best model
2023-04-27 00:36:14,155 - INFO - Epoch 431 training loss = 3.265
2023-04-27 00:36:14,254 - INFO - Epoch 432 training loss = 3.405
2023-04-27 00:36:14,353 - INFO - Epoch 433 training loss = 3.257
2023-04-27 00:36:14,452 - INFO - Epoch 434 training loss = 3.29
2023-04-27 00:36:14,552 - INFO - Epoch 435 training loss = 3.298
2023-04-27 00:36:14,650 - INFO - Epoch 436 training loss = 3.403
2023-04-27 00:36:14,749 - INFO - Epoch 437 training loss = 3.359
2023-04-27 00:36:14,905 - INFO - Epoch 438 training loss = 3.197
2023-04-27 00:36:15,003 - INFO - Epoch 439 training loss = 3.251
2023-04-27 00:36:15,102 - INFO - Epoch 440 training loss = 3.485
2023-04-27 00:36:15,129 - INFO - Validation loss = 10.94
2023-04-27 00:36:15,227 - INFO - Epoch 441 training loss = 3.156
2023-04-27 00:36:15,325 - INFO - Epoch 442 training loss = 3.236
2023-04-27 00:36:15,424 - INFO - Epoch 443 training loss = 3.345
2023-04-27 00:36:15,522 - INFO - Epoch 444 training loss = 3.181
2023-04-27 00:36:15,621 - INFO - Epoch 445 training loss = 3.224
2023-04-27 00:36:15,719 - INFO - Epoch 446 training loss = 3.235
2023-04-27 00:36:15,817 - INFO - Epoch 447 training loss = 3.365
2023-04-27 00:36:15,916 - INFO - Epoch 448 training loss = 3.175
2023-04-27 00:36:16,014 - INFO - Epoch 449 training loss = 3.249
2023-04-27 00:36:16,112 - INFO - Epoch 450 training loss = 3.309
2023-04-27 00:36:16,139 - INFO - Validation loss = 11.0
2023-04-27 00:36:16,237 - INFO - Epoch 451 training loss = 3.18
2023-04-27 00:36:16,335 - INFO - Epoch 452 training loss = 3.082
2023-04-27 00:36:16,433 - INFO - Epoch 453 training loss = 3.362
2023-04-27 00:36:16,532 - INFO - Epoch 454 training loss = 3.134
2023-04-27 00:36:16,631 - INFO - Epoch 455 training loss =  3.2
2023-04-27 00:36:16,729 - INFO - Epoch 456 training loss = 3.035
2023-04-27 00:36:16,828 - INFO - Epoch 457 training loss = 3.108
2023-04-27 00:36:16,928 - INFO - Epoch 458 training loss = 3.135
2023-04-27 00:36:17,027 - INFO - Epoch 459 training loss = 3.186
2023-04-27 00:36:17,126 - INFO - Epoch 460 training loss = 3.129
2023-04-27 00:36:17,154 - INFO - Validation loss = 10.92
2023-04-27 00:36:17,253 - INFO - Epoch 461 training loss = 3.139
2023-04-27 00:36:17,352 - INFO - Epoch 462 training loss = 3.059
2023-04-27 00:36:17,450 - INFO - Epoch 463 training loss = 3.064
2023-04-27 00:36:17,549 - INFO - Epoch 464 training loss = 3.097
2023-04-27 00:36:17,650 - INFO - Epoch 465 training loss = 3.044
2023-04-27 00:36:17,749 - INFO - Epoch 466 training loss = 3.137
2023-04-27 00:36:17,848 - INFO - Epoch 467 training loss = 3.021
2023-04-27 00:36:17,947 - INFO - Epoch 468 training loss = 3.07
2023-04-27 00:36:18,105 - INFO - Epoch 469 training loss = 3.106
2023-04-27 00:36:18,203 - INFO - Epoch 470 training loss = 3.011
2023-04-27 00:36:18,231 - INFO - Validation loss = 11.03
2023-04-27 00:36:18,330 - INFO - Epoch 471 training loss = 3.075
2023-04-27 00:36:18,429 - INFO - Epoch 472 training loss = 2.914
2023-04-27 00:36:18,528 - INFO - Epoch 473 training loss = 3.171
2023-04-27 00:36:18,627 - INFO - Epoch 474 training loss = 3.032
2023-04-27 00:36:18,726 - INFO - Epoch 475 training loss = 2.991
2023-04-27 00:36:18,825 - INFO - Epoch 476 training loss = 3.101
2023-04-27 00:36:18,924 - INFO - Epoch 477 training loss = 2.977
2023-04-27 00:36:19,023 - INFO - Epoch 478 training loss = 2.937
2023-04-27 00:36:19,122 - INFO - Epoch 479 training loss = 3.001
2023-04-27 00:36:19,221 - INFO - Epoch 480 training loss = 2.934
2023-04-27 00:36:19,248 - INFO - Validation loss = 10.85
2023-04-27 00:36:19,347 - INFO - Epoch 481 training loss = 2.939
2023-04-27 00:36:19,446 - INFO - Epoch 482 training loss =  3.0
2023-04-27 00:36:19,545 - INFO - Epoch 483 training loss = 2.987
2023-04-27 00:36:19,644 - INFO - Epoch 484 training loss = 2.902
2023-04-27 00:36:19,743 - INFO - Epoch 485 training loss = 3.026
2023-04-27 00:36:19,842 - INFO - Epoch 486 training loss = 2.916
2023-04-27 00:36:19,941 - INFO - Epoch 487 training loss = 2.947
2023-04-27 00:36:20,040 - INFO - Epoch 488 training loss = 2.899
2023-04-27 00:36:20,139 - INFO - Epoch 489 training loss = 3.103
2023-04-27 00:36:20,238 - INFO - Epoch 490 training loss = 2.813
2023-04-27 00:36:20,265 - INFO - Validation loss = 11.18
2023-04-27 00:36:20,364 - INFO - Epoch 491 training loss = 2.907
2023-04-27 00:36:20,463 - INFO - Epoch 492 training loss = 2.857
2023-04-27 00:36:20,562 - INFO - Epoch 493 training loss = 2.831
2023-04-27 00:36:20,661 - INFO - Epoch 494 training loss = 2.928
2023-04-27 00:36:20,760 - INFO - Epoch 495 training loss = 2.868
2023-04-27 00:36:20,859 - INFO - Epoch 496 training loss = 2.956
2023-04-27 00:36:20,958 - INFO - Epoch 497 training loss = 2.793
2023-04-27 00:36:21,056 - INFO - Epoch 498 training loss = 2.838
2023-04-27 00:36:21,156 - INFO - Epoch 499 training loss = 2.799
2023-04-27 00:36:21,313 - INFO - Epoch 500 training loss = 2.848
2023-04-27 00:36:21,341 - INFO - Validation loss = 10.92
2023-04-27 00:36:21,440 - INFO - Epoch 501 training loss = 2.859
2023-04-27 00:36:21,539 - INFO - Epoch 502 training loss = 2.83
2023-04-27 00:36:21,638 - INFO - Epoch 503 training loss = 2.803
2023-04-27 00:36:21,737 - INFO - Epoch 504 training loss = 2.767
2023-04-27 00:36:21,836 - INFO - Epoch 505 training loss = 2.845
2023-04-27 00:36:21,935 - INFO - Epoch 506 training loss = 2.779
2023-04-27 00:36:22,034 - INFO - Epoch 507 training loss = 2.757
2023-04-27 00:36:22,133 - INFO - Epoch 508 training loss = 2.853
2023-04-27 00:36:22,232 - INFO - Epoch 509 training loss = 2.797
2023-04-27 00:36:22,331 - INFO - Epoch 510 training loss = 2.767
2023-04-27 00:36:22,358 - INFO - Validation loss = 10.85
2023-04-27 00:36:22,457 - INFO - Epoch 511 training loss = 2.782
2023-04-27 00:36:22,556 - INFO - Epoch 512 training loss = 2.789
2023-04-27 00:36:22,655 - INFO - Epoch 513 training loss = 2.697
2023-04-27 00:36:22,754 - INFO - Epoch 514 training loss = 2.83
2023-04-27 00:36:22,854 - INFO - Epoch 515 training loss = 2.702
2023-04-27 00:36:22,953 - INFO - Epoch 516 training loss = 2.753
2023-04-27 00:36:23,052 - INFO - Epoch 517 training loss = 2.767
2023-04-27 00:36:23,151 - INFO - Epoch 518 training loss = 2.75
2023-04-27 00:36:23,250 - INFO - Epoch 519 training loss = 2.707
2023-04-27 00:36:23,349 - INFO - Epoch 520 training loss = 2.737
2023-04-27 00:36:23,376 - INFO - Validation loss = 11.02
2023-04-27 00:36:23,475 - INFO - Epoch 521 training loss = 2.688
2023-04-27 00:36:23,574 - INFO - Epoch 522 training loss = 2.703
2023-04-27 00:36:23,673 - INFO - Epoch 523 training loss = 2.78
2023-04-27 00:36:23,772 - INFO - Epoch 524 training loss = 2.651
2023-04-27 00:36:23,871 - INFO - Epoch 525 training loss = 2.721
2023-04-27 00:36:23,970 - INFO - Epoch 526 training loss =  2.7
2023-04-27 00:36:24,069 - INFO - Epoch 527 training loss = 2.668
2023-04-27 00:36:24,168 - INFO - Epoch 528 training loss = 2.627
2023-04-27 00:36:24,267 - INFO - Epoch 529 training loss = 2.653
2023-04-27 00:36:24,366 - INFO - Epoch 530 training loss = 2.77
2023-04-27 00:36:24,393 - INFO - Validation loss = 10.88
2023-04-27 00:36:24,492 - INFO - Epoch 531 training loss = 2.645
2023-04-27 00:36:24,650 - INFO - Epoch 532 training loss = 2.638
2023-04-27 00:36:24,749 - INFO - Epoch 533 training loss = 2.671
2023-04-27 00:36:24,848 - INFO - Epoch 534 training loss = 2.593
2023-04-27 00:36:24,947 - INFO - Epoch 535 training loss = 2.572
2023-04-27 00:36:25,046 - INFO - Epoch 536 training loss = 2.65
2023-04-27 00:36:25,145 - INFO - Epoch 537 training loss = 2.624
2023-04-27 00:36:25,245 - INFO - Epoch 538 training loss = 2.601
2023-04-27 00:36:25,343 - INFO - Epoch 539 training loss = 2.669
2023-04-27 00:36:25,442 - INFO - Epoch 540 training loss = 2.582
2023-04-27 00:36:25,470 - INFO - Validation loss = 10.86
2023-04-27 00:36:25,569 - INFO - Epoch 541 training loss = 2.594
2023-04-27 00:36:25,668 - INFO - Epoch 542 training loss = 2.567
2023-04-27 00:36:25,767 - INFO - Epoch 543 training loss = 2.592
2023-04-27 00:36:25,865 - INFO - Epoch 544 training loss = 2.562
2023-04-27 00:36:25,964 - INFO - Epoch 545 training loss = 2.553
2023-04-27 00:36:26,063 - INFO - Epoch 546 training loss = 2.561
2023-04-27 00:36:26,162 - INFO - Epoch 547 training loss = 2.547
2023-04-27 00:36:26,262 - INFO - Epoch 548 training loss = 2.523
2023-04-27 00:36:26,361 - INFO - Epoch 549 training loss = 2.538
2023-04-27 00:36:26,460 - INFO - Epoch 550 training loss = 2.543
2023-04-27 00:36:26,487 - INFO - Validation loss = 10.83
2023-04-27 00:36:26,487 - INFO - best model
2023-04-27 00:36:26,598 - INFO - Epoch 551 training loss = 2.556
2023-04-27 00:36:26,696 - INFO - Epoch 552 training loss = 2.56
2023-04-27 00:36:26,795 - INFO - Epoch 553 training loss = 2.527
2023-04-27 00:36:26,895 - INFO - Epoch 554 training loss = 2.496
2023-04-27 00:36:26,994 - INFO - Epoch 555 training loss = 2.575
2023-04-27 00:36:27,093 - INFO - Epoch 556 training loss = 2.526
2023-04-27 00:36:27,192 - INFO - Epoch 557 training loss = 2.501
2023-04-27 00:36:27,291 - INFO - Epoch 558 training loss = 2.502
2023-04-27 00:36:27,390 - INFO - Epoch 559 training loss = 2.474
2023-04-27 00:36:27,489 - INFO - Epoch 560 training loss = 2.491
2023-04-27 00:36:27,516 - INFO - Validation loss = 11.01
2023-04-27 00:36:27,615 - INFO - Epoch 561 training loss = 2.479
2023-04-27 00:36:27,714 - INFO - Epoch 562 training loss = 2.509
2023-04-27 00:36:27,872 - INFO - Epoch 563 training loss = 2.499
2023-04-27 00:36:27,971 - INFO - Epoch 564 training loss = 2.446
2023-04-27 00:36:28,070 - INFO - Epoch 565 training loss = 2.469
2023-04-27 00:36:28,169 - INFO - Epoch 566 training loss = 2.449
2023-04-27 00:36:28,268 - INFO - Epoch 567 training loss = 2.49
2023-04-27 00:36:28,367 - INFO - Epoch 568 training loss = 2.435
2023-04-27 00:36:28,466 - INFO - Epoch 569 training loss = 2.419
2023-04-27 00:36:28,565 - INFO - Epoch 570 training loss = 2.414
2023-04-27 00:36:28,592 - INFO - Validation loss = 10.86
2023-04-27 00:36:28,691 - INFO - Epoch 571 training loss = 2.401
2023-04-27 00:36:28,790 - INFO - Epoch 572 training loss = 2.444
2023-04-27 00:36:28,890 - INFO - Epoch 573 training loss = 2.43
2023-04-27 00:36:28,989 - INFO - Epoch 574 training loss = 2.466
2023-04-27 00:36:29,088 - INFO - Epoch 575 training loss = 2.406
2023-04-27 00:36:29,187 - INFO - Epoch 576 training loss = 2.456
2023-04-27 00:36:29,285 - INFO - Epoch 577 training loss = 2.379
2023-04-27 00:36:29,384 - INFO - Epoch 578 training loss = 2.371
2023-04-27 00:36:29,483 - INFO - Epoch 579 training loss = 2.399
2023-04-27 00:36:29,582 - INFO - Epoch 580 training loss = 2.402
2023-04-27 00:36:29,609 - INFO - Validation loss = 10.77
2023-04-27 00:36:29,610 - INFO - best model
2023-04-27 00:36:29,720 - INFO - Epoch 581 training loss = 2.38
2023-04-27 00:36:29,819 - INFO - Epoch 582 training loss = 2.396
2023-04-27 00:36:29,918 - INFO - Epoch 583 training loss = 2.392
2023-04-27 00:36:30,017 - INFO - Epoch 584 training loss = 2.383
2023-04-27 00:36:30,116 - INFO - Epoch 585 training loss = 2.356
2023-04-27 00:36:30,215 - INFO - Epoch 586 training loss = 2.37
2023-04-27 00:36:30,314 - INFO - Epoch 587 training loss = 2.359
2023-04-27 00:36:30,413 - INFO - Epoch 588 training loss = 2.362
2023-04-27 00:36:30,512 - INFO - Epoch 589 training loss = 2.322
2023-04-27 00:36:30,611 - INFO - Epoch 590 training loss = 2.319
2023-04-27 00:36:30,638 - INFO - Validation loss = 10.89
2023-04-27 00:36:30,737 - INFO - Epoch 591 training loss = 2.333
2023-04-27 00:36:30,837 - INFO - Epoch 592 training loss = 2.343
2023-04-27 00:36:30,936 - INFO - Epoch 593 training loss = 2.331
2023-04-27 00:36:31,094 - INFO - Epoch 594 training loss = 2.336
2023-04-27 00:36:31,193 - INFO - Epoch 595 training loss = 2.285
2023-04-27 00:36:31,292 - INFO - Epoch 596 training loss = 2.309
2023-04-27 00:36:31,391 - INFO - Epoch 597 training loss = 2.321
2023-04-27 00:36:31,490 - INFO - Epoch 598 training loss = 2.273
2023-04-27 00:36:31,588 - INFO - Epoch 599 training loss = 2.296
2023-04-27 00:36:31,687 - INFO - Epoch 600 training loss = 2.31
2023-04-27 00:36:31,715 - INFO - Validation loss = 10.79
2023-04-27 00:36:31,814 - INFO - Epoch 601 training loss = 2.314
2023-04-27 00:36:31,913 - INFO - Epoch 602 training loss = 2.27
2023-04-27 00:36:32,013 - INFO - Epoch 603 training loss = 2.281
2023-04-27 00:36:32,112 - INFO - Epoch 604 training loss = 2.28
2023-04-27 00:36:32,211 - INFO - Epoch 605 training loss = 2.262
2023-04-27 00:36:32,310 - INFO - Epoch 606 training loss = 2.286
2023-04-27 00:36:32,409 - INFO - Epoch 607 training loss = 2.271
2023-04-27 00:36:32,508 - INFO - Epoch 608 training loss = 2.259
2023-04-27 00:36:32,607 - INFO - Epoch 609 training loss = 2.264
2023-04-27 00:36:32,706 - INFO - Epoch 610 training loss = 2.257
2023-04-27 00:36:32,734 - INFO - Validation loss = 10.79
2023-04-27 00:36:32,833 - INFO - Epoch 611 training loss = 2.244
2023-04-27 00:36:32,932 - INFO - Epoch 612 training loss = 2.239
2023-04-27 00:36:33,031 - INFO - Epoch 613 training loss = 2.246
2023-04-27 00:36:33,130 - INFO - Epoch 614 training loss = 2.217
2023-04-27 00:36:33,229 - INFO - Epoch 615 training loss = 2.248
2023-04-27 00:36:33,328 - INFO - Epoch 616 training loss = 2.22
2023-04-27 00:36:33,427 - INFO - Epoch 617 training loss = 2.247
2023-04-27 00:36:33,526 - INFO - Epoch 618 training loss = 2.237
2023-04-27 00:36:33,625 - INFO - Epoch 619 training loss = 2.237
2023-04-27 00:36:33,723 - INFO - Epoch 620 training loss = 2.232
2023-04-27 00:36:33,751 - INFO - Validation loss = 10.76
2023-04-27 00:36:33,751 - INFO - best model
2023-04-27 00:36:33,861 - INFO - Epoch 621 training loss = 2.226
2023-04-27 00:36:33,960 - INFO - Epoch 622 training loss = 2.177
2023-04-27 00:36:34,060 - INFO - Epoch 623 training loss = 2.251
2023-04-27 00:36:34,159 - INFO - Epoch 624 training loss = 2.225
2023-04-27 00:36:34,258 - INFO - Epoch 625 training loss = 2.179
2023-04-27 00:36:34,415 - INFO - Epoch 626 training loss = 2.195
2023-04-27 00:36:34,514 - INFO - Epoch 627 training loss = 2.215
2023-04-27 00:36:34,614 - INFO - Epoch 628 training loss = 2.184
2023-04-27 00:36:34,712 - INFO - Epoch 629 training loss = 2.174
2023-04-27 00:36:34,812 - INFO - Epoch 630 training loss = 2.187
2023-04-27 00:36:34,839 - INFO - Validation loss = 10.87
2023-04-27 00:36:34,939 - INFO - Epoch 631 training loss = 2.159
2023-04-27 00:36:35,038 - INFO - Epoch 632 training loss = 2.162
2023-04-27 00:36:35,137 - INFO - Epoch 633 training loss = 2.166
2023-04-27 00:36:35,236 - INFO - Epoch 634 training loss = 2.169
2023-04-27 00:36:35,336 - INFO - Epoch 635 training loss = 2.16
2023-04-27 00:36:35,435 - INFO - Epoch 636 training loss = 2.152
2023-04-27 00:36:35,534 - INFO - Epoch 637 training loss = 2.152
2023-04-27 00:36:35,633 - INFO - Epoch 638 training loss = 2.13
2023-04-27 00:36:35,732 - INFO - Epoch 639 training loss = 2.137
2023-04-27 00:36:35,831 - INFO - Epoch 640 training loss = 2.15
2023-04-27 00:36:35,858 - INFO - Validation loss = 10.88
2023-04-27 00:36:35,957 - INFO - Epoch 641 training loss = 2.121
2023-04-27 00:36:36,056 - INFO - Epoch 642 training loss = 2.133
2023-04-27 00:36:36,155 - INFO - Epoch 643 training loss = 2.135
2023-04-27 00:36:36,254 - INFO - Epoch 644 training loss = 2.144
2023-04-27 00:36:36,353 - INFO - Epoch 645 training loss = 2.095
2023-04-27 00:36:36,452 - INFO - Epoch 646 training loss = 2.112
2023-04-27 00:36:36,552 - INFO - Epoch 647 training loss = 2.127
2023-04-27 00:36:36,651 - INFO - Epoch 648 training loss = 2.095
2023-04-27 00:36:36,749 - INFO - Epoch 649 training loss = 2.136
2023-04-27 00:36:36,849 - INFO - Epoch 650 training loss = 2.107
2023-04-27 00:36:36,876 - INFO - Validation loss = 10.78
2023-04-27 00:36:36,975 - INFO - Epoch 651 training loss = 2.125
2023-04-27 00:36:37,075 - INFO - Epoch 652 training loss = 2.093
2023-04-27 00:36:37,173 - INFO - Epoch 653 training loss = 2.088
2023-04-27 00:36:37,271 - INFO - Epoch 654 training loss = 2.09
2023-04-27 00:36:37,369 - INFO - Epoch 655 training loss = 2.093
2023-04-27 00:36:37,467 - INFO - Epoch 656 training loss = 2.075
2023-04-27 00:36:37,622 - INFO - Epoch 657 training loss = 2.083
2023-04-27 00:36:37,721 - INFO - Epoch 658 training loss = 2.098
2023-04-27 00:36:37,819 - INFO - Epoch 659 training loss = 2.054
2023-04-27 00:36:37,918 - INFO - Epoch 660 training loss = 2.078
2023-04-27 00:36:37,945 - INFO - Validation loss = 10.81
2023-04-27 00:36:38,044 - INFO - Epoch 661 training loss = 2.071
2023-04-27 00:36:38,143 - INFO - Epoch 662 training loss = 2.071
2023-04-27 00:36:38,242 - INFO - Epoch 663 training loss = 2.056
2023-04-27 00:36:38,341 - INFO - Epoch 664 training loss = 2.055
2023-04-27 00:36:38,440 - INFO - Epoch 665 training loss = 2.069
2023-04-27 00:36:38,539 - INFO - Epoch 666 training loss = 2.05
2023-04-27 00:36:38,638 - INFO - Epoch 667 training loss = 2.083
2023-04-27 00:36:38,737 - INFO - Epoch 668 training loss = 2.034
2023-04-27 00:36:38,836 - INFO - Epoch 669 training loss = 2.033
2023-04-27 00:36:38,935 - INFO - Epoch 670 training loss = 2.052
2023-04-27 00:36:38,962 - INFO - Validation loss = 10.83
2023-04-27 00:36:39,061 - INFO - Epoch 671 training loss = 2.028
2023-04-27 00:36:39,160 - INFO - Epoch 672 training loss = 2.052
2023-04-27 00:36:39,258 - INFO - Epoch 673 training loss = 2.022
2023-04-27 00:36:39,356 - INFO - Epoch 674 training loss = 2.035
2023-04-27 00:36:39,454 - INFO - Epoch 675 training loss = 2.045
2023-04-27 00:36:39,552 - INFO - Epoch 676 training loss = 1.997
2023-04-27 00:36:39,650 - INFO - Epoch 677 training loss = 2.021
2023-04-27 00:36:39,748 - INFO - Epoch 678 training loss = 2.008
2023-04-27 00:36:39,847 - INFO - Epoch 679 training loss = 2.016
2023-04-27 00:36:39,944 - INFO - Epoch 680 training loss = 2.006
2023-04-27 00:36:39,972 - INFO - Validation loss = 10.86
2023-04-27 00:36:40,070 - INFO - Epoch 681 training loss = 1.997
2023-04-27 00:36:40,168 - INFO - Epoch 682 training loss = 2.007
2023-04-27 00:36:40,266 - INFO - Epoch 683 training loss = 2.009
2023-04-27 00:36:40,364 - INFO - Epoch 684 training loss = 1.994
2023-04-27 00:36:40,462 - INFO - Epoch 685 training loss =  2.0
2023-04-27 00:36:40,560 - INFO - Epoch 686 training loss = 1.978
2023-04-27 00:36:40,659 - INFO - Epoch 687 training loss = 1.987
2023-04-27 00:36:40,757 - INFO - Epoch 688 training loss = 1.998
2023-04-27 00:36:40,913 - INFO - Epoch 689 training loss = 1.984
2023-04-27 00:36:41,013 - INFO - Epoch 690 training loss = 1.974
2023-04-27 00:36:41,040 - INFO - Validation loss = 10.83
2023-04-27 00:36:41,140 - INFO - Epoch 691 training loss = 1.992
2023-04-27 00:36:41,239 - INFO - Epoch 692 training loss = 1.968
2023-04-27 00:36:41,338 - INFO - Epoch 693 training loss = 1.962
2023-04-27 00:36:41,437 - INFO - Epoch 694 training loss = 1.955
2023-04-27 00:36:41,537 - INFO - Epoch 695 training loss = 1.963
2023-04-27 00:36:41,636 - INFO - Epoch 696 training loss = 1.967
2023-04-27 00:36:41,735 - INFO - Epoch 697 training loss = 1.97
2023-04-27 00:36:41,835 - INFO - Epoch 698 training loss = 1.959
2023-04-27 00:36:41,934 - INFO - Epoch 699 training loss = 1.948
2023-04-27 00:36:42,035 - INFO - Epoch 700 training loss = 1.96
2023-04-27 00:36:42,063 - INFO - Validation loss = 11.02
2023-04-27 00:36:42,162 - INFO - Epoch 701 training loss = 1.983
2023-04-27 00:36:42,262 - INFO - Epoch 702 training loss = 1.937
2023-04-27 00:36:42,361 - INFO - Epoch 703 training loss = 1.943
2023-04-27 00:36:42,461 - INFO - Epoch 704 training loss = 1.953
2023-04-27 00:36:42,560 - INFO - Epoch 705 training loss = 1.934
2023-04-27 00:36:42,659 - INFO - Epoch 706 training loss = 1.952
2023-04-27 00:36:42,759 - INFO - Epoch 707 training loss = 1.931
2023-04-27 00:36:42,858 - INFO - Epoch 708 training loss = 1.948
2023-04-27 00:36:42,957 - INFO - Epoch 709 training loss = 1.932
2023-04-27 00:36:43,058 - INFO - Epoch 710 training loss = 1.916
2023-04-27 00:36:43,085 - INFO - Validation loss = 10.9
2023-04-27 00:36:43,185 - INFO - Epoch 711 training loss = 1.938
2023-04-27 00:36:43,284 - INFO - Epoch 712 training loss = 1.917
2023-04-27 00:36:43,383 - INFO - Epoch 713 training loss = 1.916
2023-04-27 00:36:43,483 - INFO - Epoch 714 training loss = 1.919
2023-04-27 00:36:43,582 - INFO - Epoch 715 training loss = 1.909
2023-04-27 00:36:43,681 - INFO - Epoch 716 training loss = 1.908
2023-04-27 00:36:43,780 - INFO - Epoch 717 training loss = 1.922
2023-04-27 00:36:43,880 - INFO - Epoch 718 training loss = 1.895
2023-04-27 00:36:43,979 - INFO - Epoch 719 training loss = 1.913
2023-04-27 00:36:44,139 - INFO - Epoch 720 training loss = 1.893
2023-04-27 00:36:44,166 - INFO - Validation loss = 10.89
2023-04-27 00:36:44,265 - INFO - Epoch 721 training loss = 1.903
2023-04-27 00:36:44,365 - INFO - Epoch 722 training loss =  1.9
2023-04-27 00:36:44,464 - INFO - Epoch 723 training loss = 1.889
2023-04-27 00:36:44,563 - INFO - Epoch 724 training loss = 1.893
2023-04-27 00:36:44,661 - INFO - Epoch 725 training loss = 1.891
2023-04-27 00:36:44,759 - INFO - Epoch 726 training loss = 1.888
2023-04-27 00:36:44,858 - INFO - Epoch 727 training loss = 1.884
2023-04-27 00:36:44,956 - INFO - Epoch 728 training loss = 1.887
2023-04-27 00:36:45,056 - INFO - Epoch 729 training loss = 1.885
2023-04-27 00:36:45,157 - INFO - Epoch 730 training loss = 1.882
2023-04-27 00:36:45,184 - INFO - Validation loss = 10.94
2023-04-27 00:36:45,283 - INFO - Epoch 731 training loss = 1.874
2023-04-27 00:36:45,383 - INFO - Epoch 732 training loss = 1.872
2023-04-27 00:36:45,482 - INFO - Epoch 733 training loss = 1.877
2023-04-27 00:36:45,581 - INFO - Epoch 734 training loss = 1.87
2023-04-27 00:36:45,680 - INFO - Epoch 735 training loss = 1.868
2023-04-27 00:36:45,779 - INFO - Epoch 736 training loss = 1.867
2023-04-27 00:36:45,877 - INFO - Epoch 737 training loss = 1.859
2023-04-27 00:36:45,975 - INFO - Epoch 738 training loss = 1.856
2023-04-27 00:36:46,074 - INFO - Epoch 739 training loss = 1.868
2023-04-27 00:36:46,173 - INFO - Epoch 740 training loss = 1.848
2023-04-27 00:36:46,200 - INFO - Validation loss = 10.9
2023-04-27 00:36:46,299 - INFO - Epoch 741 training loss = 1.848
2023-04-27 00:36:46,397 - INFO - Epoch 742 training loss = 1.854
2023-04-27 00:36:46,496 - INFO - Epoch 743 training loss = 1.872
2023-04-27 00:36:46,594 - INFO - Epoch 744 training loss = 1.836
2023-04-27 00:36:46,693 - INFO - Epoch 745 training loss = 1.845
2023-04-27 00:36:46,791 - INFO - Epoch 746 training loss = 1.831
2023-04-27 00:36:46,891 - INFO - Epoch 747 training loss = 1.843
2023-04-27 00:36:46,991 - INFO - Epoch 748 training loss = 1.836
2023-04-27 00:36:47,091 - INFO - Epoch 749 training loss = 1.835
2023-04-27 00:36:47,191 - INFO - Epoch 750 training loss = 1.83
2023-04-27 00:36:47,218 - INFO - Validation loss = 10.95
2023-04-27 00:36:47,374 - INFO - Epoch 751 training loss = 1.833
2023-04-27 00:36:47,472 - INFO - Epoch 752 training loss = 1.823
2023-04-27 00:36:47,570 - INFO - Epoch 753 training loss = 1.836
2023-04-27 00:36:47,668 - INFO - Epoch 754 training loss = 1.825
2023-04-27 00:36:47,769 - INFO - Epoch 755 training loss = 1.838
2023-04-27 00:36:47,868 - INFO - Epoch 756 training loss = 1.826
2023-04-27 00:36:47,965 - INFO - Epoch 757 training loss = 1.811
2023-04-27 00:36:48,064 - INFO - Epoch 758 training loss = 1.825
2023-04-27 00:36:48,162 - INFO - Epoch 759 training loss = 1.817
2023-04-27 00:36:48,260 - INFO - Epoch 760 training loss = 1.817
2023-04-27 00:36:48,287 - INFO - Validation loss = 10.96
2023-04-27 00:36:48,385 - INFO - Epoch 761 training loss = 1.809
2023-04-27 00:36:48,483 - INFO - Epoch 762 training loss = 1.808
2023-04-27 00:36:48,581 - INFO - Epoch 763 training loss = 1.813
2023-04-27 00:36:48,679 - INFO - Epoch 764 training loss = 1.812
2023-04-27 00:36:48,777 - INFO - Epoch 765 training loss = 1.804
2023-04-27 00:36:48,876 - INFO - Epoch 766 training loss = 1.808
2023-04-27 00:36:48,974 - INFO - Epoch 767 training loss = 1.808
2023-04-27 00:36:49,072 - INFO - Epoch 768 training loss = 1.802
2023-04-27 00:36:49,170 - INFO - Epoch 769 training loss = 1.796
2023-04-27 00:36:49,268 - INFO - Epoch 770 training loss = 1.79
2023-04-27 00:36:49,295 - INFO - Validation loss = 10.95
2023-04-27 00:36:49,393 - INFO - Epoch 771 training loss = 1.795
2023-04-27 00:36:49,491 - INFO - Epoch 772 training loss = 1.795
2023-04-27 00:36:49,589 - INFO - Epoch 773 training loss = 1.789
2023-04-27 00:36:49,688 - INFO - Epoch 774 training loss = 1.791
2023-04-27 00:36:49,787 - INFO - Epoch 775 training loss = 1.796
2023-04-27 00:36:49,886 - INFO - Epoch 776 training loss = 1.792
2023-04-27 00:36:49,985 - INFO - Epoch 777 training loss = 1.785
2023-04-27 00:36:50,083 - INFO - Epoch 778 training loss = 1.777
2023-04-27 00:36:50,182 - INFO - Epoch 779 training loss = 1.779
2023-04-27 00:36:50,281 - INFO - Epoch 780 training loss = 1.782
2023-04-27 00:36:50,308 - INFO - Validation loss = 10.96
2023-04-27 00:36:50,407 - INFO - Epoch 781 training loss = 1.775
2023-04-27 00:36:50,506 - INFO - Epoch 782 training loss = 1.777
2023-04-27 00:36:50,663 - INFO - Epoch 783 training loss = 1.777
2023-04-27 00:36:50,761 - INFO - Epoch 784 training loss = 1.773
2023-04-27 00:36:50,860 - INFO - Epoch 785 training loss = 1.769
2023-04-27 00:36:50,959 - INFO - Epoch 786 training loss = 1.76
2023-04-27 00:36:51,058 - INFO - Epoch 787 training loss = 1.767
2023-04-27 00:36:51,157 - INFO - Epoch 788 training loss = 1.766
2023-04-27 00:36:51,256 - INFO - Epoch 789 training loss = 1.766
2023-04-27 00:36:51,355 - INFO - Epoch 790 training loss = 1.763
2023-04-27 00:36:51,382 - INFO - Validation loss = 11.01
2023-04-27 00:36:51,480 - INFO - Epoch 791 training loss = 1.764
2023-04-27 00:36:51,579 - INFO - Epoch 792 training loss = 1.763
2023-04-27 00:36:51,678 - INFO - Epoch 793 training loss = 1.756
2023-04-27 00:36:51,777 - INFO - Epoch 794 training loss = 1.757
2023-04-27 00:36:51,875 - INFO - Epoch 795 training loss = 1.757
2023-04-27 00:36:51,974 - INFO - Epoch 796 training loss = 1.753
2023-04-27 00:36:52,073 - INFO - Epoch 797 training loss = 1.749
2023-04-27 00:36:52,172 - INFO - Epoch 798 training loss = 1.752
2023-04-27 00:36:52,271 - INFO - Epoch 799 training loss = 1.748
2023-04-27 00:36:52,370 - INFO - Epoch 800 training loss = 1.749
2023-04-27 00:36:52,397 - INFO - Validation loss = 11.03
2023-04-27 00:36:52,495 - INFO - Epoch 801 training loss = 1.747
2023-04-27 00:36:52,594 - INFO - Epoch 802 training loss = 1.742
2023-04-27 00:36:52,693 - INFO - Epoch 803 training loss = 1.743
2023-04-27 00:36:52,792 - INFO - Epoch 804 training loss = 1.744
2023-04-27 00:36:52,891 - INFO - Epoch 805 training loss = 1.742
2023-04-27 00:36:52,990 - INFO - Epoch 806 training loss = 1.738
2023-04-27 00:36:53,088 - INFO - Epoch 807 training loss = 1.734
2023-04-27 00:36:53,187 - INFO - Epoch 808 training loss = 1.734
2023-04-27 00:36:53,286 - INFO - Epoch 809 training loss = 1.734
2023-04-27 00:36:53,385 - INFO - Epoch 810 training loss = 1.727
2023-04-27 00:36:53,412 - INFO - Validation loss = 11.03
2023-04-27 00:36:53,511 - INFO - Epoch 811 training loss = 1.731
2023-04-27 00:36:53,610 - INFO - Epoch 812 training loss = 1.732
2023-04-27 00:36:53,708 - INFO - Epoch 813 training loss = 1.73
2023-04-27 00:36:53,865 - INFO - Epoch 814 training loss = 1.725
2023-04-27 00:36:53,964 - INFO - Epoch 815 training loss = 1.724
2023-04-27 00:36:54,063 - INFO - Epoch 816 training loss = 1.727
2023-04-27 00:36:54,162 - INFO - Epoch 817 training loss = 1.721
2023-04-27 00:36:54,261 - INFO - Epoch 818 training loss = 1.722
2023-04-27 00:36:54,359 - INFO - Epoch 819 training loss = 1.723
2023-04-27 00:36:54,458 - INFO - Epoch 820 training loss = 1.717
2023-04-27 00:36:54,485 - INFO - Validation loss = 11.02
2023-04-27 00:36:54,584 - INFO - Epoch 821 training loss = 1.72
2023-04-27 00:36:54,683 - INFO - Epoch 822 training loss = 1.717
2023-04-27 00:36:54,782 - INFO - Epoch 823 training loss = 1.715
2023-04-27 00:36:54,881 - INFO - Epoch 824 training loss = 1.715
2023-04-27 00:36:54,979 - INFO - Epoch 825 training loss = 1.712
2023-04-27 00:36:55,078 - INFO - Epoch 826 training loss = 1.709
2023-04-27 00:36:55,177 - INFO - Epoch 827 training loss = 1.711
2023-04-27 00:36:55,276 - INFO - Epoch 828 training loss = 1.709
2023-04-27 00:36:55,375 - INFO - Epoch 829 training loss = 1.706
2023-04-27 00:36:55,473 - INFO - Epoch 830 training loss = 1.704
2023-04-27 00:36:55,500 - INFO - Validation loss = 11.04
2023-04-27 00:36:55,599 - INFO - Epoch 831 training loss = 1.706
2023-04-27 00:36:55,698 - INFO - Epoch 832 training loss = 1.705
2023-04-27 00:36:55,796 - INFO - Epoch 833 training loss = 1.701
2023-04-27 00:36:55,895 - INFO - Epoch 834 training loss = 1.701
2023-04-27 00:36:55,994 - INFO - Epoch 835 training loss =  1.7
2023-04-27 00:36:56,093 - INFO - Epoch 836 training loss = 1.697
2023-04-27 00:36:56,191 - INFO - Epoch 837 training loss = 1.703
2023-04-27 00:36:56,290 - INFO - Epoch 838 training loss = 1.695
2023-04-27 00:36:56,389 - INFO - Epoch 839 training loss = 1.695
2023-04-27 00:36:56,488 - INFO - Epoch 840 training loss = 1.697
2023-04-27 00:36:56,515 - INFO - Validation loss = 11.05
2023-04-27 00:36:56,614 - INFO - Epoch 841 training loss = 1.694
2023-04-27 00:36:56,713 - INFO - Epoch 842 training loss = 1.688
2023-04-27 00:36:56,812 - INFO - Epoch 843 training loss = 1.693
2023-04-27 00:36:56,911 - INFO - Epoch 844 training loss = 1.691
2023-04-27 00:36:57,009 - INFO - Epoch 845 training loss = 1.692
2023-04-27 00:36:57,166 - INFO - Epoch 846 training loss = 1.687
2023-04-27 00:36:57,264 - INFO - Epoch 847 training loss = 1.686
2023-04-27 00:36:57,363 - INFO - Epoch 848 training loss = 1.687
2023-04-27 00:36:57,462 - INFO - Epoch 849 training loss = 1.684
2023-04-27 00:36:57,561 - INFO - Epoch 850 training loss = 1.682
2023-04-27 00:36:57,588 - INFO - Validation loss = 11.06
2023-04-27 00:36:57,687 - INFO - Epoch 851 training loss = 1.683
2023-04-27 00:36:57,785 - INFO - Epoch 852 training loss = 1.682
2023-04-27 00:36:57,884 - INFO - Epoch 853 training loss = 1.681
2023-04-27 00:36:57,983 - INFO - Epoch 854 training loss = 1.679
2023-04-27 00:36:58,082 - INFO - Epoch 855 training loss = 1.678
2023-04-27 00:36:58,180 - INFO - Epoch 856 training loss = 1.68
2023-04-27 00:36:58,279 - INFO - Epoch 857 training loss = 1.677
2023-04-27 00:36:58,377 - INFO - Epoch 858 training loss = 1.675
2023-04-27 00:36:58,477 - INFO - Epoch 859 training loss = 1.676
2023-04-27 00:36:58,575 - INFO - Epoch 860 training loss = 1.676
2023-04-27 00:36:58,602 - INFO - Validation loss = 11.05
2023-04-27 00:36:58,701 - INFO - Epoch 861 training loss = 1.676
2023-04-27 00:36:58,800 - INFO - Epoch 862 training loss = 1.673
2023-04-27 00:36:58,899 - INFO - Epoch 863 training loss = 1.672
2023-04-27 00:36:58,998 - INFO - Epoch 864 training loss = 1.67
2023-04-27 00:36:59,096 - INFO - Epoch 865 training loss = 1.668
2023-04-27 00:36:59,195 - INFO - Epoch 866 training loss = 1.67
2023-04-27 00:36:59,294 - INFO - Epoch 867 training loss = 1.671
2023-04-27 00:36:59,393 - INFO - Epoch 868 training loss = 1.665
2023-04-27 00:36:59,492 - INFO - Epoch 869 training loss = 1.666
2023-04-27 00:36:59,590 - INFO - Epoch 870 training loss = 1.665
2023-04-27 00:36:59,617 - INFO - Validation loss = 11.05
2023-04-27 00:36:59,716 - INFO - Epoch 871 training loss = 1.664
2023-04-27 00:36:59,815 - INFO - Epoch 872 training loss = 1.664
2023-04-27 00:36:59,914 - INFO - Epoch 873 training loss = 1.663
2023-04-27 00:37:00,013 - INFO - Epoch 874 training loss = 1.662
2023-04-27 00:37:00,111 - INFO - Epoch 875 training loss = 1.66
2023-04-27 00:37:00,210 - INFO - Epoch 876 training loss = 1.659
2023-04-27 00:37:00,366 - INFO - Epoch 877 training loss = 1.658
2023-04-27 00:37:00,466 - INFO - Epoch 878 training loss = 1.657
2023-04-27 00:37:00,565 - INFO - Epoch 879 training loss = 1.657
2023-04-27 00:37:00,663 - INFO - Epoch 880 training loss = 1.659
2023-04-27 00:37:00,690 - INFO - Validation loss = 11.08
2023-04-27 00:37:00,789 - INFO - Epoch 881 training loss = 1.655
2023-04-27 00:37:00,888 - INFO - Epoch 882 training loss = 1.656
2023-04-27 00:37:00,987 - INFO - Epoch 883 training loss = 1.653
2023-04-27 00:37:01,086 - INFO - Epoch 884 training loss = 1.655
2023-04-27 00:37:01,185 - INFO - Epoch 885 training loss = 1.653
2023-04-27 00:37:01,283 - INFO - Epoch 886 training loss = 1.655
2023-04-27 00:37:01,382 - INFO - Epoch 887 training loss = 1.649
2023-04-27 00:37:01,481 - INFO - Epoch 888 training loss = 1.65
2023-04-27 00:37:01,580 - INFO - Epoch 889 training loss = 1.651
2023-04-27 00:37:01,679 - INFO - Epoch 890 training loss = 1.649
2023-04-27 00:37:01,706 - INFO - Validation loss = 11.08
2023-04-27 00:37:01,805 - INFO - Epoch 891 training loss = 1.648
2023-04-27 00:37:01,904 - INFO - Epoch 892 training loss = 1.645
2023-04-27 00:37:02,004 - INFO - Epoch 893 training loss = 1.647
2023-04-27 00:37:02,103 - INFO - Epoch 894 training loss = 1.645
2023-04-27 00:37:02,201 - INFO - Epoch 895 training loss = 1.645
2023-04-27 00:37:02,300 - INFO - Epoch 896 training loss = 1.647
2023-04-27 00:37:02,399 - INFO - Epoch 897 training loss = 1.645
2023-04-27 00:37:02,498 - INFO - Epoch 898 training loss = 1.641
2023-04-27 00:37:02,597 - INFO - Epoch 899 training loss = 1.645
2023-04-27 00:37:02,696 - INFO - Epoch 900 training loss = 1.641
2023-04-27 00:37:02,723 - INFO - Validation loss = 11.08
2023-04-27 00:37:02,822 - INFO - Epoch 901 training loss = 1.643
2023-04-27 00:37:02,920 - INFO - Epoch 902 training loss = 1.641
2023-04-27 00:37:03,019 - INFO - Epoch 903 training loss = 1.642
2023-04-27 00:37:03,118 - INFO - Epoch 904 training loss = 1.64
2023-04-27 00:37:03,217 - INFO - Epoch 905 training loss = 1.639
2023-04-27 00:37:03,316 - INFO - Epoch 906 training loss = 1.638
2023-04-27 00:37:03,414 - INFO - Epoch 907 training loss = 1.639
2023-04-27 00:37:03,571 - INFO - Epoch 908 training loss = 1.636
2023-04-27 00:37:03,670 - INFO - Epoch 909 training loss = 1.637
2023-04-27 00:37:03,769 - INFO - Epoch 910 training loss = 1.634
2023-04-27 00:37:03,796 - INFO - Validation loss = 11.09
2023-04-27 00:37:03,895 - INFO - Epoch 911 training loss = 1.635
2023-04-27 00:37:03,994 - INFO - Epoch 912 training loss = 1.634
2023-04-27 00:37:04,092 - INFO - Epoch 913 training loss = 1.635
2023-04-27 00:37:04,191 - INFO - Epoch 914 training loss = 1.633
2023-04-27 00:37:04,290 - INFO - Epoch 915 training loss = 1.633
2023-04-27 00:37:04,389 - INFO - Epoch 916 training loss = 1.633
2023-04-27 00:37:04,487 - INFO - Epoch 917 training loss = 1.632
2023-04-27 00:37:04,586 - INFO - Epoch 918 training loss = 1.63
2023-04-27 00:37:04,685 - INFO - Epoch 919 training loss = 1.631
2023-04-27 00:37:04,784 - INFO - Epoch 920 training loss = 1.63
2023-04-27 00:37:04,811 - INFO - Validation loss = 11.1
2023-04-27 00:37:04,910 - INFO - Epoch 921 training loss = 1.631
2023-04-27 00:37:05,008 - INFO - Epoch 922 training loss = 1.63
2023-04-27 00:37:05,107 - INFO - Epoch 923 training loss = 1.629
2023-04-27 00:37:05,206 - INFO - Epoch 924 training loss = 1.629
2023-04-27 00:37:05,305 - INFO - Epoch 925 training loss = 1.628
2023-04-27 00:37:05,403 - INFO - Epoch 926 training loss = 1.629
2023-04-27 00:37:05,502 - INFO - Epoch 927 training loss = 1.627
2023-04-27 00:37:05,601 - INFO - Epoch 928 training loss = 1.628
2023-04-27 00:37:05,699 - INFO - Epoch 929 training loss = 1.627
2023-04-27 00:37:05,798 - INFO - Epoch 930 training loss = 1.625
2023-04-27 00:37:05,825 - INFO - Validation loss = 11.09
2023-04-27 00:37:05,924 - INFO - Epoch 931 training loss = 1.623
2023-04-27 00:37:06,022 - INFO - Epoch 932 training loss = 1.624
2023-04-27 00:37:06,121 - INFO - Epoch 933 training loss = 1.623
2023-04-27 00:37:06,219 - INFO - Epoch 934 training loss = 1.625
2023-04-27 00:37:06,318 - INFO - Epoch 935 training loss = 1.622
2023-04-27 00:37:06,417 - INFO - Epoch 936 training loss = 1.623
2023-04-27 00:37:06,516 - INFO - Epoch 937 training loss = 1.623
2023-04-27 00:37:06,615 - INFO - Epoch 938 training loss = 1.623
2023-04-27 00:37:06,713 - INFO - Epoch 939 training loss = 1.624
2023-04-27 00:37:06,870 - INFO - Epoch 940 training loss = 1.623
2023-04-27 00:37:06,897 - INFO - Validation loss = 11.1
2023-04-27 00:37:06,996 - INFO - Epoch 941 training loss = 1.621
2023-04-27 00:37:07,095 - INFO - Epoch 942 training loss = 1.619
2023-04-27 00:37:07,194 - INFO - Epoch 943 training loss = 1.621
2023-04-27 00:37:07,292 - INFO - Epoch 944 training loss = 1.62
2023-04-27 00:37:07,391 - INFO - Epoch 945 training loss = 1.621
2023-04-27 00:37:07,489 - INFO - Epoch 946 training loss = 1.618
2023-04-27 00:37:07,587 - INFO - Epoch 947 training loss = 1.62
2023-04-27 00:37:07,685 - INFO - Epoch 948 training loss = 1.619
2023-04-27 00:37:07,784 - INFO - Epoch 949 training loss = 1.616
2023-04-27 00:37:07,882 - INFO - Epoch 950 training loss = 1.619
2023-04-27 00:37:07,909 - INFO - Validation loss = 11.1
2023-04-27 00:37:08,007 - INFO - Epoch 951 training loss = 1.619
2023-04-27 00:37:08,105 - INFO - Epoch 952 training loss = 1.616
2023-04-27 00:37:08,203 - INFO - Epoch 953 training loss = 1.616
2023-04-27 00:37:08,301 - INFO - Epoch 954 training loss = 1.618
2023-04-27 00:37:08,399 - INFO - Epoch 955 training loss = 1.619
2023-04-27 00:37:08,497 - INFO - Epoch 956 training loss = 1.617
2023-04-27 00:37:08,595 - INFO - Epoch 957 training loss = 1.615
2023-04-27 00:37:08,693 - INFO - Epoch 958 training loss = 1.613
2023-04-27 00:37:08,792 - INFO - Epoch 959 training loss = 1.616
2023-04-27 00:37:08,890 - INFO - Epoch 960 training loss = 1.613
2023-04-27 00:37:08,917 - INFO - Validation loss = 11.1
2023-04-27 00:37:09,015 - INFO - Epoch 961 training loss = 1.617
2023-04-27 00:37:09,113 - INFO - Epoch 962 training loss = 1.614
2023-04-27 00:37:09,211 - INFO - Epoch 963 training loss = 1.615
2023-04-27 00:37:09,310 - INFO - Epoch 964 training loss = 1.615
2023-04-27 00:37:09,408 - INFO - Epoch 965 training loss = 1.613
2023-04-27 00:37:09,506 - INFO - Epoch 966 training loss = 1.614
2023-04-27 00:37:09,605 - INFO - Epoch 967 training loss = 1.614
2023-04-27 00:37:09,703 - INFO - Epoch 968 training loss = 1.611
2023-04-27 00:37:09,801 - INFO - Epoch 969 training loss = 1.612
2023-04-27 00:37:09,899 - INFO - Epoch 970 training loss = 1.613
2023-04-27 00:37:09,926 - INFO - Validation loss = 11.11
2023-04-27 00:37:10,083 - INFO - Epoch 971 training loss = 1.613
2023-04-27 00:37:10,181 - INFO - Epoch 972 training loss = 1.613
2023-04-27 00:37:10,279 - INFO - Epoch 973 training loss = 1.612
2023-04-27 00:37:10,377 - INFO - Epoch 974 training loss = 1.612
2023-04-27 00:37:10,475 - INFO - Epoch 975 training loss = 1.612
2023-04-27 00:37:10,573 - INFO - Epoch 976 training loss = 1.61
2023-04-27 00:37:10,671 - INFO - Epoch 977 training loss = 1.612
2023-04-27 00:37:10,769 - INFO - Epoch 978 training loss = 1.612
2023-04-27 00:37:10,868 - INFO - Epoch 979 training loss = 1.611
2023-04-27 00:37:10,966 - INFO - Epoch 980 training loss = 1.612
2023-04-27 00:37:10,993 - INFO - Validation loss = 11.11
2023-04-27 00:37:11,091 - INFO - Epoch 981 training loss = 1.609
2023-04-27 00:37:11,190 - INFO - Epoch 982 training loss = 1.612
2023-04-27 00:37:11,288 - INFO - Epoch 983 training loss = 1.613
2023-04-27 00:37:11,387 - INFO - Epoch 984 training loss = 1.61
2023-04-27 00:37:11,486 - INFO - Epoch 985 training loss = 1.612
2023-04-27 00:37:11,585 - INFO - Epoch 986 training loss = 1.612
2023-04-27 00:37:11,685 - INFO - Epoch 987 training loss = 1.61
2023-04-27 00:37:11,784 - INFO - Epoch 988 training loss = 1.61
2023-04-27 00:37:11,883 - INFO - Epoch 989 training loss = 1.609
2023-04-27 00:37:11,982 - INFO - Epoch 990 training loss = 1.61
2023-04-27 00:37:12,009 - INFO - Validation loss = 11.11
2023-04-27 00:37:12,108 - INFO - Epoch 991 training loss = 1.61
2023-04-27 00:37:12,206 - INFO - Epoch 992 training loss = 1.612
2023-04-27 00:37:12,305 - INFO - Epoch 993 training loss = 1.611
2023-04-27 00:37:12,404 - INFO - Epoch 994 training loss = 1.612
2023-04-27 00:37:12,503 - INFO - Epoch 995 training loss = 1.608
2023-04-27 00:37:12,601 - INFO - Epoch 996 training loss = 1.61
2023-04-27 00:37:12,700 - INFO - Epoch 997 training loss = 1.609
2023-04-27 00:37:12,799 - INFO - Epoch 998 training loss = 1.611
2023-04-27 00:37:12,897 - INFO - Epoch 999 training loss = 1.61
2023-04-27 00:37:12,912 - INFO - Validation loss = 11.11
