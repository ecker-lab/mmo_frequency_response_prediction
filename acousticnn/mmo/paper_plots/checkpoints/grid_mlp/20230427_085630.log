2023-04-27 08:56:30,502 - INFO - Config:
Namespace(config='configs/explicit_mlp.yaml', dir='/user/delden/acoustics/spp_ai_acoustics/acousticNN/experiments/arch/no_encoding/explicitmlp', epochs=100, device='cuda', seed=3, parameter_list='configs/data.yaml', encoding='none', dir_name='arch/no_encoding/explicitmlp')
2023-04-27 08:56:30,502 - INFO - Config:
Munch({'optimizer': Munch({'type': 'AdamW', 'kwargs': Munch({'lr': 0.001, 'weight_decay': 0.005, 'betas': [0.9, 0.95]})}), 'scheduler': Munch({'type': 'CosLR', 'kwargs': Munch({'epochs': 1000, 'initial_epochs': 101})}), 'model': Munch({'name': 'ExplicitMLP', 'input_encoding': True, 'encoding_dim': 16, 'embed_dim': 64, 'depth': 6, 'mlp_width': 256, 'num_frequencies': 200}), 'generation_frequency': 5001, 'validation_frequency': 10, 'batch_size': 128, 'epochs': 1000, 'gradient_clip': 10})
2023-04-27 08:56:44,796 - INFO - ==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ExplicitMLP                              [128, 200, 4]             --
├─GroupwiseProjection: 1-1               [128, 14, 64]             --
│    └─ModuleList: 2-1                   --                        --
│    │    └─Linear: 3-1                  [128, 4, 64]              128
│    │    └─Linear: 3-2                  [128, 5, 64]              128
│    │    └─Linear: 3-3                  [128, 5, 64]              128
├─MLP: 1-2                               [128, 256]                --
│    └─Linear: 2-2                       [128, 256]                229,632
│    └─ReLU: 2-3                         [128, 256]                --
│    └─Dropout: 2-4                      [128, 256]                --
│    └─Linear: 2-5                       [128, 256]                65,792
│    └─ReLU: 2-6                         [128, 256]                --
│    └─Dropout: 2-7                      [128, 256]                --
│    └─Linear: 2-8                       [128, 256]                65,792
│    └─ReLU: 2-9                         [128, 256]                --
│    └─Dropout: 2-10                     [128, 256]                --
│    └─Linear: 2-11                      [128, 256]                65,792
│    └─ReLU: 2-12                        [128, 256]                --
│    └─Dropout: 2-13                     [128, 256]                --
│    └─Linear: 2-14                      [128, 256]                65,792
│    └─ReLU: 2-15                        [128, 256]                --
│    └─Dropout: 2-16                     [128, 256]                --
│    └─Linear: 2-17                      [128, 256]                65,792
│    └─Dropout: 2-18                     [128, 256]                --
├─Linear: 1-3                            [128, 800]                205,600
==========================================================================================
Total params: 764,576
Trainable params: 764,576
Non-trainable params: 0
Total mult-adds (M): 97.87
==========================================================================================
Input size (MB): 0.11
Forward/backward pass size (MB): 3.31
Params size (MB): 3.06
Estimated Total Size (MB): 6.48
==========================================================================================
2023-04-27 08:56:44,903 - INFO - Epoch 0 training loss = 3.381e+03
2023-04-27 08:56:44,938 - INFO - Validation loss = 3.407e+03
2023-04-27 08:56:44,938 - INFO - best model
2023-04-27 08:56:45,058 - INFO - Epoch 1 training loss = 3.373e+03
2023-04-27 08:56:45,162 - INFO - Epoch 2 training loss = 3.36e+03
2023-04-27 08:56:45,266 - INFO - Epoch 3 training loss = 3.295e+03
2023-04-27 08:56:45,371 - INFO - Epoch 4 training loss = 3.086e+03
2023-04-27 08:56:45,475 - INFO - Epoch 5 training loss = 2.453e+03
2023-04-27 08:56:45,580 - INFO - Epoch 6 training loss = 1.323e+03
2023-04-27 08:56:45,685 - INFO - Epoch 7 training loss = 511.0
2023-04-27 08:56:45,790 - INFO - Epoch 8 training loss = 252.6
2023-04-27 08:56:45,895 - INFO - Epoch 9 training loss = 219.4
2023-04-27 08:56:46,000 - INFO - Epoch 10 training loss = 210.3
2023-04-27 08:56:46,028 - INFO - Validation loss = 203.6
2023-04-27 08:56:46,029 - INFO - best model
2023-04-27 08:56:46,144 - INFO - Epoch 11 training loss = 200.1
2023-04-27 08:56:46,249 - INFO - Epoch 12 training loss = 187.0
2023-04-27 08:56:46,354 - INFO - Epoch 13 training loss = 171.1
2023-04-27 08:56:46,458 - INFO - Epoch 14 training loss = 154.1
2023-04-27 08:56:46,562 - INFO - Epoch 15 training loss = 140.6
2023-04-27 08:56:46,667 - INFO - Epoch 16 training loss = 129.4
2023-04-27 08:56:46,771 - INFO - Epoch 17 training loss = 120.6
2023-04-27 08:56:46,877 - INFO - Epoch 18 training loss = 111.7
2023-04-27 08:56:46,982 - INFO - Epoch 19 training loss = 104.0
2023-04-27 08:56:47,086 - INFO - Epoch 20 training loss = 95.66
2023-04-27 08:56:47,115 - INFO - Validation loss = 98.55
2023-04-27 08:56:47,115 - INFO - best model
2023-04-27 08:56:47,232 - INFO - Epoch 21 training loss = 87.86
2023-04-27 08:56:47,337 - INFO - Epoch 22 training loss = 80.78
2023-04-27 08:56:47,442 - INFO - Epoch 23 training loss = 73.97
2023-04-27 08:56:47,546 - INFO - Epoch 24 training loss = 68.5
2023-04-27 08:56:47,650 - INFO - Epoch 25 training loss = 65.21
2023-04-27 08:56:47,755 - INFO - Epoch 26 training loss = 61.73
2023-04-27 08:56:47,860 - INFO - Epoch 27 training loss = 58.99
2023-04-27 08:56:47,964 - INFO - Epoch 28 training loss = 57.13
2023-04-27 08:56:48,128 - INFO - Epoch 29 training loss = 54.56
2023-04-27 08:56:48,231 - INFO - Epoch 30 training loss = 52.32
2023-04-27 08:56:48,260 - INFO - Validation loss = 54.79
2023-04-27 08:56:48,260 - INFO - best model
2023-04-27 08:56:48,375 - INFO - Epoch 31 training loss = 50.4
2023-04-27 08:56:48,479 - INFO - Epoch 32 training loss = 49.13
2023-04-27 08:56:48,582 - INFO - Epoch 33 training loss = 47.53
2023-04-27 08:56:48,686 - INFO - Epoch 34 training loss = 46.15
2023-04-27 08:56:48,791 - INFO - Epoch 35 training loss = 44.8
2023-04-27 08:56:48,895 - INFO - Epoch 36 training loss = 43.64
2023-04-27 08:56:49,000 - INFO - Epoch 37 training loss = 42.71
2023-04-27 08:56:49,105 - INFO - Epoch 38 training loss = 41.13
2023-04-27 08:56:49,210 - INFO - Epoch 39 training loss = 40.26
2023-04-27 08:56:49,315 - INFO - Epoch 40 training loss = 39.78
2023-04-27 08:56:49,343 - INFO - Validation loss = 42.39
2023-04-27 08:56:49,344 - INFO - best model
2023-04-27 08:56:49,460 - INFO - Epoch 41 training loss = 37.83
2023-04-27 08:56:49,569 - INFO - Epoch 42 training loss = 37.83
2023-04-27 08:56:49,676 - INFO - Epoch 43 training loss = 36.74
2023-04-27 08:56:49,784 - INFO - Epoch 44 training loss = 35.64
2023-04-27 08:56:49,890 - INFO - Epoch 45 training loss = 34.71
2023-04-27 08:56:49,996 - INFO - Epoch 46 training loss = 33.73
2023-04-27 08:56:50,101 - INFO - Epoch 47 training loss = 33.03
2023-04-27 08:56:50,206 - INFO - Epoch 48 training loss = 32.35
2023-04-27 08:56:50,312 - INFO - Epoch 49 training loss = 32.01
2023-04-27 08:56:50,418 - INFO - Epoch 50 training loss = 31.58
2023-04-27 08:56:50,446 - INFO - Validation loss = 33.83
2023-04-27 08:56:50,447 - INFO - best model
2023-04-27 08:56:50,563 - INFO - Epoch 51 training loss = 30.45
2023-04-27 08:56:50,668 - INFO - Epoch 52 training loss = 30.45
2023-04-27 08:56:50,772 - INFO - Epoch 53 training loss = 29.27
2023-04-27 08:56:50,876 - INFO - Epoch 54 training loss = 29.32
2023-04-27 08:56:50,981 - INFO - Epoch 55 training loss = 27.96
2023-04-27 08:56:51,084 - INFO - Epoch 56 training loss = 28.66
2023-04-27 08:56:51,188 - INFO - Epoch 57 training loss = 26.75
2023-04-27 08:56:51,292 - INFO - Epoch 58 training loss = 26.66
2023-04-27 08:56:51,396 - INFO - Epoch 59 training loss = 26.38
2023-04-27 08:56:51,500 - INFO - Epoch 60 training loss = 26.61
2023-04-27 08:56:51,528 - INFO - Validation loss = 27.9
2023-04-27 08:56:51,528 - INFO - best model
2023-04-27 08:56:51,703 - INFO - Epoch 61 training loss = 25.65
2023-04-27 08:56:51,807 - INFO - Epoch 62 training loss = 24.94
2023-04-27 08:56:51,908 - INFO - Epoch 63 training loss = 24.73
2023-04-27 08:56:52,012 - INFO - Epoch 64 training loss = 23.87
2023-04-27 08:56:52,116 - INFO - Epoch 65 training loss = 23.9
2023-04-27 08:56:52,220 - INFO - Epoch 66 training loss = 23.5
2023-04-27 08:56:52,324 - INFO - Epoch 67 training loss = 23.13
2023-04-27 08:56:52,428 - INFO - Epoch 68 training loss = 23.46
2023-04-27 08:56:52,532 - INFO - Epoch 69 training loss = 23.73
2023-04-27 08:56:52,635 - INFO - Epoch 70 training loss = 22.23
2023-04-27 08:56:52,664 - INFO - Validation loss = 26.37
2023-04-27 08:56:52,664 - INFO - best model
2023-04-27 08:56:52,779 - INFO - Epoch 71 training loss = 22.58
2023-04-27 08:56:52,881 - INFO - Epoch 72 training loss = 21.88
2023-04-27 08:56:52,985 - INFO - Epoch 73 training loss = 22.12
2023-04-27 08:56:53,089 - INFO - Epoch 74 training loss = 20.56
2023-04-27 08:56:53,191 - INFO - Epoch 75 training loss = 21.22
2023-04-27 08:56:53,295 - INFO - Epoch 76 training loss = 20.78
2023-04-27 08:56:53,399 - INFO - Epoch 77 training loss = 21.93
2023-04-27 08:56:53,503 - INFO - Epoch 78 training loss = 20.07
2023-04-27 08:56:53,606 - INFO - Epoch 79 training loss = 19.91
2023-04-27 08:56:53,710 - INFO - Epoch 80 training loss = 20.28
2023-04-27 08:56:53,738 - INFO - Validation loss = 25.37
2023-04-27 08:56:53,739 - INFO - best model
2023-04-27 08:56:53,854 - INFO - Epoch 81 training loss = 19.97
2023-04-27 08:56:53,959 - INFO - Epoch 82 training loss = 18.9
2023-04-27 08:56:54,063 - INFO - Epoch 83 training loss = 20.66
2023-04-27 08:56:54,167 - INFO - Epoch 84 training loss = 18.74
2023-04-27 08:56:54,271 - INFO - Epoch 85 training loss = 18.58
2023-04-27 08:56:54,375 - INFO - Epoch 86 training loss = 18.68
2023-04-27 08:56:54,479 - INFO - Epoch 87 training loss = 18.66
2023-04-27 08:56:54,584 - INFO - Epoch 88 training loss = 18.68
2023-04-27 08:56:54,688 - INFO - Epoch 89 training loss = 17.79
2023-04-27 08:56:54,796 - INFO - Epoch 90 training loss = 17.64
2023-04-27 08:56:54,824 - INFO - Validation loss = 20.64
2023-04-27 08:56:54,825 - INFO - best model
2023-04-27 08:56:54,940 - INFO - Epoch 91 training loss = 17.64
2023-04-27 08:56:55,104 - INFO - Epoch 92 training loss = 17.18
2023-04-27 08:56:55,204 - INFO - Epoch 93 training loss = 18.0
2023-04-27 08:56:55,309 - INFO - Epoch 94 training loss = 17.08
2023-04-27 08:56:55,414 - INFO - Epoch 95 training loss = 16.85
2023-04-27 08:56:55,518 - INFO - Epoch 96 training loss = 16.58
2023-04-27 08:56:55,622 - INFO - Epoch 97 training loss = 16.32
2023-04-27 08:56:55,726 - INFO - Epoch 98 training loss = 20.42
2023-04-27 08:56:55,830 - INFO - Epoch 99 training loss = 15.81
2023-04-27 08:56:55,935 - INFO - Epoch 100 training loss = 16.12
2023-04-27 08:56:55,963 - INFO - Validation loss = 20.29
2023-04-27 08:56:55,964 - INFO - best model
2023-04-27 08:56:56,079 - INFO - Epoch 101 training loss = 16.38
2023-04-27 08:56:56,184 - INFO - Epoch 102 training loss = 15.91
2023-04-27 08:56:56,286 - INFO - Epoch 103 training loss = 15.49
2023-04-27 08:56:56,389 - INFO - Epoch 104 training loss = 16.32
2023-04-27 08:56:56,491 - INFO - Epoch 105 training loss = 15.77
2023-04-27 08:56:56,595 - INFO - Epoch 106 training loss = 15.36
2023-04-27 08:56:56,699 - INFO - Epoch 107 training loss = 14.99
2023-04-27 08:56:56,804 - INFO - Epoch 108 training loss = 14.73
2023-04-27 08:56:56,908 - INFO - Epoch 109 training loss = 14.75
2023-04-27 08:56:57,012 - INFO - Epoch 110 training loss = 14.04
2023-04-27 08:56:57,040 - INFO - Validation loss = 17.35
2023-04-27 08:56:57,040 - INFO - best model
2023-04-27 08:56:57,156 - INFO - Epoch 111 training loss = 14.38
2023-04-27 08:56:57,261 - INFO - Epoch 112 training loss = 15.23
2023-04-27 08:56:57,365 - INFO - Epoch 113 training loss = 13.69
2023-04-27 08:56:57,469 - INFO - Epoch 114 training loss = 13.51
2023-04-27 08:56:57,573 - INFO - Epoch 115 training loss = 14.79
2023-04-27 08:56:57,678 - INFO - Epoch 116 training loss = 13.51
2023-04-27 08:56:57,782 - INFO - Epoch 117 training loss = 13.52
2023-04-27 08:56:57,887 - INFO - Epoch 118 training loss = 12.96
2023-04-27 08:56:57,991 - INFO - Epoch 119 training loss = 14.42
2023-04-27 08:56:58,095 - INFO - Epoch 120 training loss = 13.7
2023-04-27 08:56:58,124 - INFO - Validation loss = 16.47
2023-04-27 08:56:58,124 - INFO - best model
2023-04-27 08:56:58,241 - INFO - Epoch 121 training loss = 12.57
2023-04-27 08:56:58,347 - INFO - Epoch 122 training loss = 12.6
2023-04-27 08:56:58,514 - INFO - Epoch 123 training loss = 13.04
2023-04-27 08:56:58,620 - INFO - Epoch 124 training loss = 11.77
2023-04-27 08:56:58,726 - INFO - Epoch 125 training loss = 12.25
2023-04-27 08:56:58,832 - INFO - Epoch 126 training loss = 12.21
2023-04-27 08:56:58,939 - INFO - Epoch 127 training loss = 14.52
2023-04-27 08:56:59,045 - INFO - Epoch 128 training loss = 11.97
2023-04-27 08:56:59,151 - INFO - Epoch 129 training loss = 12.4
2023-04-27 08:56:59,256 - INFO - Epoch 130 training loss = 11.72
2023-04-27 08:56:59,285 - INFO - Validation loss = 14.84
2023-04-27 08:56:59,285 - INFO - best model
2023-04-27 08:56:59,401 - INFO - Epoch 131 training loss = 11.79
2023-04-27 08:56:59,506 - INFO - Epoch 132 training loss = 11.43
2023-04-27 08:56:59,611 - INFO - Epoch 133 training loss = 11.69
2023-04-27 08:56:59,716 - INFO - Epoch 134 training loss = 11.98
2023-04-27 08:56:59,820 - INFO - Epoch 135 training loss = 11.34
2023-04-27 08:56:59,927 - INFO - Epoch 136 training loss = 11.35
2023-04-27 08:57:00,034 - INFO - Epoch 137 training loss = 10.96
2023-04-27 08:57:00,140 - INFO - Epoch 138 training loss = 11.08
2023-04-27 08:57:00,246 - INFO - Epoch 139 training loss = 10.99
2023-04-27 08:57:00,347 - INFO - Epoch 140 training loss = 11.1
2023-04-27 08:57:00,374 - INFO - Validation loss = 14.49
2023-04-27 08:57:00,375 - INFO - best model
2023-04-27 08:57:00,487 - INFO - Epoch 141 training loss = 11.3
2023-04-27 08:57:00,589 - INFO - Epoch 142 training loss = 10.24
2023-04-27 08:57:00,695 - INFO - Epoch 143 training loss = 10.55
2023-04-27 08:57:00,800 - INFO - Epoch 144 training loss = 10.49
2023-04-27 08:57:00,906 - INFO - Epoch 145 training loss = 10.39
2023-04-27 08:57:01,012 - INFO - Epoch 146 training loss = 10.57
2023-04-27 08:57:01,118 - INFO - Epoch 147 training loss = 10.65
2023-04-27 08:57:01,222 - INFO - Epoch 148 training loss = 10.32
2023-04-27 08:57:01,327 - INFO - Epoch 149 training loss = 10.38
2023-04-27 08:57:01,433 - INFO - Epoch 150 training loss = 10.57
2023-04-27 08:57:01,461 - INFO - Validation loss = 14.45
2023-04-27 08:57:01,461 - INFO - best model
2023-04-27 08:57:01,578 - INFO - Epoch 151 training loss = 10.01
2023-04-27 08:57:01,683 - INFO - Epoch 152 training loss = 9.761
2023-04-27 08:57:01,788 - INFO - Epoch 153 training loss = 10.05
2023-04-27 08:57:01,891 - INFO - Epoch 154 training loss = 9.208
2023-04-27 08:57:02,054 - INFO - Epoch 155 training loss = 10.36
2023-04-27 08:57:02,158 - INFO - Epoch 156 training loss = 9.359
2023-04-27 08:57:02,263 - INFO - Epoch 157 training loss = 10.5
2023-04-27 08:57:02,370 - INFO - Epoch 158 training loss = 9.768
2023-04-27 08:57:02,476 - INFO - Epoch 159 training loss = 9.006
2023-04-27 08:57:02,579 - INFO - Epoch 160 training loss = 9.549
2023-04-27 08:57:02,608 - INFO - Validation loss = 13.41
2023-04-27 08:57:02,608 - INFO - best model
2023-04-27 08:57:02,725 - INFO - Epoch 161 training loss = 9.004
2023-04-27 08:57:02,831 - INFO - Epoch 162 training loss = 10.25
2023-04-27 08:57:02,937 - INFO - Epoch 163 training loss = 8.854
2023-04-27 08:57:03,043 - INFO - Epoch 164 training loss = 9.42
2023-04-27 08:57:03,148 - INFO - Epoch 165 training loss = 9.553
2023-04-27 08:57:03,254 - INFO - Epoch 166 training loss = 8.626
2023-04-27 08:57:03,360 - INFO - Epoch 167 training loss = 9.001
2023-04-27 08:57:03,468 - INFO - Epoch 168 training loss = 9.229
2023-04-27 08:57:03,574 - INFO - Epoch 169 training loss = 9.218
2023-04-27 08:57:03,678 - INFO - Epoch 170 training loss = 8.366
2023-04-27 08:57:03,707 - INFO - Validation loss = 12.52
2023-04-27 08:57:03,707 - INFO - best model
2023-04-27 08:57:03,825 - INFO - Epoch 171 training loss = 8.748
2023-04-27 08:57:03,934 - INFO - Epoch 172 training loss = 8.459
2023-04-27 08:57:04,042 - INFO - Epoch 173 training loss = 8.522
2023-04-27 08:57:04,147 - INFO - Epoch 174 training loss = 9.378
2023-04-27 08:57:04,252 - INFO - Epoch 175 training loss = 8.149
2023-04-27 08:57:04,359 - INFO - Epoch 176 training loss = 8.209
2023-04-27 08:57:04,464 - INFO - Epoch 177 training loss = 8.741
2023-04-27 08:57:04,570 - INFO - Epoch 178 training loss = 8.143
2023-04-27 08:57:04,676 - INFO - Epoch 179 training loss = 8.138
2023-04-27 08:57:04,782 - INFO - Epoch 180 training loss = 8.527
2023-04-27 08:57:04,811 - INFO - Validation loss = 12.32
2023-04-27 08:57:04,811 - INFO - best model
2023-04-27 08:57:04,929 - INFO - Epoch 181 training loss = 7.951
2023-04-27 08:57:05,035 - INFO - Epoch 182 training loss = 8.076
2023-04-27 08:57:05,139 - INFO - Epoch 183 training loss = 8.193
2023-04-27 08:57:05,244 - INFO - Epoch 184 training loss = 7.865
2023-04-27 08:57:05,349 - INFO - Epoch 185 training loss = 7.85
2023-04-27 08:57:05,515 - INFO - Epoch 186 training loss = 7.661
2023-04-27 08:57:05,618 - INFO - Epoch 187 training loss = 8.002
2023-04-27 08:57:05,724 - INFO - Epoch 188 training loss = 7.872
2023-04-27 08:57:05,829 - INFO - Epoch 189 training loss = 7.775
2023-04-27 08:57:05,935 - INFO - Epoch 190 training loss = 7.569
2023-04-27 08:57:05,964 - INFO - Validation loss = 11.57
2023-04-27 08:57:05,965 - INFO - best model
2023-04-27 08:57:06,082 - INFO - Epoch 191 training loss = 8.096
2023-04-27 08:57:06,185 - INFO - Epoch 192 training loss = 7.569
2023-04-27 08:57:06,290 - INFO - Epoch 193 training loss = 7.497
2023-04-27 08:57:06,396 - INFO - Epoch 194 training loss = 7.85
2023-04-27 08:57:06,501 - INFO - Epoch 195 training loss = 7.667
2023-04-27 08:57:06,604 - INFO - Epoch 196 training loss = 7.331
2023-04-27 08:57:06,709 - INFO - Epoch 197 training loss = 8.275
2023-04-27 08:57:06,815 - INFO - Epoch 198 training loss = 7.723
2023-04-27 08:57:06,920 - INFO - Epoch 199 training loss = 8.214
2023-04-27 08:57:07,025 - INFO - Epoch 200 training loss = 6.96
2023-04-27 08:57:07,054 - INFO - Validation loss = 12.23
2023-04-27 08:57:07,159 - INFO - Epoch 201 training loss = 7.354
2023-04-27 08:57:07,265 - INFO - Epoch 202 training loss = 7.34
2023-04-27 08:57:07,371 - INFO - Epoch 203 training loss = 7.369
2023-04-27 08:57:07,476 - INFO - Epoch 204 training loss = 7.165
2023-04-27 08:57:07,581 - INFO - Epoch 205 training loss = 6.959
2023-04-27 08:57:07,685 - INFO - Epoch 206 training loss = 7.472
2023-04-27 08:57:07,791 - INFO - Epoch 207 training loss = 7.421
2023-04-27 08:57:07,899 - INFO - Epoch 208 training loss = 7.14
2023-04-27 08:57:08,008 - INFO - Epoch 209 training loss = 6.792
2023-04-27 08:57:08,114 - INFO - Epoch 210 training loss = 6.938
2023-04-27 08:57:08,144 - INFO - Validation loss = 11.21
2023-04-27 08:57:08,144 - INFO - best model
2023-04-27 08:57:08,264 - INFO - Epoch 211 training loss = 6.952
2023-04-27 08:57:08,373 - INFO - Epoch 212 training loss = 6.858
2023-04-27 08:57:08,481 - INFO - Epoch 213 training loss = 6.656
2023-04-27 08:57:08,587 - INFO - Epoch 214 training loss = 6.792
2023-04-27 08:57:08,696 - INFO - Epoch 215 training loss = 7.193
2023-04-27 08:57:08,802 - INFO - Epoch 216 training loss = 6.712
2023-04-27 08:57:08,969 - INFO - Epoch 217 training loss = 6.491
2023-04-27 08:57:09,073 - INFO - Epoch 218 training loss = 7.052
2023-04-27 08:57:09,178 - INFO - Epoch 219 training loss = 7.011
2023-04-27 08:57:09,284 - INFO - Epoch 220 training loss = 6.613
2023-04-27 08:57:09,313 - INFO - Validation loss = 11.45
2023-04-27 08:57:09,419 - INFO - Epoch 221 training loss = 6.856
2023-04-27 08:57:09,525 - INFO - Epoch 222 training loss = 6.349
2023-04-27 08:57:09,630 - INFO - Epoch 223 training loss = 6.671
2023-04-27 08:57:09,736 - INFO - Epoch 224 training loss = 6.279
2023-04-27 08:57:09,842 - INFO - Epoch 225 training loss = 6.441
2023-04-27 08:57:09,949 - INFO - Epoch 226 training loss = 6.316
2023-04-27 08:57:10,055 - INFO - Epoch 227 training loss = 6.253
2023-04-27 08:57:10,160 - INFO - Epoch 228 training loss = 6.709
2023-04-27 08:57:10,262 - INFO - Epoch 229 training loss = 6.998
2023-04-27 08:57:10,362 - INFO - Epoch 230 training loss = 6.533
2023-04-27 08:57:10,392 - INFO - Validation loss = 10.53
2023-04-27 08:57:10,392 - INFO - best model
2023-04-27 08:57:10,510 - INFO - Epoch 231 training loss = 6.011
2023-04-27 08:57:10,611 - INFO - Epoch 232 training loss = 6.169
2023-04-27 08:57:10,712 - INFO - Epoch 233 training loss = 6.26
2023-04-27 08:57:10,817 - INFO - Epoch 234 training loss = 6.498
2023-04-27 08:57:10,924 - INFO - Epoch 235 training loss = 6.414
2023-04-27 08:57:11,029 - INFO - Epoch 236 training loss = 6.202
2023-04-27 08:57:11,134 - INFO - Epoch 237 training loss = 6.004
2023-04-27 08:57:11,239 - INFO - Epoch 238 training loss = 6.096
2023-04-27 08:57:11,345 - INFO - Epoch 239 training loss = 6.344
2023-04-27 08:57:11,451 - INFO - Epoch 240 training loss = 6.139
2023-04-27 08:57:11,479 - INFO - Validation loss = 10.62
2023-04-27 08:57:11,585 - INFO - Epoch 241 training loss = 5.861
2023-04-27 08:57:11,688 - INFO - Epoch 242 training loss = 6.139
2023-04-27 08:57:11,794 - INFO - Epoch 243 training loss = 5.931
2023-04-27 08:57:11,901 - INFO - Epoch 244 training loss = 5.946
2023-04-27 08:57:12,006 - INFO - Epoch 245 training loss = 6.352
2023-04-27 08:57:12,112 - INFO - Epoch 246 training loss = 5.669
2023-04-27 08:57:12,218 - INFO - Epoch 247 training loss = 5.744
2023-04-27 08:57:12,323 - INFO - Epoch 248 training loss = 5.945
2023-04-27 08:57:12,489 - INFO - Epoch 249 training loss = 5.77
2023-04-27 08:57:12,594 - INFO - Epoch 250 training loss = 5.775
2023-04-27 08:57:12,623 - INFO - Validation loss = 10.53
2023-04-27 08:57:12,729 - INFO - Epoch 251 training loss = 5.572
2023-04-27 08:57:12,835 - INFO - Epoch 252 training loss = 5.723
2023-04-27 08:57:12,942 - INFO - Epoch 253 training loss = 5.813
2023-04-27 08:57:13,048 - INFO - Epoch 254 training loss = 6.081
2023-04-27 08:57:13,152 - INFO - Epoch 255 training loss = 5.609
2023-04-27 08:57:13,257 - INFO - Epoch 256 training loss = 5.563
2023-04-27 08:57:13,360 - INFO - Epoch 257 training loss = 5.536
2023-04-27 08:57:13,465 - INFO - Epoch 258 training loss = 5.739
2023-04-27 08:57:13,570 - INFO - Epoch 259 training loss = 5.654
2023-04-27 08:57:13,676 - INFO - Epoch 260 training loss = 5.503
2023-04-27 08:57:13,704 - INFO - Validation loss = 10.62
2023-04-27 08:57:13,810 - INFO - Epoch 261 training loss = 5.546
2023-04-27 08:57:13,916 - INFO - Epoch 262 training loss = 5.442
2023-04-27 08:57:14,022 - INFO - Epoch 263 training loss = 5.862
2023-04-27 08:57:14,127 - INFO - Epoch 264 training loss = 5.376
2023-04-27 08:57:14,233 - INFO - Epoch 265 training loss = 5.709
2023-04-27 08:57:14,338 - INFO - Epoch 266 training loss = 5.484
2023-04-27 08:57:14,443 - INFO - Epoch 267 training loss = 5.604
2023-04-27 08:57:14,549 - INFO - Epoch 268 training loss = 5.858
2023-04-27 08:57:14,654 - INFO - Epoch 269 training loss = 5.17
2023-04-27 08:57:14,759 - INFO - Epoch 270 training loss = 5.296
2023-04-27 08:57:14,787 - INFO - Validation loss = 10.19
2023-04-27 08:57:14,788 - INFO - best model
2023-04-27 08:57:14,905 - INFO - Epoch 271 training loss = 5.713
2023-04-27 08:57:15,009 - INFO - Epoch 272 training loss = 5.401
2023-04-27 08:57:15,114 - INFO - Epoch 273 training loss = 5.289
2023-04-27 08:57:15,218 - INFO - Epoch 274 training loss = 5.23
2023-04-27 08:57:15,322 - INFO - Epoch 275 training loss = 5.142
2023-04-27 08:57:15,426 - INFO - Epoch 276 training loss = 5.113
2023-04-27 08:57:15,530 - INFO - Epoch 277 training loss = 5.343
2023-04-27 08:57:15,635 - INFO - Epoch 278 training loss = 5.423
2023-04-27 08:57:15,739 - INFO - Epoch 279 training loss = 5.062
2023-04-27 08:57:15,905 - INFO - Epoch 280 training loss = 5.257
2023-04-27 08:57:15,933 - INFO - Validation loss = 10.57
2023-04-27 08:57:16,037 - INFO - Epoch 281 training loss = 5.172
2023-04-27 08:57:16,143 - INFO - Epoch 282 training loss = 5.386
2023-04-27 08:57:16,249 - INFO - Epoch 283 training loss = 4.901
2023-04-27 08:57:16,355 - INFO - Epoch 284 training loss = 5.226
2023-04-27 08:57:16,461 - INFO - Epoch 285 training loss = 5.298
2023-04-27 08:57:16,566 - INFO - Epoch 286 training loss = 5.135
2023-04-27 08:57:16,672 - INFO - Epoch 287 training loss = 5.001
2023-04-27 08:57:16,776 - INFO - Epoch 288 training loss = 5.101
2023-04-27 08:57:16,881 - INFO - Epoch 289 training loss = 4.975
2023-04-27 08:57:16,988 - INFO - Epoch 290 training loss = 5.141
2023-04-27 08:57:17,017 - INFO - Validation loss = 10.85
2023-04-27 08:57:17,123 - INFO - Epoch 291 training loss = 5.124
2023-04-27 08:57:17,228 - INFO - Epoch 292 training loss = 4.876
2023-04-27 08:57:17,333 - INFO - Epoch 293 training loss = 4.959
2023-04-27 08:57:17,438 - INFO - Epoch 294 training loss = 4.924
2023-04-27 08:57:17,543 - INFO - Epoch 295 training loss = 4.912
2023-04-27 08:57:17,648 - INFO - Epoch 296 training loss = 5.149
2023-04-27 08:57:17,753 - INFO - Epoch 297 training loss = 4.583
2023-04-27 08:57:17,858 - INFO - Epoch 298 training loss = 5.007
2023-04-27 08:57:17,966 - INFO - Epoch 299 training loss = 4.828
2023-04-27 08:57:18,071 - INFO - Epoch 300 training loss = 4.799
2023-04-27 08:57:18,100 - INFO - Validation loss = 9.862
2023-04-27 08:57:18,100 - INFO - best model
2023-04-27 08:57:18,217 - INFO - Epoch 301 training loss = 4.767
2023-04-27 08:57:18,324 - INFO - Epoch 302 training loss = 4.761
2023-04-27 08:57:18,430 - INFO - Epoch 303 training loss = 4.753
2023-04-27 08:57:18,535 - INFO - Epoch 304 training loss = 4.682
2023-04-27 08:57:18,641 - INFO - Epoch 305 training loss = 4.598
2023-04-27 08:57:18,747 - INFO - Epoch 306 training loss = 4.721
2023-04-27 08:57:18,853 - INFO - Epoch 307 training loss = 4.844
2023-04-27 08:57:18,959 - INFO - Epoch 308 training loss = 4.62
2023-04-27 08:57:19,063 - INFO - Epoch 309 training loss = 4.746
2023-04-27 08:57:19,168 - INFO - Epoch 310 training loss = 4.592
2023-04-27 08:57:19,197 - INFO - Validation loss = 9.608
2023-04-27 08:57:19,197 - INFO - best model
2023-04-27 08:57:19,374 - INFO - Epoch 311 training loss = 4.768
2023-04-27 08:57:19,476 - INFO - Epoch 312 training loss = 4.631
2023-04-27 08:57:19,579 - INFO - Epoch 313 training loss = 4.593
2023-04-27 08:57:19,685 - INFO - Epoch 314 training loss = 4.535
2023-04-27 08:57:19,790 - INFO - Epoch 315 training loss = 4.723
2023-04-27 08:57:19,897 - INFO - Epoch 316 training loss = 4.518
2023-04-27 08:57:20,003 - INFO - Epoch 317 training loss = 4.528
2023-04-27 08:57:20,108 - INFO - Epoch 318 training loss = 4.617
2023-04-27 08:57:20,211 - INFO - Epoch 319 training loss = 4.329
2023-04-27 08:57:20,315 - INFO - Epoch 320 training loss = 4.442
2023-04-27 08:57:20,343 - INFO - Validation loss = 9.843
2023-04-27 08:57:20,449 - INFO - Epoch 321 training loss = 4.556
2023-04-27 08:57:20,554 - INFO - Epoch 322 training loss = 4.404
2023-04-27 08:57:20,659 - INFO - Epoch 323 training loss = 4.46
2023-04-27 08:57:20,764 - INFO - Epoch 324 training loss = 4.449
2023-04-27 08:57:20,869 - INFO - Epoch 325 training loss = 4.344
2023-04-27 08:57:20,976 - INFO - Epoch 326 training loss = 4.462
2023-04-27 08:57:21,083 - INFO - Epoch 327 training loss = 4.483
2023-04-27 08:57:21,189 - INFO - Epoch 328 training loss = 4.475
2023-04-27 08:57:21,295 - INFO - Epoch 329 training loss = 4.374
2023-04-27 08:57:21,401 - INFO - Epoch 330 training loss = 4.446
2023-04-27 08:57:21,429 - INFO - Validation loss = 10.47
2023-04-27 08:57:21,536 - INFO - Epoch 331 training loss = 4.398
2023-04-27 08:57:21,641 - INFO - Epoch 332 training loss = 4.196
2023-04-27 08:57:21,745 - INFO - Epoch 333 training loss = 4.383
2023-04-27 08:57:21,849 - INFO - Epoch 334 training loss = 4.357
2023-04-27 08:57:21,954 - INFO - Epoch 335 training loss = 4.232
2023-04-27 08:57:22,059 - INFO - Epoch 336 training loss = 4.293
2023-04-27 08:57:22,161 - INFO - Epoch 337 training loss = 4.313
2023-04-27 08:57:22,265 - INFO - Epoch 338 training loss = 4.178
2023-04-27 08:57:22,369 - INFO - Epoch 339 training loss = 4.321
2023-04-27 08:57:22,474 - INFO - Epoch 340 training loss = 4.302
2023-04-27 08:57:22,502 - INFO - Validation loss = 9.355
2023-04-27 08:57:22,502 - INFO - best model
2023-04-27 08:57:22,617 - INFO - Epoch 341 training loss = 4.04
2023-04-27 08:57:22,722 - INFO - Epoch 342 training loss = 4.283
2023-04-27 08:57:22,887 - INFO - Epoch 343 training loss = 4.198
2023-04-27 08:57:22,991 - INFO - Epoch 344 training loss = 4.234
2023-04-27 08:57:23,092 - INFO - Epoch 345 training loss = 4.105
2023-04-27 08:57:23,194 - INFO - Epoch 346 training loss = 4.07
2023-04-27 08:57:23,298 - INFO - Epoch 347 training loss = 4.28
2023-04-27 08:57:23,403 - INFO - Epoch 348 training loss = 4.063
2023-04-27 08:57:23,507 - INFO - Epoch 349 training loss = 4.37
2023-04-27 08:57:23,611 - INFO - Epoch 350 training loss = 4.088
2023-04-27 08:57:23,640 - INFO - Validation loss = 9.601
2023-04-27 08:57:23,744 - INFO - Epoch 351 training loss = 4.114
2023-04-27 08:57:23,848 - INFO - Epoch 352 training loss = 4.105
2023-04-27 08:57:23,953 - INFO - Epoch 353 training loss = 4.108
2023-04-27 08:57:24,058 - INFO - Epoch 354 training loss = 4.132
2023-04-27 08:57:24,162 - INFO - Epoch 355 training loss = 4.167
2023-04-27 08:57:24,267 - INFO - Epoch 356 training loss = 3.912
2023-04-27 08:57:24,372 - INFO - Epoch 357 training loss = 4.036
2023-04-27 08:57:24,476 - INFO - Epoch 358 training loss = 3.939
2023-04-27 08:57:24,580 - INFO - Epoch 359 training loss = 4.104
2023-04-27 08:57:24,685 - INFO - Epoch 360 training loss = 3.961
2023-04-27 08:57:24,713 - INFO - Validation loss = 9.917
2023-04-27 08:57:24,818 - INFO - Epoch 361 training loss = 3.959
2023-04-27 08:57:24,923 - INFO - Epoch 362 training loss = 4.304
2023-04-27 08:57:25,027 - INFO - Epoch 363 training loss = 3.753
2023-04-27 08:57:25,130 - INFO - Epoch 364 training loss = 3.805
2023-04-27 08:57:25,233 - INFO - Epoch 365 training loss = 4.037
2023-04-27 08:57:25,337 - INFO - Epoch 366 training loss = 4.174
2023-04-27 08:57:25,443 - INFO - Epoch 367 training loss = 3.891
2023-04-27 08:57:25,549 - INFO - Epoch 368 training loss = 3.795
2023-04-27 08:57:25,654 - INFO - Epoch 369 training loss = 3.823
2023-04-27 08:57:25,760 - INFO - Epoch 370 training loss = 3.813
2023-04-27 08:57:25,789 - INFO - Validation loss = 9.379
2023-04-27 08:57:25,894 - INFO - Epoch 371 training loss = 3.881
2023-04-27 08:57:26,000 - INFO - Epoch 372 training loss = 3.921
2023-04-27 08:57:26,106 - INFO - Epoch 373 training loss = 3.837
2023-04-27 08:57:26,273 - INFO - Epoch 374 training loss = 3.856
2023-04-27 08:57:26,378 - INFO - Epoch 375 training loss = 3.707
2023-04-27 08:57:26,484 - INFO - Epoch 376 training loss = 3.899
2023-04-27 08:57:26,590 - INFO - Epoch 377 training loss = 3.807
2023-04-27 08:57:26,696 - INFO - Epoch 378 training loss = 3.687
2023-04-27 08:57:26,801 - INFO - Epoch 379 training loss = 3.863
2023-04-27 08:57:26,907 - INFO - Epoch 380 training loss = 3.732
2023-04-27 08:57:26,936 - INFO - Validation loss = 9.189
2023-04-27 08:57:26,936 - INFO - best model
2023-04-27 08:57:27,053 - INFO - Epoch 381 training loss = 3.905
2023-04-27 08:57:27,158 - INFO - Epoch 382 training loss = 3.69
2023-04-27 08:57:27,264 - INFO - Epoch 383 training loss = 3.749
2023-04-27 08:57:27,368 - INFO - Epoch 384 training loss = 3.869
2023-04-27 08:57:27,474 - INFO - Epoch 385 training loss = 3.73
2023-04-27 08:57:27,579 - INFO - Epoch 386 training loss = 3.655
2023-04-27 08:57:27,684 - INFO - Epoch 387 training loss = 3.646
2023-04-27 08:57:27,790 - INFO - Epoch 388 training loss = 3.666
2023-04-27 08:57:27,895 - INFO - Epoch 389 training loss = 3.546
2023-04-27 08:57:28,001 - INFO - Epoch 390 training loss = 3.577
2023-04-27 08:57:28,030 - INFO - Validation loss = 9.334
2023-04-27 08:57:28,135 - INFO - Epoch 391 training loss = 3.652
2023-04-27 08:57:28,240 - INFO - Epoch 392 training loss = 3.529
2023-04-27 08:57:28,345 - INFO - Epoch 393 training loss = 3.653
2023-04-27 08:57:28,451 - INFO - Epoch 394 training loss = 3.573
2023-04-27 08:57:28,555 - INFO - Epoch 395 training loss = 3.626
2023-04-27 08:57:28,657 - INFO - Epoch 396 training loss = 3.527
2023-04-27 08:57:28,762 - INFO - Epoch 397 training loss = 3.634
2023-04-27 08:57:28,876 - INFO - Epoch 398 training loss = 3.602
2023-04-27 08:57:28,991 - INFO - Epoch 399 training loss = 3.498
2023-04-27 08:57:29,102 - INFO - Epoch 400 training loss = 3.561
2023-04-27 08:57:29,132 - INFO - Validation loss = 9.101
2023-04-27 08:57:29,133 - INFO - best model
2023-04-27 08:57:29,257 - INFO - Epoch 401 training loss = 3.496
2023-04-27 08:57:29,368 - INFO - Epoch 402 training loss = 3.569
2023-04-27 08:57:29,481 - INFO - Epoch 403 training loss = 3.594
2023-04-27 08:57:29,592 - INFO - Epoch 404 training loss = 3.513
2023-04-27 08:57:29,703 - INFO - Epoch 405 training loss = 3.449
2023-04-27 08:57:29,874 - INFO - Epoch 406 training loss = 3.437
2023-04-27 08:57:29,987 - INFO - Epoch 407 training loss = 3.508
2023-04-27 08:57:30,103 - INFO - Epoch 408 training loss = 3.564
2023-04-27 08:57:30,218 - INFO - Epoch 409 training loss = 3.567
2023-04-27 08:57:30,335 - INFO - Epoch 410 training loss = 3.37
2023-04-27 08:57:30,365 - INFO - Validation loss = 11.06
2023-04-27 08:57:30,479 - INFO - Epoch 411 training loss = 3.594
2023-04-27 08:57:30,592 - INFO - Epoch 412 training loss = 3.417
2023-04-27 08:57:30,706 - INFO - Epoch 413 training loss = 3.397
2023-04-27 08:57:30,824 - INFO - Epoch 414 training loss = 3.441
2023-04-27 08:57:30,941 - INFO - Epoch 415 training loss = 3.41
2023-04-27 08:57:31,059 - INFO - Epoch 416 training loss = 3.33
2023-04-27 08:57:31,173 - INFO - Epoch 417 training loss = 3.325
2023-04-27 08:57:31,281 - INFO - Epoch 418 training loss = 3.455
2023-04-27 08:57:31,385 - INFO - Epoch 419 training loss = 3.37
2023-04-27 08:57:31,489 - INFO - Epoch 420 training loss = 3.38
2023-04-27 08:57:31,517 - INFO - Validation loss = 9.343
2023-04-27 08:57:31,621 - INFO - Epoch 421 training loss = 3.354
2023-04-27 08:57:31,724 - INFO - Epoch 422 training loss = 3.326
2023-04-27 08:57:31,826 - INFO - Epoch 423 training loss = 3.305
2023-04-27 08:57:31,931 - INFO - Epoch 424 training loss = 3.297
2023-04-27 08:57:32,036 - INFO - Epoch 425 training loss = 3.462
2023-04-27 08:57:32,143 - INFO - Epoch 426 training loss = 3.262
2023-04-27 08:57:32,247 - INFO - Epoch 427 training loss =  3.3
2023-04-27 08:57:32,352 - INFO - Epoch 428 training loss = 3.356
2023-04-27 08:57:32,456 - INFO - Epoch 429 training loss = 3.269
2023-04-27 08:57:32,561 - INFO - Epoch 430 training loss = 3.443
2023-04-27 08:57:32,589 - INFO - Validation loss = 9.799
2023-04-27 08:57:32,693 - INFO - Epoch 431 training loss = 3.232
2023-04-27 08:57:32,795 - INFO - Epoch 432 training loss = 3.301
2023-04-27 08:57:32,901 - INFO - Epoch 433 training loss = 3.255
2023-04-27 08:57:33,008 - INFO - Epoch 434 training loss = 3.271
2023-04-27 08:57:33,113 - INFO - Epoch 435 training loss =  3.3
2023-04-27 08:57:33,219 - INFO - Epoch 436 training loss = 3.162
2023-04-27 08:57:33,381 - INFO - Epoch 437 training loss = 3.16
2023-04-27 08:57:33,482 - INFO - Epoch 438 training loss = 3.214
2023-04-27 08:57:33,584 - INFO - Epoch 439 training loss = 3.21
2023-04-27 08:57:33,686 - INFO - Epoch 440 training loss = 3.154
2023-04-27 08:57:33,715 - INFO - Validation loss = 9.181
2023-04-27 08:57:33,819 - INFO - Epoch 441 training loss = 3.165
2023-04-27 08:57:33,921 - INFO - Epoch 442 training loss = 3.434
2023-04-27 08:57:34,027 - INFO - Epoch 443 training loss = 3.059
2023-04-27 08:57:34,131 - INFO - Epoch 444 training loss = 3.196
2023-04-27 08:57:34,235 - INFO - Epoch 445 training loss = 3.12
2023-04-27 08:57:34,339 - INFO - Epoch 446 training loss = 3.155
2023-04-27 08:57:34,442 - INFO - Epoch 447 training loss = 3.161
2023-04-27 08:57:34,545 - INFO - Epoch 448 training loss = 3.16
2023-04-27 08:57:34,650 - INFO - Epoch 449 training loss = 3.211
2023-04-27 08:57:34,754 - INFO - Epoch 450 training loss = 3.105
2023-04-27 08:57:34,783 - INFO - Validation loss = 9.211
2023-04-27 08:57:34,887 - INFO - Epoch 451 training loss = 3.136
2023-04-27 08:57:34,991 - INFO - Epoch 452 training loss = 3.198
2023-04-27 08:57:35,095 - INFO - Epoch 453 training loss = 3.027
2023-04-27 08:57:35,200 - INFO - Epoch 454 training loss = 3.079
2023-04-27 08:57:35,305 - INFO - Epoch 455 training loss = 3.05
2023-04-27 08:57:35,410 - INFO - Epoch 456 training loss = 3.108
2023-04-27 08:57:35,513 - INFO - Epoch 457 training loss = 3.09
2023-04-27 08:57:35,618 - INFO - Epoch 458 training loss = 3.028
2023-04-27 08:57:35,723 - INFO - Epoch 459 training loss = 3.052
2023-04-27 08:57:35,828 - INFO - Epoch 460 training loss = 3.07
2023-04-27 08:57:35,856 - INFO - Validation loss = 9.09
2023-04-27 08:57:35,856 - INFO - best model
2023-04-27 08:57:35,972 - INFO - Epoch 461 training loss = 3.046
2023-04-27 08:57:36,074 - INFO - Epoch 462 training loss = 2.958
2023-04-27 08:57:36,179 - INFO - Epoch 463 training loss = 3.105
2023-04-27 08:57:36,284 - INFO - Epoch 464 training loss = 2.978
2023-04-27 08:57:36,390 - INFO - Epoch 465 training loss = 2.946
2023-04-27 08:57:36,498 - INFO - Epoch 466 training loss = 3.013
2023-04-27 08:57:36,599 - INFO - Epoch 467 training loss = 3.157
2023-04-27 08:57:36,763 - INFO - Epoch 468 training loss = 2.929
2023-04-27 08:57:36,864 - INFO - Epoch 469 training loss = 2.945
2023-04-27 08:57:36,972 - INFO - Epoch 470 training loss = 2.921
2023-04-27 08:57:37,000 - INFO - Validation loss = 9.248
2023-04-27 08:57:37,105 - INFO - Epoch 471 training loss = 3.019
2023-04-27 08:57:37,207 - INFO - Epoch 472 training loss = 3.048
2023-04-27 08:57:37,309 - INFO - Epoch 473 training loss = 2.984
2023-04-27 08:57:37,411 - INFO - Epoch 474 training loss = 2.902
2023-04-27 08:57:37,515 - INFO - Epoch 475 training loss = 2.895
2023-04-27 08:57:37,619 - INFO - Epoch 476 training loss = 2.963
2023-04-27 08:57:37,723 - INFO - Epoch 477 training loss = 2.905
2023-04-27 08:57:37,827 - INFO - Epoch 478 training loss =  2.9
2023-04-27 08:57:37,934 - INFO - Epoch 479 training loss = 2.894
2023-04-27 08:57:38,042 - INFO - Epoch 480 training loss = 2.928
2023-04-27 08:57:38,072 - INFO - Validation loss = 9.088
2023-04-27 08:57:38,072 - INFO - best model
2023-04-27 08:57:38,192 - INFO - Epoch 481 training loss = 2.874
2023-04-27 08:57:38,295 - INFO - Epoch 482 training loss = 2.887
2023-04-27 08:57:38,396 - INFO - Epoch 483 training loss = 2.935
2023-04-27 08:57:38,498 - INFO - Epoch 484 training loss = 2.845
2023-04-27 08:57:38,602 - INFO - Epoch 485 training loss = 2.861
2023-04-27 08:57:38,707 - INFO - Epoch 486 training loss = 2.92
2023-04-27 08:57:38,808 - INFO - Epoch 487 training loss = 2.847
2023-04-27 08:57:38,910 - INFO - Epoch 488 training loss = 2.879
2023-04-27 08:57:39,014 - INFO - Epoch 489 training loss = 2.838
2023-04-27 08:57:39,118 - INFO - Epoch 490 training loss = 2.82
2023-04-27 08:57:39,147 - INFO - Validation loss = 9.304
2023-04-27 08:57:39,250 - INFO - Epoch 491 training loss = 2.833
2023-04-27 08:57:39,354 - INFO - Epoch 492 training loss = 2.869
2023-04-27 08:57:39,458 - INFO - Epoch 493 training loss = 2.815
2023-04-27 08:57:39,565 - INFO - Epoch 494 training loss = 2.808
2023-04-27 08:57:39,671 - INFO - Epoch 495 training loss = 2.835
2023-04-27 08:57:39,775 - INFO - Epoch 496 training loss = 2.82
2023-04-27 08:57:39,880 - INFO - Epoch 497 training loss = 2.732
2023-04-27 08:57:39,987 - INFO - Epoch 498 training loss = 2.87
2023-04-27 08:57:40,091 - INFO - Epoch 499 training loss = 2.754
2023-04-27 08:57:40,253 - INFO - Epoch 500 training loss = 2.703
2023-04-27 08:57:40,281 - INFO - Validation loss = 9.023
2023-04-27 08:57:40,282 - INFO - best model
2023-04-27 08:57:40,397 - INFO - Epoch 501 training loss = 2.758
2023-04-27 08:57:40,500 - INFO - Epoch 502 training loss = 2.799
2023-04-27 08:57:40,604 - INFO - Epoch 503 training loss = 2.752
2023-04-27 08:57:40,706 - INFO - Epoch 504 training loss = 2.761
2023-04-27 08:57:40,807 - INFO - Epoch 505 training loss = 2.747
2023-04-27 08:57:40,910 - INFO - Epoch 506 training loss = 2.725
2023-04-27 08:57:41,015 - INFO - Epoch 507 training loss = 2.764
2023-04-27 08:57:41,121 - INFO - Epoch 508 training loss = 2.793
2023-04-27 08:57:41,225 - INFO - Epoch 509 training loss = 2.702
2023-04-27 08:57:41,327 - INFO - Epoch 510 training loss = 2.738
2023-04-27 08:57:41,355 - INFO - Validation loss = 9.067
2023-04-27 08:57:41,459 - INFO - Epoch 511 training loss = 2.678
2023-04-27 08:57:41,562 - INFO - Epoch 512 training loss = 2.719
2023-04-27 08:57:41,666 - INFO - Epoch 513 training loss = 2.684
2023-04-27 08:57:41,768 - INFO - Epoch 514 training loss = 2.728
2023-04-27 08:57:41,869 - INFO - Epoch 515 training loss = 2.669
2023-04-27 08:57:41,975 - INFO - Epoch 516 training loss = 2.73
2023-04-27 08:57:42,080 - INFO - Epoch 517 training loss = 2.683
2023-04-27 08:57:42,185 - INFO - Epoch 518 training loss = 2.656
2023-04-27 08:57:42,290 - INFO - Epoch 519 training loss = 2.71
2023-04-27 08:57:42,394 - INFO - Epoch 520 training loss = 2.642
2023-04-27 08:57:42,422 - INFO - Validation loss = 9.053
2023-04-27 08:57:42,527 - INFO - Epoch 521 training loss = 2.834
2023-04-27 08:57:42,631 - INFO - Epoch 522 training loss = 2.591
2023-04-27 08:57:42,735 - INFO - Epoch 523 training loss = 2.645
2023-04-27 08:57:42,838 - INFO - Epoch 524 training loss = 2.667
2023-04-27 08:57:42,942 - INFO - Epoch 525 training loss = 2.618
2023-04-27 08:57:43,046 - INFO - Epoch 526 training loss = 2.639
2023-04-27 08:57:43,152 - INFO - Epoch 527 training loss = 2.615
2023-04-27 08:57:43,257 - INFO - Epoch 528 training loss = 2.563
2023-04-27 08:57:43,361 - INFO - Epoch 529 training loss = 2.636
2023-04-27 08:57:43,464 - INFO - Epoch 530 training loss = 2.614
2023-04-27 08:57:43,492 - INFO - Validation loss = 9.087
2023-04-27 08:57:43,654 - INFO - Epoch 531 training loss = 2.566
2023-04-27 08:57:43,756 - INFO - Epoch 532 training loss = 2.605
2023-04-27 08:57:43,859 - INFO - Epoch 533 training loss = 2.627
2023-04-27 08:57:43,964 - INFO - Epoch 534 training loss = 2.567
2023-04-27 08:57:44,069 - INFO - Epoch 535 training loss = 2.596
2023-04-27 08:57:44,174 - INFO - Epoch 536 training loss = 2.531
2023-04-27 08:57:44,279 - INFO - Epoch 537 training loss = 2.685
2023-04-27 08:57:44,383 - INFO - Epoch 538 training loss = 2.555
2023-04-27 08:57:44,488 - INFO - Epoch 539 training loss = 2.511
2023-04-27 08:57:44,592 - INFO - Epoch 540 training loss = 2.636
2023-04-27 08:57:44,621 - INFO - Validation loss = 9.02
2023-04-27 08:57:44,621 - INFO - best model
2023-04-27 08:57:44,736 - INFO - Epoch 541 training loss = 2.545
2023-04-27 08:57:44,840 - INFO - Epoch 542 training loss = 2.535
2023-04-27 08:57:44,943 - INFO - Epoch 543 training loss = 2.518
2023-04-27 08:57:45,047 - INFO - Epoch 544 training loss = 2.497
2023-04-27 08:57:45,154 - INFO - Epoch 545 training loss = 2.521
2023-04-27 08:57:45,258 - INFO - Epoch 546 training loss = 2.586
2023-04-27 08:57:45,364 - INFO - Epoch 547 training loss = 2.534
2023-04-27 08:57:45,469 - INFO - Epoch 548 training loss = 2.482
2023-04-27 08:57:45,574 - INFO - Epoch 549 training loss = 2.496
2023-04-27 08:57:45,679 - INFO - Epoch 550 training loss = 2.486
2023-04-27 08:57:45,707 - INFO - Validation loss = 9.041
2023-04-27 08:57:45,812 - INFO - Epoch 551 training loss = 2.537
2023-04-27 08:57:45,915 - INFO - Epoch 552 training loss = 2.502
2023-04-27 08:57:46,020 - INFO - Epoch 553 training loss = 2.511
2023-04-27 08:57:46,126 - INFO - Epoch 554 training loss = 2.494
2023-04-27 08:57:46,230 - INFO - Epoch 555 training loss = 2.473
2023-04-27 08:57:46,335 - INFO - Epoch 556 training loss = 2.43
2023-04-27 08:57:46,438 - INFO - Epoch 557 training loss = 2.492
2023-04-27 08:57:46,541 - INFO - Epoch 558 training loss = 2.453
2023-04-27 08:57:46,646 - INFO - Epoch 559 training loss = 2.471
2023-04-27 08:57:46,750 - INFO - Epoch 560 training loss = 2.45
2023-04-27 08:57:46,778 - INFO - Validation loss = 9.088
2023-04-27 08:57:46,882 - INFO - Epoch 561 training loss = 2.446
2023-04-27 08:57:47,045 - INFO - Epoch 562 training loss = 2.468
2023-04-27 08:57:47,150 - INFO - Epoch 563 training loss = 2.441
2023-04-27 08:57:47,255 - INFO - Epoch 564 training loss = 2.421
2023-04-27 08:57:47,357 - INFO - Epoch 565 training loss = 2.382
2023-04-27 08:57:47,461 - INFO - Epoch 566 training loss = 2.445
2023-04-27 08:57:47,562 - INFO - Epoch 567 training loss = 2.438
2023-04-27 08:57:47,666 - INFO - Epoch 568 training loss = 2.434
2023-04-27 08:57:47,770 - INFO - Epoch 569 training loss = 2.385
2023-04-27 08:57:47,871 - INFO - Epoch 570 training loss = 2.424
2023-04-27 08:57:47,900 - INFO - Validation loss = 9.071
2023-04-27 08:57:48,004 - INFO - Epoch 571 training loss = 2.388
2023-04-27 08:57:48,108 - INFO - Epoch 572 training loss = 2.377
2023-04-27 08:57:48,212 - INFO - Epoch 573 training loss = 2.415
2023-04-27 08:57:48,315 - INFO - Epoch 574 training loss = 2.398
2023-04-27 08:57:48,417 - INFO - Epoch 575 training loss = 2.376
2023-04-27 08:57:48,520 - INFO - Epoch 576 training loss = 2.345
2023-04-27 08:57:48,621 - INFO - Epoch 577 training loss = 2.381
2023-04-27 08:57:48,725 - INFO - Epoch 578 training loss = 2.343
2023-04-27 08:57:48,830 - INFO - Epoch 579 training loss = 2.364
2023-04-27 08:57:48,936 - INFO - Epoch 580 training loss = 2.356
2023-04-27 08:57:48,964 - INFO - Validation loss = 9.004
2023-04-27 08:57:48,964 - INFO - best model
2023-04-27 08:57:49,080 - INFO - Epoch 581 training loss = 2.329
2023-04-27 08:57:49,185 - INFO - Epoch 582 training loss = 2.362
2023-04-27 08:57:49,290 - INFO - Epoch 583 training loss = 2.358
2023-04-27 08:57:49,392 - INFO - Epoch 584 training loss = 2.335
2023-04-27 08:57:49,498 - INFO - Epoch 585 training loss = 2.399
2023-04-27 08:57:49,600 - INFO - Epoch 586 training loss = 2.315
2023-04-27 08:57:49,705 - INFO - Epoch 587 training loss = 2.328
2023-04-27 08:57:49,809 - INFO - Epoch 588 training loss = 2.307
2023-04-27 08:57:49,910 - INFO - Epoch 589 training loss = 2.302
2023-04-27 08:57:50,014 - INFO - Epoch 590 training loss = 2.33
2023-04-27 08:57:50,043 - INFO - Validation loss = 8.97
2023-04-27 08:57:50,043 - INFO - best model
2023-04-27 08:57:50,158 - INFO - Epoch 591 training loss =  2.3
2023-04-27 08:57:50,261 - INFO - Epoch 592 training loss = 2.303
2023-04-27 08:57:50,363 - INFO - Epoch 593 training loss = 2.304
2023-04-27 08:57:50,524 - INFO - Epoch 594 training loss = 2.285
2023-04-27 08:57:50,626 - INFO - Epoch 595 training loss = 2.28
2023-04-27 08:57:50,728 - INFO - Epoch 596 training loss = 2.292
2023-04-27 08:57:50,829 - INFO - Epoch 597 training loss = 2.262
2023-04-27 08:57:50,932 - INFO - Epoch 598 training loss = 2.317
2023-04-27 08:57:51,037 - INFO - Epoch 599 training loss = 2.297
2023-04-27 08:57:51,141 - INFO - Epoch 600 training loss = 2.315
2023-04-27 08:57:51,169 - INFO - Validation loss = 9.109
2023-04-27 08:57:51,273 - INFO - Epoch 601 training loss = 2.237
2023-04-27 08:57:51,375 - INFO - Epoch 602 training loss = 2.303
2023-04-27 08:57:51,479 - INFO - Epoch 603 training loss = 2.264
2023-04-27 08:57:51,581 - INFO - Epoch 604 training loss = 2.236
2023-04-27 08:57:51,683 - INFO - Epoch 605 training loss = 2.27
2023-04-27 08:57:51,787 - INFO - Epoch 606 training loss = 2.274
2023-04-27 08:57:51,892 - INFO - Epoch 607 training loss = 2.22
2023-04-27 08:57:51,997 - INFO - Epoch 608 training loss = 2.233
2023-04-27 08:57:52,101 - INFO - Epoch 609 training loss = 2.202
2023-04-27 08:57:52,203 - INFO - Epoch 610 training loss = 2.264
2023-04-27 08:57:52,231 - INFO - Validation loss = 9.033
2023-04-27 08:57:52,333 - INFO - Epoch 611 training loss = 2.224
2023-04-27 08:57:52,439 - INFO - Epoch 612 training loss = 2.238
2023-04-27 08:57:52,543 - INFO - Epoch 613 training loss = 2.214
2023-04-27 08:57:52,642 - INFO - Epoch 614 training loss = 2.222
2023-04-27 08:57:52,741 - INFO - Epoch 615 training loss = 2.265
2023-04-27 08:57:52,842 - INFO - Epoch 616 training loss = 2.168
2023-04-27 08:57:52,946 - INFO - Epoch 617 training loss = 2.213
2023-04-27 08:57:53,049 - INFO - Epoch 618 training loss = 2.186
2023-04-27 08:57:53,153 - INFO - Epoch 619 training loss = 2.22
2023-04-27 08:57:53,257 - INFO - Epoch 620 training loss =  2.2
2023-04-27 08:57:53,285 - INFO - Validation loss = 9.059
2023-04-27 08:57:53,389 - INFO - Epoch 621 training loss = 2.174
2023-04-27 08:57:53,490 - INFO - Epoch 622 training loss = 2.177
2023-04-27 08:57:53,590 - INFO - Epoch 623 training loss = 2.187
2023-04-27 08:57:53,691 - INFO - Epoch 624 training loss = 2.176
2023-04-27 08:57:53,848 - INFO - Epoch 625 training loss = 2.152
2023-04-27 08:57:53,949 - INFO - Epoch 626 training loss = 2.239
2023-04-27 08:57:54,050 - INFO - Epoch 627 training loss = 2.124
2023-04-27 08:57:54,151 - INFO - Epoch 628 training loss = 2.186
2023-04-27 08:57:54,254 - INFO - Epoch 629 training loss = 2.165
2023-04-27 08:57:54,358 - INFO - Epoch 630 training loss = 2.142
2023-04-27 08:57:54,386 - INFO - Validation loss = 9.003
2023-04-27 08:57:54,489 - INFO - Epoch 631 training loss = 2.129
2023-04-27 08:57:54,590 - INFO - Epoch 632 training loss = 2.128
2023-04-27 08:57:54,691 - INFO - Epoch 633 training loss = 2.149
2023-04-27 08:57:54,794 - INFO - Epoch 634 training loss = 2.165
2023-04-27 08:57:54,895 - INFO - Epoch 635 training loss = 2.131
2023-04-27 08:57:54,999 - INFO - Epoch 636 training loss = 2.165
2023-04-27 08:57:55,105 - INFO - Epoch 637 training loss = 2.147
2023-04-27 08:57:55,208 - INFO - Epoch 638 training loss = 2.103
2023-04-27 08:57:55,309 - INFO - Epoch 639 training loss = 2.124
2023-04-27 08:57:55,412 - INFO - Epoch 640 training loss = 2.148
2023-04-27 08:57:55,440 - INFO - Validation loss = 9.034
2023-04-27 08:57:55,542 - INFO - Epoch 641 training loss = 2.102
2023-04-27 08:57:55,644 - INFO - Epoch 642 training loss = 2.12
2023-04-27 08:57:55,747 - INFO - Epoch 643 training loss = 2.107
2023-04-27 08:57:55,852 - INFO - Epoch 644 training loss = 2.109
2023-04-27 08:57:55,959 - INFO - Epoch 645 training loss = 2.094
2023-04-27 08:57:56,066 - INFO - Epoch 646 training loss = 2.098
2023-04-27 08:57:56,171 - INFO - Epoch 647 training loss = 2.083
2023-04-27 08:57:56,276 - INFO - Epoch 648 training loss = 2.095
2023-04-27 08:57:56,381 - INFO - Epoch 649 training loss = 2.082
2023-04-27 08:57:56,483 - INFO - Epoch 650 training loss = 2.101
2023-04-27 08:57:56,512 - INFO - Validation loss = 9.03
2023-04-27 08:57:56,615 - INFO - Epoch 651 training loss = 2.075
2023-04-27 08:57:56,718 - INFO - Epoch 652 training loss = 2.067
2023-04-27 08:57:56,822 - INFO - Epoch 653 training loss = 2.09
2023-04-27 08:57:56,925 - INFO - Epoch 654 training loss = 2.08
2023-04-27 08:57:57,029 - INFO - Epoch 655 training loss = 2.047
2023-04-27 08:57:57,193 - INFO - Epoch 656 training loss = 2.074
2023-04-27 08:57:57,296 - INFO - Epoch 657 training loss = 2.056
2023-04-27 08:57:57,397 - INFO - Epoch 658 training loss = 2.074
2023-04-27 08:57:57,501 - INFO - Epoch 659 training loss = 2.036
2023-04-27 08:57:57,602 - INFO - Epoch 660 training loss = 2.074
2023-04-27 08:57:57,631 - INFO - Validation loss = 9.072
2023-04-27 08:57:57,735 - INFO - Epoch 661 training loss = 2.034
2023-04-27 08:57:57,839 - INFO - Epoch 662 training loss = 2.038
2023-04-27 08:57:57,943 - INFO - Epoch 663 training loss = 2.054
2023-04-27 08:57:58,047 - INFO - Epoch 664 training loss = 2.068
2023-04-27 08:57:58,152 - INFO - Epoch 665 training loss = 2.048
2023-04-27 08:57:58,256 - INFO - Epoch 666 training loss = 2.033
2023-04-27 08:57:58,360 - INFO - Epoch 667 training loss = 2.03
2023-04-27 08:57:58,462 - INFO - Epoch 668 training loss = 2.031
2023-04-27 08:57:58,566 - INFO - Epoch 669 training loss = 2.036
2023-04-27 08:57:58,670 - INFO - Epoch 670 training loss = 2.005
2023-04-27 08:57:58,699 - INFO - Validation loss = 9.152
2023-04-27 08:57:58,808 - INFO - Epoch 671 training loss = 2.02
2023-04-27 08:57:58,913 - INFO - Epoch 672 training loss = 2.015
2023-04-27 08:57:59,021 - INFO - Epoch 673 training loss = 2.005
2023-04-27 08:57:59,128 - INFO - Epoch 674 training loss = 2.025
2023-04-27 08:57:59,235 - INFO - Epoch 675 training loss = 1.998
2023-04-27 08:57:59,343 - INFO - Epoch 676 training loss = 2.017
2023-04-27 08:57:59,451 - INFO - Epoch 677 training loss = 2.015
2023-04-27 08:57:59,557 - INFO - Epoch 678 training loss = 2.02
2023-04-27 08:57:59,661 - INFO - Epoch 679 training loss = 1.993
2023-04-27 08:57:59,764 - INFO - Epoch 680 training loss = 2.005
2023-04-27 08:57:59,792 - INFO - Validation loss = 9.065
2023-04-27 08:57:59,896 - INFO - Epoch 681 training loss = 1.99
2023-04-27 08:57:59,997 - INFO - Epoch 682 training loss = 2.014
2023-04-27 08:58:00,103 - INFO - Epoch 683 training loss = 1.963
2023-04-27 08:58:00,205 - INFO - Epoch 684 training loss = 2.011
2023-04-27 08:58:00,309 - INFO - Epoch 685 training loss = 1.994
2023-04-27 08:58:00,412 - INFO - Epoch 686 training loss = 1.969
2023-04-27 08:58:00,516 - INFO - Epoch 687 training loss = 1.975
2023-04-27 08:58:00,679 - INFO - Epoch 688 training loss = 1.956
2023-04-27 08:58:00,783 - INFO - Epoch 689 training loss = 2.004
2023-04-27 08:58:00,885 - INFO - Epoch 690 training loss = 1.964
2023-04-27 08:58:00,913 - INFO - Validation loss = 9.11
2023-04-27 08:58:01,018 - INFO - Epoch 691 training loss = 1.955
2023-04-27 08:58:01,121 - INFO - Epoch 692 training loss = 1.978
2023-04-27 08:58:01,223 - INFO - Epoch 693 training loss = 1.953
2023-04-27 08:58:01,326 - INFO - Epoch 694 training loss = 1.981
2023-04-27 08:58:01,428 - INFO - Epoch 695 training loss = 1.944
2023-04-27 08:58:01,530 - INFO - Epoch 696 training loss = 1.939
2023-04-27 08:58:01,631 - INFO - Epoch 697 training loss = 1.947
2023-04-27 08:58:01,731 - INFO - Epoch 698 training loss = 1.953
2023-04-27 08:58:01,832 - INFO - Epoch 699 training loss = 1.948
2023-04-27 08:58:01,934 - INFO - Epoch 700 training loss = 1.93
2023-04-27 08:58:01,962 - INFO - Validation loss = 9.11
2023-04-27 08:58:02,067 - INFO - Epoch 701 training loss = 1.943
2023-04-27 08:58:02,172 - INFO - Epoch 702 training loss = 1.924
2023-04-27 08:58:02,277 - INFO - Epoch 703 training loss = 1.931
2023-04-27 08:58:02,383 - INFO - Epoch 704 training loss = 1.924
2023-04-27 08:58:02,489 - INFO - Epoch 705 training loss = 1.924
2023-04-27 08:58:02,591 - INFO - Epoch 706 training loss = 1.924
2023-04-27 08:58:02,697 - INFO - Epoch 707 training loss = 1.931
2023-04-27 08:58:02,803 - INFO - Epoch 708 training loss = 1.934
2023-04-27 08:58:02,908 - INFO - Epoch 709 training loss = 1.907
2023-04-27 08:58:03,011 - INFO - Epoch 710 training loss = 1.925
2023-04-27 08:58:03,040 - INFO - Validation loss = 9.101
2023-04-27 08:58:03,145 - INFO - Epoch 711 training loss = 1.914
2023-04-27 08:58:03,251 - INFO - Epoch 712 training loss = 1.919
2023-04-27 08:58:03,355 - INFO - Epoch 713 training loss = 1.908
2023-04-27 08:58:03,460 - INFO - Epoch 714 training loss = 1.903
2023-04-27 08:58:03,563 - INFO - Epoch 715 training loss = 1.909
2023-04-27 08:58:03,668 - INFO - Epoch 716 training loss = 1.907
2023-04-27 08:58:03,774 - INFO - Epoch 717 training loss = 1.889
2023-04-27 08:58:03,879 - INFO - Epoch 718 training loss = 1.904
2023-04-27 08:58:04,044 - INFO - Epoch 719 training loss = 1.882
2023-04-27 08:58:04,147 - INFO - Epoch 720 training loss = 1.896
2023-04-27 08:58:04,176 - INFO - Validation loss = 9.146
2023-04-27 08:58:04,282 - INFO - Epoch 721 training loss = 1.894
2023-04-27 08:58:04,385 - INFO - Epoch 722 training loss = 1.889
2023-04-27 08:58:04,490 - INFO - Epoch 723 training loss = 1.879
2023-04-27 08:58:04,594 - INFO - Epoch 724 training loss = 1.887
2023-04-27 08:58:04,699 - INFO - Epoch 725 training loss = 1.878
2023-04-27 08:58:04,803 - INFO - Epoch 726 training loss = 1.889
2023-04-27 08:58:04,908 - INFO - Epoch 727 training loss = 1.873
2023-04-27 08:58:05,012 - INFO - Epoch 728 training loss = 1.87
2023-04-27 08:58:05,116 - INFO - Epoch 729 training loss = 1.875
2023-04-27 08:58:05,217 - INFO - Epoch 730 training loss = 1.878
2023-04-27 08:58:05,245 - INFO - Validation loss = 9.127
2023-04-27 08:58:05,349 - INFO - Epoch 731 training loss = 1.858
2023-04-27 08:58:05,453 - INFO - Epoch 732 training loss = 1.872
2023-04-27 08:58:05,558 - INFO - Epoch 733 training loss = 1.862
2023-04-27 08:58:05,662 - INFO - Epoch 734 training loss = 1.87
2023-04-27 08:58:05,767 - INFO - Epoch 735 training loss = 1.851
2023-04-27 08:58:05,872 - INFO - Epoch 736 training loss = 1.852
2023-04-27 08:58:05,977 - INFO - Epoch 737 training loss = 1.846
2023-04-27 08:58:06,082 - INFO - Epoch 738 training loss = 1.848
2023-04-27 08:58:06,186 - INFO - Epoch 739 training loss = 1.852
2023-04-27 08:58:06,291 - INFO - Epoch 740 training loss = 1.852
2023-04-27 08:58:06,319 - INFO - Validation loss = 9.131
2023-04-27 08:58:06,423 - INFO - Epoch 741 training loss = 1.848
2023-04-27 08:58:06,526 - INFO - Epoch 742 training loss = 1.841
2023-04-27 08:58:06,630 - INFO - Epoch 743 training loss = 1.849
2023-04-27 08:58:06,731 - INFO - Epoch 744 training loss = 1.838
2023-04-27 08:58:06,834 - INFO - Epoch 745 training loss = 1.846
2023-04-27 08:58:06,938 - INFO - Epoch 746 training loss = 1.845
2023-04-27 08:58:07,040 - INFO - Epoch 747 training loss = 1.834
2023-04-27 08:58:07,142 - INFO - Epoch 748 training loss = 1.828
2023-04-27 08:58:07,246 - INFO - Epoch 749 training loss = 1.827
2023-04-27 08:58:07,407 - INFO - Epoch 750 training loss = 1.832
2023-04-27 08:58:07,436 - INFO - Validation loss = 9.147
2023-04-27 08:58:07,538 - INFO - Epoch 751 training loss = 1.827
2023-04-27 08:58:07,643 - INFO - Epoch 752 training loss = 1.822
2023-04-27 08:58:07,746 - INFO - Epoch 753 training loss = 1.822
2023-04-27 08:58:07,850 - INFO - Epoch 754 training loss = 1.819
2023-04-27 08:58:07,955 - INFO - Epoch 755 training loss = 1.816
2023-04-27 08:58:08,062 - INFO - Epoch 756 training loss = 1.813
2023-04-27 08:58:08,167 - INFO - Epoch 757 training loss = 1.82
2023-04-27 08:58:08,272 - INFO - Epoch 758 training loss = 1.815
2023-04-27 08:58:08,374 - INFO - Epoch 759 training loss = 1.812
2023-04-27 08:58:08,478 - INFO - Epoch 760 training loss = 1.808
2023-04-27 08:58:08,507 - INFO - Validation loss = 9.189
2023-04-27 08:58:08,611 - INFO - Epoch 761 training loss = 1.815
2023-04-27 08:58:08,714 - INFO - Epoch 762 training loss = 1.804
2023-04-27 08:58:08,815 - INFO - Epoch 763 training loss = 1.807
2023-04-27 08:58:08,918 - INFO - Epoch 764 training loss = 1.805
2023-04-27 08:58:09,022 - INFO - Epoch 765 training loss = 1.802
2023-04-27 08:58:09,126 - INFO - Epoch 766 training loss = 1.797
2023-04-27 08:58:09,229 - INFO - Epoch 767 training loss =  1.8
2023-04-27 08:58:09,331 - INFO - Epoch 768 training loss = 1.795
2023-04-27 08:58:09,434 - INFO - Epoch 769 training loss = 1.794
2023-04-27 08:58:09,537 - INFO - Epoch 770 training loss = 1.791
2023-04-27 08:58:09,565 - INFO - Validation loss = 9.18
2023-04-27 08:58:09,667 - INFO - Epoch 771 training loss = 1.788
2023-04-27 08:58:09,771 - INFO - Epoch 772 training loss = 1.786
2023-04-27 08:58:09,875 - INFO - Epoch 773 training loss = 1.784
2023-04-27 08:58:09,979 - INFO - Epoch 774 training loss = 1.782
2023-04-27 08:58:10,084 - INFO - Epoch 775 training loss = 1.785
2023-04-27 08:58:10,185 - INFO - Epoch 776 training loss = 1.783
2023-04-27 08:58:10,286 - INFO - Epoch 777 training loss = 1.784
2023-04-27 08:58:10,390 - INFO - Epoch 778 training loss = 1.777
2023-04-27 08:58:10,491 - INFO - Epoch 779 training loss = 1.774
2023-04-27 08:58:10,592 - INFO - Epoch 780 training loss = 1.775
2023-04-27 08:58:10,621 - INFO - Validation loss = 9.182
2023-04-27 08:58:10,724 - INFO - Epoch 781 training loss = 1.772
2023-04-27 08:58:10,887 - INFO - Epoch 782 training loss = 1.768
2023-04-27 08:58:10,989 - INFO - Epoch 783 training loss = 1.775
2023-04-27 08:58:11,090 - INFO - Epoch 784 training loss = 1.762
2023-04-27 08:58:11,191 - INFO - Epoch 785 training loss = 1.765
2023-04-27 08:58:11,292 - INFO - Epoch 786 training loss = 1.767
2023-04-27 08:58:11,393 - INFO - Epoch 787 training loss = 1.762
2023-04-27 08:58:11,495 - INFO - Epoch 788 training loss = 1.759
2023-04-27 08:58:11,599 - INFO - Epoch 789 training loss = 1.758
2023-04-27 08:58:11,699 - INFO - Epoch 790 training loss = 1.761
2023-04-27 08:58:11,728 - INFO - Validation loss = 9.228
2023-04-27 08:58:11,829 - INFO - Epoch 791 training loss = 1.767
2023-04-27 08:58:11,934 - INFO - Epoch 792 training loss = 1.748
2023-04-27 08:58:12,038 - INFO - Epoch 793 training loss = 1.757
2023-04-27 08:58:12,140 - INFO - Epoch 794 training loss = 1.75
2023-04-27 08:58:12,242 - INFO - Epoch 795 training loss = 1.753
2023-04-27 08:58:12,343 - INFO - Epoch 796 training loss = 1.749
2023-04-27 08:58:12,444 - INFO - Epoch 797 training loss = 1.746
2023-04-27 08:58:12,545 - INFO - Epoch 798 training loss = 1.746
2023-04-27 08:58:12,646 - INFO - Epoch 799 training loss = 1.744
2023-04-27 08:58:12,747 - INFO - Epoch 800 training loss = 1.744
2023-04-27 08:58:12,775 - INFO - Validation loss = 9.213
2023-04-27 08:58:12,878 - INFO - Epoch 801 training loss = 1.745
2023-04-27 08:58:12,980 - INFO - Epoch 802 training loss = 1.741
2023-04-27 08:58:13,081 - INFO - Epoch 803 training loss = 1.741
2023-04-27 08:58:13,183 - INFO - Epoch 804 training loss = 1.74
2023-04-27 08:58:13,287 - INFO - Epoch 805 training loss = 1.735
2023-04-27 08:58:13,389 - INFO - Epoch 806 training loss = 1.735
2023-04-27 08:58:13,490 - INFO - Epoch 807 training loss = 1.733
2023-04-27 08:58:13,592 - INFO - Epoch 808 training loss = 1.735
2023-04-27 08:58:13,697 - INFO - Epoch 809 training loss = 1.731
2023-04-27 08:58:13,798 - INFO - Epoch 810 training loss = 1.729
2023-04-27 08:58:13,826 - INFO - Validation loss = 9.227
2023-04-27 08:58:13,927 - INFO - Epoch 811 training loss = 1.728
2023-04-27 08:58:14,028 - INFO - Epoch 812 training loss = 1.725
2023-04-27 08:58:14,194 - INFO - Epoch 813 training loss = 1.727
2023-04-27 08:58:14,297 - INFO - Epoch 814 training loss = 1.726
2023-04-27 08:58:14,402 - INFO - Epoch 815 training loss = 1.721
2023-04-27 08:58:14,511 - INFO - Epoch 816 training loss = 1.725
2023-04-27 08:58:14,619 - INFO - Epoch 817 training loss = 1.719
2023-04-27 08:58:14,726 - INFO - Epoch 818 training loss = 1.724
2023-04-27 08:58:14,830 - INFO - Epoch 819 training loss = 1.722
2023-04-27 08:58:14,936 - INFO - Epoch 820 training loss = 1.717
2023-04-27 08:58:14,965 - INFO - Validation loss = 9.237
2023-04-27 08:58:15,071 - INFO - Epoch 821 training loss = 1.715
2023-04-27 08:58:15,175 - INFO - Epoch 822 training loss = 1.716
2023-04-27 08:58:15,279 - INFO - Epoch 823 training loss = 1.715
2023-04-27 08:58:15,383 - INFO - Epoch 824 training loss = 1.711
2023-04-27 08:58:15,488 - INFO - Epoch 825 training loss = 1.711
2023-04-27 08:58:15,591 - INFO - Epoch 826 training loss = 1.708
2023-04-27 08:58:15,695 - INFO - Epoch 827 training loss = 1.712
2023-04-27 08:58:15,799 - INFO - Epoch 828 training loss = 1.709
2023-04-27 08:58:15,903 - INFO - Epoch 829 training loss = 1.704
2023-04-27 08:58:16,006 - INFO - Epoch 830 training loss = 1.706
2023-04-27 08:58:16,035 - INFO - Validation loss = 9.239
2023-04-27 08:58:16,140 - INFO - Epoch 831 training loss = 1.705
2023-04-27 08:58:16,246 - INFO - Epoch 832 training loss = 1.702
2023-04-27 08:58:16,350 - INFO - Epoch 833 training loss = 1.701
2023-04-27 08:58:16,454 - INFO - Epoch 834 training loss = 1.698
2023-04-27 08:58:16,559 - INFO - Epoch 835 training loss = 1.699
2023-04-27 08:58:16,664 - INFO - Epoch 836 training loss =  1.7
2023-04-27 08:58:16,769 - INFO - Epoch 837 training loss = 1.699
2023-04-27 08:58:16,872 - INFO - Epoch 838 training loss = 1.696
2023-04-27 08:58:16,979 - INFO - Epoch 839 training loss = 1.696
2023-04-27 08:58:17,084 - INFO - Epoch 840 training loss = 1.695
2023-04-27 08:58:17,112 - INFO - Validation loss = 9.253
2023-04-27 08:58:17,217 - INFO - Epoch 841 training loss = 1.693
2023-04-27 08:58:17,324 - INFO - Epoch 842 training loss = 1.69
2023-04-27 08:58:17,430 - INFO - Epoch 843 training loss = 1.691
2023-04-27 08:58:17,534 - INFO - Epoch 844 training loss = 1.69
2023-04-27 08:58:17,698 - INFO - Epoch 845 training loss = 1.689
2023-04-27 08:58:17,801 - INFO - Epoch 846 training loss = 1.689
2023-04-27 08:58:17,905 - INFO - Epoch 847 training loss = 1.688
2023-04-27 08:58:18,009 - INFO - Epoch 848 training loss = 1.684
2023-04-27 08:58:18,113 - INFO - Epoch 849 training loss = 1.685
2023-04-27 08:58:18,219 - INFO - Epoch 850 training loss = 1.681
2023-04-27 08:58:18,247 - INFO - Validation loss = 9.248
2023-04-27 08:58:18,352 - INFO - Epoch 851 training loss = 1.684
2023-04-27 08:58:18,459 - INFO - Epoch 852 training loss = 1.682
2023-04-27 08:58:18,564 - INFO - Epoch 853 training loss = 1.678
2023-04-27 08:58:18,668 - INFO - Epoch 854 training loss = 1.679
2023-04-27 08:58:18,773 - INFO - Epoch 855 training loss = 1.68
2023-04-27 08:58:18,876 - INFO - Epoch 856 training loss = 1.68
2023-04-27 08:58:18,982 - INFO - Epoch 857 training loss = 1.673
2023-04-27 08:58:19,087 - INFO - Epoch 858 training loss = 1.677
2023-04-27 08:58:19,191 - INFO - Epoch 859 training loss = 1.676
2023-04-27 08:58:19,296 - INFO - Epoch 860 training loss = 1.676
2023-04-27 08:58:19,325 - INFO - Validation loss = 9.256
2023-04-27 08:58:19,429 - INFO - Epoch 861 training loss = 1.674
2023-04-27 08:58:19,532 - INFO - Epoch 862 training loss = 1.673
2023-04-27 08:58:19,634 - INFO - Epoch 863 training loss = 1.674
2023-04-27 08:58:19,738 - INFO - Epoch 864 training loss = 1.672
2023-04-27 08:58:19,841 - INFO - Epoch 865 training loss = 1.672
2023-04-27 08:58:19,946 - INFO - Epoch 866 training loss = 1.67
2023-04-27 08:58:20,051 - INFO - Epoch 867 training loss = 1.669
2023-04-27 08:58:20,156 - INFO - Epoch 868 training loss = 1.669
2023-04-27 08:58:20,263 - INFO - Epoch 869 training loss = 1.667
2023-04-27 08:58:20,368 - INFO - Epoch 870 training loss = 1.665
2023-04-27 08:58:20,397 - INFO - Validation loss = 9.273
2023-04-27 08:58:20,504 - INFO - Epoch 871 training loss = 1.666
2023-04-27 08:58:20,608 - INFO - Epoch 872 training loss = 1.663
2023-04-27 08:58:20,712 - INFO - Epoch 873 training loss = 1.66
2023-04-27 08:58:20,815 - INFO - Epoch 874 training loss = 1.66
2023-04-27 08:58:20,917 - INFO - Epoch 875 training loss = 1.663
2023-04-27 08:58:21,082 - INFO - Epoch 876 training loss = 1.66
2023-04-27 08:58:21,185 - INFO - Epoch 877 training loss = 1.661
2023-04-27 08:58:21,289 - INFO - Epoch 878 training loss = 1.66
2023-04-27 08:58:21,391 - INFO - Epoch 879 training loss = 1.658
2023-04-27 08:58:21,495 - INFO - Epoch 880 training loss = 1.658
2023-04-27 08:58:21,523 - INFO - Validation loss = 9.289
2023-04-27 08:58:21,626 - INFO - Epoch 881 training loss = 1.657
2023-04-27 08:58:21,728 - INFO - Epoch 882 training loss = 1.657
2023-04-27 08:58:21,831 - INFO - Epoch 883 training loss = 1.655
2023-04-27 08:58:21,934 - INFO - Epoch 884 training loss = 1.654
2023-04-27 08:58:22,038 - INFO - Epoch 885 training loss = 1.652
2023-04-27 08:58:22,144 - INFO - Epoch 886 training loss = 1.654
2023-04-27 08:58:22,247 - INFO - Epoch 887 training loss = 1.652
2023-04-27 08:58:22,351 - INFO - Epoch 888 training loss = 1.651
2023-04-27 08:58:22,456 - INFO - Epoch 889 training loss = 1.649
2023-04-27 08:58:22,561 - INFO - Epoch 890 training loss = 1.651
2023-04-27 08:58:22,589 - INFO - Validation loss = 9.29
2023-04-27 08:58:22,693 - INFO - Epoch 891 training loss = 1.65
2023-04-27 08:58:22,792 - INFO - Epoch 892 training loss = 1.648
2023-04-27 08:58:22,892 - INFO - Epoch 893 training loss = 1.649
2023-04-27 08:58:22,994 - INFO - Epoch 894 training loss = 1.646
2023-04-27 08:58:23,100 - INFO - Epoch 895 training loss = 1.648
2023-04-27 08:58:23,205 - INFO - Epoch 896 training loss = 1.646
2023-04-27 08:58:23,308 - INFO - Epoch 897 training loss = 1.646
2023-04-27 08:58:23,410 - INFO - Epoch 898 training loss = 1.643
2023-04-27 08:58:23,514 - INFO - Epoch 899 training loss = 1.643
2023-04-27 08:58:23,617 - INFO - Epoch 900 training loss = 1.643
2023-04-27 08:58:23,645 - INFO - Validation loss = 9.294
2023-04-27 08:58:23,749 - INFO - Epoch 901 training loss = 1.643
2023-04-27 08:58:23,853 - INFO - Epoch 902 training loss = 1.641
2023-04-27 08:58:23,957 - INFO - Epoch 903 training loss = 1.64
2023-04-27 08:58:24,060 - INFO - Epoch 904 training loss = 1.641
2023-04-27 08:58:24,161 - INFO - Epoch 905 training loss = 1.642
2023-04-27 08:58:24,265 - INFO - Epoch 906 training loss = 1.64
2023-04-27 08:58:24,428 - INFO - Epoch 907 training loss = 1.641
2023-04-27 08:58:24,529 - INFO - Epoch 908 training loss = 1.639
2023-04-27 08:58:24,635 - INFO - Epoch 909 training loss = 1.639
2023-04-27 08:58:24,741 - INFO - Epoch 910 training loss = 1.64
2023-04-27 08:58:24,770 - INFO - Validation loss = 9.294
2023-04-27 08:58:24,875 - INFO - Epoch 911 training loss = 1.635
2023-04-27 08:58:24,980 - INFO - Epoch 912 training loss = 1.638
2023-04-27 08:58:25,085 - INFO - Epoch 913 training loss = 1.635
2023-04-27 08:58:25,187 - INFO - Epoch 914 training loss = 1.634
2023-04-27 08:58:25,290 - INFO - Epoch 915 training loss = 1.632
2023-04-27 08:58:25,395 - INFO - Epoch 916 training loss = 1.633
2023-04-27 08:58:25,497 - INFO - Epoch 917 training loss = 1.634
2023-04-27 08:58:25,602 - INFO - Epoch 918 training loss = 1.633
2023-04-27 08:58:25,706 - INFO - Epoch 919 training loss = 1.634
2023-04-27 08:58:25,808 - INFO - Epoch 920 training loss = 1.631
2023-04-27 08:58:25,836 - INFO - Validation loss = 9.299
2023-04-27 08:58:25,941 - INFO - Epoch 921 training loss = 1.632
2023-04-27 08:58:26,045 - INFO - Epoch 922 training loss = 1.63
2023-04-27 08:58:26,152 - INFO - Epoch 923 training loss = 1.632
2023-04-27 08:58:26,258 - INFO - Epoch 924 training loss = 1.63
2023-04-27 08:58:26,361 - INFO - Epoch 925 training loss = 1.629
2023-04-27 08:58:26,463 - INFO - Epoch 926 training loss = 1.63
2023-04-27 08:58:26,564 - INFO - Epoch 927 training loss = 1.629
2023-04-27 08:58:26,666 - INFO - Epoch 928 training loss = 1.629
2023-04-27 08:58:26,772 - INFO - Epoch 929 training loss = 1.626
2023-04-27 08:58:26,877 - INFO - Epoch 930 training loss = 1.628
2023-04-27 08:58:26,906 - INFO - Validation loss = 9.303
2023-04-27 08:58:27,011 - INFO - Epoch 931 training loss = 1.628
2023-04-27 08:58:27,116 - INFO - Epoch 932 training loss = 1.627
2023-04-27 08:58:27,220 - INFO - Epoch 933 training loss = 1.627
2023-04-27 08:58:27,326 - INFO - Epoch 934 training loss = 1.626
2023-04-27 08:58:27,431 - INFO - Epoch 935 training loss = 1.624
2023-04-27 08:58:27,535 - INFO - Epoch 936 training loss = 1.626
2023-04-27 08:58:27,637 - INFO - Epoch 937 training loss = 1.624
2023-04-27 08:58:27,740 - INFO - Epoch 938 training loss = 1.624
2023-04-27 08:58:27,904 - INFO - Epoch 939 training loss = 1.622
2023-04-27 08:58:28,009 - INFO - Epoch 940 training loss = 1.621
2023-04-27 08:58:28,038 - INFO - Validation loss = 9.305
2023-04-27 08:58:28,140 - INFO - Epoch 941 training loss = 1.624
2023-04-27 08:58:28,245 - INFO - Epoch 942 training loss = 1.622
2023-04-27 08:58:28,350 - INFO - Epoch 943 training loss = 1.622
2023-04-27 08:58:28,454 - INFO - Epoch 944 training loss = 1.622
2023-04-27 08:58:28,555 - INFO - Epoch 945 training loss = 1.622
2023-04-27 08:58:28,662 - INFO - Epoch 946 training loss = 1.623
2023-04-27 08:58:28,767 - INFO - Epoch 947 training loss = 1.621
2023-04-27 08:58:28,872 - INFO - Epoch 948 training loss = 1.622
2023-04-27 08:58:28,981 - INFO - Epoch 949 training loss = 1.622
2023-04-27 08:58:29,086 - INFO - Epoch 950 training loss = 1.622
2023-04-27 08:58:29,115 - INFO - Validation loss = 9.306
2023-04-27 08:58:29,219 - INFO - Epoch 951 training loss = 1.62
2023-04-27 08:58:29,322 - INFO - Epoch 952 training loss = 1.62
2023-04-27 08:58:29,427 - INFO - Epoch 953 training loss = 1.619
2023-04-27 08:58:29,530 - INFO - Epoch 954 training loss = 1.621
2023-04-27 08:58:29,636 - INFO - Epoch 955 training loss = 1.619
2023-04-27 08:58:29,741 - INFO - Epoch 956 training loss = 1.619
2023-04-27 08:58:29,845 - INFO - Epoch 957 training loss = 1.619
2023-04-27 08:58:29,949 - INFO - Epoch 958 training loss = 1.62
2023-04-27 08:58:30,054 - INFO - Epoch 959 training loss = 1.62
2023-04-27 08:58:30,159 - INFO - Epoch 960 training loss = 1.619
2023-04-27 08:58:30,187 - INFO - Validation loss = 9.303
2023-04-27 08:58:30,292 - INFO - Epoch 961 training loss = 1.619
2023-04-27 08:58:30,402 - INFO - Epoch 962 training loss = 1.618
2023-04-27 08:58:30,513 - INFO - Epoch 963 training loss = 1.617
2023-04-27 08:58:30,617 - INFO - Epoch 964 training loss = 1.617
2023-04-27 08:58:30,721 - INFO - Epoch 965 training loss = 1.618
2023-04-27 08:58:30,823 - INFO - Epoch 966 training loss = 1.617
2023-04-27 08:58:30,928 - INFO - Epoch 967 training loss = 1.616
2023-04-27 08:58:31,033 - INFO - Epoch 968 training loss = 1.617
2023-04-27 08:58:31,137 - INFO - Epoch 969 training loss = 1.617
2023-04-27 08:58:31,301 - INFO - Epoch 970 training loss = 1.616
2023-04-27 08:58:31,329 - INFO - Validation loss = 9.308
2023-04-27 08:58:31,432 - INFO - Epoch 971 training loss = 1.617
2023-04-27 08:58:31,536 - INFO - Epoch 972 training loss = 1.614
2023-04-27 08:58:31,639 - INFO - Epoch 973 training loss = 1.616
2023-04-27 08:58:31,741 - INFO - Epoch 974 training loss = 1.615
2023-04-27 08:58:31,844 - INFO - Epoch 975 training loss = 1.613
2023-04-27 08:58:31,949 - INFO - Epoch 976 training loss = 1.616
2023-04-27 08:58:32,054 - INFO - Epoch 977 training loss = 1.615
2023-04-27 08:58:32,157 - INFO - Epoch 978 training loss = 1.614
2023-04-27 08:58:32,264 - INFO - Epoch 979 training loss = 1.615
2023-04-27 08:58:32,368 - INFO - Epoch 980 training loss = 1.614
2023-04-27 08:58:32,396 - INFO - Validation loss = 9.309
2023-04-27 08:58:32,500 - INFO - Epoch 981 training loss = 1.614
2023-04-27 08:58:32,605 - INFO - Epoch 982 training loss = 1.613
2023-04-27 08:58:32,707 - INFO - Epoch 983 training loss = 1.612
2023-04-27 08:58:32,809 - INFO - Epoch 984 training loss = 1.614
2023-04-27 08:58:32,914 - INFO - Epoch 985 training loss = 1.611
2023-04-27 08:58:33,018 - INFO - Epoch 986 training loss = 1.614
2023-04-27 08:58:33,123 - INFO - Epoch 987 training loss = 1.615
2023-04-27 08:58:33,228 - INFO - Epoch 988 training loss = 1.613
2023-04-27 08:58:33,334 - INFO - Epoch 989 training loss = 1.613
2023-04-27 08:58:33,438 - INFO - Epoch 990 training loss = 1.613
2023-04-27 08:58:33,466 - INFO - Validation loss = 9.311
2023-04-27 08:58:33,569 - INFO - Epoch 991 training loss = 1.613
2023-04-27 08:58:33,672 - INFO - Epoch 992 training loss = 1.612
2023-04-27 08:58:33,774 - INFO - Epoch 993 training loss = 1.613
2023-04-27 08:58:33,878 - INFO - Epoch 994 training loss = 1.613
2023-04-27 08:58:33,984 - INFO - Epoch 995 training loss = 1.614
2023-04-27 08:58:34,088 - INFO - Epoch 996 training loss = 1.61
2023-04-27 08:58:34,193 - INFO - Epoch 997 training loss = 1.614
2023-04-27 08:58:34,300 - INFO - Epoch 998 training loss = 1.613
2023-04-27 08:58:34,402 - INFO - Epoch 999 training loss = 1.611
2023-04-27 08:58:34,418 - INFO - Validation loss = 9.31
