2023-04-26 20:24:33,512 - INFO - Config:
Namespace(config='configs/explicit_mlp.yaml', dir='/user/delden/acoustics/spp_ai_acoustics/acousticNN/experiments/arch/no_encoding/explicitmlp', epochs=100, device='cuda', seed=0, parameter_list='configs/data.yaml', encoding='none', dir_name='arch/no_encoding/explicitmlp')
2023-04-26 20:24:33,512 - INFO - Config:
Munch({'optimizer': Munch({'type': 'AdamW', 'kwargs': Munch({'lr': 0.001, 'weight_decay': 0.005, 'betas': [0.9, 0.95]})}), 'scheduler': Munch({'type': 'CosLR', 'kwargs': Munch({'epochs': 1000, 'initial_epochs': 101})}), 'model': Munch({'name': 'ExplicitMLP', 'input_encoding': True, 'encoding_dim': 16, 'embed_dim': 64, 'depth': 6, 'mlp_width': 256, 'num_frequencies': 200}), 'generation_frequency': 5001, 'validation_frequency': 10, 'batch_size': 128, 'epochs': 1000, 'gradient_clip': 10})
2023-04-26 20:24:47,788 - INFO - ==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ExplicitMLP                              [128, 200, 4]             --
├─GroupwiseProjection: 1-1               [128, 14, 64]             --
│    └─ModuleList: 2-1                   --                        --
│    │    └─Linear: 3-1                  [128, 4, 64]              128
│    │    └─Linear: 3-2                  [128, 5, 64]              128
│    │    └─Linear: 3-3                  [128, 5, 64]              128
├─MLP: 1-2                               [128, 256]                --
│    └─Linear: 2-2                       [128, 256]                229,632
│    └─ReLU: 2-3                         [128, 256]                --
│    └─Dropout: 2-4                      [128, 256]                --
│    └─Linear: 2-5                       [128, 256]                65,792
│    └─ReLU: 2-6                         [128, 256]                --
│    └─Dropout: 2-7                      [128, 256]                --
│    └─Linear: 2-8                       [128, 256]                65,792
│    └─ReLU: 2-9                         [128, 256]                --
│    └─Dropout: 2-10                     [128, 256]                --
│    └─Linear: 2-11                      [128, 256]                65,792
│    └─ReLU: 2-12                        [128, 256]                --
│    └─Dropout: 2-13                     [128, 256]                --
│    └─Linear: 2-14                      [128, 256]                65,792
│    └─ReLU: 2-15                        [128, 256]                --
│    └─Dropout: 2-16                     [128, 256]                --
│    └─Linear: 2-17                      [128, 256]                65,792
│    └─Dropout: 2-18                     [128, 256]                --
├─Linear: 1-3                            [128, 800]                205,600
==========================================================================================
Total params: 764,576
Trainable params: 764,576
Non-trainable params: 0
Total mult-adds (M): 97.87
==========================================================================================
Input size (MB): 0.11
Forward/backward pass size (MB): 3.31
Params size (MB): 3.06
Estimated Total Size (MB): 6.48
==========================================================================================
2023-04-26 20:24:47,888 - INFO - Epoch 0 training loss = 3.365e+03
2023-04-26 20:24:47,920 - INFO - Validation loss = 3.436e+03
2023-04-26 20:24:47,920 - INFO - best model
2023-04-26 20:24:48,033 - INFO - Epoch 1 training loss = 3.366e+03
2023-04-26 20:24:48,132 - INFO - Epoch 2 training loss = 3.346e+03
2023-04-26 20:24:48,230 - INFO - Epoch 3 training loss = 3.292e+03
2023-04-26 20:24:48,329 - INFO - Epoch 4 training loss = 3.107e+03
2023-04-26 20:24:48,427 - INFO - Epoch 5 training loss = 2.549e+03
2023-04-26 20:24:48,526 - INFO - Epoch 6 training loss = 1.419e+03
2023-04-26 20:24:48,624 - INFO - Epoch 7 training loss = 543.4
2023-04-26 20:24:48,722 - INFO - Epoch 8 training loss = 252.2
2023-04-26 20:24:48,821 - INFO - Epoch 9 training loss = 218.0
2023-04-26 20:24:48,919 - INFO - Epoch 10 training loss = 209.6
2023-04-26 20:24:48,946 - INFO - Validation loss = 215.1
2023-04-26 20:24:48,946 - INFO - best model
2023-04-26 20:24:49,056 - INFO - Epoch 11 training loss = 202.8
2023-04-26 20:24:49,155 - INFO - Epoch 12 training loss = 190.5
2023-04-26 20:24:49,254 - INFO - Epoch 13 training loss = 176.3
2023-04-26 20:24:49,352 - INFO - Epoch 14 training loss = 159.3
2023-04-26 20:24:49,451 - INFO - Epoch 15 training loss = 144.7
2023-04-26 20:24:49,550 - INFO - Epoch 16 training loss = 133.2
2023-04-26 20:24:49,648 - INFO - Epoch 17 training loss = 122.4
2023-04-26 20:24:49,746 - INFO - Epoch 18 training loss = 111.9
2023-04-26 20:24:49,845 - INFO - Epoch 19 training loss = 103.3
2023-04-26 20:24:49,943 - INFO - Epoch 20 training loss = 95.29
2023-04-26 20:24:49,970 - INFO - Validation loss = 99.11
2023-04-26 20:24:49,971 - INFO - best model
2023-04-26 20:24:50,080 - INFO - Epoch 21 training loss = 87.89
2023-04-26 20:24:50,180 - INFO - Epoch 22 training loss = 81.08
2023-04-26 20:24:50,279 - INFO - Epoch 23 training loss = 75.99
2023-04-26 20:24:50,377 - INFO - Epoch 24 training loss = 70.61
2023-04-26 20:24:50,476 - INFO - Epoch 25 training loss = 66.61
2023-04-26 20:24:50,574 - INFO - Epoch 26 training loss = 63.35
2023-04-26 20:24:50,672 - INFO - Epoch 27 training loss = 60.31
2023-04-26 20:24:50,771 - INFO - Epoch 28 training loss = 57.3
2023-04-26 20:24:50,927 - INFO - Epoch 29 training loss = 54.98
2023-04-26 20:24:51,026 - INFO - Epoch 30 training loss = 52.85
2023-04-26 20:24:51,053 - INFO - Validation loss = 55.82
2023-04-26 20:24:51,054 - INFO - best model
2023-04-26 20:24:51,163 - INFO - Epoch 31 training loss = 51.18
2023-04-26 20:24:51,262 - INFO - Epoch 32 training loss = 49.79
2023-04-26 20:24:51,360 - INFO - Epoch 33 training loss = 47.55
2023-04-26 20:24:51,458 - INFO - Epoch 34 training loss = 46.52
2023-04-26 20:24:51,556 - INFO - Epoch 35 training loss = 44.86
2023-04-26 20:24:51,654 - INFO - Epoch 36 training loss = 44.23
2023-04-26 20:24:51,752 - INFO - Epoch 37 training loss = 42.7
2023-04-26 20:24:51,851 - INFO - Epoch 38 training loss = 41.04
2023-04-26 20:24:51,949 - INFO - Epoch 39 training loss = 40.98
2023-04-26 20:24:52,047 - INFO - Epoch 40 training loss = 39.42
2023-04-26 20:24:52,074 - INFO - Validation loss = 42.18
2023-04-26 20:24:52,074 - INFO - best model
2023-04-26 20:24:52,183 - INFO - Epoch 41 training loss = 38.23
2023-04-26 20:24:52,281 - INFO - Epoch 42 training loss = 37.51
2023-04-26 20:24:52,379 - INFO - Epoch 43 training loss = 36.17
2023-04-26 20:24:52,479 - INFO - Epoch 44 training loss = 36.13
2023-04-26 20:24:52,577 - INFO - Epoch 45 training loss = 35.06
2023-04-26 20:24:52,674 - INFO - Epoch 46 training loss = 33.64
2023-04-26 20:24:52,772 - INFO - Epoch 47 training loss = 33.41
2023-04-26 20:24:52,870 - INFO - Epoch 48 training loss = 31.92
2023-04-26 20:24:52,969 - INFO - Epoch 49 training loss = 32.09
2023-04-26 20:24:53,067 - INFO - Epoch 50 training loss = 30.9
2023-04-26 20:24:53,094 - INFO - Validation loss = 33.28
2023-04-26 20:24:53,094 - INFO - best model
2023-04-26 20:24:53,203 - INFO - Epoch 51 training loss = 30.46
2023-04-26 20:24:53,302 - INFO - Epoch 52 training loss = 29.52
2023-04-26 20:24:53,400 - INFO - Epoch 53 training loss = 29.26
2023-04-26 20:24:53,498 - INFO - Epoch 54 training loss = 28.96
2023-04-26 20:24:53,596 - INFO - Epoch 55 training loss = 28.51
2023-04-26 20:24:53,694 - INFO - Epoch 56 training loss = 27.37
2023-04-26 20:24:53,792 - INFO - Epoch 57 training loss = 27.08
2023-04-26 20:24:53,891 - INFO - Epoch 58 training loss = 27.78
2023-04-26 20:24:53,989 - INFO - Epoch 59 training loss = 26.17
2023-04-26 20:24:54,087 - INFO - Epoch 60 training loss = 26.86
2023-04-26 20:24:54,114 - INFO - Validation loss = 29.62
2023-04-26 20:24:54,115 - INFO - best model
2023-04-26 20:24:54,282 - INFO - Epoch 61 training loss = 26.1
2023-04-26 20:24:54,380 - INFO - Epoch 62 training loss = 24.96
2023-04-26 20:24:54,478 - INFO - Epoch 63 training loss = 24.88
2023-04-26 20:24:54,576 - INFO - Epoch 64 training loss = 24.12
2023-04-26 20:24:54,674 - INFO - Epoch 65 training loss = 24.45
2023-04-26 20:24:54,772 - INFO - Epoch 66 training loss = 24.78
2023-04-26 20:24:54,870 - INFO - Epoch 67 training loss = 23.35
2023-04-26 20:24:54,968 - INFO - Epoch 68 training loss = 22.84
2023-04-26 20:24:55,067 - INFO - Epoch 69 training loss = 23.01
2023-04-26 20:24:55,165 - INFO - Epoch 70 training loss = 22.64
2023-04-26 20:24:55,192 - INFO - Validation loss = 24.66
2023-04-26 20:24:55,192 - INFO - best model
2023-04-26 20:24:55,302 - INFO - Epoch 71 training loss = 21.82
2023-04-26 20:24:55,400 - INFO - Epoch 72 training loss = 21.94
2023-04-26 20:24:55,498 - INFO - Epoch 73 training loss = 21.78
2023-04-26 20:24:55,596 - INFO - Epoch 74 training loss = 21.32
2023-04-26 20:24:55,695 - INFO - Epoch 75 training loss = 21.21
2023-04-26 20:24:55,793 - INFO - Epoch 76 training loss = 20.47
2023-04-26 20:24:55,891 - INFO - Epoch 77 training loss = 22.03
2023-04-26 20:24:55,989 - INFO - Epoch 78 training loss = 20.54
2023-04-26 20:24:56,087 - INFO - Epoch 79 training loss = 19.66
2023-04-26 20:24:56,185 - INFO - Epoch 80 training loss = 20.15
2023-04-26 20:24:56,212 - INFO - Validation loss = 25.53
2023-04-26 20:24:56,311 - INFO - Epoch 81 training loss = 20.31
2023-04-26 20:24:56,409 - INFO - Epoch 82 training loss = 19.32
2023-04-26 20:24:56,508 - INFO - Epoch 83 training loss = 19.33
2023-04-26 20:24:56,606 - INFO - Epoch 84 training loss = 18.94
2023-04-26 20:24:56,704 - INFO - Epoch 85 training loss = 18.62
2023-04-26 20:24:56,802 - INFO - Epoch 86 training loss = 18.97
2023-04-26 20:24:56,900 - INFO - Epoch 87 training loss = 19.02
2023-04-26 20:24:56,998 - INFO - Epoch 88 training loss = 19.3
2023-04-26 20:24:57,097 - INFO - Epoch 89 training loss = 17.74
2023-04-26 20:24:57,195 - INFO - Epoch 90 training loss = 17.4
2023-04-26 20:24:57,222 - INFO - Validation loss = 21.53
2023-04-26 20:24:57,222 - INFO - best model
2023-04-26 20:24:57,331 - INFO - Epoch 91 training loss = 18.76
2023-04-26 20:24:57,487 - INFO - Epoch 92 training loss = 16.87
2023-04-26 20:24:57,585 - INFO - Epoch 93 training loss = 18.26
2023-04-26 20:24:57,683 - INFO - Epoch 94 training loss = 17.15
2023-04-26 20:24:57,780 - INFO - Epoch 95 training loss = 17.62
2023-04-26 20:24:57,878 - INFO - Epoch 96 training loss = 18.24
2023-04-26 20:24:57,976 - INFO - Epoch 97 training loss = 16.55
2023-04-26 20:24:58,073 - INFO - Epoch 98 training loss = 18.55
2023-04-26 20:24:58,171 - INFO - Epoch 99 training loss = 16.54
2023-04-26 20:24:58,268 - INFO - Epoch 100 training loss = 16.42
2023-04-26 20:24:58,295 - INFO - Validation loss = 19.09
2023-04-26 20:24:58,296 - INFO - best model
2023-04-26 20:24:58,404 - INFO - Epoch 101 training loss = 17.47
2023-04-26 20:24:58,502 - INFO - Epoch 102 training loss = 15.73
2023-04-26 20:24:58,600 - INFO - Epoch 103 training loss = 16.73
2023-04-26 20:24:58,698 - INFO - Epoch 104 training loss = 15.42
2023-04-26 20:24:58,795 - INFO - Epoch 105 training loss = 16.34
2023-04-26 20:24:58,893 - INFO - Epoch 106 training loss = 15.19
2023-04-26 20:24:58,991 - INFO - Epoch 107 training loss = 15.77
2023-04-26 20:24:59,089 - INFO - Epoch 108 training loss = 14.85
2023-04-26 20:24:59,187 - INFO - Epoch 109 training loss = 15.22
2023-04-26 20:24:59,285 - INFO - Epoch 110 training loss = 17.16
2023-04-26 20:24:59,312 - INFO - Validation loss = 19.75
2023-04-26 20:24:59,409 - INFO - Epoch 111 training loss = 14.98
2023-04-26 20:24:59,507 - INFO - Epoch 112 training loss = 14.55
2023-04-26 20:24:59,605 - INFO - Epoch 113 training loss = 13.85
2023-04-26 20:24:59,703 - INFO - Epoch 114 training loss = 13.86
2023-04-26 20:24:59,801 - INFO - Epoch 115 training loss = 13.75
2023-04-26 20:24:59,898 - INFO - Epoch 116 training loss = 13.66
2023-04-26 20:24:59,996 - INFO - Epoch 117 training loss = 13.56
2023-04-26 20:25:00,095 - INFO - Epoch 118 training loss = 12.78
2023-04-26 20:25:00,193 - INFO - Epoch 119 training loss = 13.16
2023-04-26 20:25:00,290 - INFO - Epoch 120 training loss = 13.45
2023-04-26 20:25:00,317 - INFO - Validation loss = 16.49
2023-04-26 20:25:00,318 - INFO - best model
2023-04-26 20:25:00,426 - INFO - Epoch 121 training loss = 12.85
2023-04-26 20:25:00,524 - INFO - Epoch 122 training loss = 12.69
2023-04-26 20:25:00,622 - INFO - Epoch 123 training loss = 12.95
2023-04-26 20:25:00,778 - INFO - Epoch 124 training loss = 11.82
2023-04-26 20:25:00,875 - INFO - Epoch 125 training loss = 14.47
2023-04-26 20:25:00,974 - INFO - Epoch 126 training loss = 12.09
2023-04-26 20:25:01,072 - INFO - Epoch 127 training loss = 12.59
2023-04-26 20:25:01,170 - INFO - Epoch 128 training loss = 11.91
2023-04-26 20:25:01,267 - INFO - Epoch 129 training loss = 11.53
2023-04-26 20:25:01,366 - INFO - Epoch 130 training loss = 11.52
2023-04-26 20:25:01,393 - INFO - Validation loss = 15.4
2023-04-26 20:25:01,393 - INFO - best model
2023-04-26 20:25:01,501 - INFO - Epoch 131 training loss = 11.91
2023-04-26 20:25:01,599 - INFO - Epoch 132 training loss = 12.1
2023-04-26 20:25:01,696 - INFO - Epoch 133 training loss = 11.38
2023-04-26 20:25:01,793 - INFO - Epoch 134 training loss = 11.31
2023-04-26 20:25:01,891 - INFO - Epoch 135 training loss = 11.03
2023-04-26 20:25:01,989 - INFO - Epoch 136 training loss = 11.19
2023-04-26 20:25:02,086 - INFO - Epoch 137 training loss = 11.09
2023-04-26 20:25:02,184 - INFO - Epoch 138 training loss = 10.93
2023-04-26 20:25:02,282 - INFO - Epoch 139 training loss = 11.24
2023-04-26 20:25:02,381 - INFO - Epoch 140 training loss = 11.89
2023-04-26 20:25:02,407 - INFO - Validation loss = 14.41
2023-04-26 20:25:02,408 - INFO - best model
2023-04-26 20:25:02,516 - INFO - Epoch 141 training loss = 11.13
2023-04-26 20:25:02,613 - INFO - Epoch 142 training loss = 11.29
2023-04-26 20:25:02,712 - INFO - Epoch 143 training loss = 10.05
2023-04-26 20:25:02,809 - INFO - Epoch 144 training loss = 11.4
2023-04-26 20:25:02,907 - INFO - Epoch 145 training loss = 10.65
2023-04-26 20:25:03,005 - INFO - Epoch 146 training loss = 11.31
2023-04-26 20:25:03,103 - INFO - Epoch 147 training loss = 10.33
2023-04-26 20:25:03,200 - INFO - Epoch 148 training loss = 10.55
2023-04-26 20:25:03,298 - INFO - Epoch 149 training loss = 10.44
2023-04-26 20:25:03,396 - INFO - Epoch 150 training loss = 10.34
2023-04-26 20:25:03,423 - INFO - Validation loss = 16.76
2023-04-26 20:25:03,521 - INFO - Epoch 151 training loss = 10.79
2023-04-26 20:25:03,618 - INFO - Epoch 152 training loss = 10.19
2023-04-26 20:25:03,716 - INFO - Epoch 153 training loss = 10.5
2023-04-26 20:25:03,813 - INFO - Epoch 154 training loss = 9.528
2023-04-26 20:25:03,969 - INFO - Epoch 155 training loss = 9.64
2023-04-26 20:25:04,067 - INFO - Epoch 156 training loss = 9.907
2023-04-26 20:25:04,164 - INFO - Epoch 157 training loss = 9.741
2023-04-26 20:25:04,262 - INFO - Epoch 158 training loss = 9.863
2023-04-26 20:25:04,359 - INFO - Epoch 159 training loss = 9.549
2023-04-26 20:25:04,457 - INFO - Epoch 160 training loss = 9.012
2023-04-26 20:25:04,484 - INFO - Validation loss = 13.02
2023-04-26 20:25:04,484 - INFO - best model
2023-04-26 20:25:04,593 - INFO - Epoch 161 training loss = 9.106
2023-04-26 20:25:04,690 - INFO - Epoch 162 training loss = 9.726
2023-04-26 20:25:04,788 - INFO - Epoch 163 training loss = 8.777
2023-04-26 20:25:04,886 - INFO - Epoch 164 training loss = 8.862
2023-04-26 20:25:04,983 - INFO - Epoch 165 training loss = 8.724
2023-04-26 20:25:05,081 - INFO - Epoch 166 training loss = 9.187
2023-04-26 20:25:05,181 - INFO - Epoch 167 training loss = 8.743
2023-04-26 20:25:05,279 - INFO - Epoch 168 training loss = 9.107
2023-04-26 20:25:05,377 - INFO - Epoch 169 training loss = 9.001
2023-04-26 20:25:05,475 - INFO - Epoch 170 training loss = 8.748
2023-04-26 20:25:05,501 - INFO - Validation loss = 16.23
2023-04-26 20:25:05,599 - INFO - Epoch 171 training loss = 8.782
2023-04-26 20:25:05,697 - INFO - Epoch 172 training loss = 9.439
2023-04-26 20:25:05,794 - INFO - Epoch 173 training loss = 8.167
2023-04-26 20:25:05,892 - INFO - Epoch 174 training loss = 8.575
2023-04-26 20:25:05,990 - INFO - Epoch 175 training loss = 9.01
2023-04-26 20:25:06,087 - INFO - Epoch 176 training loss = 7.995
2023-04-26 20:25:06,185 - INFO - Epoch 177 training loss = 8.951
2023-04-26 20:25:06,282 - INFO - Epoch 178 training loss = 8.31
2023-04-26 20:25:06,380 - INFO - Epoch 179 training loss = 8.218
2023-04-26 20:25:06,478 - INFO - Epoch 180 training loss = 8.077
2023-04-26 20:25:06,505 - INFO - Validation loss = 14.6
2023-04-26 20:25:06,602 - INFO - Epoch 181 training loss = 7.988
2023-04-26 20:25:06,700 - INFO - Epoch 182 training loss = 8.548
2023-04-26 20:25:06,798 - INFO - Epoch 183 training loss = 8.169
2023-04-26 20:25:06,896 - INFO - Epoch 184 training loss = 8.056
2023-04-26 20:25:06,994 - INFO - Epoch 185 training loss = 8.206
2023-04-26 20:25:07,150 - INFO - Epoch 186 training loss = 8.46
2023-04-26 20:25:07,247 - INFO - Epoch 187 training loss = 7.691
2023-04-26 20:25:07,345 - INFO - Epoch 188 training loss = 8.147
2023-04-26 20:25:07,443 - INFO - Epoch 189 training loss = 8.192
2023-04-26 20:25:07,541 - INFO - Epoch 190 training loss = 7.818
2023-04-26 20:25:07,568 - INFO - Validation loss = 11.88
2023-04-26 20:25:07,568 - INFO - best model
2023-04-26 20:25:07,677 - INFO - Epoch 191 training loss = 7.584
2023-04-26 20:25:07,775 - INFO - Epoch 192 training loss = 8.157
2023-04-26 20:25:07,872 - INFO - Epoch 193 training loss = 7.478
2023-04-26 20:25:07,970 - INFO - Epoch 194 training loss = 7.524
2023-04-26 20:25:08,068 - INFO - Epoch 195 training loss = 7.956
2023-04-26 20:25:08,165 - INFO - Epoch 196 training loss = 7.308
2023-04-26 20:25:08,263 - INFO - Epoch 197 training loss = 7.453
2023-04-26 20:25:08,361 - INFO - Epoch 198 training loss = 7.381
2023-04-26 20:25:08,458 - INFO - Epoch 199 training loss = 7.476
2023-04-26 20:25:08,556 - INFO - Epoch 200 training loss = 7.563
2023-04-26 20:25:08,583 - INFO - Validation loss = 12.64
2023-04-26 20:25:08,681 - INFO - Epoch 201 training loss = 6.903
2023-04-26 20:25:08,778 - INFO - Epoch 202 training loss = 7.743
2023-04-26 20:25:08,876 - INFO - Epoch 203 training loss = 6.989
2023-04-26 20:25:08,973 - INFO - Epoch 204 training loss = 7.084
2023-04-26 20:25:09,072 - INFO - Epoch 205 training loss = 6.994
2023-04-26 20:25:09,169 - INFO - Epoch 206 training loss = 7.304
2023-04-26 20:25:09,267 - INFO - Epoch 207 training loss = 7.176
2023-04-26 20:25:09,365 - INFO - Epoch 208 training loss = 7.335
2023-04-26 20:25:09,463 - INFO - Epoch 209 training loss = 7.257
2023-04-26 20:25:09,561 - INFO - Epoch 210 training loss = 7.035
2023-04-26 20:25:09,588 - INFO - Validation loss = 12.03
2023-04-26 20:25:09,686 - INFO - Epoch 211 training loss = 6.91
2023-04-26 20:25:09,783 - INFO - Epoch 212 training loss = 6.816
2023-04-26 20:25:09,881 - INFO - Epoch 213 training loss = 6.726
2023-04-26 20:25:09,979 - INFO - Epoch 214 training loss = 6.904
2023-04-26 20:25:10,077 - INFO - Epoch 215 training loss = 6.929
2023-04-26 20:25:10,175 - INFO - Epoch 216 training loss = 6.681
2023-04-26 20:25:10,273 - INFO - Epoch 217 training loss = 7.116
2023-04-26 20:25:10,428 - INFO - Epoch 218 training loss = 6.676
2023-04-26 20:25:10,526 - INFO - Epoch 219 training loss = 6.899
2023-04-26 20:25:10,623 - INFO - Epoch 220 training loss = 6.633
2023-04-26 20:25:10,650 - INFO - Validation loss = 12.58
2023-04-26 20:25:10,748 - INFO - Epoch 221 training loss = 7.065
2023-04-26 20:25:10,845 - INFO - Epoch 222 training loss = 6.634
2023-04-26 20:25:10,943 - INFO - Epoch 223 training loss = 6.339
2023-04-26 20:25:11,041 - INFO - Epoch 224 training loss = 6.924
2023-04-26 20:25:11,140 - INFO - Epoch 225 training loss = 6.546
2023-04-26 20:25:11,238 - INFO - Epoch 226 training loss = 6.074
2023-04-26 20:25:11,337 - INFO - Epoch 227 training loss = 6.221
2023-04-26 20:25:11,436 - INFO - Epoch 228 training loss = 6.664
2023-04-26 20:25:11,534 - INFO - Epoch 229 training loss = 6.218
2023-04-26 20:25:11,633 - INFO - Epoch 230 training loss = 6.196
2023-04-26 20:25:11,660 - INFO - Validation loss = 11.07
2023-04-26 20:25:11,660 - INFO - best model
2023-04-26 20:25:11,770 - INFO - Epoch 231 training loss = 6.538
2023-04-26 20:25:11,868 - INFO - Epoch 232 training loss = 6.103
2023-04-26 20:25:11,966 - INFO - Epoch 233 training loss = 6.105
2023-04-26 20:25:12,065 - INFO - Epoch 234 training loss = 6.737
2023-04-26 20:25:12,163 - INFO - Epoch 235 training loss = 6.368
2023-04-26 20:25:12,262 - INFO - Epoch 236 training loss = 6.235
2023-04-26 20:25:12,360 - INFO - Epoch 237 training loss = 5.963
2023-04-26 20:25:12,459 - INFO - Epoch 238 training loss = 6.027
2023-04-26 20:25:12,558 - INFO - Epoch 239 training loss = 6.079
2023-04-26 20:25:12,656 - INFO - Epoch 240 training loss = 6.207
2023-04-26 20:25:12,683 - INFO - Validation loss = 11.5
2023-04-26 20:25:12,781 - INFO - Epoch 241 training loss = 5.837
2023-04-26 20:25:12,879 - INFO - Epoch 242 training loss = 6.05
2023-04-26 20:25:12,978 - INFO - Epoch 243 training loss = 5.764
2023-04-26 20:25:13,077 - INFO - Epoch 244 training loss = 5.984
2023-04-26 20:25:13,175 - INFO - Epoch 245 training loss = 6.049
2023-04-26 20:25:13,273 - INFO - Epoch 246 training loss = 5.824
2023-04-26 20:25:13,372 - INFO - Epoch 247 training loss = 5.845
2023-04-26 20:25:13,471 - INFO - Epoch 248 training loss = 5.961
2023-04-26 20:25:13,569 - INFO - Epoch 249 training loss = 5.796
2023-04-26 20:25:13,725 - INFO - Epoch 250 training loss = 5.701
2023-04-26 20:25:13,752 - INFO - Validation loss = 12.44
2023-04-26 20:25:13,851 - INFO - Epoch 251 training loss = 5.898
2023-04-26 20:25:13,949 - INFO - Epoch 252 training loss = 5.889
2023-04-26 20:25:14,048 - INFO - Epoch 253 training loss = 5.726
2023-04-26 20:25:14,146 - INFO - Epoch 254 training loss = 5.571
2023-04-26 20:25:14,245 - INFO - Epoch 255 training loss = 5.593
2023-04-26 20:25:14,343 - INFO - Epoch 256 training loss = 5.782
2023-04-26 20:25:14,442 - INFO - Epoch 257 training loss = 5.621
2023-04-26 20:25:14,541 - INFO - Epoch 258 training loss = 5.872
2023-04-26 20:25:14,639 - INFO - Epoch 259 training loss = 5.49
2023-04-26 20:25:14,737 - INFO - Epoch 260 training loss = 5.509
2023-04-26 20:25:14,764 - INFO - Validation loss = 11.33
2023-04-26 20:25:14,863 - INFO - Epoch 261 training loss = 5.542
2023-04-26 20:25:14,961 - INFO - Epoch 262 training loss = 5.835
2023-04-26 20:25:15,060 - INFO - Epoch 263 training loss =  5.3
2023-04-26 20:25:15,159 - INFO - Epoch 264 training loss = 5.375
2023-04-26 20:25:15,257 - INFO - Epoch 265 training loss = 5.436
2023-04-26 20:25:15,355 - INFO - Epoch 266 training loss = 5.524
2023-04-26 20:25:15,454 - INFO - Epoch 267 training loss = 5.299
2023-04-26 20:25:15,553 - INFO - Epoch 268 training loss = 5.657
2023-04-26 20:25:15,651 - INFO - Epoch 269 training loss = 5.345
2023-04-26 20:25:15,750 - INFO - Epoch 270 training loss = 5.425
2023-04-26 20:25:15,777 - INFO - Validation loss = 10.85
2023-04-26 20:25:15,777 - INFO - best model
2023-04-26 20:25:15,886 - INFO - Epoch 271 training loss = 5.401
2023-04-26 20:25:15,985 - INFO - Epoch 272 training loss = 5.11
2023-04-26 20:25:16,083 - INFO - Epoch 273 training loss = 5.31
2023-04-26 20:25:16,182 - INFO - Epoch 274 training loss = 5.396
2023-04-26 20:25:16,280 - INFO - Epoch 275 training loss = 5.277
2023-04-26 20:25:16,378 - INFO - Epoch 276 training loss = 5.215
2023-04-26 20:25:16,477 - INFO - Epoch 277 training loss = 5.188
2023-04-26 20:25:16,576 - INFO - Epoch 278 training loss = 5.126
2023-04-26 20:25:16,674 - INFO - Epoch 279 training loss = 5.145
2023-04-26 20:25:16,772 - INFO - Epoch 280 training loss = 5.073
2023-04-26 20:25:16,800 - INFO - Validation loss = 10.85
2023-04-26 20:25:16,956 - INFO - Epoch 281 training loss = 5.168
2023-04-26 20:25:17,055 - INFO - Epoch 282 training loss = 5.091
2023-04-26 20:25:17,153 - INFO - Epoch 283 training loss = 5.03
2023-04-26 20:25:17,252 - INFO - Epoch 284 training loss = 5.254
2023-04-26 20:25:17,350 - INFO - Epoch 285 training loss = 5.164
2023-04-26 20:25:17,449 - INFO - Epoch 286 training loss = 4.863
2023-04-26 20:25:17,548 - INFO - Epoch 287 training loss = 5.027
2023-04-26 20:25:17,646 - INFO - Epoch 288 training loss = 5.202
2023-04-26 20:25:17,745 - INFO - Epoch 289 training loss = 4.945
2023-04-26 20:25:17,843 - INFO - Epoch 290 training loss = 4.98
2023-04-26 20:25:17,870 - INFO - Validation loss = 11.16
2023-04-26 20:25:17,968 - INFO - Epoch 291 training loss = 4.867
2023-04-26 20:25:18,067 - INFO - Epoch 292 training loss = 5.211
2023-04-26 20:25:18,166 - INFO - Epoch 293 training loss = 4.922
2023-04-26 20:25:18,264 - INFO - Epoch 294 training loss = 4.81
2023-04-26 20:25:18,362 - INFO - Epoch 295 training loss = 5.314
2023-04-26 20:25:18,460 - INFO - Epoch 296 training loss = 4.627
2023-04-26 20:25:18,559 - INFO - Epoch 297 training loss = 4.773
2023-04-26 20:25:18,658 - INFO - Epoch 298 training loss = 4.839
2023-04-26 20:25:18,756 - INFO - Epoch 299 training loss = 5.147
2023-04-26 20:25:18,854 - INFO - Epoch 300 training loss = 4.697
2023-04-26 20:25:18,881 - INFO - Validation loss = 11.26
2023-04-26 20:25:18,980 - INFO - Epoch 301 training loss = 4.917
2023-04-26 20:25:19,079 - INFO - Epoch 302 training loss = 4.626
2023-04-26 20:25:19,178 - INFO - Epoch 303 training loss = 5.12
2023-04-26 20:25:19,276 - INFO - Epoch 304 training loss = 4.566
2023-04-26 20:25:19,374 - INFO - Epoch 305 training loss = 4.857
2023-04-26 20:25:19,473 - INFO - Epoch 306 training loss = 4.853
2023-04-26 20:25:19,571 - INFO - Epoch 307 training loss = 4.809
2023-04-26 20:25:19,670 - INFO - Epoch 308 training loss = 4.48
2023-04-26 20:25:19,768 - INFO - Epoch 309 training loss = 4.653
2023-04-26 20:25:19,866 - INFO - Epoch 310 training loss = 4.514
2023-04-26 20:25:19,893 - INFO - Validation loss = 10.37
2023-04-26 20:25:19,894 - INFO - best model
2023-04-26 20:25:20,003 - INFO - Epoch 311 training loss = 4.412
2023-04-26 20:25:20,160 - INFO - Epoch 312 training loss = 4.509
2023-04-26 20:25:20,258 - INFO - Epoch 313 training loss = 4.565
2023-04-26 20:25:20,356 - INFO - Epoch 314 training loss = 4.686
2023-04-26 20:25:20,454 - INFO - Epoch 315 training loss = 4.641
2023-04-26 20:25:20,552 - INFO - Epoch 316 training loss = 4.877
2023-04-26 20:25:20,649 - INFO - Epoch 317 training loss = 4.57
2023-04-26 20:25:20,747 - INFO - Epoch 318 training loss = 4.445
2023-04-26 20:25:20,845 - INFO - Epoch 319 training loss = 4.359
2023-04-26 20:25:20,942 - INFO - Epoch 320 training loss = 4.613
2023-04-26 20:25:20,969 - INFO - Validation loss = 10.55
2023-04-26 20:25:21,067 - INFO - Epoch 321 training loss = 4.432
2023-04-26 20:25:21,165 - INFO - Epoch 322 training loss = 4.477
2023-04-26 20:25:21,263 - INFO - Epoch 323 training loss = 4.618
2023-04-26 20:25:21,361 - INFO - Epoch 324 training loss = 4.281
2023-04-26 20:25:21,459 - INFO - Epoch 325 training loss = 4.353
2023-04-26 20:25:21,558 - INFO - Epoch 326 training loss = 4.26
2023-04-26 20:25:21,657 - INFO - Epoch 327 training loss = 4.757
2023-04-26 20:25:21,755 - INFO - Epoch 328 training loss = 4.293
2023-04-26 20:25:21,854 - INFO - Epoch 329 training loss = 4.368
2023-04-26 20:25:21,954 - INFO - Epoch 330 training loss = 4.227
2023-04-26 20:25:21,981 - INFO - Validation loss = 10.37
2023-04-26 20:25:22,081 - INFO - Epoch 331 training loss = 4.278
2023-04-26 20:25:22,180 - INFO - Epoch 332 training loss = 4.317
2023-04-26 20:25:22,279 - INFO - Epoch 333 training loss = 4.48
2023-04-26 20:25:22,377 - INFO - Epoch 334 training loss = 4.194
2023-04-26 20:25:22,476 - INFO - Epoch 335 training loss = 4.432
2023-04-26 20:25:22,575 - INFO - Epoch 336 training loss = 4.143
2023-04-26 20:25:22,674 - INFO - Epoch 337 training loss = 4.313
2023-04-26 20:25:22,773 - INFO - Epoch 338 training loss = 4.295
2023-04-26 20:25:22,872 - INFO - Epoch 339 training loss = 4.152
2023-04-26 20:25:22,971 - INFO - Epoch 340 training loss = 4.154
2023-04-26 20:25:22,998 - INFO - Validation loss = 10.81
2023-04-26 20:25:23,098 - INFO - Epoch 341 training loss = 4.231
2023-04-26 20:25:23,196 - INFO - Epoch 342 training loss = 4.039
2023-04-26 20:25:23,294 - INFO - Epoch 343 training loss = 4.065
2023-04-26 20:25:23,450 - INFO - Epoch 344 training loss = 4.168
2023-04-26 20:25:23,548 - INFO - Epoch 345 training loss = 4.233
2023-04-26 20:25:23,647 - INFO - Epoch 346 training loss = 4.199
2023-04-26 20:25:23,745 - INFO - Epoch 347 training loss = 4.124
2023-04-26 20:25:23,843 - INFO - Epoch 348 training loss = 4.246
2023-04-26 20:25:23,942 - INFO - Epoch 349 training loss = 4.017
2023-04-26 20:25:24,041 - INFO - Epoch 350 training loss = 3.991
2023-04-26 20:25:24,068 - INFO - Validation loss = 10.26
2023-04-26 20:25:24,068 - INFO - best model
2023-04-26 20:25:24,177 - INFO - Epoch 351 training loss = 4.066
2023-04-26 20:25:24,276 - INFO - Epoch 352 training loss = 4.009
2023-04-26 20:25:24,374 - INFO - Epoch 353 training loss = 4.004
2023-04-26 20:25:24,472 - INFO - Epoch 354 training loss = 3.957
2023-04-26 20:25:24,571 - INFO - Epoch 355 training loss = 3.988
2023-04-26 20:25:24,669 - INFO - Epoch 356 training loss = 3.986
2023-04-26 20:25:24,767 - INFO - Epoch 357 training loss = 4.255
2023-04-26 20:25:24,865 - INFO - Epoch 358 training loss = 4.132
2023-04-26 20:25:24,963 - INFO - Epoch 359 training loss = 3.857
2023-04-26 20:25:25,062 - INFO - Epoch 360 training loss = 3.937
2023-04-26 20:25:25,089 - INFO - Validation loss = 10.28
2023-04-26 20:25:25,188 - INFO - Epoch 361 training loss = 3.912
2023-04-26 20:25:25,286 - INFO - Epoch 362 training loss = 3.909
2023-04-26 20:25:25,385 - INFO - Epoch 363 training loss = 4.096
2023-04-26 20:25:25,484 - INFO - Epoch 364 training loss = 3.979
2023-04-26 20:25:25,583 - INFO - Epoch 365 training loss = 3.837
2023-04-26 20:25:25,682 - INFO - Epoch 366 training loss = 3.763
2023-04-26 20:25:25,781 - INFO - Epoch 367 training loss = 3.828
2023-04-26 20:25:25,880 - INFO - Epoch 368 training loss = 3.87
2023-04-26 20:25:25,978 - INFO - Epoch 369 training loss = 3.735
2023-04-26 20:25:26,077 - INFO - Epoch 370 training loss = 3.843
2023-04-26 20:25:26,104 - INFO - Validation loss = 10.17
2023-04-26 20:25:26,104 - INFO - best model
2023-04-26 20:25:26,213 - INFO - Epoch 371 training loss = 3.867
2023-04-26 20:25:26,312 - INFO - Epoch 372 training loss = 3.807
2023-04-26 20:25:26,410 - INFO - Epoch 373 training loss = 3.841
2023-04-26 20:25:26,508 - INFO - Epoch 374 training loss = 3.835
2023-04-26 20:25:26,664 - INFO - Epoch 375 training loss = 3.613
2023-04-26 20:25:26,762 - INFO - Epoch 376 training loss = 3.764
2023-04-26 20:25:26,860 - INFO - Epoch 377 training loss = 3.715
2023-04-26 20:25:26,958 - INFO - Epoch 378 training loss = 3.737
2023-04-26 20:25:27,057 - INFO - Epoch 379 training loss = 3.653
2023-04-26 20:25:27,155 - INFO - Epoch 380 training loss = 3.668
2023-04-26 20:25:27,183 - INFO - Validation loss = 9.775
2023-04-26 20:25:27,183 - INFO - best model
2023-04-26 20:25:27,292 - INFO - Epoch 381 training loss = 3.728
2023-04-26 20:25:27,391 - INFO - Epoch 382 training loss = 3.696
2023-04-26 20:25:27,489 - INFO - Epoch 383 training loss = 3.747
2023-04-26 20:25:27,587 - INFO - Epoch 384 training loss = 3.655
2023-04-26 20:25:27,685 - INFO - Epoch 385 training loss = 3.692
2023-04-26 20:25:27,784 - INFO - Epoch 386 training loss = 3.523
2023-04-26 20:25:27,882 - INFO - Epoch 387 training loss = 3.615
2023-04-26 20:25:27,980 - INFO - Epoch 388 training loss = 3.787
2023-04-26 20:25:28,079 - INFO - Epoch 389 training loss = 3.569
2023-04-26 20:25:28,177 - INFO - Epoch 390 training loss = 3.615
2023-04-26 20:25:28,204 - INFO - Validation loss = 9.852
2023-04-26 20:25:28,302 - INFO - Epoch 391 training loss = 3.65
2023-04-26 20:25:28,400 - INFO - Epoch 392 training loss = 3.524
2023-04-26 20:25:28,499 - INFO - Epoch 393 training loss = 3.551
2023-04-26 20:25:28,597 - INFO - Epoch 394 training loss = 3.467
2023-04-26 20:25:28,696 - INFO - Epoch 395 training loss = 3.651
2023-04-26 20:25:28,794 - INFO - Epoch 396 training loss = 3.468
2023-04-26 20:25:28,892 - INFO - Epoch 397 training loss = 3.562
2023-04-26 20:25:28,989 - INFO - Epoch 398 training loss = 3.504
2023-04-26 20:25:29,088 - INFO - Epoch 399 training loss = 3.524
2023-04-26 20:25:29,185 - INFO - Epoch 400 training loss = 3.469
2023-04-26 20:25:29,212 - INFO - Validation loss = 9.764
2023-04-26 20:25:29,212 - INFO - best model
2023-04-26 20:25:29,320 - INFO - Epoch 401 training loss = 3.383
2023-04-26 20:25:29,418 - INFO - Epoch 402 training loss = 3.52
2023-04-26 20:25:29,516 - INFO - Epoch 403 training loss = 3.525
2023-04-26 20:25:29,614 - INFO - Epoch 404 training loss = 3.473
2023-04-26 20:25:29,712 - INFO - Epoch 405 training loss = 3.547
2023-04-26 20:25:29,868 - INFO - Epoch 406 training loss = 3.357
2023-04-26 20:25:29,966 - INFO - Epoch 407 training loss = 3.406
2023-04-26 20:25:30,065 - INFO - Epoch 408 training loss = 3.485
2023-04-26 20:25:30,163 - INFO - Epoch 409 training loss = 3.483
2023-04-26 20:25:30,261 - INFO - Epoch 410 training loss = 3.416
2023-04-26 20:25:30,288 - INFO - Validation loss = 9.981
2023-04-26 20:25:30,387 - INFO - Epoch 411 training loss = 3.51
2023-04-26 20:25:30,485 - INFO - Epoch 412 training loss = 3.286
2023-04-26 20:25:30,583 - INFO - Epoch 413 training loss = 3.223
2023-04-26 20:25:30,681 - INFO - Epoch 414 training loss = 3.64
2023-04-26 20:25:30,780 - INFO - Epoch 415 training loss = 3.266
2023-04-26 20:25:30,878 - INFO - Epoch 416 training loss = 3.431
2023-04-26 20:25:30,976 - INFO - Epoch 417 training loss = 3.322
2023-04-26 20:25:31,075 - INFO - Epoch 418 training loss = 3.297
2023-04-26 20:25:31,173 - INFO - Epoch 419 training loss = 3.222
2023-04-26 20:25:31,270 - INFO - Epoch 420 training loss = 3.314
2023-04-26 20:25:31,297 - INFO - Validation loss = 9.924
2023-04-26 20:25:31,395 - INFO - Epoch 421 training loss = 3.323
2023-04-26 20:25:31,493 - INFO - Epoch 422 training loss = 3.401
2023-04-26 20:25:31,590 - INFO - Epoch 423 training loss = 3.184
2023-04-26 20:25:31,688 - INFO - Epoch 424 training loss = 3.425
2023-04-26 20:25:31,786 - INFO - Epoch 425 training loss = 3.281
2023-04-26 20:25:31,883 - INFO - Epoch 426 training loss = 3.374
2023-04-26 20:25:31,981 - INFO - Epoch 427 training loss = 3.349
2023-04-26 20:25:32,079 - INFO - Epoch 428 training loss = 3.108
2023-04-26 20:25:32,176 - INFO - Epoch 429 training loss =  3.2
2023-04-26 20:25:32,274 - INFO - Epoch 430 training loss = 3.287
2023-04-26 20:25:32,301 - INFO - Validation loss = 10.07
2023-04-26 20:25:32,398 - INFO - Epoch 431 training loss = 3.349
2023-04-26 20:25:32,496 - INFO - Epoch 432 training loss = 3.121
2023-04-26 20:25:32,594 - INFO - Epoch 433 training loss = 3.161
2023-04-26 20:25:32,691 - INFO - Epoch 434 training loss = 3.291
2023-04-26 20:25:32,789 - INFO - Epoch 435 training loss = 3.14
2023-04-26 20:25:32,886 - INFO - Epoch 436 training loss =  3.2
2023-04-26 20:25:32,984 - INFO - Epoch 437 training loss = 3.084
2023-04-26 20:25:33,140 - INFO - Epoch 438 training loss = 3.279
2023-04-26 20:25:33,237 - INFO - Epoch 439 training loss = 3.098
2023-04-26 20:25:33,335 - INFO - Epoch 440 training loss = 3.021
2023-04-26 20:25:33,362 - INFO - Validation loss = 9.742
2023-04-26 20:25:33,362 - INFO - best model
2023-04-26 20:25:33,471 - INFO - Epoch 441 training loss = 3.183
2023-04-26 20:25:33,569 - INFO - Epoch 442 training loss = 3.136
2023-04-26 20:25:33,667 - INFO - Epoch 443 training loss = 3.083
2023-04-26 20:25:33,765 - INFO - Epoch 444 training loss = 3.069
2023-04-26 20:25:33,864 - INFO - Epoch 445 training loss = 3.079
2023-04-26 20:25:33,962 - INFO - Epoch 446 training loss = 3.069
2023-04-26 20:25:34,061 - INFO - Epoch 447 training loss = 3.094
2023-04-26 20:25:34,160 - INFO - Epoch 448 training loss = 3.108
2023-04-26 20:25:34,258 - INFO - Epoch 449 training loss = 2.975
2023-04-26 20:25:34,357 - INFO - Epoch 450 training loss = 3.069
2023-04-26 20:25:34,384 - INFO - Validation loss = 9.802
2023-04-26 20:25:34,482 - INFO - Epoch 451 training loss = 2.999
2023-04-26 20:25:34,581 - INFO - Epoch 452 training loss = 3.084
2023-04-26 20:25:34,679 - INFO - Epoch 453 training loss = 3.133
2023-04-26 20:25:34,778 - INFO - Epoch 454 training loss = 2.984
2023-04-26 20:25:34,876 - INFO - Epoch 455 training loss = 3.009
2023-04-26 20:25:34,974 - INFO - Epoch 456 training loss = 3.069
2023-04-26 20:25:35,073 - INFO - Epoch 457 training loss = 2.959
2023-04-26 20:25:35,174 - INFO - Epoch 458 training loss = 3.018
2023-04-26 20:25:35,272 - INFO - Epoch 459 training loss = 3.035
2023-04-26 20:25:35,371 - INFO - Epoch 460 training loss = 2.934
2023-04-26 20:25:35,398 - INFO - Validation loss = 10.3
2023-04-26 20:25:35,497 - INFO - Epoch 461 training loss = 2.918
2023-04-26 20:25:35,596 - INFO - Epoch 462 training loss = 2.93
2023-04-26 20:25:35,695 - INFO - Epoch 463 training loss = 2.991
2023-04-26 20:25:35,794 - INFO - Epoch 464 training loss = 2.923
2023-04-26 20:25:35,893 - INFO - Epoch 465 training loss = 2.896
2023-04-26 20:25:35,992 - INFO - Epoch 466 training loss = 2.993
2023-04-26 20:25:36,091 - INFO - Epoch 467 training loss = 2.991
2023-04-26 20:25:36,190 - INFO - Epoch 468 training loss = 2.842
2023-04-26 20:25:36,347 - INFO - Epoch 469 training loss = 2.871
2023-04-26 20:25:36,446 - INFO - Epoch 470 training loss = 2.901
2023-04-26 20:25:36,473 - INFO - Validation loss = 9.857
2023-04-26 20:25:36,572 - INFO - Epoch 471 training loss = 2.95
2023-04-26 20:25:36,671 - INFO - Epoch 472 training loss = 2.878
2023-04-26 20:25:36,770 - INFO - Epoch 473 training loss = 2.966
2023-04-26 20:25:36,869 - INFO - Epoch 474 training loss = 2.985
2023-04-26 20:25:36,968 - INFO - Epoch 475 training loss = 2.839
2023-04-26 20:25:37,068 - INFO - Epoch 476 training loss = 2.841
2023-04-26 20:25:37,167 - INFO - Epoch 477 training loss = 2.824
2023-04-26 20:25:37,266 - INFO - Epoch 478 training loss = 2.846
2023-04-26 20:25:37,365 - INFO - Epoch 479 training loss = 2.793
2023-04-26 20:25:37,465 - INFO - Epoch 480 training loss = 2.834
2023-04-26 20:25:37,492 - INFO - Validation loss = 9.496
2023-04-26 20:25:37,492 - INFO - best model
2023-04-26 20:25:37,602 - INFO - Epoch 481 training loss = 2.862
2023-04-26 20:25:37,701 - INFO - Epoch 482 training loss = 2.777
2023-04-26 20:25:37,800 - INFO - Epoch 483 training loss = 2.799
2023-04-26 20:25:37,899 - INFO - Epoch 484 training loss = 2.839
2023-04-26 20:25:37,998 - INFO - Epoch 485 training loss = 2.758
2023-04-26 20:25:38,098 - INFO - Epoch 486 training loss = 2.78
2023-04-26 20:25:38,196 - INFO - Epoch 487 training loss = 2.794
2023-04-26 20:25:38,296 - INFO - Epoch 488 training loss = 2.743
2023-04-26 20:25:38,395 - INFO - Epoch 489 training loss = 2.754
2023-04-26 20:25:38,494 - INFO - Epoch 490 training loss = 2.721
2023-04-26 20:25:38,521 - INFO - Validation loss = 9.541
2023-04-26 20:25:38,620 - INFO - Epoch 491 training loss = 2.718
2023-04-26 20:25:38,719 - INFO - Epoch 492 training loss = 2.833
2023-04-26 20:25:38,818 - INFO - Epoch 493 training loss = 2.85
2023-04-26 20:25:38,917 - INFO - Epoch 494 training loss = 2.711
2023-04-26 20:25:39,017 - INFO - Epoch 495 training loss = 2.773
2023-04-26 20:25:39,116 - INFO - Epoch 496 training loss = 2.695
2023-04-26 20:25:39,215 - INFO - Epoch 497 training loss = 2.709
2023-04-26 20:25:39,314 - INFO - Epoch 498 training loss = 2.666
2023-04-26 20:25:39,413 - INFO - Epoch 499 training loss = 2.783
2023-04-26 20:25:39,570 - INFO - Epoch 500 training loss = 2.762
2023-04-26 20:25:39,597 - INFO - Validation loss = 9.902
2023-04-26 20:25:39,696 - INFO - Epoch 501 training loss = 2.701
2023-04-26 20:25:39,795 - INFO - Epoch 502 training loss = 2.67
2023-04-26 20:25:39,894 - INFO - Epoch 503 training loss = 2.669
2023-04-26 20:25:39,993 - INFO - Epoch 504 training loss = 2.708
2023-04-26 20:25:40,092 - INFO - Epoch 505 training loss = 2.633
2023-04-26 20:25:40,191 - INFO - Epoch 506 training loss = 2.635
2023-04-26 20:25:40,290 - INFO - Epoch 507 training loss = 2.663
2023-04-26 20:25:40,389 - INFO - Epoch 508 training loss = 2.592
2023-04-26 20:25:40,488 - INFO - Epoch 509 training loss = 2.652
2023-04-26 20:25:40,587 - INFO - Epoch 510 training loss = 2.622
2023-04-26 20:25:40,614 - INFO - Validation loss = 9.597
2023-04-26 20:25:40,714 - INFO - Epoch 511 training loss = 2.67
2023-04-26 20:25:40,812 - INFO - Epoch 512 training loss = 2.698
2023-04-26 20:25:40,911 - INFO - Epoch 513 training loss = 2.576
2023-04-26 20:25:41,011 - INFO - Epoch 514 training loss = 2.595
2023-04-26 20:25:41,110 - INFO - Epoch 515 training loss = 2.648
2023-04-26 20:25:41,209 - INFO - Epoch 516 training loss =  2.6
2023-04-26 20:25:41,308 - INFO - Epoch 517 training loss = 2.566
2023-04-26 20:25:41,407 - INFO - Epoch 518 training loss = 2.597
2023-04-26 20:25:41,506 - INFO - Epoch 519 training loss = 2.59
2023-04-26 20:25:41,605 - INFO - Epoch 520 training loss = 2.554
2023-04-26 20:25:41,632 - INFO - Validation loss = 9.735
2023-04-26 20:25:41,732 - INFO - Epoch 521 training loss = 2.628
2023-04-26 20:25:41,831 - INFO - Epoch 522 training loss = 2.574
2023-04-26 20:25:41,930 - INFO - Epoch 523 training loss = 2.536
2023-04-26 20:25:42,029 - INFO - Epoch 524 training loss = 2.523
2023-04-26 20:25:42,128 - INFO - Epoch 525 training loss = 2.556
2023-04-26 20:25:42,227 - INFO - Epoch 526 training loss = 2.557
2023-04-26 20:25:42,326 - INFO - Epoch 527 training loss = 2.509
2023-04-26 20:25:42,425 - INFO - Epoch 528 training loss = 2.542
2023-04-26 20:25:42,524 - INFO - Epoch 529 training loss = 2.436
2023-04-26 20:25:42,623 - INFO - Epoch 530 training loss = 2.561
2023-04-26 20:25:42,650 - INFO - Validation loss = 9.592
2023-04-26 20:25:42,751 - INFO - Epoch 531 training loss = 2.506
2023-04-26 20:25:42,907 - INFO - Epoch 532 training loss = 2.494
2023-04-26 20:25:43,006 - INFO - Epoch 533 training loss = 2.459
2023-04-26 20:25:43,106 - INFO - Epoch 534 training loss = 2.481
2023-04-26 20:25:43,205 - INFO - Epoch 535 training loss = 2.515
2023-04-26 20:25:43,304 - INFO - Epoch 536 training loss = 2.474
2023-04-26 20:25:43,403 - INFO - Epoch 537 training loss = 2.466
2023-04-26 20:25:43,502 - INFO - Epoch 538 training loss = 2.508
2023-04-26 20:25:43,600 - INFO - Epoch 539 training loss = 2.454
2023-04-26 20:25:43,701 - INFO - Epoch 540 training loss = 2.469
2023-04-26 20:25:43,728 - INFO - Validation loss = 9.536
2023-04-26 20:25:43,827 - INFO - Epoch 541 training loss = 2.464
2023-04-26 20:25:43,926 - INFO - Epoch 542 training loss = 2.488
2023-04-26 20:25:44,025 - INFO - Epoch 543 training loss = 2.381
2023-04-26 20:25:44,125 - INFO - Epoch 544 training loss = 2.436
2023-04-26 20:25:44,224 - INFO - Epoch 545 training loss = 2.429
2023-04-26 20:25:44,322 - INFO - Epoch 546 training loss = 2.443
2023-04-26 20:25:44,421 - INFO - Epoch 547 training loss = 2.415
2023-04-26 20:25:44,520 - INFO - Epoch 548 training loss = 2.399
2023-04-26 20:25:44,619 - INFO - Epoch 549 training loss = 2.439
2023-04-26 20:25:44,720 - INFO - Epoch 550 training loss = 2.437
2023-04-26 20:25:44,747 - INFO - Validation loss = 9.66
2023-04-26 20:25:44,846 - INFO - Epoch 551 training loss = 2.38
2023-04-26 20:25:44,945 - INFO - Epoch 552 training loss = 2.36
2023-04-26 20:25:45,044 - INFO - Epoch 553 training loss = 2.399
2023-04-26 20:25:45,144 - INFO - Epoch 554 training loss = 2.445
2023-04-26 20:25:45,244 - INFO - Epoch 555 training loss = 2.362
2023-04-26 20:25:45,342 - INFO - Epoch 556 training loss = 2.372
2023-04-26 20:25:45,441 - INFO - Epoch 557 training loss = 2.394
2023-04-26 20:25:45,540 - INFO - Epoch 558 training loss = 2.33
2023-04-26 20:25:45,640 - INFO - Epoch 559 training loss = 2.348
2023-04-26 20:25:45,740 - INFO - Epoch 560 training loss = 2.368
2023-04-26 20:25:45,767 - INFO - Validation loss = 10.11
2023-04-26 20:25:45,866 - INFO - Epoch 561 training loss = 2.532
2023-04-26 20:25:45,965 - INFO - Epoch 562 training loss = 2.319
2023-04-26 20:25:46,122 - INFO - Epoch 563 training loss = 2.345
2023-04-26 20:25:46,221 - INFO - Epoch 564 training loss = 2.383
2023-04-26 20:25:46,320 - INFO - Epoch 565 training loss = 2.34
2023-04-26 20:25:46,419 - INFO - Epoch 566 training loss = 2.354
2023-04-26 20:25:46,518 - INFO - Epoch 567 training loss = 2.292
2023-04-26 20:25:46,617 - INFO - Epoch 568 training loss = 2.332
2023-04-26 20:25:46,717 - INFO - Epoch 569 training loss = 2.323
2023-04-26 20:25:46,816 - INFO - Epoch 570 training loss = 2.327
2023-04-26 20:25:46,844 - INFO - Validation loss = 9.494
2023-04-26 20:25:46,844 - INFO - best model
2023-04-26 20:25:46,954 - INFO - Epoch 571 training loss = 2.307
2023-04-26 20:25:47,054 - INFO - Epoch 572 training loss = 2.324
2023-04-26 20:25:47,153 - INFO - Epoch 573 training loss = 2.298
2023-04-26 20:25:47,252 - INFO - Epoch 574 training loss = 2.272
2023-04-26 20:25:47,352 - INFO - Epoch 575 training loss = 2.283
2023-04-26 20:25:47,451 - INFO - Epoch 576 training loss = 2.273
2023-04-26 20:25:47,550 - INFO - Epoch 577 training loss = 2.269
2023-04-26 20:25:47,649 - INFO - Epoch 578 training loss = 2.28
2023-04-26 20:25:47,750 - INFO - Epoch 579 training loss = 2.254
2023-04-26 20:25:47,849 - INFO - Epoch 580 training loss = 2.278
2023-04-26 20:25:47,876 - INFO - Validation loss = 9.669
2023-04-26 20:25:47,975 - INFO - Epoch 581 training loss = 2.238
2023-04-26 20:25:48,074 - INFO - Epoch 582 training loss = 2.33
2023-04-26 20:25:48,173 - INFO - Epoch 583 training loss = 2.283
2023-04-26 20:25:48,272 - INFO - Epoch 584 training loss = 2.216
2023-04-26 20:25:48,372 - INFO - Epoch 585 training loss = 2.237
2023-04-26 20:25:48,471 - INFO - Epoch 586 training loss = 2.249
2023-04-26 20:25:48,570 - INFO - Epoch 587 training loss = 2.25
2023-04-26 20:25:48,669 - INFO - Epoch 588 training loss = 2.244
2023-04-26 20:25:48,768 - INFO - Epoch 589 training loss = 2.213
2023-04-26 20:25:48,867 - INFO - Epoch 590 training loss = 2.195
2023-04-26 20:25:48,894 - INFO - Validation loss = 9.614
2023-04-26 20:25:48,993 - INFO - Epoch 591 training loss = 2.208
2023-04-26 20:25:49,093 - INFO - Epoch 592 training loss = 2.179
2023-04-26 20:25:49,192 - INFO - Epoch 593 training loss = 2.228
2023-04-26 20:25:49,349 - INFO - Epoch 594 training loss = 2.218
2023-04-26 20:25:49,448 - INFO - Epoch 595 training loss = 2.194
2023-04-26 20:25:49,547 - INFO - Epoch 596 training loss = 2.191
2023-04-26 20:25:49,646 - INFO - Epoch 597 training loss = 2.205
2023-04-26 20:25:49,745 - INFO - Epoch 598 training loss = 2.171
2023-04-26 20:25:49,844 - INFO - Epoch 599 training loss = 2.18
2023-04-26 20:25:49,943 - INFO - Epoch 600 training loss = 2.166
2023-04-26 20:25:49,970 - INFO - Validation loss = 9.477
2023-04-26 20:25:49,970 - INFO - best model
2023-04-26 20:25:50,080 - INFO - Epoch 601 training loss = 2.175
2023-04-26 20:25:50,179 - INFO - Epoch 602 training loss = 2.153
2023-04-26 20:25:50,277 - INFO - Epoch 603 training loss = 2.149
2023-04-26 20:25:50,376 - INFO - Epoch 604 training loss = 2.207
2023-04-26 20:25:50,475 - INFO - Epoch 605 training loss = 2.135
2023-04-26 20:25:50,573 - INFO - Epoch 606 training loss = 2.169
2023-04-26 20:25:50,672 - INFO - Epoch 607 training loss = 2.15
2023-04-26 20:25:50,770 - INFO - Epoch 608 training loss = 2.127
2023-04-26 20:25:50,868 - INFO - Epoch 609 training loss = 2.134
2023-04-26 20:25:50,966 - INFO - Epoch 610 training loss = 2.106
2023-04-26 20:25:50,993 - INFO - Validation loss = 9.455
2023-04-26 20:25:50,993 - INFO - best model
2023-04-26 20:25:51,102 - INFO - Epoch 611 training loss = 2.174
2023-04-26 20:25:51,200 - INFO - Epoch 612 training loss = 2.115
2023-04-26 20:25:51,298 - INFO - Epoch 613 training loss = 2.198
2023-04-26 20:25:51,395 - INFO - Epoch 614 training loss = 2.095
2023-04-26 20:25:51,493 - INFO - Epoch 615 training loss = 2.113
2023-04-26 20:25:51,591 - INFO - Epoch 616 training loss = 2.105
2023-04-26 20:25:51,688 - INFO - Epoch 617 training loss = 2.092
2023-04-26 20:25:51,786 - INFO - Epoch 618 training loss = 2.142
2023-04-26 20:25:51,884 - INFO - Epoch 619 training loss = 2.108
2023-04-26 20:25:51,982 - INFO - Epoch 620 training loss = 2.108
2023-04-26 20:25:52,009 - INFO - Validation loss = 9.543
2023-04-26 20:25:52,107 - INFO - Epoch 621 training loss = 2.079
2023-04-26 20:25:52,204 - INFO - Epoch 622 training loss = 2.09
2023-04-26 20:25:52,302 - INFO - Epoch 623 training loss = 2.061
2023-04-26 20:25:52,399 - INFO - Epoch 624 training loss = 2.084
2023-04-26 20:25:52,497 - INFO - Epoch 625 training loss = 2.104
2023-04-26 20:25:52,653 - INFO - Epoch 626 training loss = 2.05
2023-04-26 20:25:52,750 - INFO - Epoch 627 training loss = 2.067
2023-04-26 20:25:52,848 - INFO - Epoch 628 training loss = 2.052
2023-04-26 20:25:52,946 - INFO - Epoch 629 training loss = 2.075
2023-04-26 20:25:53,044 - INFO - Epoch 630 training loss = 2.064
2023-04-26 20:25:53,071 - INFO - Validation loss = 9.491
2023-04-26 20:25:53,168 - INFO - Epoch 631 training loss = 2.061
2023-04-26 20:25:53,266 - INFO - Epoch 632 training loss = 2.04
2023-04-26 20:25:53,364 - INFO - Epoch 633 training loss = 2.041
2023-04-26 20:25:53,461 - INFO - Epoch 634 training loss = 2.03
2023-04-26 20:25:53,559 - INFO - Epoch 635 training loss = 2.032
2023-04-26 20:25:53,657 - INFO - Epoch 636 training loss = 2.05
2023-04-26 20:25:53,754 - INFO - Epoch 637 training loss = 2.024
2023-04-26 20:25:53,852 - INFO - Epoch 638 training loss = 2.041
2023-04-26 20:25:53,950 - INFO - Epoch 639 training loss = 2.025
2023-04-26 20:25:54,048 - INFO - Epoch 640 training loss = 2.012
2023-04-26 20:25:54,075 - INFO - Validation loss = 9.534
2023-04-26 20:25:54,172 - INFO - Epoch 641 training loss = 2.029
2023-04-26 20:25:54,270 - INFO - Epoch 642 training loss = 2.024
2023-04-26 20:25:54,368 - INFO - Epoch 643 training loss = 2.007
2023-04-26 20:25:54,466 - INFO - Epoch 644 training loss = 1.997
2023-04-26 20:25:54,563 - INFO - Epoch 645 training loss = 2.01
2023-04-26 20:25:54,661 - INFO - Epoch 646 training loss = 1.99
2023-04-26 20:25:54,759 - INFO - Epoch 647 training loss = 2.008
2023-04-26 20:25:54,856 - INFO - Epoch 648 training loss = 1.997
2023-04-26 20:25:54,954 - INFO - Epoch 649 training loss = 1.982
2023-04-26 20:25:55,052 - INFO - Epoch 650 training loss = 1.996
2023-04-26 20:25:55,079 - INFO - Validation loss = 9.524
2023-04-26 20:25:55,177 - INFO - Epoch 651 training loss = 1.971
2023-04-26 20:25:55,276 - INFO - Epoch 652 training loss = 2.002
2023-04-26 20:25:55,374 - INFO - Epoch 653 training loss = 1.973
2023-04-26 20:25:55,472 - INFO - Epoch 654 training loss = 1.989
2023-04-26 20:25:55,570 - INFO - Epoch 655 training loss = 1.961
2023-04-26 20:25:55,668 - INFO - Epoch 656 training loss = 1.962
2023-04-26 20:25:55,824 - INFO - Epoch 657 training loss = 1.994
2023-04-26 20:25:55,923 - INFO - Epoch 658 training loss = 1.973
2023-04-26 20:25:56,020 - INFO - Epoch 659 training loss = 1.942
2023-04-26 20:25:56,118 - INFO - Epoch 660 training loss = 2.01
2023-04-26 20:25:56,145 - INFO - Validation loss = 9.529
2023-04-26 20:25:56,242 - INFO - Epoch 661 training loss = 1.966
2023-04-26 20:25:56,340 - INFO - Epoch 662 training loss = 1.935
2023-04-26 20:25:56,437 - INFO - Epoch 663 training loss = 1.949
2023-04-26 20:25:56,535 - INFO - Epoch 664 training loss = 1.952
2023-04-26 20:25:56,632 - INFO - Epoch 665 training loss = 1.95
2023-04-26 20:25:56,730 - INFO - Epoch 666 training loss = 1.929
2023-04-26 20:25:56,828 - INFO - Epoch 667 training loss = 1.923
2023-04-26 20:25:56,925 - INFO - Epoch 668 training loss = 1.956
2023-04-26 20:25:57,023 - INFO - Epoch 669 training loss = 1.931
2023-04-26 20:25:57,121 - INFO - Epoch 670 training loss = 1.915
2023-04-26 20:25:57,148 - INFO - Validation loss = 9.605
2023-04-26 20:25:57,246 - INFO - Epoch 671 training loss = 1.928
2023-04-26 20:25:57,344 - INFO - Epoch 672 training loss = 1.924
2023-04-26 20:25:57,442 - INFO - Epoch 673 training loss = 1.911
2023-04-26 20:25:57,540 - INFO - Epoch 674 training loss = 1.937
2023-04-26 20:25:57,637 - INFO - Epoch 675 training loss = 1.906
2023-04-26 20:25:57,735 - INFO - Epoch 676 training loss = 1.896
2023-04-26 20:25:57,833 - INFO - Epoch 677 training loss = 1.906
2023-04-26 20:25:57,931 - INFO - Epoch 678 training loss = 1.889
2023-04-26 20:25:58,030 - INFO - Epoch 679 training loss = 1.889
2023-04-26 20:25:58,129 - INFO - Epoch 680 training loss = 1.899
2023-04-26 20:25:58,156 - INFO - Validation loss = 9.528
2023-04-26 20:25:58,254 - INFO - Epoch 681 training loss = 1.889
2023-04-26 20:25:58,353 - INFO - Epoch 682 training loss = 1.885
2023-04-26 20:25:58,452 - INFO - Epoch 683 training loss = 1.88
2023-04-26 20:25:58,551 - INFO - Epoch 684 training loss = 1.89
2023-04-26 20:25:58,649 - INFO - Epoch 685 training loss = 1.891
2023-04-26 20:25:58,748 - INFO - Epoch 686 training loss = 1.872
2023-04-26 20:25:58,846 - INFO - Epoch 687 training loss = 1.878
2023-04-26 20:25:58,945 - INFO - Epoch 688 training loss = 1.882
2023-04-26 20:25:59,102 - INFO - Epoch 689 training loss = 1.868
2023-04-26 20:25:59,200 - INFO - Epoch 690 training loss = 1.871
2023-04-26 20:25:59,227 - INFO - Validation loss = 9.597
2023-04-26 20:25:59,326 - INFO - Epoch 691 training loss = 1.852
2023-04-26 20:25:59,425 - INFO - Epoch 692 training loss = 1.871
2023-04-26 20:25:59,523 - INFO - Epoch 693 training loss = 1.85
2023-04-26 20:25:59,622 - INFO - Epoch 694 training loss = 1.869
2023-04-26 20:25:59,720 - INFO - Epoch 695 training loss = 1.853
2023-04-26 20:25:59,819 - INFO - Epoch 696 training loss = 1.846
2023-04-26 20:25:59,917 - INFO - Epoch 697 training loss = 1.849
2023-04-26 20:26:00,016 - INFO - Epoch 698 training loss = 1.839
2023-04-26 20:26:00,115 - INFO - Epoch 699 training loss = 1.846
2023-04-26 20:26:00,213 - INFO - Epoch 700 training loss = 1.838
2023-04-26 20:26:00,240 - INFO - Validation loss = 9.562
2023-04-26 20:26:00,339 - INFO - Epoch 701 training loss = 1.829
2023-04-26 20:26:00,437 - INFO - Epoch 702 training loss = 1.837
2023-04-26 20:26:00,536 - INFO - Epoch 703 training loss = 1.836
2023-04-26 20:26:00,634 - INFO - Epoch 704 training loss = 1.819
2023-04-26 20:26:00,733 - INFO - Epoch 705 training loss = 1.845
2023-04-26 20:26:00,832 - INFO - Epoch 706 training loss = 1.823
2023-04-26 20:26:00,931 - INFO - Epoch 707 training loss = 1.82
2023-04-26 20:26:01,029 - INFO - Epoch 708 training loss = 1.821
2023-04-26 20:26:01,128 - INFO - Epoch 709 training loss = 1.824
2023-04-26 20:26:01,227 - INFO - Epoch 710 training loss = 1.811
2023-04-26 20:26:01,254 - INFO - Validation loss = 9.604
2023-04-26 20:26:01,353 - INFO - Epoch 711 training loss = 1.823
2023-04-26 20:26:01,451 - INFO - Epoch 712 training loss = 1.807
2023-04-26 20:26:01,550 - INFO - Epoch 713 training loss = 1.805
2023-04-26 20:26:01,649 - INFO - Epoch 714 training loss =  1.8
2023-04-26 20:26:01,747 - INFO - Epoch 715 training loss = 1.819
2023-04-26 20:26:01,846 - INFO - Epoch 716 training loss = 1.797
2023-04-26 20:26:01,945 - INFO - Epoch 717 training loss = 1.818
2023-04-26 20:26:02,043 - INFO - Epoch 718 training loss = 1.785
2023-04-26 20:26:02,142 - INFO - Epoch 719 training loss = 1.797
2023-04-26 20:26:02,298 - INFO - Epoch 720 training loss = 1.789
2023-04-26 20:26:02,325 - INFO - Validation loss = 9.587
2023-04-26 20:26:02,423 - INFO - Epoch 721 training loss = 1.783
2023-04-26 20:26:02,522 - INFO - Epoch 722 training loss = 1.785
2023-04-26 20:26:02,620 - INFO - Epoch 723 training loss = 1.794
2023-04-26 20:26:02,718 - INFO - Epoch 724 training loss = 1.777
2023-04-26 20:26:02,817 - INFO - Epoch 725 training loss = 1.799
2023-04-26 20:26:02,915 - INFO - Epoch 726 training loss = 1.769
2023-04-26 20:26:03,014 - INFO - Epoch 727 training loss = 1.774
2023-04-26 20:26:03,113 - INFO - Epoch 728 training loss = 1.77
2023-04-26 20:26:03,211 - INFO - Epoch 729 training loss = 1.777
2023-04-26 20:26:03,310 - INFO - Epoch 730 training loss = 1.778
2023-04-26 20:26:03,337 - INFO - Validation loss = 9.587
2023-04-26 20:26:03,436 - INFO - Epoch 731 training loss = 1.76
2023-04-26 20:26:03,534 - INFO - Epoch 732 training loss = 1.77
2023-04-26 20:26:03,632 - INFO - Epoch 733 training loss = 1.761
2023-04-26 20:26:03,731 - INFO - Epoch 734 training loss = 1.764
2023-04-26 20:26:03,830 - INFO - Epoch 735 training loss = 1.763
2023-04-26 20:26:03,928 - INFO - Epoch 736 training loss = 1.752
2023-04-26 20:26:04,027 - INFO - Epoch 737 training loss = 1.757
2023-04-26 20:26:04,126 - INFO - Epoch 738 training loss = 1.748
2023-04-26 20:26:04,224 - INFO - Epoch 739 training loss = 1.75
2023-04-26 20:26:04,322 - INFO - Epoch 740 training loss = 1.754
2023-04-26 20:26:04,349 - INFO - Validation loss = 9.675
2023-04-26 20:26:04,448 - INFO - Epoch 741 training loss = 1.75
2023-04-26 20:26:04,546 - INFO - Epoch 742 training loss = 1.735
2023-04-26 20:26:04,644 - INFO - Epoch 743 training loss = 1.746
2023-04-26 20:26:04,743 - INFO - Epoch 744 training loss = 1.739
2023-04-26 20:26:04,841 - INFO - Epoch 745 training loss = 1.742
2023-04-26 20:26:04,939 - INFO - Epoch 746 training loss = 1.731
2023-04-26 20:26:05,038 - INFO - Epoch 747 training loss = 1.725
2023-04-26 20:26:05,137 - INFO - Epoch 748 training loss = 1.726
2023-04-26 20:26:05,237 - INFO - Epoch 749 training loss = 1.731
2023-04-26 20:26:05,336 - INFO - Epoch 750 training loss = 1.724
2023-04-26 20:26:05,363 - INFO - Validation loss = 9.63
2023-04-26 20:26:05,519 - INFO - Epoch 751 training loss = 1.73
2023-04-26 20:26:05,617 - INFO - Epoch 752 training loss = 1.726
2023-04-26 20:26:05,716 - INFO - Epoch 753 training loss = 1.717
2023-04-26 20:26:05,814 - INFO - Epoch 754 training loss = 1.717
2023-04-26 20:26:05,912 - INFO - Epoch 755 training loss = 1.721
2023-04-26 20:26:06,011 - INFO - Epoch 756 training loss = 1.719
2023-04-26 20:26:06,109 - INFO - Epoch 757 training loss = 1.711
2023-04-26 20:26:06,208 - INFO - Epoch 758 training loss = 1.71
2023-04-26 20:26:06,306 - INFO - Epoch 759 training loss = 1.719
2023-04-26 20:26:06,404 - INFO - Epoch 760 training loss = 1.697
2023-04-26 20:26:06,432 - INFO - Validation loss = 9.661
2023-04-26 20:26:06,530 - INFO - Epoch 761 training loss = 1.704
2023-04-26 20:26:06,628 - INFO - Epoch 762 training loss = 1.707
2023-04-26 20:26:06,727 - INFO - Epoch 763 training loss = 1.705
2023-04-26 20:26:06,825 - INFO - Epoch 764 training loss = 1.698
2023-04-26 20:26:06,923 - INFO - Epoch 765 training loss =  1.7
2023-04-26 20:26:07,022 - INFO - Epoch 766 training loss = 1.691
2023-04-26 20:26:07,121 - INFO - Epoch 767 training loss = 1.693
2023-04-26 20:26:07,219 - INFO - Epoch 768 training loss = 1.693
2023-04-26 20:26:07,318 - INFO - Epoch 769 training loss = 1.692
2023-04-26 20:26:07,416 - INFO - Epoch 770 training loss = 1.691
2023-04-26 20:26:07,443 - INFO - Validation loss = 9.672
2023-04-26 20:26:07,542 - INFO - Epoch 771 training loss = 1.693
2023-04-26 20:26:07,640 - INFO - Epoch 772 training loss = 1.69
2023-04-26 20:26:07,738 - INFO - Epoch 773 training loss = 1.676
2023-04-26 20:26:07,837 - INFO - Epoch 774 training loss = 1.69
2023-04-26 20:26:07,935 - INFO - Epoch 775 training loss = 1.686
2023-04-26 20:26:08,034 - INFO - Epoch 776 training loss = 1.676
2023-04-26 20:26:08,132 - INFO - Epoch 777 training loss = 1.676
2023-04-26 20:26:08,230 - INFO - Epoch 778 training loss = 1.679
2023-04-26 20:26:08,329 - INFO - Epoch 779 training loss = 1.677
2023-04-26 20:26:08,427 - INFO - Epoch 780 training loss = 1.68
2023-04-26 20:26:08,454 - INFO - Validation loss = 9.665
2023-04-26 20:26:08,555 - INFO - Epoch 781 training loss = 1.669
2023-04-26 20:26:08,654 - INFO - Epoch 782 training loss = 1.674
2023-04-26 20:26:08,810 - INFO - Epoch 783 training loss = 1.671
2023-04-26 20:26:08,908 - INFO - Epoch 784 training loss = 1.666
2023-04-26 20:26:09,006 - INFO - Epoch 785 training loss = 1.666
2023-04-26 20:26:09,105 - INFO - Epoch 786 training loss = 1.664
2023-04-26 20:26:09,204 - INFO - Epoch 787 training loss = 1.661
2023-04-26 20:26:09,302 - INFO - Epoch 788 training loss = 1.658
2023-04-26 20:26:09,401 - INFO - Epoch 789 training loss = 1.658
2023-04-26 20:26:09,499 - INFO - Epoch 790 training loss = 1.658
2023-04-26 20:26:09,526 - INFO - Validation loss = 9.698
2023-04-26 20:26:09,625 - INFO - Epoch 791 training loss = 1.66
2023-04-26 20:26:09,723 - INFO - Epoch 792 training loss = 1.653
2023-04-26 20:26:09,821 - INFO - Epoch 793 training loss = 1.657
2023-04-26 20:26:09,920 - INFO - Epoch 794 training loss = 1.652
2023-04-26 20:26:10,019 - INFO - Epoch 795 training loss = 1.648
2023-04-26 20:26:10,119 - INFO - Epoch 796 training loss = 1.649
2023-04-26 20:26:10,217 - INFO - Epoch 797 training loss = 1.644
2023-04-26 20:26:10,315 - INFO - Epoch 798 training loss = 1.645
2023-04-26 20:26:10,414 - INFO - Epoch 799 training loss = 1.649
2023-04-26 20:26:10,512 - INFO - Epoch 800 training loss = 1.642
2023-04-26 20:26:10,539 - INFO - Validation loss = 9.678
2023-04-26 20:26:10,637 - INFO - Epoch 801 training loss = 1.642
2023-04-26 20:26:10,736 - INFO - Epoch 802 training loss = 1.64
2023-04-26 20:26:10,834 - INFO - Epoch 803 training loss = 1.638
2023-04-26 20:26:10,932 - INFO - Epoch 804 training loss = 1.637
2023-04-26 20:26:11,031 - INFO - Epoch 805 training loss = 1.636
2023-04-26 20:26:11,129 - INFO - Epoch 806 training loss = 1.636
2023-04-26 20:26:11,228 - INFO - Epoch 807 training loss = 1.633
2023-04-26 20:26:11,326 - INFO - Epoch 808 training loss = 1.633
2023-04-26 20:26:11,425 - INFO - Epoch 809 training loss = 1.63
2023-04-26 20:26:11,523 - INFO - Epoch 810 training loss = 1.63
2023-04-26 20:26:11,550 - INFO - Validation loss = 9.725
2023-04-26 20:26:11,648 - INFO - Epoch 811 training loss = 1.63
2023-04-26 20:26:11,747 - INFO - Epoch 812 training loss = 1.624
2023-04-26 20:26:11,845 - INFO - Epoch 813 training loss = 1.626
2023-04-26 20:26:12,002 - INFO - Epoch 814 training loss = 1.622
2023-04-26 20:26:12,101 - INFO - Epoch 815 training loss = 1.623
2023-04-26 20:26:12,199 - INFO - Epoch 816 training loss = 1.623
2023-04-26 20:26:12,298 - INFO - Epoch 817 training loss = 1.618
2023-04-26 20:26:12,396 - INFO - Epoch 818 training loss = 1.622
2023-04-26 20:26:12,494 - INFO - Epoch 819 training loss = 1.619
2023-04-26 20:26:12,592 - INFO - Epoch 820 training loss = 1.614
2023-04-26 20:26:12,620 - INFO - Validation loss = 9.731
2023-04-26 20:26:12,718 - INFO - Epoch 821 training loss = 1.616
2023-04-26 20:26:12,817 - INFO - Epoch 822 training loss = 1.614
2023-04-26 20:26:12,915 - INFO - Epoch 823 training loss = 1.614
2023-04-26 20:26:13,015 - INFO - Epoch 824 training loss = 1.61
2023-04-26 20:26:13,113 - INFO - Epoch 825 training loss = 1.61
2023-04-26 20:26:13,212 - INFO - Epoch 826 training loss = 1.61
2023-04-26 20:26:13,310 - INFO - Epoch 827 training loss = 1.61
2023-04-26 20:26:13,409 - INFO - Epoch 828 training loss = 1.604
2023-04-26 20:26:13,507 - INFO - Epoch 829 training loss = 1.603
2023-04-26 20:26:13,605 - INFO - Epoch 830 training loss = 1.604
2023-04-26 20:26:13,632 - INFO - Validation loss = 9.732
2023-04-26 20:26:13,731 - INFO - Epoch 831 training loss = 1.601
2023-04-26 20:26:13,829 - INFO - Epoch 832 training loss = 1.604
2023-04-26 20:26:13,928 - INFO - Epoch 833 training loss = 1.601
2023-04-26 20:26:14,028 - INFO - Epoch 834 training loss = 1.598
2023-04-26 20:26:14,126 - INFO - Epoch 835 training loss = 1.599
2023-04-26 20:26:14,225 - INFO - Epoch 836 training loss = 1.598
2023-04-26 20:26:14,323 - INFO - Epoch 837 training loss = 1.594
2023-04-26 20:26:14,422 - INFO - Epoch 838 training loss = 1.599
2023-04-26 20:26:14,520 - INFO - Epoch 839 training loss = 1.596
2023-04-26 20:26:14,618 - INFO - Epoch 840 training loss = 1.592
2023-04-26 20:26:14,645 - INFO - Validation loss = 9.741
2023-04-26 20:26:14,743 - INFO - Epoch 841 training loss = 1.593
2023-04-26 20:26:14,842 - INFO - Epoch 842 training loss = 1.592
2023-04-26 20:26:14,940 - INFO - Epoch 843 training loss = 1.589
2023-04-26 20:26:15,040 - INFO - Epoch 844 training loss = 1.59
2023-04-26 20:26:15,139 - INFO - Epoch 845 training loss = 1.586
2023-04-26 20:26:15,295 - INFO - Epoch 846 training loss = 1.588
2023-04-26 20:26:15,393 - INFO - Epoch 847 training loss = 1.585
2023-04-26 20:26:15,491 - INFO - Epoch 848 training loss = 1.584
2023-04-26 20:26:15,590 - INFO - Epoch 849 training loss = 1.584
2023-04-26 20:26:15,688 - INFO - Epoch 850 training loss = 1.584
2023-04-26 20:26:15,715 - INFO - Validation loss = 9.765
2023-04-26 20:26:15,814 - INFO - Epoch 851 training loss = 1.583
2023-04-26 20:26:15,912 - INFO - Epoch 852 training loss = 1.585
2023-04-26 20:26:16,012 - INFO - Epoch 853 training loss = 1.577
2023-04-26 20:26:16,110 - INFO - Epoch 854 training loss = 1.578
2023-04-26 20:26:16,209 - INFO - Epoch 855 training loss = 1.579
2023-04-26 20:26:16,307 - INFO - Epoch 856 training loss = 1.579
2023-04-26 20:26:16,405 - INFO - Epoch 857 training loss = 1.576
2023-04-26 20:26:16,503 - INFO - Epoch 858 training loss = 1.574
2023-04-26 20:26:16,603 - INFO - Epoch 859 training loss = 1.575
2023-04-26 20:26:16,701 - INFO - Epoch 860 training loss = 1.572
2023-04-26 20:26:16,728 - INFO - Validation loss = 9.759
2023-04-26 20:26:16,827 - INFO - Epoch 861 training loss = 1.571
2023-04-26 20:26:16,925 - INFO - Epoch 862 training loss = 1.575
2023-04-26 20:26:17,025 - INFO - Epoch 863 training loss = 1.57
2023-04-26 20:26:17,124 - INFO - Epoch 864 training loss = 1.571
2023-04-26 20:26:17,222 - INFO - Epoch 865 training loss = 1.571
2023-04-26 20:26:17,321 - INFO - Epoch 866 training loss = 1.571
2023-04-26 20:26:17,419 - INFO - Epoch 867 training loss = 1.565
2023-04-26 20:26:17,517 - INFO - Epoch 868 training loss = 1.567
2023-04-26 20:26:17,616 - INFO - Epoch 869 training loss = 1.565
2023-04-26 20:26:17,714 - INFO - Epoch 870 training loss = 1.565
2023-04-26 20:26:17,741 - INFO - Validation loss = 9.753
2023-04-26 20:26:17,840 - INFO - Epoch 871 training loss = 1.564
2023-04-26 20:26:17,938 - INFO - Epoch 872 training loss = 1.567
2023-04-26 20:26:18,038 - INFO - Epoch 873 training loss = 1.564
2023-04-26 20:26:18,136 - INFO - Epoch 874 training loss = 1.561
2023-04-26 20:26:18,235 - INFO - Epoch 875 training loss = 1.562
2023-04-26 20:26:18,333 - INFO - Epoch 876 training loss = 1.562
2023-04-26 20:26:18,489 - INFO - Epoch 877 training loss = 1.561
2023-04-26 20:26:18,587 - INFO - Epoch 878 training loss = 1.559
2023-04-26 20:26:18,686 - INFO - Epoch 879 training loss = 1.559
2023-04-26 20:26:18,784 - INFO - Epoch 880 training loss = 1.559
2023-04-26 20:26:18,811 - INFO - Validation loss = 9.765
2023-04-26 20:26:18,910 - INFO - Epoch 881 training loss = 1.556
2023-04-26 20:26:19,008 - INFO - Epoch 882 training loss = 1.557
2023-04-26 20:26:19,107 - INFO - Epoch 883 training loss = 1.555
2023-04-26 20:26:19,205 - INFO - Epoch 884 training loss = 1.551
2023-04-26 20:26:19,304 - INFO - Epoch 885 training loss = 1.553
2023-04-26 20:26:19,402 - INFO - Epoch 886 training loss = 1.553
2023-04-26 20:26:19,501 - INFO - Epoch 887 training loss = 1.552
2023-04-26 20:26:19,599 - INFO - Epoch 888 training loss = 1.548
2023-04-26 20:26:19,697 - INFO - Epoch 889 training loss = 1.55
2023-04-26 20:26:19,796 - INFO - Epoch 890 training loss = 1.549
2023-04-26 20:26:19,823 - INFO - Validation loss = 9.776
2023-04-26 20:26:19,921 - INFO - Epoch 891 training loss = 1.549
2023-04-26 20:26:20,020 - INFO - Epoch 892 training loss = 1.551
2023-04-26 20:26:20,118 - INFO - Epoch 893 training loss = 1.548
2023-04-26 20:26:20,217 - INFO - Epoch 894 training loss = 1.548
2023-04-26 20:26:20,315 - INFO - Epoch 895 training loss = 1.547
2023-04-26 20:26:20,414 - INFO - Epoch 896 training loss = 1.545
2023-04-26 20:26:20,512 - INFO - Epoch 897 training loss = 1.546
2023-04-26 20:26:20,611 - INFO - Epoch 898 training loss = 1.545
2023-04-26 20:26:20,709 - INFO - Epoch 899 training loss = 1.545
2023-04-26 20:26:20,807 - INFO - Epoch 900 training loss = 1.542
2023-04-26 20:26:20,834 - INFO - Validation loss = 9.797
2023-04-26 20:26:20,932 - INFO - Epoch 901 training loss = 1.542
2023-04-26 20:26:21,031 - INFO - Epoch 902 training loss = 1.542
2023-04-26 20:26:21,129 - INFO - Epoch 903 training loss = 1.54
2023-04-26 20:26:21,228 - INFO - Epoch 904 training loss = 1.542
2023-04-26 20:26:21,326 - INFO - Epoch 905 training loss = 1.542
2023-04-26 20:26:21,424 - INFO - Epoch 906 training loss = 1.539
2023-04-26 20:26:21,523 - INFO - Epoch 907 training loss = 1.54
2023-04-26 20:26:21,679 - INFO - Epoch 908 training loss = 1.54
2023-04-26 20:26:21,777 - INFO - Epoch 909 training loss = 1.537
2023-04-26 20:26:21,875 - INFO - Epoch 910 training loss = 1.54
2023-04-26 20:26:21,903 - INFO - Validation loss = 9.783
2023-04-26 20:26:22,001 - INFO - Epoch 911 training loss = 1.537
2023-04-26 20:26:22,100 - INFO - Epoch 912 training loss = 1.537
2023-04-26 20:26:22,198 - INFO - Epoch 913 training loss = 1.537
2023-04-26 20:26:22,296 - INFO - Epoch 914 training loss = 1.536
2023-04-26 20:26:22,394 - INFO - Epoch 915 training loss = 1.534
2023-04-26 20:26:22,492 - INFO - Epoch 916 training loss = 1.536
2023-04-26 20:26:22,591 - INFO - Epoch 917 training loss = 1.535
2023-04-26 20:26:22,689 - INFO - Epoch 918 training loss = 1.534
2023-04-26 20:26:22,787 - INFO - Epoch 919 training loss = 1.535
2023-04-26 20:26:22,886 - INFO - Epoch 920 training loss = 1.534
2023-04-26 20:26:22,913 - INFO - Validation loss = 9.79
2023-04-26 20:26:23,011 - INFO - Epoch 921 training loss = 1.532
2023-04-26 20:26:23,110 - INFO - Epoch 922 training loss = 1.534
2023-04-26 20:26:23,208 - INFO - Epoch 923 training loss = 1.532
2023-04-26 20:26:23,306 - INFO - Epoch 924 training loss = 1.532
2023-04-26 20:26:23,405 - INFO - Epoch 925 training loss = 1.529
2023-04-26 20:26:23,503 - INFO - Epoch 926 training loss = 1.529
2023-04-26 20:26:23,601 - INFO - Epoch 927 training loss = 1.529
2023-04-26 20:26:23,699 - INFO - Epoch 928 training loss = 1.529
2023-04-26 20:26:23,798 - INFO - Epoch 929 training loss = 1.53
2023-04-26 20:26:23,896 - INFO - Epoch 930 training loss = 1.529
2023-04-26 20:26:23,923 - INFO - Validation loss = 9.792
2023-04-26 20:26:24,022 - INFO - Epoch 931 training loss = 1.529
2023-04-26 20:26:24,120 - INFO - Epoch 932 training loss = 1.527
2023-04-26 20:26:24,218 - INFO - Epoch 933 training loss = 1.525
2023-04-26 20:26:24,316 - INFO - Epoch 934 training loss = 1.526
2023-04-26 20:26:24,414 - INFO - Epoch 935 training loss = 1.526
2023-04-26 20:26:24,513 - INFO - Epoch 936 training loss = 1.525
2023-04-26 20:26:24,611 - INFO - Epoch 937 training loss = 1.526
2023-04-26 20:26:24,709 - INFO - Epoch 938 training loss = 1.526
2023-04-26 20:26:24,807 - INFO - Epoch 939 training loss = 1.525
2023-04-26 20:26:24,964 - INFO - Epoch 940 training loss = 1.525
2023-04-26 20:26:24,991 - INFO - Validation loss = 9.796
2023-04-26 20:26:25,090 - INFO - Epoch 941 training loss = 1.524
2023-04-26 20:26:25,188 - INFO - Epoch 942 training loss = 1.524
2023-04-26 20:26:25,286 - INFO - Epoch 943 training loss = 1.525
2023-04-26 20:26:25,385 - INFO - Epoch 944 training loss = 1.521
2023-04-26 20:26:25,483 - INFO - Epoch 945 training loss = 1.523
2023-04-26 20:26:25,581 - INFO - Epoch 946 training loss = 1.524
2023-04-26 20:26:25,679 - INFO - Epoch 947 training loss = 1.521
2023-04-26 20:26:25,778 - INFO - Epoch 948 training loss = 1.522
2023-04-26 20:26:25,876 - INFO - Epoch 949 training loss = 1.523
2023-04-26 20:26:25,974 - INFO - Epoch 950 training loss = 1.521
2023-04-26 20:26:26,002 - INFO - Validation loss = 9.802
2023-04-26 20:26:26,100 - INFO - Epoch 951 training loss = 1.52
2023-04-26 20:26:26,198 - INFO - Epoch 952 training loss = 1.52
2023-04-26 20:26:26,297 - INFO - Epoch 953 training loss = 1.521
2023-04-26 20:26:26,395 - INFO - Epoch 954 training loss = 1.519
2023-04-26 20:26:26,494 - INFO - Epoch 955 training loss = 1.522
2023-04-26 20:26:26,592 - INFO - Epoch 956 training loss = 1.52
2023-04-26 20:26:26,690 - INFO - Epoch 957 training loss = 1.52
2023-04-26 20:26:26,788 - INFO - Epoch 958 training loss = 1.518
2023-04-26 20:26:26,887 - INFO - Epoch 959 training loss = 1.519
2023-04-26 20:26:26,985 - INFO - Epoch 960 training loss = 1.518
2023-04-26 20:26:27,012 - INFO - Validation loss =  9.8
2023-04-26 20:26:27,111 - INFO - Epoch 961 training loss = 1.518
2023-04-26 20:26:27,209 - INFO - Epoch 962 training loss = 1.518
2023-04-26 20:26:27,307 - INFO - Epoch 963 training loss = 1.518
2023-04-26 20:26:27,405 - INFO - Epoch 964 training loss = 1.518
2023-04-26 20:26:27,503 - INFO - Epoch 965 training loss = 1.517
2023-04-26 20:26:27,602 - INFO - Epoch 966 training loss = 1.517
2023-04-26 20:26:27,700 - INFO - Epoch 967 training loss = 1.517
2023-04-26 20:26:27,798 - INFO - Epoch 968 training loss = 1.517
2023-04-26 20:26:27,896 - INFO - Epoch 969 training loss = 1.516
2023-04-26 20:26:27,994 - INFO - Epoch 970 training loss = 1.517
2023-04-26 20:26:28,021 - INFO - Validation loss =  9.8
2023-04-26 20:26:28,178 - INFO - Epoch 971 training loss = 1.517
2023-04-26 20:26:28,276 - INFO - Epoch 972 training loss = 1.517
2023-04-26 20:26:28,374 - INFO - Epoch 973 training loss = 1.515
2023-04-26 20:26:28,473 - INFO - Epoch 974 training loss = 1.514
2023-04-26 20:26:28,571 - INFO - Epoch 975 training loss = 1.516
2023-04-26 20:26:28,669 - INFO - Epoch 976 training loss = 1.515
2023-04-26 20:26:28,768 - INFO - Epoch 977 training loss = 1.515
2023-04-26 20:26:28,866 - INFO - Epoch 978 training loss = 1.516
2023-04-26 20:26:28,964 - INFO - Epoch 979 training loss = 1.515
2023-04-26 20:26:29,063 - INFO - Epoch 980 training loss = 1.516
2023-04-26 20:26:29,090 - INFO - Validation loss =  9.8
2023-04-26 20:26:29,188 - INFO - Epoch 981 training loss = 1.516
2023-04-26 20:26:29,287 - INFO - Epoch 982 training loss = 1.515
2023-04-26 20:26:29,385 - INFO - Epoch 983 training loss = 1.516
2023-04-26 20:26:29,484 - INFO - Epoch 984 training loss = 1.515
2023-04-26 20:26:29,582 - INFO - Epoch 985 training loss = 1.514
2023-04-26 20:26:29,680 - INFO - Epoch 986 training loss = 1.515
2023-04-26 20:26:29,778 - INFO - Epoch 987 training loss = 1.513
2023-04-26 20:26:29,877 - INFO - Epoch 988 training loss = 1.513
2023-04-26 20:26:29,975 - INFO - Epoch 989 training loss = 1.513
2023-04-26 20:26:30,074 - INFO - Epoch 990 training loss = 1.514
2023-04-26 20:26:30,101 - INFO - Validation loss = 9.801
2023-04-26 20:26:30,199 - INFO - Epoch 991 training loss = 1.514
2023-04-26 20:26:30,297 - INFO - Epoch 992 training loss = 1.512
2023-04-26 20:26:30,396 - INFO - Epoch 993 training loss = 1.514
2023-04-26 20:26:30,494 - INFO - Epoch 994 training loss = 1.514
2023-04-26 20:26:30,592 - INFO - Epoch 995 training loss = 1.514
2023-04-26 20:26:30,691 - INFO - Epoch 996 training loss = 1.514
2023-04-26 20:26:30,789 - INFO - Epoch 997 training loss = 1.514
2023-04-26 20:26:30,887 - INFO - Epoch 998 training loss = 1.514
2023-04-26 20:26:30,986 - INFO - Epoch 999 training loss = 1.513
2023-04-26 20:26:31,001 - INFO - Validation loss = 9.802
