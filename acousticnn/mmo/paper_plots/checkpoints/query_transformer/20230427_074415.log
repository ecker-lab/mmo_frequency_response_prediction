2023-04-27 07:44:15,123 - INFO - Config:
Namespace(config='configs/implicit_transformer.yaml', dir='/user/delden/acoustics/spp_ai_acoustics/acousticNN/experiments/arch/no_encoding/implicit_transformer', epochs=100, device='cuda', seed=3, parameter_list='configs/data.yaml', encoding='none', dir_name='arch/no_encoding/implicit_transformer')
2023-04-27 07:44:15,123 - INFO - Config:
Munch({'optimizer': Munch({'type': 'AdamW', 'kwargs': Munch({'lr': 0.0025, 'weight_decay': 0.005, 'betas': [0.9, 0.95]})}), 'scheduler': Munch({'type': 'CosLR', 'kwargs': Munch({'epochs': 1000, 'initial_epochs': 25})}), 'model': Munch({'name': 'ImplicitTransformer', 'input_encoding': True, 'encoding_dim': 16, 'encoder': Munch({'embed_dim': 66, 'num_heads': 3, 'depth': 4}), 'decoder': Munch({'embed_dim': 99, 'num_heads': 3, 'depth': 3})}), 'generation_frequency': 5000, 'validation_frequency': 10, 'batch_size': 16, 'epochs': 1000, 'gradient_clip': 10})
2023-04-27 07:44:29,155 - INFO - ==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ImplicitTransformer                      [16, 200, 4]              --
├─PositionalEncoding: 1-1                [16, 14, 66]              --
│    └─SinosoidalEncoding: 2-1           [16, 14, 66]              --
├─GroupwiseProjection: 1-2               [16, 14, 66]              --
│    └─ModuleList: 2-2                   --                        --
│    │    └─Linear: 3-1                  [16, 4, 66]               132
│    │    └─Linear: 3-2                  [16, 5, 66]               132
│    │    └─Linear: 3-3                  [16, 5, 66]               132
├─TransformerEncoder: 1-3                [16, 14, 66]              --
│    └─ModuleList: 2-3                   --                        --
│    │    └─Block: 3-4                   [16, 14, 66]              52,932
│    │    └─Block: 3-5                   [16, 14, 66]              52,932
│    │    └─Block: 3-6                   [16, 14, 66]              52,932
│    │    └─Block: 3-7                   [16, 14, 66]              52,932
├─Linear: 1-4                            [3200, 4, 99]             6,633
├─Linear: 1-5                            [16, 200, 99]             198
├─TransformerEncoder: 1-6                [3200, 4, 99]             --
│    └─ModuleList: 2-4                   --                        --
│    │    └─Block: 3-8                   [3200, 4, 99]             118,602
│    │    └─Block: 3-9                   [3200, 4, 99]             118,602
│    │    └─Block: 3-10                  [3200, 4, 99]             118,602
├─Linear: 1-7                            [3200, 4, 1]              100
==========================================================================================
Total params: 574,861
Trainable params: 574,861
Non-trainable params: 0
Total mult-adds (G): 1.16
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 352.64
Params size (MB): 2.30
Estimated Total Size (MB): 354.95
==========================================================================================
2023-04-27 07:44:33,214 - INFO - Epoch 0 training loss = 1.669e+03
2023-04-27 07:44:33,528 - INFO - Validation loss = 1.053e+03
2023-04-27 07:44:33,528 - INFO - best model
2023-04-27 07:44:37,554 - INFO - Epoch 1 training loss = 774.6
2023-04-27 07:44:41,560 - INFO - Epoch 2 training loss = 328.3
2023-04-27 07:44:45,567 - INFO - Epoch 3 training loss = 226.6
2023-04-27 07:44:49,571 - INFO - Epoch 4 training loss = 138.2
2023-04-27 07:44:53,573 - INFO - Epoch 5 training loss = 111.7
2023-04-27 07:44:57,574 - INFO - Epoch 6 training loss = 85.82
2023-04-27 07:45:01,577 - INFO - Epoch 7 training loss = 66.39
2023-04-27 07:45:05,579 - INFO - Epoch 8 training loss = 56.09
2023-04-27 07:45:09,582 - INFO - Epoch 9 training loss = 50.52
2023-04-27 07:45:13,587 - INFO - Epoch 10 training loss = 45.65
2023-04-27 07:45:13,899 - INFO - Validation loss = 34.52
2023-04-27 07:45:13,899 - INFO - best model
2023-04-27 07:45:17,926 - INFO - Epoch 11 training loss = 42.2
2023-04-27 07:45:21,927 - INFO - Epoch 12 training loss = 39.84
2023-04-27 07:45:25,927 - INFO - Epoch 13 training loss = 37.3
2023-04-27 07:45:29,929 - INFO - Epoch 14 training loss = 34.5
2023-04-27 07:45:33,930 - INFO - Epoch 15 training loss = 30.53
2023-04-27 07:45:37,933 - INFO - Epoch 16 training loss = 29.52
2023-04-27 07:45:41,936 - INFO - Epoch 17 training loss = 25.89
2023-04-27 07:45:45,940 - INFO - Epoch 18 training loss = 26.0
2023-04-27 07:45:49,944 - INFO - Epoch 19 training loss = 25.6
2023-04-27 07:45:53,946 - INFO - Epoch 20 training loss = 24.13
2023-04-27 07:45:54,258 - INFO - Validation loss = 31.78
2023-04-27 07:45:54,259 - INFO - best model
2023-04-27 07:45:58,282 - INFO - Epoch 21 training loss = 22.17
2023-04-27 07:46:02,285 - INFO - Epoch 22 training loss = 21.55
2023-04-27 07:46:06,287 - INFO - Epoch 23 training loss = 22.34
2023-04-27 07:46:10,290 - INFO - Epoch 24 training loss = 19.05
2023-04-27 07:46:14,297 - INFO - Epoch 25 training loss = 19.11
2023-04-27 07:46:18,310 - INFO - Epoch 26 training loss = 20.2
2023-04-27 07:46:22,312 - INFO - Epoch 27 training loss = 18.13
2023-04-27 07:46:26,315 - INFO - Epoch 28 training loss = 17.14
2023-04-27 07:46:30,318 - INFO - Epoch 29 training loss = 14.28
2023-04-27 07:46:34,321 - INFO - Epoch 30 training loss = 13.79
2023-04-27 07:46:34,633 - INFO - Validation loss = 9.298
2023-04-27 07:46:34,634 - INFO - best model
2023-04-27 07:46:38,658 - INFO - Epoch 31 training loss = 13.11
2023-04-27 07:46:42,663 - INFO - Epoch 32 training loss = 11.6
2023-04-27 07:46:46,669 - INFO - Epoch 33 training loss = 12.46
2023-04-27 07:46:50,678 - INFO - Epoch 34 training loss = 11.17
2023-04-27 07:46:54,693 - INFO - Epoch 35 training loss = 11.34
2023-04-27 07:46:58,708 - INFO - Epoch 36 training loss = 9.658
2023-04-27 07:47:02,725 - INFO - Epoch 37 training loss = 9.93
2023-04-27 07:47:06,740 - INFO - Epoch 38 training loss = 9.205
2023-04-27 07:47:10,756 - INFO - Epoch 39 training loss = 9.392
2023-04-27 07:47:14,774 - INFO - Epoch 40 training loss = 8.799
2023-04-27 07:47:15,087 - INFO - Validation loss = 10.65
2023-04-27 07:47:19,105 - INFO - Epoch 41 training loss = 8.86
2023-04-27 07:47:23,119 - INFO - Epoch 42 training loss = 8.014
2023-04-27 07:47:27,143 - INFO - Epoch 43 training loss = 7.558
2023-04-27 07:47:31,158 - INFO - Epoch 44 training loss = 7.344
2023-04-27 07:47:35,172 - INFO - Epoch 45 training loss = 7.275
2023-04-27 07:47:39,185 - INFO - Epoch 46 training loss = 6.871
2023-04-27 07:47:43,188 - INFO - Epoch 47 training loss = 6.82
2023-04-27 07:47:47,194 - INFO - Epoch 48 training loss = 6.735
2023-04-27 07:47:51,197 - INFO - Epoch 49 training loss = 5.961
2023-04-27 07:47:55,205 - INFO - Epoch 50 training loss = 6.451
2023-04-27 07:47:55,518 - INFO - Validation loss = 8.964
2023-04-27 07:47:55,518 - INFO - best model
2023-04-27 07:47:59,553 - INFO - Epoch 51 training loss = 6.721
2023-04-27 07:48:03,570 - INFO - Epoch 52 training loss = 5.682
2023-04-27 07:48:07,585 - INFO - Epoch 53 training loss = 5.846
2023-04-27 07:48:11,600 - INFO - Epoch 54 training loss = 5.481
2023-04-27 07:48:15,618 - INFO - Epoch 55 training loss = 5.894
2023-04-27 07:48:19,636 - INFO - Epoch 56 training loss = 5.158
2023-04-27 07:48:23,651 - INFO - Epoch 57 training loss = 5.576
2023-04-27 07:48:27,665 - INFO - Epoch 58 training loss = 5.128
2023-04-27 07:48:31,677 - INFO - Epoch 59 training loss = 5.249
2023-04-27 07:48:35,680 - INFO - Epoch 60 training loss = 5.235
2023-04-27 07:48:35,993 - INFO - Validation loss = 4.371
2023-04-27 07:48:35,993 - INFO - best model
2023-04-27 07:48:40,017 - INFO - Epoch 61 training loss = 4.578
2023-04-27 07:48:44,022 - INFO - Epoch 62 training loss = 4.574
2023-04-27 07:48:48,030 - INFO - Epoch 63 training loss = 4.946
2023-04-27 07:48:52,033 - INFO - Epoch 64 training loss = 4.581
2023-04-27 07:48:56,036 - INFO - Epoch 65 training loss = 4.604
2023-04-27 07:49:00,039 - INFO - Epoch 66 training loss = 3.995
2023-04-27 07:49:04,043 - INFO - Epoch 67 training loss = 4.516
2023-04-27 07:49:08,045 - INFO - Epoch 68 training loss = 4.101
2023-04-27 07:49:12,048 - INFO - Epoch 69 training loss = 4.546
2023-04-27 07:49:16,054 - INFO - Epoch 70 training loss = 4.259
2023-04-27 07:49:16,366 - INFO - Validation loss = 3.399
2023-04-27 07:49:16,367 - INFO - best model
2023-04-27 07:49:20,393 - INFO - Epoch 71 training loss = 3.863
2023-04-27 07:49:24,395 - INFO - Epoch 72 training loss = 3.967
2023-04-27 07:49:28,398 - INFO - Epoch 73 training loss = 4.101
2023-04-27 07:49:32,400 - INFO - Epoch 74 training loss = 3.786
2023-04-27 07:49:36,402 - INFO - Epoch 75 training loss = 4.007
2023-04-27 07:49:40,405 - INFO - Epoch 76 training loss = 3.805
2023-04-27 07:49:44,410 - INFO - Epoch 77 training loss = 3.542
2023-04-27 07:49:48,417 - INFO - Epoch 78 training loss = 3.633
2023-04-27 07:49:52,420 - INFO - Epoch 79 training loss = 3.936
2023-04-27 07:49:56,424 - INFO - Epoch 80 training loss = 3.531
2023-04-27 07:49:56,736 - INFO - Validation loss = 2.453
2023-04-27 07:49:56,736 - INFO - best model
2023-04-27 07:50:00,762 - INFO - Epoch 81 training loss = 3.892
2023-04-27 07:50:04,769 - INFO - Epoch 82 training loss = 3.455
2023-04-27 07:50:08,772 - INFO - Epoch 83 training loss = 3.206
2023-04-27 07:50:12,777 - INFO - Epoch 84 training loss = 3.448
2023-04-27 07:50:16,784 - INFO - Epoch 85 training loss = 3.488
2023-04-27 07:50:20,786 - INFO - Epoch 86 training loss = 3.163
2023-04-27 07:50:24,788 - INFO - Epoch 87 training loss = 3.211
2023-04-27 07:50:28,790 - INFO - Epoch 88 training loss = 3.32
2023-04-27 07:50:32,791 - INFO - Epoch 89 training loss = 3.263
2023-04-27 07:50:36,794 - INFO - Epoch 90 training loss = 3.161
2023-04-27 07:50:37,106 - INFO - Validation loss = 3.562
2023-04-27 07:50:41,109 - INFO - Epoch 91 training loss = 2.869
2023-04-27 07:50:45,115 - INFO - Epoch 92 training loss = 3.01
2023-04-27 07:50:49,121 - INFO - Epoch 93 training loss = 2.745
2023-04-27 07:50:53,125 - INFO - Epoch 94 training loss = 3.004
2023-04-27 07:50:57,129 - INFO - Epoch 95 training loss = 3.292
2023-04-27 07:51:01,134 - INFO - Epoch 96 training loss = 3.102
2023-04-27 07:51:05,138 - INFO - Epoch 97 training loss = 3.011
2023-04-27 07:51:09,140 - INFO - Epoch 98 training loss = 3.078
2023-04-27 07:51:13,146 - INFO - Epoch 99 training loss = 2.563
2023-04-27 07:51:17,152 - INFO - Epoch 100 training loss = 3.062
2023-04-27 07:51:17,465 - INFO - Validation loss = 3.341
2023-04-27 07:51:21,468 - INFO - Epoch 101 training loss = 2.82
2023-04-27 07:51:25,471 - INFO - Epoch 102 training loss = 2.722
2023-04-27 07:51:29,473 - INFO - Epoch 103 training loss = 2.868
2023-04-27 07:51:33,477 - INFO - Epoch 104 training loss = 2.681
2023-04-27 07:51:37,480 - INFO - Epoch 105 training loss = 2.595
2023-04-27 07:51:41,483 - INFO - Epoch 106 training loss = 2.664
2023-04-27 07:51:45,490 - INFO - Epoch 107 training loss = 2.739
2023-04-27 07:51:49,496 - INFO - Epoch 108 training loss = 2.426
2023-04-27 07:51:53,498 - INFO - Epoch 109 training loss = 2.619
2023-04-27 07:51:57,502 - INFO - Epoch 110 training loss = 2.509
2023-04-27 07:51:57,815 - INFO - Validation loss = 2.754
2023-04-27 07:52:01,819 - INFO - Epoch 111 training loss = 2.413
2023-04-27 07:52:05,822 - INFO - Epoch 112 training loss = 2.516
2023-04-27 07:52:09,824 - INFO - Epoch 113 training loss = 2.672
2023-04-27 07:52:13,829 - INFO - Epoch 114 training loss = 2.507
2023-04-27 07:52:17,837 - INFO - Epoch 115 training loss = 2.339
2023-04-27 07:52:21,838 - INFO - Epoch 116 training loss = 2.473
2023-04-27 07:52:25,839 - INFO - Epoch 117 training loss = 2.43
2023-04-27 07:52:29,842 - INFO - Epoch 118 training loss = 2.336
2023-04-27 07:52:33,846 - INFO - Epoch 119 training loss = 2.195
2023-04-27 07:52:37,849 - INFO - Epoch 120 training loss = 2.386
2023-04-27 07:52:38,161 - INFO - Validation loss = 4.03
2023-04-27 07:52:42,165 - INFO - Epoch 121 training loss = 2.259
2023-04-27 07:52:46,170 - INFO - Epoch 122 training loss = 2.426
2023-04-27 07:52:50,178 - INFO - Epoch 123 training loss = 2.189
2023-04-27 07:52:54,181 - INFO - Epoch 124 training loss = 2.37
2023-04-27 07:52:58,185 - INFO - Epoch 125 training loss = 2.337
2023-04-27 07:53:02,190 - INFO - Epoch 126 training loss = 2.652
2023-04-27 07:53:06,193 - INFO - Epoch 127 training loss = 2.504
2023-04-27 07:53:10,209 - INFO - Epoch 128 training loss = 2.273
2023-04-27 07:53:14,226 - INFO - Epoch 129 training loss = 2.154
2023-04-27 07:53:18,245 - INFO - Epoch 130 training loss = 2.38
2023-04-27 07:53:18,559 - INFO - Validation loss = 2.246
2023-04-27 07:53:18,559 - INFO - best model
2023-04-27 07:53:22,595 - INFO - Epoch 131 training loss = 2.388
2023-04-27 07:53:26,609 - INFO - Epoch 132 training loss = 2.405
2023-04-27 07:53:30,624 - INFO - Epoch 133 training loss = 2.404
2023-04-27 07:53:34,639 - INFO - Epoch 134 training loss = 2.189
2023-04-27 07:53:38,654 - INFO - Epoch 135 training loss = 2.33
2023-04-27 07:53:42,670 - INFO - Epoch 136 training loss = 2.257
2023-04-27 07:53:46,687 - INFO - Epoch 137 training loss = 2.355
2023-04-27 07:53:50,694 - INFO - Epoch 138 training loss = 2.289
2023-04-27 07:53:54,697 - INFO - Epoch 139 training loss = 2.207
2023-04-27 07:53:58,698 - INFO - Epoch 140 training loss = 2.185
2023-04-27 07:53:59,010 - INFO - Validation loss = 4.286
2023-04-27 07:54:03,024 - INFO - Epoch 141 training loss = 2.219
2023-04-27 07:54:07,055 - INFO - Epoch 142 training loss = 1.909
2023-04-27 07:54:11,087 - INFO - Epoch 143 training loss = 1.933
2023-04-27 07:54:15,121 - INFO - Epoch 144 training loss = 1.961
2023-04-27 07:54:19,139 - INFO - Epoch 145 training loss = 2.055
2023-04-27 07:54:23,141 - INFO - Epoch 146 training loss = 2.079
2023-04-27 07:54:27,143 - INFO - Epoch 147 training loss = 2.052
2023-04-27 07:54:31,145 - INFO - Epoch 148 training loss = 2.108
2023-04-27 07:54:35,147 - INFO - Epoch 149 training loss = 2.203
2023-04-27 07:54:39,148 - INFO - Epoch 150 training loss = 1.989
2023-04-27 07:54:39,461 - INFO - Validation loss = 1.423
2023-04-27 07:54:39,462 - INFO - best model
2023-04-27 07:54:43,488 - INFO - Epoch 151 training loss = 1.908
2023-04-27 07:54:47,494 - INFO - Epoch 152 training loss = 1.784
2023-04-27 07:54:51,497 - INFO - Epoch 153 training loss = 2.107
2023-04-27 07:54:55,498 - INFO - Epoch 154 training loss = 2.171
2023-04-27 07:54:59,500 - INFO - Epoch 155 training loss = 2.02
2023-04-27 07:55:03,508 - INFO - Epoch 156 training loss = 1.921
2023-04-27 07:55:07,536 - INFO - Epoch 157 training loss = 2.011
2023-04-27 07:55:11,565 - INFO - Epoch 158 training loss = 1.848
2023-04-27 07:55:15,596 - INFO - Epoch 159 training loss = 1.877
2023-04-27 07:55:19,628 - INFO - Epoch 160 training loss = 1.921
2023-04-27 07:55:19,942 - INFO - Validation loss = 2.831
2023-04-27 07:55:23,964 - INFO - Epoch 161 training loss = 2.046
2023-04-27 07:55:27,965 - INFO - Epoch 162 training loss = 1.964
2023-04-27 07:55:31,967 - INFO - Epoch 163 training loss = 1.634
2023-04-27 07:55:35,968 - INFO - Epoch 164 training loss = 1.926
2023-04-27 07:55:39,971 - INFO - Epoch 165 training loss = 1.92
2023-04-27 07:55:43,975 - INFO - Epoch 166 training loss = 2.009
2023-04-27 07:55:47,981 - INFO - Epoch 167 training loss = 1.966
2023-04-27 07:55:51,983 - INFO - Epoch 168 training loss = 1.779
2023-04-27 07:55:55,985 - INFO - Epoch 169 training loss = 1.837
2023-04-27 07:55:59,986 - INFO - Epoch 170 training loss = 1.87
2023-04-27 07:56:00,299 - INFO - Validation loss = 1.573
2023-04-27 07:56:04,303 - INFO - Epoch 171 training loss = 1.682
2023-04-27 07:56:08,306 - INFO - Epoch 172 training loss = 1.831
2023-04-27 07:56:12,311 - INFO - Epoch 173 training loss = 2.007
2023-04-27 07:56:16,316 - INFO - Epoch 174 training loss = 1.771
2023-04-27 07:56:20,322 - INFO - Epoch 175 training loss = 1.825
2023-04-27 07:56:24,324 - INFO - Epoch 176 training loss = 1.824
2023-04-27 07:56:28,326 - INFO - Epoch 177 training loss = 1.683
2023-04-27 07:56:32,328 - INFO - Epoch 178 training loss = 1.841
2023-04-27 07:56:36,331 - INFO - Epoch 179 training loss = 1.756
2023-04-27 07:56:40,334 - INFO - Epoch 180 training loss = 1.853
2023-04-27 07:56:40,647 - INFO - Validation loss = 1.491
2023-04-27 07:56:44,651 - INFO - Epoch 181 training loss = 1.693
2023-04-27 07:56:48,658 - INFO - Epoch 182 training loss = 1.569
2023-04-27 07:56:52,659 - INFO - Epoch 183 training loss = 1.588
2023-04-27 07:56:56,661 - INFO - Epoch 184 training loss = 1.77
2023-04-27 07:57:00,664 - INFO - Epoch 185 training loss = 1.682
2023-04-27 07:57:04,669 - INFO - Epoch 186 training loss = 1.832
2023-04-27 07:57:08,670 - INFO - Epoch 187 training loss = 1.683
2023-04-27 07:57:12,674 - INFO - Epoch 188 training loss = 1.713
2023-04-27 07:57:16,679 - INFO - Epoch 189 training loss = 1.633
2023-04-27 07:57:20,683 - INFO - Epoch 190 training loss = 1.737
2023-04-27 07:57:20,995 - INFO - Validation loss = 1.288
2023-04-27 07:57:20,995 - INFO - best model
2023-04-27 07:57:25,019 - INFO - Epoch 191 training loss = 1.569
2023-04-27 07:57:29,022 - INFO - Epoch 192 training loss = 1.68
2023-04-27 07:57:33,024 - INFO - Epoch 193 training loss = 1.615
2023-04-27 07:57:37,027 - INFO - Epoch 194 training loss = 1.562
2023-04-27 07:57:41,040 - INFO - Epoch 195 training loss = 1.623
2023-04-27 07:57:45,057 - INFO - Epoch 196 training loss = 1.827
2023-04-27 07:57:49,075 - INFO - Epoch 197 training loss = 1.817
2023-04-27 07:57:53,090 - INFO - Epoch 198 training loss = 1.501
2023-04-27 07:57:57,105 - INFO - Epoch 199 training loss = 1.861
2023-04-27 07:58:01,120 - INFO - Epoch 200 training loss = 1.591
2023-04-27 07:58:01,433 - INFO - Validation loss = 2.792
2023-04-27 07:58:05,450 - INFO - Epoch 201 training loss = 1.721
2023-04-27 07:58:09,465 - INFO - Epoch 202 training loss = 1.574
2023-04-27 07:58:13,482 - INFO - Epoch 203 training loss = 1.612
2023-04-27 07:58:17,500 - INFO - Epoch 204 training loss = 1.578
2023-04-27 07:58:21,516 - INFO - Epoch 205 training loss = 1.526
2023-04-27 07:58:25,530 - INFO - Epoch 206 training loss = 1.562
2023-04-27 07:58:29,545 - INFO - Epoch 207 training loss = 1.455
2023-04-27 07:58:33,559 - INFO - Epoch 208 training loss = 1.653
2023-04-27 07:58:37,573 - INFO - Epoch 209 training loss = 1.513
2023-04-27 07:58:41,588 - INFO - Epoch 210 training loss = 1.604
2023-04-27 07:58:41,901 - INFO - Validation loss = 1.464
2023-04-27 07:58:45,919 - INFO - Epoch 211 training loss = 1.57
2023-04-27 07:58:49,937 - INFO - Epoch 212 training loss = 1.504
2023-04-27 07:58:53,951 - INFO - Epoch 213 training loss = 1.461
2023-04-27 07:58:57,966 - INFO - Epoch 214 training loss = 1.491
2023-04-27 07:59:01,981 - INFO - Epoch 215 training loss = 1.636
2023-04-27 07:59:05,995 - INFO - Epoch 216 training loss = 1.53
2023-04-27 07:59:10,009 - INFO - Epoch 217 training loss = 1.587
2023-04-27 07:59:14,026 - INFO - Epoch 218 training loss = 1.518
2023-04-27 07:59:18,044 - INFO - Epoch 219 training loss = 1.489
2023-04-27 07:59:22,058 - INFO - Epoch 220 training loss = 1.487
2023-04-27 07:59:22,371 - INFO - Validation loss = 1.566
2023-04-27 07:59:26,385 - INFO - Epoch 221 training loss = 1.427
2023-04-27 07:59:30,399 - INFO - Epoch 222 training loss = 1.454
2023-04-27 07:59:34,414 - INFO - Epoch 223 training loss = 1.356
2023-04-27 07:59:38,428 - INFO - Epoch 224 training loss = 1.49
2023-04-27 07:59:42,443 - INFO - Epoch 225 training loss = 1.424
2023-04-27 07:59:46,460 - INFO - Epoch 226 training loss = 1.398
2023-04-27 07:59:50,478 - INFO - Epoch 227 training loss = 1.371
2023-04-27 07:59:54,493 - INFO - Epoch 228 training loss = 1.43
2023-04-27 07:59:58,507 - INFO - Epoch 229 training loss = 1.376
2023-04-27 08:00:02,526 - INFO - Epoch 230 training loss = 1.414
2023-04-27 08:00:02,839 - INFO - Validation loss = 1.489
2023-04-27 08:00:06,856 - INFO - Epoch 231 training loss = 1.447
2023-04-27 08:00:10,861 - INFO - Epoch 232 training loss = 1.303
2023-04-27 08:00:14,866 - INFO - Epoch 233 training loss = 1.548
2023-04-27 08:00:18,872 - INFO - Epoch 234 training loss = 1.355
2023-04-27 08:00:22,874 - INFO - Epoch 235 training loss = 1.288
2023-04-27 08:00:26,877 - INFO - Epoch 236 training loss = 1.43
2023-04-27 08:00:30,879 - INFO - Epoch 237 training loss = 1.315
2023-04-27 08:00:34,891 - INFO - Epoch 238 training loss = 1.426
2023-04-27 08:00:38,906 - INFO - Epoch 239 training loss = 1.381
2023-04-27 08:00:42,921 - INFO - Epoch 240 training loss = 1.357
2023-04-27 08:00:43,235 - INFO - Validation loss = 1.643
2023-04-27 08:00:47,254 - INFO - Epoch 241 training loss = 1.411
2023-04-27 08:00:51,268 - INFO - Epoch 242 training loss = 1.321
2023-04-27 08:00:55,282 - INFO - Epoch 243 training loss = 1.351
2023-04-27 08:00:59,297 - INFO - Epoch 244 training loss = 1.448
2023-04-27 08:01:03,313 - INFO - Epoch 245 training loss = 1.29
2023-04-27 08:01:07,328 - INFO - Epoch 246 training loss = 1.392
2023-04-27 08:01:11,349 - INFO - Epoch 247 training loss = 1.352
2023-04-27 08:01:15,370 - INFO - Epoch 248 training loss = 1.301
2023-04-27 08:01:19,394 - INFO - Epoch 249 training loss = 1.523
2023-04-27 08:01:23,423 - INFO - Epoch 250 training loss = 1.259
2023-04-27 08:01:23,737 - INFO - Validation loss = 1.255
2023-04-27 08:01:23,737 - INFO - best model
2023-04-27 08:01:27,788 - INFO - Epoch 251 training loss = 1.376
2023-04-27 08:01:31,816 - INFO - Epoch 252 training loss = 1.298
2023-04-27 08:01:35,844 - INFO - Epoch 253 training loss = 1.312
2023-04-27 08:01:39,874 - INFO - Epoch 254 training loss = 1.272
2023-04-27 08:01:43,905 - INFO - Epoch 255 training loss = 1.31
2023-04-27 08:01:47,938 - INFO - Epoch 256 training loss = 1.187
2023-04-27 08:01:51,968 - INFO - Epoch 257 training loss = 1.269
2023-04-27 08:01:55,997 - INFO - Epoch 258 training loss = 1.213
2023-04-27 08:02:00,026 - INFO - Epoch 259 training loss = 1.377
2023-04-27 08:02:04,049 - INFO - Epoch 260 training loss = 1.248
2023-04-27 08:02:04,362 - INFO - Validation loss = 0.98
2023-04-27 08:02:04,362 - INFO - best model
2023-04-27 08:02:08,410 - INFO - Epoch 261 training loss = 1.364
2023-04-27 08:02:12,426 - INFO - Epoch 262 training loss = 1.196
2023-04-27 08:02:16,442 - INFO - Epoch 263 training loss = 1.154
2023-04-27 08:02:20,460 - INFO - Epoch 264 training loss = 1.258
2023-04-27 08:02:24,474 - INFO - Epoch 265 training loss = 1.218
2023-04-27 08:02:28,488 - INFO - Epoch 266 training loss = 1.264
2023-04-27 08:02:32,503 - INFO - Epoch 267 training loss = 1.062
2023-04-27 08:02:36,521 - INFO - Epoch 268 training loss = 1.324
2023-04-27 08:02:40,550 - INFO - Epoch 269 training loss = 1.176
2023-04-27 08:02:44,581 - INFO - Epoch 270 training loss = 1.158
2023-04-27 08:02:44,895 - INFO - Validation loss = 1.265
2023-04-27 08:02:48,921 - INFO - Epoch 271 training loss = 1.257
2023-04-27 08:02:52,936 - INFO - Epoch 272 training loss = 1.182
2023-04-27 08:02:56,951 - INFO - Epoch 273 training loss = 1.148
2023-04-27 08:03:00,967 - INFO - Epoch 274 training loss = 1.287
2023-04-27 08:03:04,984 - INFO - Epoch 275 training loss = 1.24
2023-04-27 08:03:09,013 - INFO - Epoch 276 training loss = 1.121
2023-04-27 08:03:13,045 - INFO - Epoch 277 training loss = 1.193
2023-04-27 08:03:17,078 - INFO - Epoch 278 training loss = 1.113
2023-04-27 08:03:21,107 - INFO - Epoch 279 training loss = 1.127
2023-04-27 08:03:25,122 - INFO - Epoch 280 training loss = 1.211
2023-04-27 08:03:25,435 - INFO - Validation loss = 2.235
2023-04-27 08:03:29,451 - INFO - Epoch 281 training loss = 1.224
2023-04-27 08:03:33,466 - INFO - Epoch 282 training loss = 1.118
2023-04-27 08:03:37,482 - INFO - Epoch 283 training loss = 1.221
2023-04-27 08:03:41,497 - INFO - Epoch 284 training loss = 1.087
2023-04-27 08:03:45,515 - INFO - Epoch 285 training loss = 1.074
2023-04-27 08:03:49,534 - INFO - Epoch 286 training loss = 1.136
2023-04-27 08:03:53,549 - INFO - Epoch 287 training loss = 1.153
2023-04-27 08:03:57,564 - INFO - Epoch 288 training loss = 1.225
2023-04-27 08:04:01,580 - INFO - Epoch 289 training loss = 1.108
2023-04-27 08:04:05,597 - INFO - Epoch 290 training loss = 1.064
2023-04-27 08:04:05,910 - INFO - Validation loss = 1.916
2023-04-27 08:04:09,926 - INFO - Epoch 291 training loss = 1.137
2023-04-27 08:04:13,942 - INFO - Epoch 292 training loss = 1.236
2023-04-27 08:04:17,960 - INFO - Epoch 293 training loss = 1.13
2023-04-27 08:04:21,975 - INFO - Epoch 294 training loss = 1.131
2023-04-27 08:04:25,988 - INFO - Epoch 295 training loss = 1.117
2023-04-27 08:04:30,004 - INFO - Epoch 296 training loss = 1.126
2023-04-27 08:04:34,020 - INFO - Epoch 297 training loss = 1.169
2023-04-27 08:04:38,035 - INFO - Epoch 298 training loss = 1.117
2023-04-27 08:04:42,051 - INFO - Epoch 299 training loss = 1.045
2023-04-27 08:04:46,071 - INFO - Epoch 300 training loss = 1.062
2023-04-27 08:04:46,383 - INFO - Validation loss = 1.237
2023-04-27 08:04:50,402 - INFO - Epoch 301 training loss = 1.045
2023-04-27 08:04:54,417 - INFO - Epoch 302 training loss = 1.114
2023-04-27 08:04:58,433 - INFO - Epoch 303 training loss = 1.077
2023-04-27 08:05:02,449 - INFO - Epoch 304 training loss = 1.056
2023-04-27 08:05:06,465 - INFO - Epoch 305 training loss = 1.069
2023-04-27 08:05:10,482 - INFO - Epoch 306 training loss = 1.101
2023-04-27 08:05:14,500 - INFO - Epoch 307 training loss = 1.161
2023-04-27 08:05:18,518 - INFO - Epoch 308 training loss = 1.195
2023-04-27 08:05:22,533 - INFO - Epoch 309 training loss = 1.071
2023-04-27 08:05:26,547 - INFO - Epoch 310 training loss = 1.077
2023-04-27 08:05:26,860 - INFO - Validation loss = 1.164
2023-04-27 08:05:30,875 - INFO - Epoch 311 training loss = 1.048
2023-04-27 08:05:34,891 - INFO - Epoch 312 training loss = 1.101
2023-04-27 08:05:38,905 - INFO - Epoch 313 training loss = 1.004
2023-04-27 08:05:42,921 - INFO - Epoch 314 training loss = 1.062
2023-04-27 08:05:46,939 - INFO - Epoch 315 training loss = 1.099
2023-04-27 08:05:50,954 - INFO - Epoch 316 training loss = 1.124
2023-04-27 08:05:54,969 - INFO - Epoch 317 training loss = 1.069
2023-04-27 08:05:58,985 - INFO - Epoch 318 training loss = 0.9365
2023-04-27 08:06:03,002 - INFO - Epoch 319 training loss = 1.064
2023-04-27 08:06:07,015 - INFO - Epoch 320 training loss = 1.027
2023-04-27 08:06:07,329 - INFO - Validation loss = 2.07
2023-04-27 08:06:11,344 - INFO - Epoch 321 training loss = 1.069
2023-04-27 08:06:15,361 - INFO - Epoch 322 training loss = 1.029
2023-04-27 08:06:19,379 - INFO - Epoch 323 training loss = 0.9368
2023-04-27 08:06:23,393 - INFO - Epoch 324 training loss = 0.9489
2023-04-27 08:06:27,407 - INFO - Epoch 325 training loss = 1.053
2023-04-27 08:06:31,421 - INFO - Epoch 326 training loss = 1.081
2023-04-27 08:06:35,437 - INFO - Epoch 327 training loss = 1.021
2023-04-27 08:06:39,452 - INFO - Epoch 328 training loss = 0.9259
2023-04-27 08:06:43,470 - INFO - Epoch 329 training loss = 1.013
2023-04-27 08:06:47,489 - INFO - Epoch 330 training loss = 0.9897
2023-04-27 08:06:47,814 - INFO - Validation loss = 0.9268
2023-04-27 08:06:47,814 - INFO - best model
2023-04-27 08:06:51,853 - INFO - Epoch 331 training loss = 1.046
2023-04-27 08:06:55,869 - INFO - Epoch 332 training loss = 1.042
2023-04-27 08:06:59,884 - INFO - Epoch 333 training loss = 1.092
2023-04-27 08:07:03,903 - INFO - Epoch 334 training loss = 0.9896
2023-04-27 08:07:07,917 - INFO - Epoch 335 training loss = 0.9639
2023-04-27 08:07:11,935 - INFO - Epoch 336 training loss = 1.028
2023-04-27 08:07:15,953 - INFO - Epoch 337 training loss = 1.001
2023-04-27 08:07:19,971 - INFO - Epoch 338 training loss = 1.024
2023-04-27 08:07:23,986 - INFO - Epoch 339 training loss = 0.9726
2023-04-27 08:07:28,000 - INFO - Epoch 340 training loss = 0.9167
2023-04-27 08:07:28,313 - INFO - Validation loss = 0.5828
2023-04-27 08:07:28,313 - INFO - best model
2023-04-27 08:07:32,350 - INFO - Epoch 341 training loss = 1.027
2023-04-27 08:07:36,365 - INFO - Epoch 342 training loss = 0.8201
2023-04-27 08:07:40,380 - INFO - Epoch 343 training loss = 0.9229
2023-04-27 08:07:44,397 - INFO - Epoch 344 training loss = 0.9626
2023-04-27 08:07:48,415 - INFO - Epoch 345 training loss = 1.015
2023-04-27 08:07:52,431 - INFO - Epoch 346 training loss = 0.939
2023-04-27 08:07:56,447 - INFO - Epoch 347 training loss = 1.001
2023-04-27 08:08:00,463 - INFO - Epoch 348 training loss = 0.9589
2023-04-27 08:08:04,479 - INFO - Epoch 349 training loss = 0.9774
2023-04-27 08:08:08,494 - INFO - Epoch 350 training loss = 1.032
2023-04-27 08:08:08,807 - INFO - Validation loss = 1.71
2023-04-27 08:08:12,824 - INFO - Epoch 351 training loss = 0.9735
2023-04-27 08:08:16,843 - INFO - Epoch 352 training loss = 0.9181
2023-04-27 08:08:20,859 - INFO - Epoch 353 training loss = 0.9528
2023-04-27 08:08:24,874 - INFO - Epoch 354 training loss = 0.9132
2023-04-27 08:08:28,888 - INFO - Epoch 355 training loss = 0.9888
2023-04-27 08:08:32,907 - INFO - Epoch 356 training loss = 0.8197
2023-04-27 08:08:36,935 - INFO - Epoch 357 training loss = 0.9195
2023-04-27 08:08:40,963 - INFO - Epoch 358 training loss = 0.9233
2023-04-27 08:08:44,996 - INFO - Epoch 359 training loss = 0.8664
2023-04-27 08:08:49,030 - INFO - Epoch 360 training loss = 0.8786
2023-04-27 08:08:49,344 - INFO - Validation loss = 0.797
2023-04-27 08:08:53,373 - INFO - Epoch 361 training loss = 0.8802
2023-04-27 08:08:57,402 - INFO - Epoch 362 training loss = 0.9581
2023-04-27 08:09:01,432 - INFO - Epoch 363 training loss = 0.9665
2023-04-27 08:09:05,450 - INFO - Epoch 364 training loss = 0.9304
2023-04-27 08:09:09,465 - INFO - Epoch 365 training loss = 0.8362
2023-04-27 08:09:13,481 - INFO - Epoch 366 training loss = 0.9058
2023-04-27 08:09:17,499 - INFO - Epoch 367 training loss = 0.8744
2023-04-27 08:09:21,515 - INFO - Epoch 368 training loss = 0.933
2023-04-27 08:09:25,529 - INFO - Epoch 369 training loss = 0.8817
2023-04-27 08:09:29,543 - INFO - Epoch 370 training loss = 0.9009
2023-04-27 08:09:29,856 - INFO - Validation loss = 0.6559
2023-04-27 08:09:33,877 - INFO - Epoch 371 training loss = 0.8952
2023-04-27 08:09:37,905 - INFO - Epoch 372 training loss = 0.8859
2023-04-27 08:09:41,928 - INFO - Epoch 373 training loss = 0.8993
2023-04-27 08:09:45,946 - INFO - Epoch 374 training loss = 0.8199
2023-04-27 08:09:49,964 - INFO - Epoch 375 training loss = 0.898
2023-04-27 08:09:53,979 - INFO - Epoch 376 training loss = 0.8779
2023-04-27 08:09:57,994 - INFO - Epoch 377 training loss = 0.775
2023-04-27 08:10:02,013 - INFO - Epoch 378 training loss = 0.8238
2023-04-27 08:10:06,029 - INFO - Epoch 379 training loss = 0.8602
2023-04-27 08:10:10,044 - INFO - Epoch 380 training loss = 0.8355
2023-04-27 08:10:10,357 - INFO - Validation loss = 0.6576
2023-04-27 08:10:14,374 - INFO - Epoch 381 training loss = 0.872
2023-04-27 08:10:18,393 - INFO - Epoch 382 training loss = 0.8362
2023-04-27 08:10:22,408 - INFO - Epoch 383 training loss = 0.8045
2023-04-27 08:10:26,422 - INFO - Epoch 384 training loss = 0.8059
2023-04-27 08:10:30,436 - INFO - Epoch 385 training loss = 0.7919
2023-04-27 08:10:34,452 - INFO - Epoch 386 training loss = 0.754
2023-04-27 08:10:38,467 - INFO - Epoch 387 training loss = 0.8375
2023-04-27 08:10:42,482 - INFO - Epoch 388 training loss = 0.8304
2023-04-27 08:10:46,499 - INFO - Epoch 389 training loss = 0.8058
2023-04-27 08:10:50,518 - INFO - Epoch 390 training loss = 0.7837
2023-04-27 08:10:50,831 - INFO - Validation loss = 0.7548
2023-04-27 08:10:54,846 - INFO - Epoch 391 training loss = 0.7952
2023-04-27 08:10:58,861 - INFO - Epoch 392 training loss = 0.7924
2023-04-27 08:11:02,878 - INFO - Epoch 393 training loss = 0.8223
2023-04-27 08:11:06,892 - INFO - Epoch 394 training loss = 0.792
2023-04-27 08:11:10,908 - INFO - Epoch 395 training loss = 0.7836
2023-04-27 08:11:14,924 - INFO - Epoch 396 training loss = 0.6868
2023-04-27 08:11:18,941 - INFO - Epoch 397 training loss = 0.7705
2023-04-27 08:11:22,967 - INFO - Epoch 398 training loss = 0.9285
2023-04-27 08:11:26,995 - INFO - Epoch 399 training loss = 0.8538
2023-04-27 08:11:31,023 - INFO - Epoch 400 training loss = 0.7434
2023-04-27 08:11:31,337 - INFO - Validation loss = 0.8864
2023-04-27 08:11:35,367 - INFO - Epoch 401 training loss = 0.7793
2023-04-27 08:11:39,395 - INFO - Epoch 402 training loss = 0.6995
2023-04-27 08:11:43,425 - INFO - Epoch 403 training loss = 0.7568
2023-04-27 08:11:47,454 - INFO - Epoch 404 training loss = 0.7752
2023-04-27 08:11:51,459 - INFO - Epoch 405 training loss = 0.7341
2023-04-27 08:11:55,463 - INFO - Epoch 406 training loss = 0.764
2023-04-27 08:11:59,468 - INFO - Epoch 407 training loss = 0.7666
2023-04-27 08:12:03,499 - INFO - Epoch 408 training loss = 0.7343
2023-04-27 08:12:07,527 - INFO - Epoch 409 training loss = 0.6927
2023-04-27 08:12:11,558 - INFO - Epoch 410 training loss = 0.7748
2023-04-27 08:12:11,871 - INFO - Validation loss = 0.6493
2023-04-27 08:12:15,903 - INFO - Epoch 411 training loss = 0.8496
2023-04-27 08:12:19,915 - INFO - Epoch 412 training loss = 0.6758
2023-04-27 08:12:23,918 - INFO - Epoch 413 training loss = 0.7635
2023-04-27 08:12:27,921 - INFO - Epoch 414 training loss = 0.7086
2023-04-27 08:12:31,925 - INFO - Epoch 415 training loss = 0.7685
2023-04-27 08:12:35,929 - INFO - Epoch 416 training loss = 0.722
2023-04-27 08:12:39,932 - INFO - Epoch 417 training loss = 0.7682
2023-04-27 08:12:43,938 - INFO - Epoch 418 training loss = 0.7945
2023-04-27 08:12:47,945 - INFO - Epoch 419 training loss = 0.802
2023-04-27 08:12:51,950 - INFO - Epoch 420 training loss = 0.712
2023-04-27 08:12:52,263 - INFO - Validation loss = 0.8433
2023-04-27 08:12:56,267 - INFO - Epoch 421 training loss = 0.711
2023-04-27 08:13:00,271 - INFO - Epoch 422 training loss = 0.6592
2023-04-27 08:13:04,276 - INFO - Epoch 423 training loss = 0.7661
2023-04-27 08:13:08,278 - INFO - Epoch 424 training loss = 0.7049
2023-04-27 08:13:12,285 - INFO - Epoch 425 training loss = 0.6671
2023-04-27 08:13:16,292 - INFO - Epoch 426 training loss = 0.6547
2023-04-27 08:13:20,297 - INFO - Epoch 427 training loss = 0.7192
2023-04-27 08:13:24,302 - INFO - Epoch 428 training loss = 0.6738
2023-04-27 08:13:28,305 - INFO - Epoch 429 training loss = 0.7249
2023-04-27 08:13:32,309 - INFO - Epoch 430 training loss = 0.7817
2023-04-27 08:13:32,623 - INFO - Validation loss = 0.5388
2023-04-27 08:13:32,623 - INFO - best model
2023-04-27 08:13:36,649 - INFO - Epoch 431 training loss = 0.6618
2023-04-27 08:13:40,653 - INFO - Epoch 432 training loss = 0.686
2023-04-27 08:13:44,661 - INFO - Epoch 433 training loss = 0.6711
2023-04-27 08:13:48,668 - INFO - Epoch 434 training loss = 0.7482
2023-04-27 08:13:52,673 - INFO - Epoch 435 training loss = 0.7098
2023-04-27 08:13:56,677 - INFO - Epoch 436 training loss = 0.6867
2023-04-27 08:14:00,693 - INFO - Epoch 437 training loss = 0.6832
2023-04-27 08:14:04,723 - INFO - Epoch 438 training loss = 0.6885
2023-04-27 08:14:08,752 - INFO - Epoch 439 training loss = 0.62
2023-04-27 08:14:12,783 - INFO - Epoch 440 training loss = 0.6595
2023-04-27 08:14:13,096 - INFO - Validation loss = 0.7618
2023-04-27 08:14:17,128 - INFO - Epoch 441 training loss = 0.6785
2023-04-27 08:14:21,158 - INFO - Epoch 442 training loss = 0.5903
2023-04-27 08:14:25,186 - INFO - Epoch 443 training loss = 0.6375
2023-04-27 08:14:29,214 - INFO - Epoch 444 training loss = 0.6583
2023-04-27 08:14:33,235 - INFO - Epoch 445 training loss = 0.6428
2023-04-27 08:14:37,238 - INFO - Epoch 446 training loss = 0.6067
2023-04-27 08:14:41,241 - INFO - Epoch 447 training loss = 0.6589
2023-04-27 08:14:45,246 - INFO - Epoch 448 training loss = 0.5993
2023-04-27 08:14:49,253 - INFO - Epoch 449 training loss = 0.6332
2023-04-27 08:14:53,256 - INFO - Epoch 450 training loss = 0.6176
2023-04-27 08:14:53,569 - INFO - Validation loss = 0.7459
2023-04-27 08:14:57,573 - INFO - Epoch 451 training loss = 0.5614
2023-04-27 08:15:01,578 - INFO - Epoch 452 training loss = 0.5694
2023-04-27 08:15:05,582 - INFO - Epoch 453 training loss = 0.6413
2023-04-27 08:15:09,586 - INFO - Epoch 454 training loss = 0.6711
2023-04-27 08:15:13,592 - INFO - Epoch 455 training loss = 0.6306
2023-04-27 08:15:17,600 - INFO - Epoch 456 training loss = 0.6758
2023-04-27 08:15:21,605 - INFO - Epoch 457 training loss = 0.6144
2023-04-27 08:15:25,608 - INFO - Epoch 458 training loss = 0.6134
2023-04-27 08:15:29,611 - INFO - Epoch 459 training loss = 0.6227
2023-04-27 08:15:33,615 - INFO - Epoch 460 training loss = 0.6297
2023-04-27 08:15:33,928 - INFO - Validation loss = 0.5694
2023-04-27 08:15:37,933 - INFO - Epoch 461 training loss = 0.6041
2023-04-27 08:15:41,938 - INFO - Epoch 462 training loss = 0.544
2023-04-27 08:15:45,945 - INFO - Epoch 463 training loss = 0.6027
2023-04-27 08:15:49,951 - INFO - Epoch 464 training loss = 0.5707
2023-04-27 08:15:53,956 - INFO - Epoch 465 training loss = 0.6504
2023-04-27 08:15:57,961 - INFO - Epoch 466 training loss = 0.5503
2023-04-27 08:16:01,967 - INFO - Epoch 467 training loss = 0.5686
2023-04-27 08:16:05,974 - INFO - Epoch 468 training loss = 0.5529
2023-04-27 08:16:09,979 - INFO - Epoch 469 training loss = 0.6262
2023-04-27 08:16:13,985 - INFO - Epoch 470 training loss = 0.5429
2023-04-27 08:16:14,298 - INFO - Validation loss = 0.6078
2023-04-27 08:16:18,307 - INFO - Epoch 471 training loss = 0.5614
2023-04-27 08:16:22,312 - INFO - Epoch 472 training loss = 0.5681
2023-04-27 08:16:26,317 - INFO - Epoch 473 training loss = 0.5889
2023-04-27 08:16:30,321 - INFO - Epoch 474 training loss = 0.5651
2023-04-27 08:16:34,326 - INFO - Epoch 475 training loss = 0.5542
2023-04-27 08:16:38,330 - INFO - Epoch 476 training loss = 0.5817
2023-04-27 08:16:42,337 - INFO - Epoch 477 training loss = 0.5521
2023-04-27 08:16:46,344 - INFO - Epoch 478 training loss = 0.5838
2023-04-27 08:16:50,350 - INFO - Epoch 479 training loss = 0.5375
2023-04-27 08:16:54,355 - INFO - Epoch 480 training loss = 0.57
2023-04-27 08:16:54,668 - INFO - Validation loss = 0.7664
2023-04-27 08:16:58,673 - INFO - Epoch 481 training loss = 0.565
2023-04-27 08:17:02,680 - INFO - Epoch 482 training loss = 0.5109
2023-04-27 08:17:06,684 - INFO - Epoch 483 training loss = 0.6242
2023-04-27 08:17:10,689 - INFO - Epoch 484 training loss = 0.5474
2023-04-27 08:17:14,696 - INFO - Epoch 485 training loss = 0.5489
2023-04-27 08:17:18,702 - INFO - Epoch 486 training loss = 0.5179
2023-04-27 08:17:22,706 - INFO - Epoch 487 training loss = 0.5418
2023-04-27 08:17:26,709 - INFO - Epoch 488 training loss = 0.5366
2023-04-27 08:17:30,713 - INFO - Epoch 489 training loss = 0.6043
2023-04-27 08:17:34,718 - INFO - Epoch 490 training loss = 0.5023
2023-04-27 08:17:35,030 - INFO - Validation loss = 0.5793
2023-04-27 08:17:39,034 - INFO - Epoch 491 training loss = 0.5467
2023-04-27 08:17:43,038 - INFO - Epoch 492 training loss = 0.5246
2023-04-27 08:17:47,045 - INFO - Epoch 493 training loss = 0.5459
2023-04-27 08:17:51,050 - INFO - Epoch 494 training loss = 0.5421
2023-04-27 08:17:55,053 - INFO - Epoch 495 training loss = 0.5504
2023-04-27 08:17:59,057 - INFO - Epoch 496 training loss = 0.5275
2023-04-27 08:18:03,062 - INFO - Epoch 497 training loss = 0.5173
2023-04-27 08:18:07,065 - INFO - Epoch 498 training loss = 0.472
2023-04-27 08:18:11,070 - INFO - Epoch 499 training loss = 0.5432
2023-04-27 08:18:15,075 - INFO - Epoch 500 training loss = 0.5142
2023-04-27 08:18:15,388 - INFO - Validation loss = 0.5716
2023-04-27 08:18:19,395 - INFO - Epoch 501 training loss = 0.5313
2023-04-27 08:18:23,398 - INFO - Epoch 502 training loss = 0.4385
2023-04-27 08:18:27,402 - INFO - Epoch 503 training loss = 0.519
2023-04-27 08:18:31,405 - INFO - Epoch 504 training loss = 0.5174
2023-04-27 08:18:35,409 - INFO - Epoch 505 training loss = 0.4653
2023-04-27 08:18:39,412 - INFO - Epoch 506 training loss = 0.5669
2023-04-27 08:18:43,417 - INFO - Epoch 507 training loss = 0.4974
2023-04-27 08:18:47,424 - INFO - Epoch 508 training loss = 0.5285
2023-04-27 08:18:51,429 - INFO - Epoch 509 training loss = 0.5041
2023-04-27 08:18:55,431 - INFO - Epoch 510 training loss = 0.4712
2023-04-27 08:18:55,744 - INFO - Validation loss = 0.4418
2023-04-27 08:18:55,744 - INFO - best model
2023-04-27 08:18:59,770 - INFO - Epoch 511 training loss = 0.4822
2023-04-27 08:19:03,775 - INFO - Epoch 512 training loss = 0.488
2023-04-27 08:19:07,779 - INFO - Epoch 513 training loss = 0.4657
2023-04-27 08:19:11,784 - INFO - Epoch 514 training loss = 0.5075
2023-04-27 08:19:15,790 - INFO - Epoch 515 training loss = 0.479
2023-04-27 08:19:19,795 - INFO - Epoch 516 training loss = 0.4816
2023-04-27 08:19:23,797 - INFO - Epoch 517 training loss = 0.4472
2023-04-27 08:19:27,799 - INFO - Epoch 518 training loss = 0.4726
2023-04-27 08:19:31,802 - INFO - Epoch 519 training loss = 0.4375
2023-04-27 08:19:35,805 - INFO - Epoch 520 training loss = 0.4793
2023-04-27 08:19:36,119 - INFO - Validation loss = 0.765
2023-04-27 08:19:40,122 - INFO - Epoch 521 training loss = 0.4716
2023-04-27 08:19:44,127 - INFO - Epoch 522 training loss = 0.4161
2023-04-27 08:19:48,133 - INFO - Epoch 523 training loss = 0.4332
2023-04-27 08:19:52,136 - INFO - Epoch 524 training loss = 0.4439
2023-04-27 08:19:56,138 - INFO - Epoch 525 training loss = 0.4344
2023-04-27 08:20:00,142 - INFO - Epoch 526 training loss = 0.4999
2023-04-27 08:20:04,149 - INFO - Epoch 527 training loss = 0.4517
2023-04-27 08:20:08,151 - INFO - Epoch 528 training loss = 0.425
2023-04-27 08:20:12,156 - INFO - Epoch 529 training loss = 0.4787
2023-04-27 08:20:16,161 - INFO - Epoch 530 training loss = 0.4447
2023-04-27 08:20:16,473 - INFO - Validation loss = 0.4216
2023-04-27 08:20:16,474 - INFO - best model
2023-04-27 08:20:20,500 - INFO - Epoch 531 training loss = 0.4561
2023-04-27 08:20:24,501 - INFO - Epoch 532 training loss = 0.3884
2023-04-27 08:20:28,503 - INFO - Epoch 533 training loss = 0.4183
2023-04-27 08:20:32,506 - INFO - Epoch 534 training loss = 0.4414
2023-04-27 08:20:36,509 - INFO - Epoch 535 training loss = 0.4336
2023-04-27 08:20:40,512 - INFO - Epoch 536 training loss = 0.4087
2023-04-27 08:20:44,516 - INFO - Epoch 537 training loss = 0.3842
2023-04-27 08:20:48,522 - INFO - Epoch 538 training loss = 0.3931
2023-04-27 08:20:52,525 - INFO - Epoch 539 training loss = 0.43
2023-04-27 08:20:56,528 - INFO - Epoch 540 training loss = 0.4127
2023-04-27 08:20:56,841 - INFO - Validation loss = 0.7449
2023-04-27 08:21:00,846 - INFO - Epoch 541 training loss = 0.4403
2023-04-27 08:21:04,852 - INFO - Epoch 542 training loss = 0.4635
2023-04-27 08:21:08,855 - INFO - Epoch 543 training loss = 0.4362
2023-04-27 08:21:12,861 - INFO - Epoch 544 training loss = 0.3892
2023-04-27 08:21:16,868 - INFO - Epoch 545 training loss = 0.3907
2023-04-27 08:21:20,874 - INFO - Epoch 546 training loss = 0.4083
2023-04-27 08:21:24,877 - INFO - Epoch 547 training loss = 0.3985
2023-04-27 08:21:28,881 - INFO - Epoch 548 training loss = 0.3799
2023-04-27 08:21:32,885 - INFO - Epoch 549 training loss = 0.4298
2023-04-27 08:21:36,889 - INFO - Epoch 550 training loss = 0.3502
2023-04-27 08:21:37,201 - INFO - Validation loss = 0.3727
2023-04-27 08:21:37,202 - INFO - best model
2023-04-27 08:21:41,227 - INFO - Epoch 551 training loss = 0.3743
2023-04-27 08:21:45,233 - INFO - Epoch 552 training loss = 0.3991
2023-04-27 08:21:49,239 - INFO - Epoch 553 training loss = 0.3907
2023-04-27 08:21:53,242 - INFO - Epoch 554 training loss = 0.3855
2023-04-27 08:21:57,245 - INFO - Epoch 555 training loss = 0.3735
2023-04-27 08:22:01,248 - INFO - Epoch 556 training loss = 0.3725
2023-04-27 08:22:05,252 - INFO - Epoch 557 training loss = 0.3859
2023-04-27 08:22:09,256 - INFO - Epoch 558 training loss = 0.3925
2023-04-27 08:22:13,262 - INFO - Epoch 559 training loss = 0.372
2023-04-27 08:22:17,268 - INFO - Epoch 560 training loss = 0.3572
2023-04-27 08:22:17,581 - INFO - Validation loss = 0.592
2023-04-27 08:22:21,587 - INFO - Epoch 561 training loss = 0.3843
2023-04-27 08:22:25,590 - INFO - Epoch 562 training loss = 0.37
2023-04-27 08:22:29,594 - INFO - Epoch 563 training loss = 0.3491
2023-04-27 08:22:33,599 - INFO - Epoch 564 training loss = 0.3787
2023-04-27 08:22:37,603 - INFO - Epoch 565 training loss = 0.34
2023-04-27 08:22:41,608 - INFO - Epoch 566 training loss = 0.3175
2023-04-27 08:22:45,614 - INFO - Epoch 567 training loss = 0.362
2023-04-27 08:22:49,620 - INFO - Epoch 568 training loss = 0.3208
2023-04-27 08:22:53,625 - INFO - Epoch 569 training loss = 0.3515
2023-04-27 08:22:57,629 - INFO - Epoch 570 training loss = 0.3387
2023-04-27 08:22:57,942 - INFO - Validation loss = 0.4282
2023-04-27 08:23:01,949 - INFO - Epoch 571 training loss = 0.342
2023-04-27 08:23:05,954 - INFO - Epoch 572 training loss = 0.3134
2023-04-27 08:23:09,960 - INFO - Epoch 573 training loss = 0.3278
2023-04-27 08:23:13,967 - INFO - Epoch 574 training loss = 0.3052
2023-04-27 08:23:17,975 - INFO - Epoch 575 training loss = 0.3013
2023-04-27 08:23:21,980 - INFO - Epoch 576 training loss = 0.3217
2023-04-27 08:23:25,985 - INFO - Epoch 577 training loss = 0.3008
2023-04-27 08:23:29,989 - INFO - Epoch 578 training loss = 0.3564
2023-04-27 08:23:33,994 - INFO - Epoch 579 training loss = 0.3197
2023-04-27 08:23:37,998 - INFO - Epoch 580 training loss = 0.3171
2023-04-27 08:23:38,311 - INFO - Validation loss = 0.4655
2023-04-27 08:23:42,316 - INFO - Epoch 581 training loss = 0.3732
2023-04-27 08:23:46,322 - INFO - Epoch 582 training loss = 0.3359
2023-04-27 08:23:50,327 - INFO - Epoch 583 training loss = 0.309
2023-04-27 08:23:54,330 - INFO - Epoch 584 training loss = 0.3108
2023-04-27 08:23:58,334 - INFO - Epoch 585 training loss = 0.3005
2023-04-27 08:24:02,338 - INFO - Epoch 586 training loss = 0.3035
2023-04-27 08:24:06,342 - INFO - Epoch 587 training loss = 0.315
2023-04-27 08:24:10,346 - INFO - Epoch 588 training loss = 0.2843
2023-04-27 08:24:14,351 - INFO - Epoch 589 training loss = 0.31
2023-04-27 08:24:18,357 - INFO - Epoch 590 training loss = 0.3279
2023-04-27 08:24:18,669 - INFO - Validation loss = 0.4432
2023-04-27 08:24:22,673 - INFO - Epoch 591 training loss = 0.3435
2023-04-27 08:24:26,675 - INFO - Epoch 592 training loss = 0.2948
2023-04-27 08:24:30,678 - INFO - Epoch 593 training loss = 0.2928
2023-04-27 08:24:34,682 - INFO - Epoch 594 training loss = 0.2917
2023-04-27 08:24:38,685 - INFO - Epoch 595 training loss = 0.3192
2023-04-27 08:24:42,689 - INFO - Epoch 596 training loss = 0.298
2023-04-27 08:24:46,695 - INFO - Epoch 597 training loss = 0.3263
2023-04-27 08:24:50,701 - INFO - Epoch 598 training loss = 0.2832
2023-04-27 08:24:54,705 - INFO - Epoch 599 training loss = 0.3067
2023-04-27 08:24:58,709 - INFO - Epoch 600 training loss = 0.3138
2023-04-27 08:24:59,022 - INFO - Validation loss = 0.44
2023-04-27 08:25:03,027 - INFO - Epoch 601 training loss = 0.2772
2023-04-27 08:25:07,030 - INFO - Epoch 602 training loss = 0.2669
2023-04-27 08:25:11,034 - INFO - Epoch 603 training loss = 0.2811
2023-04-27 08:25:15,040 - INFO - Epoch 604 training loss = 0.2897
2023-04-27 08:25:19,045 - INFO - Epoch 605 training loss = 0.2841
2023-04-27 08:25:23,063 - INFO - Epoch 606 training loss = 0.2776
2023-04-27 08:25:27,077 - INFO - Epoch 607 training loss = 0.2801
2023-04-27 08:25:31,091 - INFO - Epoch 608 training loss = 0.2706
2023-04-27 08:25:35,106 - INFO - Epoch 609 training loss = 0.2818
2023-04-27 08:25:39,121 - INFO - Epoch 610 training loss = 0.2428
2023-04-27 08:25:39,434 - INFO - Validation loss = 0.3033
2023-04-27 08:25:39,434 - INFO - best model
2023-04-27 08:25:43,473 - INFO - Epoch 611 training loss = 0.2975
2023-04-27 08:25:47,491 - INFO - Epoch 612 training loss = 0.2809
2023-04-27 08:25:51,506 - INFO - Epoch 613 training loss = 0.2719
2023-04-27 08:25:55,521 - INFO - Epoch 614 training loss = 0.2392
2023-04-27 08:25:59,536 - INFO - Epoch 615 training loss = 0.2558
2023-04-27 08:26:03,552 - INFO - Epoch 616 training loss = 0.2575
2023-04-27 08:26:07,566 - INFO - Epoch 617 training loss = 0.2741
2023-04-27 08:26:11,581 - INFO - Epoch 618 training loss = 0.2498
2023-04-27 08:26:15,597 - INFO - Epoch 619 training loss = 0.2876
2023-04-27 08:26:19,614 - INFO - Epoch 620 training loss = 0.2324
2023-04-27 08:26:19,928 - INFO - Validation loss = 0.2262
2023-04-27 08:26:19,928 - INFO - best model
2023-04-27 08:26:23,965 - INFO - Epoch 621 training loss = 0.297
2023-04-27 08:26:27,980 - INFO - Epoch 622 training loss = 0.2462
2023-04-27 08:26:31,996 - INFO - Epoch 623 training loss = 0.2444
2023-04-27 08:26:36,012 - INFO - Epoch 624 training loss = 0.2637
2023-04-27 08:26:40,028 - INFO - Epoch 625 training loss = 0.2469
2023-04-27 08:26:44,045 - INFO - Epoch 626 training loss = 0.2339
2023-04-27 08:26:48,064 - INFO - Epoch 627 training loss = 0.2506
2023-04-27 08:26:52,081 - INFO - Epoch 628 training loss = 0.2432
2023-04-27 08:26:56,097 - INFO - Epoch 629 training loss = 0.2374
2023-04-27 08:27:00,113 - INFO - Epoch 630 training loss = 0.2185
2023-04-27 08:27:00,427 - INFO - Validation loss = 0.2359
2023-04-27 08:27:04,444 - INFO - Epoch 631 training loss = 0.2341
2023-04-27 08:27:08,459 - INFO - Epoch 632 training loss = 0.2302
2023-04-27 08:27:12,478 - INFO - Epoch 633 training loss = 0.2628
2023-04-27 08:27:16,496 - INFO - Epoch 634 training loss = 0.2279
2023-04-27 08:27:20,513 - INFO - Epoch 635 training loss = 0.2171
2023-04-27 08:27:24,528 - INFO - Epoch 636 training loss = 0.2213
2023-04-27 08:27:28,543 - INFO - Epoch 637 training loss = 0.2228
2023-04-27 08:27:32,559 - INFO - Epoch 638 training loss = 0.2105
2023-04-27 08:27:36,575 - INFO - Epoch 639 training loss = 0.2287
2023-04-27 08:27:40,591 - INFO - Epoch 640 training loss = 0.2197
2023-04-27 08:27:40,904 - INFO - Validation loss = 0.25
2023-04-27 08:27:44,922 - INFO - Epoch 641 training loss = 0.2125
2023-04-27 08:27:48,939 - INFO - Epoch 642 training loss = 0.2174
2023-04-27 08:27:52,954 - INFO - Epoch 643 training loss = 0.2012
2023-04-27 08:27:56,968 - INFO - Epoch 644 training loss = 0.2178
2023-04-27 08:28:00,984 - INFO - Epoch 645 training loss = 0.2408
2023-04-27 08:28:04,999 - INFO - Epoch 646 training loss = 0.1856
2023-04-27 08:28:09,013 - INFO - Epoch 647 training loss = 0.2024
2023-04-27 08:28:13,021 - INFO - Epoch 648 training loss = 0.2184
2023-04-27 08:28:17,026 - INFO - Epoch 649 training loss = 0.183
2023-04-27 08:28:21,028 - INFO - Epoch 650 training loss = 0.2124
2023-04-27 08:28:21,341 - INFO - Validation loss = 0.2986
2023-04-27 08:28:25,343 - INFO - Epoch 651 training loss = 0.193
2023-04-27 08:28:29,345 - INFO - Epoch 652 training loss = 0.1772
2023-04-27 08:28:33,346 - INFO - Epoch 653 training loss = 0.2269
2023-04-27 08:28:37,364 - INFO - Epoch 654 training loss = 0.1907
2023-04-27 08:28:41,393 - INFO - Epoch 655 training loss = 0.1868
2023-04-27 08:28:45,423 - INFO - Epoch 656 training loss = 0.1768
2023-04-27 08:28:49,453 - INFO - Epoch 657 training loss = 0.2012
2023-04-27 08:28:53,482 - INFO - Epoch 658 training loss = 0.1863
2023-04-27 08:28:57,509 - INFO - Epoch 659 training loss = 0.1879
2023-04-27 08:29:01,538 - INFO - Epoch 660 training loss = 0.1979
2023-04-27 08:29:01,851 - INFO - Validation loss = 0.1948
2023-04-27 08:29:01,851 - INFO - best model
2023-04-27 08:29:05,904 - INFO - Epoch 661 training loss = 0.1801
2023-04-27 08:29:09,933 - INFO - Epoch 662 training loss = 0.1642
2023-04-27 08:29:13,965 - INFO - Epoch 663 training loss = 0.1803
2023-04-27 08:29:17,997 - INFO - Epoch 664 training loss = 0.1917
2023-04-27 08:29:22,028 - INFO - Epoch 665 training loss = 0.1788
2023-04-27 08:29:26,058 - INFO - Epoch 666 training loss = 0.1799
2023-04-27 08:29:30,088 - INFO - Epoch 667 training loss = 0.1804
2023-04-27 08:29:34,118 - INFO - Epoch 668 training loss = 0.1642
2023-04-27 08:29:38,147 - INFO - Epoch 669 training loss = 0.1669
2023-04-27 08:29:42,178 - INFO - Epoch 670 training loss = 0.1782
2023-04-27 08:29:42,492 - INFO - Validation loss = 0.2877
2023-04-27 08:29:46,525 - INFO - Epoch 671 training loss = 0.157
2023-04-27 08:29:50,557 - INFO - Epoch 672 training loss = 0.1471
2023-04-27 08:29:54,588 - INFO - Epoch 673 training loss = 0.1532
2023-04-27 08:29:58,617 - INFO - Epoch 674 training loss = 0.1653
2023-04-27 08:30:02,652 - INFO - Epoch 675 training loss = 0.1681
2023-04-27 08:30:06,683 - INFO - Epoch 676 training loss = 0.1711
2023-04-27 08:30:10,715 - INFO - Epoch 677 training loss = 0.1712
2023-04-27 08:30:14,746 - INFO - Epoch 678 training loss = 0.1499
2023-04-27 08:30:18,779 - INFO - Epoch 679 training loss = 0.1516
2023-04-27 08:30:22,809 - INFO - Epoch 680 training loss = 0.1488
2023-04-27 08:30:23,122 - INFO - Validation loss = 0.169
2023-04-27 08:30:23,122 - INFO - best model
2023-04-27 08:30:27,174 - INFO - Epoch 681 training loss = 0.1442
2023-04-27 08:30:31,204 - INFO - Epoch 682 training loss = 0.145
2023-04-27 08:30:35,233 - INFO - Epoch 683 training loss = 0.1609
2023-04-27 08:30:39,262 - INFO - Epoch 684 training loss = 0.147
2023-04-27 08:30:43,293 - INFO - Epoch 685 training loss = 0.134
2023-04-27 08:30:47,327 - INFO - Epoch 686 training loss = 0.1478
2023-04-27 08:30:51,357 - INFO - Epoch 687 training loss = 0.1645
2023-04-27 08:30:55,388 - INFO - Epoch 688 training loss = 0.1352
2023-04-27 08:30:59,417 - INFO - Epoch 689 training loss = 0.1379
2023-04-27 08:31:03,451 - INFO - Epoch 690 training loss = 0.1398
2023-04-27 08:31:03,764 - INFO - Validation loss = 0.1567
2023-04-27 08:31:03,765 - INFO - best model
2023-04-27 08:31:07,817 - INFO - Epoch 691 training loss = 0.136
2023-04-27 08:31:11,823 - INFO - Epoch 692 training loss = 0.1277
2023-04-27 08:31:15,830 - INFO - Epoch 693 training loss = 0.1343
2023-04-27 08:31:19,835 - INFO - Epoch 694 training loss = 0.1457
2023-04-27 08:31:23,840 - INFO - Epoch 695 training loss = 0.135
2023-04-27 08:31:27,844 - INFO - Epoch 696 training loss = 0.1109
2023-04-27 08:31:31,847 - INFO - Epoch 697 training loss = 0.1364
2023-04-27 08:31:35,851 - INFO - Epoch 698 training loss = 0.1377
2023-04-27 08:31:39,857 - INFO - Epoch 699 training loss = 0.131
2023-04-27 08:31:43,863 - INFO - Epoch 700 training loss = 0.1171
2023-04-27 08:31:44,176 - INFO - Validation loss = 0.1579
2023-04-27 08:31:48,185 - INFO - Epoch 701 training loss = 0.1506
2023-04-27 08:31:52,191 - INFO - Epoch 702 training loss = 0.1241
2023-04-27 08:31:56,195 - INFO - Epoch 703 training loss = 0.1292
2023-04-27 08:32:00,204 - INFO - Epoch 704 training loss = 0.1315
2023-04-27 08:32:04,221 - INFO - Epoch 705 training loss = 0.1245
2023-04-27 08:32:08,237 - INFO - Epoch 706 training loss = 0.1253
2023-04-27 08:32:12,253 - INFO - Epoch 707 training loss = 0.1127
2023-04-27 08:32:16,272 - INFO - Epoch 708 training loss = 0.1271
2023-04-27 08:32:20,289 - INFO - Epoch 709 training loss = 0.1136
2023-04-27 08:32:24,304 - INFO - Epoch 710 training loss = 0.1226
2023-04-27 08:32:24,617 - INFO - Validation loss = 0.1723
2023-04-27 08:32:28,632 - INFO - Epoch 711 training loss = 0.1061
2023-04-27 08:32:32,648 - INFO - Epoch 712 training loss = 0.1019
2023-04-27 08:32:36,663 - INFO - Epoch 713 training loss = 0.1063
2023-04-27 08:32:40,678 - INFO - Epoch 714 training loss = 0.135
2023-04-27 08:32:44,696 - INFO - Epoch 715 training loss = 0.119
2023-04-27 08:32:48,714 - INFO - Epoch 716 training loss = 0.1138
2023-04-27 08:32:52,730 - INFO - Epoch 717 training loss = 0.1077
2023-04-27 08:32:56,745 - INFO - Epoch 718 training loss = 0.1088
2023-04-27 08:33:00,762 - INFO - Epoch 719 training loss = 0.1105
2023-04-27 08:33:04,778 - INFO - Epoch 720 training loss = 0.1091
2023-04-27 08:33:05,092 - INFO - Validation loss = 0.1703
2023-04-27 08:33:09,107 - INFO - Epoch 721 training loss = 0.1068
2023-04-27 08:33:13,124 - INFO - Epoch 722 training loss = 0.1063
2023-04-27 08:33:17,142 - INFO - Epoch 723 training loss = 0.1137
2023-04-27 08:33:21,157 - INFO - Epoch 724 training loss = 0.08832
2023-04-27 08:33:25,172 - INFO - Epoch 725 training loss = 0.09955
2023-04-27 08:33:29,187 - INFO - Epoch 726 training loss = 0.1042
2023-04-27 08:33:33,201 - INFO - Epoch 727 training loss = 0.1099
2023-04-27 08:33:37,215 - INFO - Epoch 728 training loss = 0.1079
2023-04-27 08:33:41,229 - INFO - Epoch 729 training loss = 0.09168
2023-04-27 08:33:45,246 - INFO - Epoch 730 training loss = 0.09219
2023-04-27 08:33:45,559 - INFO - Validation loss = 0.1534
2023-04-27 08:33:45,560 - INFO - best model
2023-04-27 08:33:49,600 - INFO - Epoch 731 training loss = 0.09046
2023-04-27 08:33:53,616 - INFO - Epoch 732 training loss = 0.08756
2023-04-27 08:33:57,631 - INFO - Epoch 733 training loss = 0.08822
2023-04-27 08:34:01,647 - INFO - Epoch 734 training loss = 0.09689
2023-04-27 08:34:05,663 - INFO - Epoch 735 training loss = 0.09631
2023-04-27 08:34:09,678 - INFO - Epoch 736 training loss = 0.0979
2023-04-27 08:34:13,695 - INFO - Epoch 737 training loss = 0.08135
2023-04-27 08:34:17,711 - INFO - Epoch 738 training loss = 0.09018
2023-04-27 08:34:21,714 - INFO - Epoch 739 training loss = 0.09148
2023-04-27 08:34:25,717 - INFO - Epoch 740 training loss = 0.08378
2023-04-27 08:34:26,029 - INFO - Validation loss = 0.1274
2023-04-27 08:34:26,030 - INFO - best model
2023-04-27 08:34:30,053 - INFO - Epoch 741 training loss = 0.08941
2023-04-27 08:34:34,055 - INFO - Epoch 742 training loss = 0.09128
2023-04-27 08:34:38,056 - INFO - Epoch 743 training loss = 0.07992
2023-04-27 08:34:42,058 - INFO - Epoch 744 training loss = 0.08497
2023-04-27 08:34:46,066 - INFO - Epoch 745 training loss = 0.08067
2023-04-27 08:34:50,082 - INFO - Epoch 746 training loss = 0.08223
2023-04-27 08:34:54,096 - INFO - Epoch 747 training loss = 0.08226
2023-04-27 08:34:58,111 - INFO - Epoch 748 training loss = 0.07635
2023-04-27 08:35:02,127 - INFO - Epoch 749 training loss = 0.07673
2023-04-27 08:35:06,140 - INFO - Epoch 750 training loss = 0.09137
2023-04-27 08:35:06,453 - INFO - Validation loss = 0.1118
2023-04-27 08:35:06,454 - INFO - best model
2023-04-27 08:35:10,492 - INFO - Epoch 751 training loss = 0.07243
2023-04-27 08:35:14,509 - INFO - Epoch 752 training loss = 0.07701
2023-04-27 08:35:18,528 - INFO - Epoch 753 training loss = 0.07521
2023-04-27 08:35:22,544 - INFO - Epoch 754 training loss = 0.06977
2023-04-27 08:35:26,559 - INFO - Epoch 755 training loss = 0.07478
2023-04-27 08:35:30,568 - INFO - Epoch 756 training loss = 0.07178
2023-04-27 08:35:34,572 - INFO - Epoch 757 training loss = 0.08082
2023-04-27 08:35:38,575 - INFO - Epoch 758 training loss = 0.0786
2023-04-27 08:35:42,580 - INFO - Epoch 759 training loss = 0.0666
2023-04-27 08:35:46,586 - INFO - Epoch 760 training loss = 0.07021
2023-04-27 08:35:46,899 - INFO - Validation loss = 0.1511
2023-04-27 08:35:50,904 - INFO - Epoch 761 training loss = 0.0717
2023-04-27 08:35:54,908 - INFO - Epoch 762 training loss = 0.06647
2023-04-27 08:35:58,911 - INFO - Epoch 763 training loss = 0.06902
2023-04-27 08:36:02,915 - INFO - Epoch 764 training loss = 0.06669
2023-04-27 08:36:06,918 - INFO - Epoch 765 training loss = 0.07029
2023-04-27 08:36:10,921 - INFO - Epoch 766 training loss = 0.06062
2023-04-27 08:36:14,926 - INFO - Epoch 767 training loss = 0.0617
2023-04-27 08:36:18,930 - INFO - Epoch 768 training loss = 0.06229
2023-04-27 08:36:22,933 - INFO - Epoch 769 training loss = 0.06715
2023-04-27 08:36:26,935 - INFO - Epoch 770 training loss = 0.06461
2023-04-27 08:36:27,248 - INFO - Validation loss = 0.1076
2023-04-27 08:36:27,248 - INFO - best model
2023-04-27 08:36:31,274 - INFO - Epoch 771 training loss = 0.05951
2023-04-27 08:36:35,278 - INFO - Epoch 772 training loss = 0.05854
2023-04-27 08:36:39,281 - INFO - Epoch 773 training loss = 0.06122
2023-04-27 08:36:43,285 - INFO - Epoch 774 training loss = 0.05624
2023-04-27 08:36:47,293 - INFO - Epoch 775 training loss = 0.05438
2023-04-27 08:36:51,297 - INFO - Epoch 776 training loss = 0.05933
2023-04-27 08:36:55,301 - INFO - Epoch 777 training loss = 0.06079
2023-04-27 08:36:59,305 - INFO - Epoch 778 training loss = 0.05774
2023-04-27 08:37:03,311 - INFO - Epoch 779 training loss = 0.06013
2023-04-27 08:37:07,315 - INFO - Epoch 780 training loss = 0.05533
2023-04-27 08:37:07,627 - INFO - Validation loss = 0.1009
2023-04-27 08:37:07,628 - INFO - best model
2023-04-27 08:37:11,654 - INFO - Epoch 781 training loss = 0.0519
2023-04-27 08:37:15,661 - INFO - Epoch 782 training loss = 0.05688
2023-04-27 08:37:19,669 - INFO - Epoch 783 training loss = 0.05276
2023-04-27 08:37:23,672 - INFO - Epoch 784 training loss = 0.0509
2023-04-27 08:37:27,676 - INFO - Epoch 785 training loss = 0.04915
2023-04-27 08:37:31,679 - INFO - Epoch 786 training loss = 0.05107
2023-04-27 08:37:35,683 - INFO - Epoch 787 training loss = 0.04865
2023-04-27 08:37:39,687 - INFO - Epoch 788 training loss = 0.04925
2023-04-27 08:37:43,692 - INFO - Epoch 789 training loss = 0.05251
2023-04-27 08:37:47,698 - INFO - Epoch 790 training loss = 0.04676
2023-04-27 08:37:48,011 - INFO - Validation loss = 0.09208
2023-04-27 08:37:48,011 - INFO - best model
2023-04-27 08:37:52,036 - INFO - Epoch 791 training loss = 0.04961
2023-04-27 08:37:56,039 - INFO - Epoch 792 training loss = 0.05185
2023-04-27 08:38:00,042 - INFO - Epoch 793 training loss = 0.0472
2023-04-27 08:38:04,046 - INFO - Epoch 794 training loss = 0.05067
2023-04-27 08:38:08,050 - INFO - Epoch 795 training loss = 0.05095
2023-04-27 08:38:12,055 - INFO - Epoch 796 training loss = 0.04577
2023-04-27 08:38:16,060 - INFO - Epoch 797 training loss = 0.04829
2023-04-27 08:38:20,065 - INFO - Epoch 798 training loss = 0.04505
2023-04-27 08:38:24,069 - INFO - Epoch 799 training loss = 0.04632
2023-04-27 08:38:28,071 - INFO - Epoch 800 training loss = 0.04493
2023-04-27 08:38:28,384 - INFO - Validation loss = 0.08698
2023-04-27 08:38:28,385 - INFO - best model
2023-04-27 08:38:32,410 - INFO - Epoch 801 training loss = 0.04102
2023-04-27 08:38:36,413 - INFO - Epoch 802 training loss = 0.04522
2023-04-27 08:38:40,416 - INFO - Epoch 803 training loss = 0.04304
2023-04-27 08:38:44,420 - INFO - Epoch 804 training loss = 0.04315
2023-04-27 08:38:48,426 - INFO - Epoch 805 training loss = 0.04185
2023-04-27 08:38:52,427 - INFO - Epoch 806 training loss = 0.04627
2023-04-27 08:38:56,431 - INFO - Epoch 807 training loss = 0.04425
2023-04-27 08:39:00,433 - INFO - Epoch 808 training loss = 0.0388
2023-04-27 08:39:04,437 - INFO - Epoch 809 training loss = 0.04049
2023-04-27 08:39:08,438 - INFO - Epoch 810 training loss = 0.03798
2023-04-27 08:39:08,751 - INFO - Validation loss = 0.0785
2023-04-27 08:39:08,751 - INFO - best model
2023-04-27 08:39:12,778 - INFO - Epoch 811 training loss = 0.03885
2023-04-27 08:39:16,783 - INFO - Epoch 812 training loss = 0.03916
2023-04-27 08:39:20,786 - INFO - Epoch 813 training loss = 0.03717
2023-04-27 08:39:24,788 - INFO - Epoch 814 training loss = 0.03866
2023-04-27 08:39:28,790 - INFO - Epoch 815 training loss = 0.03814
2023-04-27 08:39:32,791 - INFO - Epoch 816 training loss = 0.03575
2023-04-27 08:39:36,794 - INFO - Epoch 817 training loss = 0.03692
2023-04-27 08:39:40,795 - INFO - Epoch 818 training loss = 0.03455
2023-04-27 08:39:44,799 - INFO - Epoch 819 training loss = 0.03267
2023-04-27 08:39:48,804 - INFO - Epoch 820 training loss = 0.03552
2023-04-27 08:39:49,117 - INFO - Validation loss = 0.08286
2023-04-27 08:39:53,122 - INFO - Epoch 821 training loss = 0.03614
2023-04-27 08:39:57,125 - INFO - Epoch 822 training loss = 0.03457
2023-04-27 08:40:01,128 - INFO - Epoch 823 training loss = 0.03367
2023-04-27 08:40:05,136 - INFO - Epoch 824 training loss = 0.03347
2023-04-27 08:40:09,139 - INFO - Epoch 825 training loss = 0.03366
2023-04-27 08:40:13,146 - INFO - Epoch 826 training loss = 0.03569
2023-04-27 08:40:17,152 - INFO - Epoch 827 training loss = 0.03401
2023-04-27 08:40:21,156 - INFO - Epoch 828 training loss = 0.03161
2023-04-27 08:40:25,161 - INFO - Epoch 829 training loss = 0.03247
2023-04-27 08:40:29,163 - INFO - Epoch 830 training loss = 0.03146
2023-04-27 08:40:29,476 - INFO - Validation loss = 0.06506
2023-04-27 08:40:29,476 - INFO - best model
2023-04-27 08:40:33,502 - INFO - Epoch 831 training loss = 0.03061
2023-04-27 08:40:37,504 - INFO - Epoch 832 training loss = 0.03052
2023-04-27 08:40:41,507 - INFO - Epoch 833 training loss = 0.03148
2023-04-27 08:40:45,512 - INFO - Epoch 834 training loss = 0.02949
2023-04-27 08:40:49,517 - INFO - Epoch 835 training loss = 0.03019
2023-04-27 08:40:53,519 - INFO - Epoch 836 training loss = 0.02892
2023-04-27 08:40:57,522 - INFO - Epoch 837 training loss = 0.02896
2023-04-27 08:41:01,524 - INFO - Epoch 838 training loss = 0.02842
2023-04-27 08:41:05,527 - INFO - Epoch 839 training loss = 0.02878
2023-04-27 08:41:09,529 - INFO - Epoch 840 training loss = 0.02785
2023-04-27 08:41:09,841 - INFO - Validation loss = 0.06782
2023-04-27 08:41:13,848 - INFO - Epoch 841 training loss = 0.02612
2023-04-27 08:41:17,856 - INFO - Epoch 842 training loss = 0.02774
2023-04-27 08:41:21,859 - INFO - Epoch 843 training loss = 0.02723
2023-04-27 08:41:25,863 - INFO - Epoch 844 training loss = 0.02625
2023-04-27 08:41:29,867 - INFO - Epoch 845 training loss = 0.02715
2023-04-27 08:41:33,871 - INFO - Epoch 846 training loss = 0.02572
2023-04-27 08:41:37,884 - INFO - Epoch 847 training loss = 0.02424
2023-04-27 08:41:41,915 - INFO - Epoch 848 training loss = 0.02804
2023-04-27 08:41:45,945 - INFO - Epoch 849 training loss = 0.02494
2023-04-27 08:41:49,964 - INFO - Epoch 850 training loss = 0.02442
2023-04-27 08:41:50,276 - INFO - Validation loss = 0.05738
2023-04-27 08:41:50,277 - INFO - best model
2023-04-27 08:41:54,316 - INFO - Epoch 851 training loss = 0.0248
2023-04-27 08:41:58,331 - INFO - Epoch 852 training loss = 0.02494
2023-04-27 08:42:02,348 - INFO - Epoch 853 training loss = 0.02549
2023-04-27 08:42:06,364 - INFO - Epoch 854 training loss = 0.02371
2023-04-27 08:42:10,381 - INFO - Epoch 855 training loss = 0.02334
2023-04-27 08:42:14,399 - INFO - Epoch 856 training loss = 0.02418
2023-04-27 08:42:18,422 - INFO - Epoch 857 training loss = 0.02302
2023-04-27 08:42:22,438 - INFO - Epoch 858 training loss = 0.02391
2023-04-27 08:42:26,454 - INFO - Epoch 859 training loss = 0.02351
2023-04-27 08:42:30,470 - INFO - Epoch 860 training loss = 0.02274
2023-04-27 08:42:30,783 - INFO - Validation loss = 0.05688
2023-04-27 08:42:30,783 - INFO - best model
2023-04-27 08:42:34,821 - INFO - Epoch 861 training loss = 0.02247
2023-04-27 08:42:38,836 - INFO - Epoch 862 training loss = 0.02237
2023-04-27 08:42:42,853 - INFO - Epoch 863 training loss = 0.02154
2023-04-27 08:42:46,871 - INFO - Epoch 864 training loss = 0.02191
2023-04-27 08:42:50,886 - INFO - Epoch 865 training loss = 0.02228
2023-04-27 08:42:54,902 - INFO - Epoch 866 training loss = 0.02173
2023-04-27 08:42:58,918 - INFO - Epoch 867 training loss = 0.02146
2023-04-27 08:43:02,935 - INFO - Epoch 868 training loss = 0.02136
2023-04-27 08:43:06,953 - INFO - Epoch 869 training loss = 0.02065
2023-04-27 08:43:10,976 - INFO - Epoch 870 training loss = 0.02059
2023-04-27 08:43:11,290 - INFO - Validation loss = 0.05945
2023-04-27 08:43:15,308 - INFO - Epoch 871 training loss = 0.02135
2023-04-27 08:43:19,326 - INFO - Epoch 872 training loss = 0.02011
2023-04-27 08:43:23,341 - INFO - Epoch 873 training loss = 0.02007
2023-04-27 08:43:27,356 - INFO - Epoch 874 training loss = 0.01963
2023-04-27 08:43:31,370 - INFO - Epoch 875 training loss = 0.01989
2023-04-27 08:43:35,386 - INFO - Epoch 876 training loss = 0.01976
2023-04-27 08:43:39,401 - INFO - Epoch 877 training loss = 0.01957
2023-04-27 08:43:43,418 - INFO - Epoch 878 training loss = 0.01875
2023-04-27 08:43:47,440 - INFO - Epoch 879 training loss = 0.019
2023-04-27 08:43:51,455 - INFO - Epoch 880 training loss = 0.01868
2023-04-27 08:43:51,768 - INFO - Validation loss = 0.0546
2023-04-27 08:43:51,769 - INFO - best model
2023-04-27 08:43:55,805 - INFO - Epoch 881 training loss = 0.0187
2023-04-27 08:43:59,819 - INFO - Epoch 882 training loss = 0.01843
2023-04-27 08:44:03,834 - INFO - Epoch 883 training loss = 0.01881
2023-04-27 08:44:07,855 - INFO - Epoch 884 training loss = 0.01815
2023-04-27 08:44:11,885 - INFO - Epoch 885 training loss = 0.01791
2023-04-27 08:44:15,915 - INFO - Epoch 886 training loss = 0.01815
2023-04-27 08:44:19,945 - INFO - Epoch 887 training loss = 0.0177
2023-04-27 08:44:23,973 - INFO - Epoch 888 training loss = 0.01791
2023-04-27 08:44:28,000 - INFO - Epoch 889 training loss = 0.01743
2023-04-27 08:44:32,015 - INFO - Epoch 890 training loss = 0.01767
2023-04-27 08:44:32,328 - INFO - Validation loss = 0.05342
2023-04-27 08:44:32,328 - INFO - best model
2023-04-27 08:44:36,363 - INFO - Epoch 891 training loss = 0.01714
2023-04-27 08:44:40,378 - INFO - Epoch 892 training loss = 0.01716
2023-04-27 08:44:44,395 - INFO - Epoch 893 training loss = 0.01743
2023-04-27 08:44:48,415 - INFO - Epoch 894 training loss = 0.01704
2023-04-27 08:44:52,429 - INFO - Epoch 895 training loss = 0.01705
2023-04-27 08:44:56,445 - INFO - Epoch 896 training loss = 0.01679
2023-04-27 08:45:00,461 - INFO - Epoch 897 training loss = 0.01652
2023-04-27 08:45:04,478 - INFO - Epoch 898 training loss = 0.01652
2023-04-27 08:45:08,493 - INFO - Epoch 899 training loss = 0.01611
2023-04-27 08:45:12,511 - INFO - Epoch 900 training loss = 0.01616
2023-04-27 08:45:12,824 - INFO - Validation loss = 0.05084
2023-04-27 08:45:12,824 - INFO - best model
2023-04-27 08:45:16,865 - INFO - Epoch 901 training loss = 0.01598
2023-04-27 08:45:20,885 - INFO - Epoch 902 training loss = 0.01578
2023-04-27 08:45:24,901 - INFO - Epoch 903 training loss = 0.01572
2023-04-27 08:45:28,916 - INFO - Epoch 904 training loss = 0.01575
2023-04-27 08:45:32,933 - INFO - Epoch 905 training loss = 0.01572
2023-04-27 08:45:36,948 - INFO - Epoch 906 training loss = 0.01557
2023-04-27 08:45:40,963 - INFO - Epoch 907 training loss = 0.01547
2023-04-27 08:45:44,982 - INFO - Epoch 908 training loss = 0.01531
2023-04-27 08:45:49,008 - INFO - Epoch 909 training loss = 0.0152
2023-04-27 08:45:53,036 - INFO - Epoch 910 training loss = 0.01511
2023-04-27 08:45:53,351 - INFO - Validation loss = 0.05102
2023-04-27 08:45:57,381 - INFO - Epoch 911 training loss = 0.01537
2023-04-27 08:46:01,410 - INFO - Epoch 912 training loss = 0.01478
2023-04-27 08:46:05,440 - INFO - Epoch 913 training loss = 0.01488
2023-04-27 08:46:09,468 - INFO - Epoch 914 training loss = 0.01476
2023-04-27 08:46:13,499 - INFO - Epoch 915 training loss = 0.01457
2023-04-27 08:46:17,531 - INFO - Epoch 916 training loss = 0.01455
2023-04-27 08:46:21,559 - INFO - Epoch 917 training loss = 0.01466
2023-04-27 08:46:25,579 - INFO - Epoch 918 training loss = 0.01439
2023-04-27 08:46:29,593 - INFO - Epoch 919 training loss = 0.01438
2023-04-27 08:46:33,608 - INFO - Epoch 920 training loss = 0.01431
2023-04-27 08:46:33,921 - INFO - Validation loss = 0.04829
2023-04-27 08:46:33,921 - INFO - best model
2023-04-27 08:46:37,958 - INFO - Epoch 921 training loss = 0.01413
2023-04-27 08:46:41,975 - INFO - Epoch 922 training loss = 0.01408
2023-04-27 08:46:45,993 - INFO - Epoch 923 training loss = 0.01419
2023-04-27 08:46:50,022 - INFO - Epoch 924 training loss = 0.01395
2023-04-27 08:46:54,046 - INFO - Epoch 925 training loss = 0.01384
2023-04-27 08:46:58,062 - INFO - Epoch 926 training loss = 0.01385
2023-04-27 08:47:02,088 - INFO - Epoch 927 training loss = 0.01377
2023-04-27 08:47:06,118 - INFO - Epoch 928 training loss = 0.01372
2023-04-27 08:47:10,149 - INFO - Epoch 929 training loss = 0.0137
2023-04-27 08:47:14,181 - INFO - Epoch 930 training loss = 0.0136
2023-04-27 08:47:14,495 - INFO - Validation loss = 0.04709
2023-04-27 08:47:14,495 - INFO - best model
2023-04-27 08:47:18,549 - INFO - Epoch 931 training loss = 0.01351
2023-04-27 08:47:22,578 - INFO - Epoch 932 training loss = 0.01342
2023-04-27 08:47:26,607 - INFO - Epoch 933 training loss = 0.01347
2023-04-27 08:47:30,636 - INFO - Epoch 934 training loss = 0.01331
2023-04-27 08:47:34,666 - INFO - Epoch 935 training loss = 0.01331
2023-04-27 08:47:38,695 - INFO - Epoch 936 training loss = 0.0132
2023-04-27 08:47:42,725 - INFO - Epoch 937 training loss = 0.01314
2023-04-27 08:47:46,758 - INFO - Epoch 938 training loss = 0.01323
2023-04-27 08:47:50,789 - INFO - Epoch 939 training loss = 0.01309
2023-04-27 08:47:54,818 - INFO - Epoch 940 training loss = 0.01301
2023-04-27 08:47:55,132 - INFO - Validation loss = 0.04659
2023-04-27 08:47:55,132 - INFO - best model
2023-04-27 08:47:59,184 - INFO - Epoch 941 training loss = 0.01303
2023-04-27 08:48:03,215 - INFO - Epoch 942 training loss = 0.01295
2023-04-27 08:48:07,244 - INFO - Epoch 943 training loss = 0.013
2023-04-27 08:48:11,274 - INFO - Epoch 944 training loss = 0.01282
2023-04-27 08:48:15,305 - INFO - Epoch 945 training loss = 0.01284
2023-04-27 08:48:19,335 - INFO - Epoch 946 training loss = 0.01276
2023-04-27 08:48:23,363 - INFO - Epoch 947 training loss = 0.0127
2023-04-27 08:48:27,392 - INFO - Epoch 948 training loss = 0.01268
2023-04-27 08:48:31,420 - INFO - Epoch 949 training loss = 0.01258
2023-04-27 08:48:35,448 - INFO - Epoch 950 training loss = 0.01261
2023-04-27 08:48:35,761 - INFO - Validation loss = 0.04576
2023-04-27 08:48:35,762 - INFO - best model
2023-04-27 08:48:39,809 - INFO - Epoch 951 training loss = 0.01254
2023-04-27 08:48:43,840 - INFO - Epoch 952 training loss = 0.01253
2023-04-27 08:48:47,872 - INFO - Epoch 953 training loss = 0.01248
2023-04-27 08:48:51,901 - INFO - Epoch 954 training loss = 0.01241
2023-04-27 08:48:55,930 - INFO - Epoch 955 training loss = 0.01242
2023-04-27 08:48:59,959 - INFO - Epoch 956 training loss = 0.01239
2023-04-27 08:49:03,991 - INFO - Epoch 957 training loss = 0.01236
2023-04-27 08:49:08,020 - INFO - Epoch 958 training loss = 0.01233
2023-04-27 08:49:12,050 - INFO - Epoch 959 training loss = 0.01229
2023-04-27 08:49:16,082 - INFO - Epoch 960 training loss = 0.01225
2023-04-27 08:49:16,396 - INFO - Validation loss = 0.04546
2023-04-27 08:49:16,396 - INFO - best model
2023-04-27 08:49:20,448 - INFO - Epoch 961 training loss = 0.01221
2023-04-27 08:49:24,476 - INFO - Epoch 962 training loss = 0.01221
2023-04-27 08:49:28,503 - INFO - Epoch 963 training loss = 0.0122
2023-04-27 08:49:32,531 - INFO - Epoch 964 training loss = 0.01218
2023-04-27 08:49:36,559 - INFO - Epoch 965 training loss = 0.01214
2023-04-27 08:49:40,588 - INFO - Epoch 966 training loss = 0.0121
2023-04-27 08:49:44,620 - INFO - Epoch 967 training loss = 0.01206
2023-04-27 08:49:48,652 - INFO - Epoch 968 training loss = 0.01204
2023-04-27 08:49:52,681 - INFO - Epoch 969 training loss = 0.01204
2023-04-27 08:49:56,710 - INFO - Epoch 970 training loss = 0.012
2023-04-27 08:49:57,024 - INFO - Validation loss = 0.0451
2023-04-27 08:49:57,024 - INFO - best model
2023-04-27 08:50:01,077 - INFO - Epoch 971 training loss = 0.01197
2023-04-27 08:50:05,109 - INFO - Epoch 972 training loss = 0.01194
2023-04-27 08:50:09,138 - INFO - Epoch 973 training loss = 0.01194
2023-04-27 08:50:13,170 - INFO - Epoch 974 training loss = 0.01193
2023-04-27 08:50:17,202 - INFO - Epoch 975 training loss = 0.01191
2023-04-27 08:50:21,232 - INFO - Epoch 976 training loss = 0.01188
2023-04-27 08:50:25,260 - INFO - Epoch 977 training loss = 0.01185
2023-04-27 08:50:29,289 - INFO - Epoch 978 training loss = 0.01185
2023-04-27 08:50:33,318 - INFO - Epoch 979 training loss = 0.01182
2023-04-27 08:50:37,346 - INFO - Epoch 980 training loss = 0.01182
2023-04-27 08:50:37,659 - INFO - Validation loss = 0.04495
2023-04-27 08:50:37,659 - INFO - best model
2023-04-27 08:50:41,710 - INFO - Epoch 981 training loss = 0.0118
2023-04-27 08:50:45,742 - INFO - Epoch 982 training loss = 0.01178
2023-04-27 08:50:49,773 - INFO - Epoch 983 training loss = 0.01178
2023-04-27 08:50:53,801 - INFO - Epoch 984 training loss = 0.01176
2023-04-27 08:50:57,831 - INFO - Epoch 985 training loss = 0.01175
2023-04-27 08:51:01,861 - INFO - Epoch 986 training loss = 0.01174
2023-04-27 08:51:05,891 - INFO - Epoch 987 training loss = 0.01173
2023-04-27 08:51:09,920 - INFO - Epoch 988 training loss = 0.01171
2023-04-27 08:51:13,952 - INFO - Epoch 989 training loss = 0.01171
2023-04-27 08:51:17,985 - INFO - Epoch 990 training loss = 0.0117
2023-04-27 08:51:18,298 - INFO - Validation loss = 0.04483
2023-04-27 08:51:18,298 - INFO - best model
2023-04-27 08:51:22,347 - INFO - Epoch 991 training loss = 0.01169
2023-04-27 08:51:26,376 - INFO - Epoch 992 training loss = 0.01168
2023-04-27 08:51:30,403 - INFO - Epoch 993 training loss = 0.01168
2023-04-27 08:51:34,432 - INFO - Epoch 994 training loss = 0.01167
2023-04-27 08:51:38,460 - INFO - Epoch 995 training loss = 0.01167
2023-04-27 08:51:42,489 - INFO - Epoch 996 training loss = 0.01167
2023-04-27 08:51:46,520 - INFO - Epoch 997 training loss = 0.01166
2023-04-27 08:51:50,551 - INFO - Epoch 998 training loss = 0.01166
2023-04-27 08:51:54,580 - INFO - Epoch 999 training loss = 0.01165
2023-04-27 08:51:54,871 - INFO - Validation loss = 0.0448
