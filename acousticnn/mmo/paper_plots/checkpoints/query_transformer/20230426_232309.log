2023-04-26 23:23:09,141 - INFO - Config:
Namespace(config='configs/implicit_transformer.yaml', dir='/user/delden/acoustics/spp_ai_acoustics/acousticNN/experiments/arch/no_encoding/implicit_transformer', epochs=100, device='cuda', seed=1, parameter_list='configs/data.yaml', encoding='none', dir_name='arch/no_encoding/implicit_transformer')
2023-04-26 23:23:09,141 - INFO - Config:
Munch({'optimizer': Munch({'type': 'AdamW', 'kwargs': Munch({'lr': 0.0025, 'weight_decay': 0.005, 'betas': [0.9, 0.95]})}), 'scheduler': Munch({'type': 'CosLR', 'kwargs': Munch({'epochs': 1000, 'initial_epochs': 25})}), 'model': Munch({'name': 'ImplicitTransformer', 'input_encoding': True, 'encoding_dim': 16, 'encoder': Munch({'embed_dim': 66, 'num_heads': 3, 'depth': 4}), 'decoder': Munch({'embed_dim': 99, 'num_heads': 3, 'depth': 3})}), 'generation_frequency': 5000, 'validation_frequency': 10, 'batch_size': 16, 'epochs': 1000, 'gradient_clip': 10})
2023-04-26 23:23:23,419 - INFO - ==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ImplicitTransformer                      [16, 200, 4]              --
├─PositionalEncoding: 1-1                [16, 14, 66]              --
│    └─SinosoidalEncoding: 2-1           [16, 14, 66]              --
├─GroupwiseProjection: 1-2               [16, 14, 66]              --
│    └─ModuleList: 2-2                   --                        --
│    │    └─Linear: 3-1                  [16, 4, 66]               132
│    │    └─Linear: 3-2                  [16, 5, 66]               132
│    │    └─Linear: 3-3                  [16, 5, 66]               132
├─TransformerEncoder: 1-3                [16, 14, 66]              --
│    └─ModuleList: 2-3                   --                        --
│    │    └─Block: 3-4                   [16, 14, 66]              52,932
│    │    └─Block: 3-5                   [16, 14, 66]              52,932
│    │    └─Block: 3-6                   [16, 14, 66]              52,932
│    │    └─Block: 3-7                   [16, 14, 66]              52,932
├─Linear: 1-4                            [3200, 4, 99]             6,633
├─Linear: 1-5                            [16, 200, 99]             198
├─TransformerEncoder: 1-6                [3200, 4, 99]             --
│    └─ModuleList: 2-4                   --                        --
│    │    └─Block: 3-8                   [3200, 4, 99]             118,602
│    │    └─Block: 3-9                   [3200, 4, 99]             118,602
│    │    └─Block: 3-10                  [3200, 4, 99]             118,602
├─Linear: 1-7                            [3200, 4, 1]              100
==========================================================================================
Total params: 574,861
Trainable params: 574,861
Non-trainable params: 0
Total mult-adds (G): 1.16
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 352.64
Params size (MB): 2.30
Estimated Total Size (MB): 354.95
==========================================================================================
2023-04-26 23:23:27,479 - INFO - Epoch 0 training loss = 4.015e+04
2023-04-26 23:23:27,794 - INFO - Validation loss = 3.502e+04
2023-04-26 23:23:27,794 - INFO - best model
2023-04-26 23:23:31,831 - INFO - Epoch 1 training loss = 3.059e+04
2023-04-26 23:23:35,856 - INFO - Epoch 2 training loss = 890.7
2023-04-26 23:23:39,880 - INFO - Epoch 3 training loss = 206.8
2023-04-26 23:23:43,907 - INFO - Epoch 4 training loss = 165.4
2023-04-26 23:23:47,932 - INFO - Epoch 5 training loss = 109.8
2023-04-26 23:23:51,956 - INFO - Epoch 6 training loss = 89.5
2023-04-26 23:23:55,980 - INFO - Epoch 7 training loss = 72.81
2023-04-26 23:24:00,003 - INFO - Epoch 8 training loss = 58.07
2023-04-26 23:24:04,028 - INFO - Epoch 9 training loss = 50.41
2023-04-26 23:24:08,051 - INFO - Epoch 10 training loss = 44.9
2023-04-26 23:24:08,365 - INFO - Validation loss = 35.56
2023-04-26 23:24:08,365 - INFO - best model
2023-04-26 23:24:12,415 - INFO - Epoch 11 training loss = 39.11
2023-04-26 23:24:16,441 - INFO - Epoch 12 training loss = 36.34
2023-04-26 23:24:20,466 - INFO - Epoch 13 training loss = 33.54
2023-04-26 23:24:24,489 - INFO - Epoch 14 training loss = 33.65
2023-04-26 23:24:28,504 - INFO - Epoch 15 training loss = 30.21
2023-04-26 23:24:32,513 - INFO - Epoch 16 training loss = 28.24
2023-04-26 23:24:36,523 - INFO - Epoch 17 training loss = 27.0
2023-04-26 23:24:40,542 - INFO - Epoch 18 training loss = 24.81
2023-04-26 23:24:44,569 - INFO - Epoch 19 training loss = 22.65
2023-04-26 23:24:48,595 - INFO - Epoch 20 training loss = 22.36
2023-04-26 23:24:48,910 - INFO - Validation loss = 19.2
2023-04-26 23:24:48,910 - INFO - best model
2023-04-26 23:24:52,956 - INFO - Epoch 21 training loss = 22.43
2023-04-26 23:24:56,981 - INFO - Epoch 22 training loss = 22.42
2023-04-26 23:25:01,004 - INFO - Epoch 23 training loss = 20.6
2023-04-26 23:25:05,029 - INFO - Epoch 24 training loss = 20.85
2023-04-26 23:25:09,052 - INFO - Epoch 25 training loss = 19.05
2023-04-26 23:25:13,066 - INFO - Epoch 26 training loss = 19.15
2023-04-26 23:25:17,080 - INFO - Epoch 27 training loss = 17.71
2023-04-26 23:25:21,090 - INFO - Epoch 28 training loss = 15.66
2023-04-26 23:25:25,100 - INFO - Epoch 29 training loss = 14.46
2023-04-26 23:25:29,113 - INFO - Epoch 30 training loss = 14.43
2023-04-26 23:25:29,426 - INFO - Validation loss = 10.32
2023-04-26 23:25:29,427 - INFO - best model
2023-04-26 23:25:33,459 - INFO - Epoch 31 training loss = 12.63
2023-04-26 23:25:37,469 - INFO - Epoch 32 training loss = 12.01
2023-04-26 23:25:41,481 - INFO - Epoch 33 training loss = 10.73
2023-04-26 23:25:45,495 - INFO - Epoch 34 training loss = 10.26
2023-04-26 23:25:49,508 - INFO - Epoch 35 training loss = 10.76
2023-04-26 23:25:53,519 - INFO - Epoch 36 training loss = 10.69
2023-04-26 23:25:57,530 - INFO - Epoch 37 training loss = 9.504
2023-04-26 23:26:01,542 - INFO - Epoch 38 training loss = 9.776
2023-04-26 23:26:05,553 - INFO - Epoch 39 training loss = 9.352
2023-04-26 23:26:09,565 - INFO - Epoch 40 training loss = 8.203
2023-04-26 23:26:09,878 - INFO - Validation loss = 10.67
2023-04-26 23:26:13,894 - INFO - Epoch 41 training loss = 8.384
2023-04-26 23:26:17,895 - INFO - Epoch 42 training loss = 7.613
2023-04-26 23:26:21,893 - INFO - Epoch 43 training loss = 7.918
2023-04-26 23:26:25,892 - INFO - Epoch 44 training loss = 7.146
2023-04-26 23:26:29,890 - INFO - Epoch 45 training loss = 7.15
2023-04-26 23:26:33,888 - INFO - Epoch 46 training loss = 6.964
2023-04-26 23:26:37,887 - INFO - Epoch 47 training loss = 6.881
2023-04-26 23:26:41,887 - INFO - Epoch 48 training loss = 6.895
2023-04-26 23:26:45,889 - INFO - Epoch 49 training loss = 6.368
2023-04-26 23:26:49,890 - INFO - Epoch 50 training loss = 6.706
2023-04-26 23:26:50,203 - INFO - Validation loss = 5.424
2023-04-26 23:26:50,203 - INFO - best model
2023-04-26 23:26:54,225 - INFO - Epoch 51 training loss = 6.275
2023-04-26 23:26:58,225 - INFO - Epoch 52 training loss = 6.356
2023-04-26 23:27:02,223 - INFO - Epoch 53 training loss = 6.086
2023-04-26 23:27:06,222 - INFO - Epoch 54 training loss = 5.403
2023-04-26 23:27:10,222 - INFO - Epoch 55 training loss = 6.112
2023-04-26 23:27:14,225 - INFO - Epoch 56 training loss = 5.25
2023-04-26 23:27:18,226 - INFO - Epoch 57 training loss = 5.798
2023-04-26 23:27:22,224 - INFO - Epoch 58 training loss = 6.024
2023-04-26 23:27:26,223 - INFO - Epoch 59 training loss = 5.031
2023-04-26 23:27:30,234 - INFO - Epoch 60 training loss = 4.807
2023-04-26 23:27:30,547 - INFO - Validation loss = 4.524
2023-04-26 23:27:30,548 - INFO - best model
2023-04-26 23:27:34,581 - INFO - Epoch 61 training loss = 4.611
2023-04-26 23:27:38,591 - INFO - Epoch 62 training loss = 4.94
2023-04-26 23:27:42,604 - INFO - Epoch 63 training loss = 4.529
2023-04-26 23:27:46,618 - INFO - Epoch 64 training loss = 5.279
2023-04-26 23:27:50,624 - INFO - Epoch 65 training loss = 4.404
2023-04-26 23:27:54,623 - INFO - Epoch 66 training loss = 4.49
2023-04-26 23:27:58,633 - INFO - Epoch 67 training loss = 4.752
2023-04-26 23:28:02,644 - INFO - Epoch 68 training loss = 4.755
2023-04-26 23:28:06,654 - INFO - Epoch 69 training loss = 4.604
2023-04-26 23:28:10,667 - INFO - Epoch 70 training loss = 4.47
2023-04-26 23:28:10,980 - INFO - Validation loss = 4.144
2023-04-26 23:28:10,981 - INFO - best model
2023-04-26 23:28:15,014 - INFO - Epoch 71 training loss = 4.112
2023-04-26 23:28:19,028 - INFO - Epoch 72 training loss = 3.669
2023-04-26 23:28:23,038 - INFO - Epoch 73 training loss = 4.049
2023-04-26 23:28:27,049 - INFO - Epoch 74 training loss = 4.174
2023-04-26 23:28:31,059 - INFO - Epoch 75 training loss = 4.101
2023-04-26 23:28:35,070 - INFO - Epoch 76 training loss = 3.983
2023-04-26 23:28:39,081 - INFO - Epoch 77 training loss = 3.832
2023-04-26 23:28:43,087 - INFO - Epoch 78 training loss = 3.576
2023-04-26 23:28:47,089 - INFO - Epoch 79 training loss = 3.821
2023-04-26 23:28:51,088 - INFO - Epoch 80 training loss = 3.713
2023-04-26 23:28:51,401 - INFO - Validation loss = 3.577
2023-04-26 23:28:51,401 - INFO - best model
2023-04-26 23:28:55,422 - INFO - Epoch 81 training loss = 3.37
2023-04-26 23:28:59,423 - INFO - Epoch 82 training loss = 3.414
2023-04-26 23:29:03,423 - INFO - Epoch 83 training loss = 3.358
2023-04-26 23:29:07,422 - INFO - Epoch 84 training loss = 3.314
2023-04-26 23:29:11,424 - INFO - Epoch 85 training loss = 3.392
2023-04-26 23:29:15,425 - INFO - Epoch 86 training loss = 3.259
2023-04-26 23:29:19,424 - INFO - Epoch 87 training loss = 3.23
2023-04-26 23:29:23,422 - INFO - Epoch 88 training loss = 3.28
2023-04-26 23:29:27,420 - INFO - Epoch 89 training loss = 3.419
2023-04-26 23:29:31,416 - INFO - Epoch 90 training loss = 3.108
2023-04-26 23:29:31,729 - INFO - Validation loss = 3.38
2023-04-26 23:29:31,730 - INFO - best model
2023-04-26 23:29:35,750 - INFO - Epoch 91 training loss = 3.129
2023-04-26 23:29:39,749 - INFO - Epoch 92 training loss = 3.127
2023-04-26 23:29:43,750 - INFO - Epoch 93 training loss = 2.96
2023-04-26 23:29:47,750 - INFO - Epoch 94 training loss = 3.048
2023-04-26 23:29:51,749 - INFO - Epoch 95 training loss = 2.966
2023-04-26 23:29:55,748 - INFO - Epoch 96 training loss = 2.796
2023-04-26 23:29:59,747 - INFO - Epoch 97 training loss = 3.146
2023-04-26 23:30:03,751 - INFO - Epoch 98 training loss = 2.891
2023-04-26 23:30:07,751 - INFO - Epoch 99 training loss = 3.097
2023-04-26 23:30:11,753 - INFO - Epoch 100 training loss = 2.65
2023-04-26 23:30:12,064 - INFO - Validation loss = 3.224
2023-04-26 23:30:12,065 - INFO - best model
2023-04-26 23:30:16,084 - INFO - Epoch 101 training loss = 2.642
2023-04-26 23:30:20,084 - INFO - Epoch 102 training loss = 2.567
2023-04-26 23:30:24,082 - INFO - Epoch 103 training loss = 2.865
2023-04-26 23:30:28,080 - INFO - Epoch 104 training loss = 2.707
2023-04-26 23:30:32,077 - INFO - Epoch 105 training loss = 2.762
2023-04-26 23:30:36,075 - INFO - Epoch 106 training loss = 2.633
2023-04-26 23:30:40,073 - INFO - Epoch 107 training loss = 2.522
2023-04-26 23:30:44,074 - INFO - Epoch 108 training loss = 2.523
2023-04-26 23:30:48,074 - INFO - Epoch 109 training loss = 2.749
2023-04-26 23:30:52,072 - INFO - Epoch 110 training loss = 2.459
2023-04-26 23:30:52,385 - INFO - Validation loss = 3.669
2023-04-26 23:30:56,384 - INFO - Epoch 111 training loss = 2.415
2023-04-26 23:31:00,383 - INFO - Epoch 112 training loss = 2.491
2023-04-26 23:31:04,384 - INFO - Epoch 113 training loss = 2.542
2023-04-26 23:31:08,382 - INFO - Epoch 114 training loss = 2.281
2023-04-26 23:31:12,385 - INFO - Epoch 115 training loss = 2.573
2023-04-26 23:31:16,387 - INFO - Epoch 116 training loss = 2.453
2023-04-26 23:31:20,387 - INFO - Epoch 117 training loss = 2.543
2023-04-26 23:31:24,385 - INFO - Epoch 118 training loss = 2.421
2023-04-26 23:31:28,383 - INFO - Epoch 119 training loss = 2.313
2023-04-26 23:31:32,382 - INFO - Epoch 120 training loss = 2.24
2023-04-26 23:31:32,695 - INFO - Validation loss = 2.07
2023-04-26 23:31:32,695 - INFO - best model
2023-04-26 23:31:36,715 - INFO - Epoch 121 training loss = 2.46
2023-04-26 23:31:40,714 - INFO - Epoch 122 training loss = 2.428
2023-04-26 23:31:44,714 - INFO - Epoch 123 training loss = 2.337
2023-04-26 23:31:48,715 - INFO - Epoch 124 training loss = 2.133
2023-04-26 23:31:52,713 - INFO - Epoch 125 training loss = 2.35
2023-04-26 23:31:56,712 - INFO - Epoch 126 training loss = 2.548
2023-04-26 23:32:00,710 - INFO - Epoch 127 training loss = 2.332
2023-04-26 23:32:04,708 - INFO - Epoch 128 training loss = 2.163
2023-04-26 23:32:08,706 - INFO - Epoch 129 training loss = 2.373
2023-04-26 23:32:12,711 - INFO - Epoch 130 training loss = 2.148
2023-04-26 23:32:13,024 - INFO - Validation loss = 2.707
2023-04-26 23:32:17,026 - INFO - Epoch 131 training loss = 2.167
2023-04-26 23:32:21,025 - INFO - Epoch 132 training loss = 2.178
2023-04-26 23:32:25,023 - INFO - Epoch 133 training loss = 2.42
2023-04-26 23:32:29,021 - INFO - Epoch 134 training loss = 2.247
2023-04-26 23:32:33,020 - INFO - Epoch 135 training loss = 2.322
2023-04-26 23:32:37,018 - INFO - Epoch 136 training loss = 2.169
2023-04-26 23:32:41,017 - INFO - Epoch 137 training loss = 1.99
2023-04-26 23:32:45,018 - INFO - Epoch 138 training loss = 1.96
2023-04-26 23:32:49,019 - INFO - Epoch 139 training loss = 2.018
2023-04-26 23:32:53,017 - INFO - Epoch 140 training loss = 2.343
2023-04-26 23:32:53,330 - INFO - Validation loss = 4.254
2023-04-26 23:32:57,330 - INFO - Epoch 141 training loss = 2.147
2023-04-26 23:33:01,329 - INFO - Epoch 142 training loss = 2.146
2023-04-26 23:33:05,328 - INFO - Epoch 143 training loss = 2.063
2023-04-26 23:33:09,326 - INFO - Epoch 144 training loss = 2.048
2023-04-26 23:33:13,328 - INFO - Epoch 145 training loss = 2.042
2023-04-26 23:33:17,328 - INFO - Epoch 146 training loss = 2.159
2023-04-26 23:33:21,327 - INFO - Epoch 147 training loss = 1.987
2023-04-26 23:33:25,325 - INFO - Epoch 148 training loss = 2.059
2023-04-26 23:33:29,323 - INFO - Epoch 149 training loss = 1.951
2023-04-26 23:33:33,321 - INFO - Epoch 150 training loss = 1.993
2023-04-26 23:33:33,634 - INFO - Validation loss = 1.199
2023-04-26 23:33:33,634 - INFO - best model
2023-04-26 23:33:37,654 - INFO - Epoch 151 training loss = 2.121
2023-04-26 23:33:41,654 - INFO - Epoch 152 training loss = 1.897
2023-04-26 23:33:45,655 - INFO - Epoch 153 training loss = 2.056
2023-04-26 23:33:49,656 - INFO - Epoch 154 training loss = 1.865
2023-04-26 23:33:53,655 - INFO - Epoch 155 training loss = 1.89
2023-04-26 23:33:57,654 - INFO - Epoch 156 training loss = 1.941
2023-04-26 23:34:01,654 - INFO - Epoch 157 training loss = 1.728
2023-04-26 23:34:05,653 - INFO - Epoch 158 training loss = 1.899
2023-04-26 23:34:09,653 - INFO - Epoch 159 training loss = 1.891
2023-04-26 23:34:13,657 - INFO - Epoch 160 training loss = 1.831
2023-04-26 23:34:13,970 - INFO - Validation loss = 1.248
2023-04-26 23:34:17,972 - INFO - Epoch 161 training loss = 1.692
2023-04-26 23:34:21,969 - INFO - Epoch 162 training loss = 1.784
2023-04-26 23:34:25,968 - INFO - Epoch 163 training loss = 1.816
2023-04-26 23:34:29,965 - INFO - Epoch 164 training loss = 1.937
2023-04-26 23:34:33,963 - INFO - Epoch 165 training loss = 1.822
2023-04-26 23:34:37,961 - INFO - Epoch 166 training loss = 1.937
2023-04-26 23:34:41,962 - INFO - Epoch 167 training loss = 1.773
2023-04-26 23:34:45,963 - INFO - Epoch 168 training loss = 1.847
2023-04-26 23:34:49,963 - INFO - Epoch 169 training loss = 1.792
2023-04-26 23:34:53,960 - INFO - Epoch 170 training loss = 1.86
2023-04-26 23:34:54,273 - INFO - Validation loss = 1.91
2023-04-26 23:34:58,273 - INFO - Epoch 171 training loss = 1.831
2023-04-26 23:35:02,272 - INFO - Epoch 172 training loss = 1.86
2023-04-26 23:35:06,272 - INFO - Epoch 173 training loss = 1.931
2023-04-26 23:35:10,270 - INFO - Epoch 174 training loss = 1.842
2023-04-26 23:35:14,272 - INFO - Epoch 175 training loss = 1.767
2023-04-26 23:35:18,273 - INFO - Epoch 176 training loss = 1.787
2023-04-26 23:35:22,272 - INFO - Epoch 177 training loss = 1.761
2023-04-26 23:35:26,271 - INFO - Epoch 178 training loss = 1.806
2023-04-26 23:35:30,269 - INFO - Epoch 179 training loss = 1.819
2023-04-26 23:35:34,268 - INFO - Epoch 180 training loss = 1.676
2023-04-26 23:35:34,581 - INFO - Validation loss = 2.444
2023-04-26 23:35:38,580 - INFO - Epoch 181 training loss = 1.573
2023-04-26 23:35:42,581 - INFO - Epoch 182 training loss = 1.738
2023-04-26 23:35:46,581 - INFO - Epoch 183 training loss = 1.698
2023-04-26 23:35:50,581 - INFO - Epoch 184 training loss = 1.803
2023-04-26 23:35:54,580 - INFO - Epoch 185 training loss = 1.735
2023-04-26 23:35:58,580 - INFO - Epoch 186 training loss = 1.684
2023-04-26 23:36:02,580 - INFO - Epoch 187 training loss = 1.733
2023-04-26 23:36:06,579 - INFO - Epoch 188 training loss = 1.622
2023-04-26 23:36:10,579 - INFO - Epoch 189 training loss = 1.572
2023-04-26 23:36:14,581 - INFO - Epoch 190 training loss = 1.441
2023-04-26 23:36:14,894 - INFO - Validation loss = 1.086
2023-04-26 23:36:14,894 - INFO - best model
2023-04-26 23:36:18,916 - INFO - Epoch 191 training loss = 1.779
2023-04-26 23:36:22,914 - INFO - Epoch 192 training loss = 1.615
2023-04-26 23:36:26,912 - INFO - Epoch 193 training loss = 1.559
2023-04-26 23:36:30,911 - INFO - Epoch 194 training loss = 1.615
2023-04-26 23:36:34,910 - INFO - Epoch 195 training loss = 1.572
2023-04-26 23:36:38,909 - INFO - Epoch 196 training loss = 1.657
2023-04-26 23:36:42,912 - INFO - Epoch 197 training loss = 1.771
2023-04-26 23:36:46,913 - INFO - Epoch 198 training loss = 1.565
2023-04-26 23:36:50,912 - INFO - Epoch 199 training loss = 1.696
2023-04-26 23:36:54,911 - INFO - Epoch 200 training loss = 1.508
2023-04-26 23:36:55,224 - INFO - Validation loss = 1.145
2023-04-26 23:36:59,224 - INFO - Epoch 201 training loss = 1.575
2023-04-26 23:37:03,224 - INFO - Epoch 202 training loss = 1.541
2023-04-26 23:37:07,223 - INFO - Epoch 203 training loss = 1.49
2023-04-26 23:37:11,224 - INFO - Epoch 204 training loss = 1.424
2023-04-26 23:37:15,227 - INFO - Epoch 205 training loss = 1.613
2023-04-26 23:37:19,227 - INFO - Epoch 206 training loss = 1.564
2023-04-26 23:37:23,225 - INFO - Epoch 207 training loss = 1.532
2023-04-26 23:37:27,224 - INFO - Epoch 208 training loss = 1.527
2023-04-26 23:37:31,223 - INFO - Epoch 209 training loss = 1.504
2023-04-26 23:37:35,222 - INFO - Epoch 210 training loss = 1.548
2023-04-26 23:37:35,534 - INFO - Validation loss = 2.613
2023-04-26 23:37:39,533 - INFO - Epoch 211 training loss = 1.432
2023-04-26 23:37:43,537 - INFO - Epoch 212 training loss = 1.621
2023-04-26 23:37:47,538 - INFO - Epoch 213 training loss = 1.459
2023-04-26 23:37:51,538 - INFO - Epoch 214 training loss = 1.516
2023-04-26 23:37:55,542 - INFO - Epoch 215 training loss = 1.543
2023-04-26 23:37:59,555 - INFO - Epoch 216 training loss = 1.438
2023-04-26 23:38:03,567 - INFO - Epoch 217 training loss = 1.658
2023-04-26 23:38:07,577 - INFO - Epoch 218 training loss = 1.441
2023-04-26 23:38:11,590 - INFO - Epoch 219 training loss = 1.441
2023-04-26 23:38:15,604 - INFO - Epoch 220 training loss = 1.65
2023-04-26 23:38:15,917 - INFO - Validation loss = 1.494
2023-04-26 23:38:19,929 - INFO - Epoch 221 training loss = 1.468
2023-04-26 23:38:23,939 - INFO - Epoch 222 training loss = 1.405
2023-04-26 23:38:27,951 - INFO - Epoch 223 training loss = 1.469
2023-04-26 23:38:31,960 - INFO - Epoch 224 training loss = 1.324
2023-04-26 23:38:35,969 - INFO - Epoch 225 training loss = 1.447
2023-04-26 23:38:39,978 - INFO - Epoch 226 training loss = 1.565
2023-04-26 23:38:43,983 - INFO - Epoch 227 training loss = 1.336
2023-04-26 23:38:47,983 - INFO - Epoch 228 training loss = 1.313
2023-04-26 23:38:51,981 - INFO - Epoch 229 training loss = 1.212
2023-04-26 23:38:55,981 - INFO - Epoch 230 training loss = 1.539
2023-04-26 23:38:56,293 - INFO - Validation loss = 1.835
2023-04-26 23:39:00,303 - INFO - Epoch 231 training loss = 1.258
2023-04-26 23:39:04,314 - INFO - Epoch 232 training loss = 1.411
2023-04-26 23:39:08,324 - INFO - Epoch 233 training loss = 1.437
2023-04-26 23:39:12,338 - INFO - Epoch 234 training loss = 1.323
2023-04-26 23:39:16,352 - INFO - Epoch 235 training loss = 1.218
2023-04-26 23:39:20,362 - INFO - Epoch 236 training loss = 1.311
2023-04-26 23:39:24,372 - INFO - Epoch 237 training loss = 1.242
2023-04-26 23:39:28,383 - INFO - Epoch 238 training loss = 1.21
2023-04-26 23:39:32,392 - INFO - Epoch 239 training loss = 1.47
2023-04-26 23:39:36,400 - INFO - Epoch 240 training loss = 1.286
2023-04-26 23:39:36,714 - INFO - Validation loss = 1.079
2023-04-26 23:39:36,714 - INFO - best model
2023-04-26 23:39:40,746 - INFO - Epoch 241 training loss = 1.354
2023-04-26 23:39:44,758 - INFO - Epoch 242 training loss = 1.432
2023-04-26 23:39:48,770 - INFO - Epoch 243 training loss = 1.405
2023-04-26 23:39:52,780 - INFO - Epoch 244 training loss = 1.395
2023-04-26 23:39:56,789 - INFO - Epoch 245 training loss = 1.226
2023-04-26 23:40:00,800 - INFO - Epoch 246 training loss = 1.382
2023-04-26 23:40:04,812 - INFO - Epoch 247 training loss = 1.288
2023-04-26 23:40:08,821 - INFO - Epoch 248 training loss = 1.334
2023-04-26 23:40:12,825 - INFO - Epoch 249 training loss = 1.335
2023-04-26 23:40:16,826 - INFO - Epoch 250 training loss = 1.222
2023-04-26 23:40:17,138 - INFO - Validation loss = 1.519
2023-04-26 23:40:21,137 - INFO - Epoch 251 training loss = 1.291
2023-04-26 23:40:25,134 - INFO - Epoch 252 training loss = 1.27
2023-04-26 23:40:29,132 - INFO - Epoch 253 training loss = 1.337
2023-04-26 23:40:33,130 - INFO - Epoch 254 training loss = 1.289
2023-04-26 23:40:37,127 - INFO - Epoch 255 training loss = 1.231
2023-04-26 23:40:41,126 - INFO - Epoch 256 training loss = 1.156
2023-04-26 23:40:45,127 - INFO - Epoch 257 training loss = 1.263
2023-04-26 23:40:49,125 - INFO - Epoch 258 training loss = 1.268
2023-04-26 23:40:53,123 - INFO - Epoch 259 training loss = 1.287
2023-04-26 23:40:57,120 - INFO - Epoch 260 training loss = 1.246
2023-04-26 23:40:57,434 - INFO - Validation loss = 1.032
2023-04-26 23:40:57,434 - INFO - best model
2023-04-26 23:41:01,454 - INFO - Epoch 261 training loss = 1.252
2023-04-26 23:41:05,453 - INFO - Epoch 262 training loss = 1.191
2023-04-26 23:41:09,450 - INFO - Epoch 263 training loss = 1.152
2023-04-26 23:41:13,452 - INFO - Epoch 264 training loss = 1.192
2023-04-26 23:41:17,450 - INFO - Epoch 265 training loss = 1.215
2023-04-26 23:41:21,448 - INFO - Epoch 266 training loss = 1.123
2023-04-26 23:41:25,444 - INFO - Epoch 267 training loss = 1.364
2023-04-26 23:41:29,443 - INFO - Epoch 268 training loss = 1.258
2023-04-26 23:41:33,442 - INFO - Epoch 269 training loss = 1.33
2023-04-26 23:41:37,452 - INFO - Epoch 270 training loss = 1.185
2023-04-26 23:41:37,766 - INFO - Validation loss = 1.926
2023-04-26 23:41:41,776 - INFO - Epoch 271 training loss = 1.293
2023-04-26 23:41:45,777 - INFO - Epoch 272 training loss = 1.18
2023-04-26 23:41:49,776 - INFO - Epoch 273 training loss = 1.236
2023-04-26 23:41:53,773 - INFO - Epoch 274 training loss = 1.098
2023-04-26 23:41:57,778 - INFO - Epoch 275 training loss = 1.206
2023-04-26 23:42:01,788 - INFO - Epoch 276 training loss = 1.178
2023-04-26 23:42:05,797 - INFO - Epoch 277 training loss = 1.161
2023-04-26 23:42:09,807 - INFO - Epoch 278 training loss = 1.252
2023-04-26 23:42:13,822 - INFO - Epoch 279 training loss = 1.29
2023-04-26 23:42:17,834 - INFO - Epoch 280 training loss = 1.337
2023-04-26 23:42:18,148 - INFO - Validation loss = 1.079
2023-04-26 23:42:22,157 - INFO - Epoch 281 training loss = 1.274
2023-04-26 23:42:26,167 - INFO - Epoch 282 training loss = 1.195
2023-04-26 23:42:30,166 - INFO - Epoch 283 training loss = 1.153
2023-04-26 23:42:34,164 - INFO - Epoch 284 training loss = 1.299
2023-04-26 23:42:38,164 - INFO - Epoch 285 training loss = 1.14
2023-04-26 23:42:42,165 - INFO - Epoch 286 training loss = 1.161
2023-04-26 23:42:46,167 - INFO - Epoch 287 training loss = 1.166
2023-04-26 23:42:50,166 - INFO - Epoch 288 training loss = 1.208
2023-04-26 23:42:54,164 - INFO - Epoch 289 training loss = 1.192
2023-04-26 23:42:58,164 - INFO - Epoch 290 training loss = 1.095
2023-04-26 23:42:58,476 - INFO - Validation loss = 0.7238
2023-04-26 23:42:58,476 - INFO - best model
2023-04-26 23:43:02,496 - INFO - Epoch 291 training loss = 1.133
2023-04-26 23:43:06,495 - INFO - Epoch 292 training loss = 1.232
2023-04-26 23:43:10,495 - INFO - Epoch 293 training loss = 1.124
2023-04-26 23:43:14,496 - INFO - Epoch 294 training loss = 1.027
2023-04-26 23:43:18,495 - INFO - Epoch 295 training loss = 1.059
2023-04-26 23:43:22,493 - INFO - Epoch 296 training loss = 1.16
2023-04-26 23:43:26,494 - INFO - Epoch 297 training loss = 1.07
2023-04-26 23:43:30,504 - INFO - Epoch 298 training loss = 1.142
2023-04-26 23:43:34,513 - INFO - Epoch 299 training loss = 1.034
2023-04-26 23:43:38,522 - INFO - Epoch 300 training loss = 1.068
2023-04-26 23:43:38,836 - INFO - Validation loss = 0.9588
2023-04-26 23:43:42,850 - INFO - Epoch 301 training loss = 1.093
2023-04-26 23:43:46,863 - INFO - Epoch 302 training loss = 1.041
2023-04-26 23:43:50,874 - INFO - Epoch 303 training loss = 1.093
2023-04-26 23:43:54,884 - INFO - Epoch 304 training loss = 1.141
2023-04-26 23:43:58,884 - INFO - Epoch 305 training loss = 1.126
2023-04-26 23:44:02,883 - INFO - Epoch 306 training loss = 1.009
2023-04-26 23:44:06,882 - INFO - Epoch 307 training loss = 1.093
2023-04-26 23:44:10,881 - INFO - Epoch 308 training loss = 0.9947
2023-04-26 23:44:14,884 - INFO - Epoch 309 training loss = 0.9657
2023-04-26 23:44:18,885 - INFO - Epoch 310 training loss = 1.064
2023-04-26 23:44:19,198 - INFO - Validation loss = 0.8184
2023-04-26 23:44:23,197 - INFO - Epoch 311 training loss = 1.07
2023-04-26 23:44:27,197 - INFO - Epoch 312 training loss = 1.039
2023-04-26 23:44:31,195 - INFO - Epoch 313 training loss = 0.9949
2023-04-26 23:44:35,194 - INFO - Epoch 314 training loss = 1.027
2023-04-26 23:44:39,193 - INFO - Epoch 315 training loss =  1.0
2023-04-26 23:44:43,200 - INFO - Epoch 316 training loss = 1.062
2023-04-26 23:44:47,201 - INFO - Epoch 317 training loss = 0.9555
2023-04-26 23:44:51,200 - INFO - Epoch 318 training loss = 0.9873
2023-04-26 23:44:55,198 - INFO - Epoch 319 training loss = 0.991
2023-04-26 23:44:59,197 - INFO - Epoch 320 training loss = 1.061
2023-04-26 23:44:59,509 - INFO - Validation loss = 0.9434
2023-04-26 23:45:03,508 - INFO - Epoch 321 training loss = 0.9998
2023-04-26 23:45:07,509 - INFO - Epoch 322 training loss = 0.9455
2023-04-26 23:45:11,521 - INFO - Epoch 323 training loss = 0.9085
2023-04-26 23:45:15,522 - INFO - Epoch 324 training loss = 0.9492
2023-04-26 23:45:19,522 - INFO - Epoch 325 training loss = 1.049
2023-04-26 23:45:23,519 - INFO - Epoch 326 training loss = 0.9945
2023-04-26 23:45:27,524 - INFO - Epoch 327 training loss = 0.9733
2023-04-26 23:45:31,534 - INFO - Epoch 328 training loss = 1.031
2023-04-26 23:45:35,544 - INFO - Epoch 329 training loss = 0.9334
2023-04-26 23:45:39,554 - INFO - Epoch 330 training loss = 0.9291
2023-04-26 23:45:39,868 - INFO - Validation loss = 1.714
2023-04-26 23:45:43,882 - INFO - Epoch 331 training loss = 0.957
2023-04-26 23:45:47,895 - INFO - Epoch 332 training loss = 0.9304
2023-04-26 23:45:51,905 - INFO - Epoch 333 training loss = 0.9855
2023-04-26 23:45:55,916 - INFO - Epoch 334 training loss = 1.046
2023-04-26 23:45:59,925 - INFO - Epoch 335 training loss = 0.9202
2023-04-26 23:46:03,936 - INFO - Epoch 336 training loss = 1.013
2023-04-26 23:46:07,947 - INFO - Epoch 337 training loss = 0.9566
2023-04-26 23:46:11,961 - INFO - Epoch 338 training loss = 0.9074
2023-04-26 23:46:15,975 - INFO - Epoch 339 training loss = 0.9278
2023-04-26 23:46:19,987 - INFO - Epoch 340 training loss = 0.951
2023-04-26 23:46:20,300 - INFO - Validation loss = 1.235
2023-04-26 23:46:24,310 - INFO - Epoch 341 training loss = 1.046
2023-04-26 23:46:28,322 - INFO - Epoch 342 training loss = 1.119
2023-04-26 23:46:32,333 - INFO - Epoch 343 training loss = 0.9271
2023-04-26 23:46:36,344 - INFO - Epoch 344 training loss = 1.003
2023-04-26 23:46:40,357 - INFO - Epoch 345 training loss = 1.104
2023-04-26 23:46:44,373 - INFO - Epoch 346 training loss = 0.9852
2023-04-26 23:46:48,387 - INFO - Epoch 347 training loss = 0.9269
2023-04-26 23:46:52,398 - INFO - Epoch 348 training loss = 0.8845
2023-04-26 23:46:56,412 - INFO - Epoch 349 training loss = 0.9678
2023-04-26 23:47:00,433 - INFO - Epoch 350 training loss = 0.9974
2023-04-26 23:47:00,747 - INFO - Validation loss = 1.18
2023-04-26 23:47:04,759 - INFO - Epoch 351 training loss = 0.8799
2023-04-26 23:47:08,769 - INFO - Epoch 352 training loss = 0.9017
2023-04-26 23:47:12,782 - INFO - Epoch 353 training loss = 0.9776
2023-04-26 23:47:16,795 - INFO - Epoch 354 training loss = 0.8822
2023-04-26 23:47:20,807 - INFO - Epoch 355 training loss = 0.9106
2023-04-26 23:47:24,818 - INFO - Epoch 356 training loss = 0.9693
2023-04-26 23:47:28,828 - INFO - Epoch 357 training loss = 0.9205
2023-04-26 23:47:32,837 - INFO - Epoch 358 training loss = 0.9863
2023-04-26 23:47:36,848 - INFO - Epoch 359 training loss = 0.8581
2023-04-26 23:47:40,866 - INFO - Epoch 360 training loss = 0.8422
2023-04-26 23:47:41,180 - INFO - Validation loss = 0.8291
2023-04-26 23:47:45,209 - INFO - Epoch 361 training loss = 0.8935
2023-04-26 23:47:49,236 - INFO - Epoch 362 training loss = 0.9175
2023-04-26 23:47:53,261 - INFO - Epoch 363 training loss = 0.85
2023-04-26 23:47:57,287 - INFO - Epoch 364 training loss = 0.9166
2023-04-26 23:48:01,313 - INFO - Epoch 365 training loss = 0.83
2023-04-26 23:48:05,340 - INFO - Epoch 366 training loss = 0.8743
2023-04-26 23:48:09,364 - INFO - Epoch 367 training loss = 0.8061
2023-04-26 23:48:13,384 - INFO - Epoch 368 training loss = 0.8444
2023-04-26 23:48:17,396 - INFO - Epoch 369 training loss = 0.9086
2023-04-26 23:48:21,407 - INFO - Epoch 370 training loss = 0.7851
2023-04-26 23:48:21,721 - INFO - Validation loss = 0.6603
2023-04-26 23:48:21,721 - INFO - best model
2023-04-26 23:48:25,754 - INFO - Epoch 371 training loss = 0.9415
2023-04-26 23:48:29,764 - INFO - Epoch 372 training loss = 0.9469
2023-04-26 23:48:33,774 - INFO - Epoch 373 training loss = 0.7762
2023-04-26 23:48:37,785 - INFO - Epoch 374 training loss = 0.7511
2023-04-26 23:48:41,798 - INFO - Epoch 375 training loss = 0.7959
2023-04-26 23:48:45,813 - INFO - Epoch 376 training loss = 0.811
2023-04-26 23:48:49,826 - INFO - Epoch 377 training loss = 0.8348
2023-04-26 23:48:53,837 - INFO - Epoch 378 training loss = 0.8672
2023-04-26 23:48:57,848 - INFO - Epoch 379 training loss = 0.7409
2023-04-26 23:49:01,858 - INFO - Epoch 380 training loss = 0.8945
2023-04-26 23:49:02,172 - INFO - Validation loss = 1.194
2023-04-26 23:49:06,182 - INFO - Epoch 381 training loss = 0.8251
2023-04-26 23:49:10,192 - INFO - Epoch 382 training loss = 0.8277
2023-04-26 23:49:14,206 - INFO - Epoch 383 training loss = 0.7933
2023-04-26 23:49:18,218 - INFO - Epoch 384 training loss = 0.8119
2023-04-26 23:49:22,227 - INFO - Epoch 385 training loss = 0.8778
2023-04-26 23:49:26,237 - INFO - Epoch 386 training loss = 0.83
2023-04-26 23:49:30,246 - INFO - Epoch 387 training loss = 0.8494
2023-04-26 23:49:34,255 - INFO - Epoch 388 training loss = 0.7482
2023-04-26 23:49:38,264 - INFO - Epoch 389 training loss = 0.8384
2023-04-26 23:49:42,275 - INFO - Epoch 390 training loss = 0.7737
2023-04-26 23:49:42,588 - INFO - Validation loss = 0.8838
2023-04-26 23:49:46,603 - INFO - Epoch 391 training loss = 0.7555
2023-04-26 23:49:50,616 - INFO - Epoch 392 training loss = 0.8527
2023-04-26 23:49:54,627 - INFO - Epoch 393 training loss = 0.8189
2023-04-26 23:49:58,638 - INFO - Epoch 394 training loss = 0.8391
2023-04-26 23:50:02,653 - INFO - Epoch 395 training loss = 0.783
2023-04-26 23:50:06,664 - INFO - Epoch 396 training loss = 0.822
2023-04-26 23:50:10,676 - INFO - Epoch 397 training loss = 0.7328
2023-04-26 23:50:14,690 - INFO - Epoch 398 training loss = 0.7277
2023-04-26 23:50:18,703 - INFO - Epoch 399 training loss = 0.7438
2023-04-26 23:50:22,713 - INFO - Epoch 400 training loss = 0.7763
2023-04-26 23:50:23,027 - INFO - Validation loss = 0.6336
2023-04-26 23:50:23,027 - INFO - best model
2023-04-26 23:50:27,061 - INFO - Epoch 401 training loss = 0.7595
2023-04-26 23:50:31,072 - INFO - Epoch 402 training loss = 0.7613
2023-04-26 23:50:35,082 - INFO - Epoch 403 training loss = 0.7756
2023-04-26 23:50:39,092 - INFO - Epoch 404 training loss = 0.7075
2023-04-26 23:50:43,106 - INFO - Epoch 405 training loss = 0.7423
2023-04-26 23:50:47,120 - INFO - Epoch 406 training loss = 0.7553
2023-04-26 23:50:51,131 - INFO - Epoch 407 training loss = 0.7922
2023-04-26 23:50:55,141 - INFO - Epoch 408 training loss = 0.8083
2023-04-26 23:50:59,153 - INFO - Epoch 409 training loss = 0.6846
2023-04-26 23:51:03,165 - INFO - Epoch 410 training loss = 0.7687
2023-04-26 23:51:03,478 - INFO - Validation loss = 0.7134
2023-04-26 23:51:07,490 - INFO - Epoch 411 training loss = 0.7387
2023-04-26 23:51:11,503 - INFO - Epoch 412 training loss = 0.6814
2023-04-26 23:51:15,517 - INFO - Epoch 413 training loss = 0.6947
2023-04-26 23:51:19,529 - INFO - Epoch 414 training loss = 0.6983
2023-04-26 23:51:23,540 - INFO - Epoch 415 training loss = 0.6562
2023-04-26 23:51:27,551 - INFO - Epoch 416 training loss = 0.7086
2023-04-26 23:51:31,561 - INFO - Epoch 417 training loss = 0.6928
2023-04-26 23:51:35,572 - INFO - Epoch 418 training loss = 0.7163
2023-04-26 23:51:39,583 - INFO - Epoch 419 training loss = 0.6611
2023-04-26 23:51:43,597 - INFO - Epoch 420 training loss = 0.6688
2023-04-26 23:51:43,911 - INFO - Validation loss = 0.7578
2023-04-26 23:51:47,925 - INFO - Epoch 421 training loss = 0.7775
2023-04-26 23:51:51,935 - INFO - Epoch 422 training loss = 0.6581
2023-04-26 23:51:55,946 - INFO - Epoch 423 training loss = 0.6969
2023-04-26 23:51:59,956 - INFO - Epoch 424 training loss = 0.6641
2023-04-26 23:52:03,968 - INFO - Epoch 425 training loss = 0.6872
2023-04-26 23:52:07,977 - INFO - Epoch 426 training loss = 0.7132
2023-04-26 23:52:11,990 - INFO - Epoch 427 training loss = 0.6869
2023-04-26 23:52:16,003 - INFO - Epoch 428 training loss = 0.7367
2023-04-26 23:52:20,014 - INFO - Epoch 429 training loss = 0.6771
2023-04-26 23:52:24,023 - INFO - Epoch 430 training loss = 0.6648
2023-04-26 23:52:24,336 - INFO - Validation loss = 0.6679
2023-04-26 23:52:28,346 - INFO - Epoch 431 training loss = 0.6314
2023-04-26 23:52:32,354 - INFO - Epoch 432 training loss = 0.6548
2023-04-26 23:52:36,363 - INFO - Epoch 433 training loss = 0.6557
2023-04-26 23:52:40,372 - INFO - Epoch 434 training loss = 0.6352
2023-04-26 23:52:44,385 - INFO - Epoch 435 training loss = 0.6508
2023-04-26 23:52:48,397 - INFO - Epoch 436 training loss = 0.6389
2023-04-26 23:52:52,406 - INFO - Epoch 437 training loss = 0.6425
2023-04-26 23:52:56,415 - INFO - Epoch 438 training loss = 0.7274
2023-04-26 23:53:00,425 - INFO - Epoch 439 training loss = 0.5769
2023-04-26 23:53:04,435 - INFO - Epoch 440 training loss = 0.6477
2023-04-26 23:53:04,749 - INFO - Validation loss = 0.8639
2023-04-26 23:53:08,759 - INFO - Epoch 441 training loss = 0.627
2023-04-26 23:53:12,772 - INFO - Epoch 442 training loss = 0.7527
2023-04-26 23:53:16,785 - INFO - Epoch 443 training loss = 0.6266
2023-04-26 23:53:20,794 - INFO - Epoch 444 training loss = 0.6361
2023-04-26 23:53:24,804 - INFO - Epoch 445 training loss = 0.5875
2023-04-26 23:53:28,814 - INFO - Epoch 446 training loss = 0.5632
2023-04-26 23:53:32,823 - INFO - Epoch 447 training loss = 0.6827
2023-04-26 23:53:36,833 - INFO - Epoch 448 training loss = 0.5966
2023-04-26 23:53:40,843 - INFO - Epoch 449 training loss = 0.6044
2023-04-26 23:53:44,856 - INFO - Epoch 450 training loss = 0.5952
2023-04-26 23:53:45,170 - INFO - Validation loss = 0.6992
2023-04-26 23:53:49,182 - INFO - Epoch 451 training loss = 0.5952
2023-04-26 23:53:53,192 - INFO - Epoch 452 training loss = 0.6288
2023-04-26 23:53:57,201 - INFO - Epoch 453 training loss = 0.6023
2023-04-26 23:54:01,212 - INFO - Epoch 454 training loss = 0.5886
2023-04-26 23:54:05,223 - INFO - Epoch 455 training loss = 0.6291
2023-04-26 23:54:09,233 - INFO - Epoch 456 training loss = 0.5473
2023-04-26 23:54:13,246 - INFO - Epoch 457 training loss = 0.6608
2023-04-26 23:54:17,259 - INFO - Epoch 458 training loss = 0.528
2023-04-26 23:54:21,270 - INFO - Epoch 459 training loss = 0.6054
2023-04-26 23:54:25,280 - INFO - Epoch 460 training loss = 0.5678
2023-04-26 23:54:25,592 - INFO - Validation loss = 0.6476
2023-04-26 23:54:29,604 - INFO - Epoch 461 training loss = 0.5843
2023-04-26 23:54:33,614 - INFO - Epoch 462 training loss = 0.6151
2023-04-26 23:54:37,626 - INFO - Epoch 463 training loss =  0.6
2023-04-26 23:54:41,639 - INFO - Epoch 464 training loss = 0.585
2023-04-26 23:54:45,653 - INFO - Epoch 465 training loss = 0.5733
2023-04-26 23:54:49,668 - INFO - Epoch 466 training loss = 0.5588
2023-04-26 23:54:53,680 - INFO - Epoch 467 training loss = 0.5836
2023-04-26 23:54:57,692 - INFO - Epoch 468 training loss = 0.6106
2023-04-26 23:55:01,704 - INFO - Epoch 469 training loss = 0.5855
2023-04-26 23:55:05,715 - INFO - Epoch 470 training loss = 0.5985
2023-04-26 23:55:06,029 - INFO - Validation loss = 0.5651
2023-04-26 23:55:06,029 - INFO - best model
2023-04-26 23:55:10,062 - INFO - Epoch 471 training loss = 0.6039
2023-04-26 23:55:14,076 - INFO - Epoch 472 training loss = 0.5863
2023-04-26 23:55:18,089 - INFO - Epoch 473 training loss = 0.5811
2023-04-26 23:55:22,099 - INFO - Epoch 474 training loss = 0.5834
2023-04-26 23:55:26,110 - INFO - Epoch 475 training loss = 0.4767
2023-04-26 23:55:30,120 - INFO - Epoch 476 training loss = 0.5676
2023-04-26 23:55:34,130 - INFO - Epoch 477 training loss = 0.6082
2023-04-26 23:55:38,140 - INFO - Epoch 478 training loss = 0.5072
2023-04-26 23:55:42,153 - INFO - Epoch 479 training loss = 0.5184
2023-04-26 23:55:46,168 - INFO - Epoch 480 training loss = 0.5458
2023-04-26 23:55:46,481 - INFO - Validation loss = 0.6504
2023-04-26 23:55:50,493 - INFO - Epoch 481 training loss = 0.5226
2023-04-26 23:55:54,503 - INFO - Epoch 482 training loss = 0.5816
2023-04-26 23:55:58,514 - INFO - Epoch 483 training loss = 0.5194
2023-04-26 23:56:02,522 - INFO - Epoch 484 training loss = 0.5828
2023-04-26 23:56:06,531 - INFO - Epoch 485 training loss = 0.6212
2023-04-26 23:56:10,541 - INFO - Epoch 486 training loss = 0.5236
2023-04-26 23:56:14,553 - INFO - Epoch 487 training loss = 0.4887
2023-04-26 23:56:18,565 - INFO - Epoch 488 training loss = 0.5216
2023-04-26 23:56:22,574 - INFO - Epoch 489 training loss = 0.4746
2023-04-26 23:56:26,582 - INFO - Epoch 490 training loss = 0.4777
2023-04-26 23:56:26,896 - INFO - Validation loss = 0.9737
2023-04-26 23:56:30,905 - INFO - Epoch 491 training loss = 0.5115
2023-04-26 23:56:34,915 - INFO - Epoch 492 training loss = 0.5229
2023-04-26 23:56:38,925 - INFO - Epoch 493 training loss = 0.5051
2023-04-26 23:56:42,937 - INFO - Epoch 494 training loss = 0.4677
2023-04-26 23:56:46,950 - INFO - Epoch 495 training loss = 0.5467
2023-04-26 23:56:50,962 - INFO - Epoch 496 training loss = 0.6007
2023-04-26 23:56:54,971 - INFO - Epoch 497 training loss = 0.5033
2023-04-26 23:56:58,973 - INFO - Epoch 498 training loss = 0.547
2023-04-26 23:57:02,973 - INFO - Epoch 499 training loss = 0.4675
2023-04-26 23:57:06,972 - INFO - Epoch 500 training loss = 0.4852
2023-04-26 23:57:07,285 - INFO - Validation loss = 0.9296
2023-04-26 23:57:11,286 - INFO - Epoch 501 training loss = 0.4718
2023-04-26 23:57:15,287 - INFO - Epoch 502 training loss = 0.4698
2023-04-26 23:57:19,287 - INFO - Epoch 503 training loss = 0.5307
2023-04-26 23:57:23,285 - INFO - Epoch 504 training loss = 0.4366
2023-04-26 23:57:27,284 - INFO - Epoch 505 training loss = 0.4273
2023-04-26 23:57:31,282 - INFO - Epoch 506 training loss = 0.5358
2023-04-26 23:57:35,279 - INFO - Epoch 507 training loss = 0.4786
2023-04-26 23:57:39,277 - INFO - Epoch 508 training loss = 0.4247
2023-04-26 23:57:43,278 - INFO - Epoch 509 training loss = 0.4775
2023-04-26 23:57:47,280 - INFO - Epoch 510 training loss = 0.4475
2023-04-26 23:57:47,593 - INFO - Validation loss = 0.6469
2023-04-26 23:57:51,592 - INFO - Epoch 511 training loss = 0.5323
2023-04-26 23:57:55,590 - INFO - Epoch 512 training loss = 0.4579
2023-04-26 23:57:59,590 - INFO - Epoch 513 training loss = 0.4851
2023-04-26 23:58:03,590 - INFO - Epoch 514 training loss = 0.4778
2023-04-26 23:58:07,589 - INFO - Epoch 515 training loss = 0.4385
2023-04-26 23:58:11,591 - INFO - Epoch 516 training loss = 0.4588
2023-04-26 23:58:15,593 - INFO - Epoch 517 training loss = 0.4381
2023-04-26 23:58:19,593 - INFO - Epoch 518 training loss = 0.4969
2023-04-26 23:58:23,591 - INFO - Epoch 519 training loss = 0.4682
2023-04-26 23:58:27,589 - INFO - Epoch 520 training loss = 0.464
2023-04-26 23:58:27,902 - INFO - Validation loss = 0.4825
2023-04-26 23:58:27,902 - INFO - best model
2023-04-26 23:58:31,921 - INFO - Epoch 521 training loss = 0.4283
2023-04-26 23:58:35,919 - INFO - Epoch 522 training loss = 0.404
2023-04-26 23:58:39,917 - INFO - Epoch 523 training loss = 0.4396
2023-04-26 23:58:43,917 - INFO - Epoch 524 training loss = 0.4878
2023-04-26 23:58:47,920 - INFO - Epoch 525 training loss = 0.4478
2023-04-26 23:58:51,919 - INFO - Epoch 526 training loss = 0.4352
2023-04-26 23:58:55,919 - INFO - Epoch 527 training loss = 0.4283
2023-04-26 23:58:59,917 - INFO - Epoch 528 training loss = 0.4233
2023-04-26 23:59:03,916 - INFO - Epoch 529 training loss = 0.4827
2023-04-26 23:59:07,915 - INFO - Epoch 530 training loss = 0.4407
2023-04-26 23:59:08,228 - INFO - Validation loss = 0.7539
2023-04-26 23:59:12,230 - INFO - Epoch 531 training loss = 0.3966
2023-04-26 23:59:16,231 - INFO - Epoch 532 training loss = 0.4551
2023-04-26 23:59:20,230 - INFO - Epoch 533 training loss = 0.417
2023-04-26 23:59:24,228 - INFO - Epoch 534 training loss = 0.4082
2023-04-26 23:59:28,227 - INFO - Epoch 535 training loss = 0.4123
2023-04-26 23:59:32,224 - INFO - Epoch 536 training loss = 0.4336
2023-04-26 23:59:36,222 - INFO - Epoch 537 training loss = 0.4378
2023-04-26 23:59:40,221 - INFO - Epoch 538 training loss = 0.4277
2023-04-26 23:59:44,224 - INFO - Epoch 539 training loss = 0.4562
2023-04-26 23:59:48,225 - INFO - Epoch 540 training loss = 0.4022
2023-04-26 23:59:48,538 - INFO - Validation loss = 0.5918
2023-04-26 23:59:52,537 - INFO - Epoch 541 training loss = 0.4066
2023-04-26 23:59:56,536 - INFO - Epoch 542 training loss = 0.3685
2023-04-27 00:00:00,534 - INFO - Epoch 543 training loss = 0.3964
2023-04-27 00:00:04,552 - INFO - Epoch 544 training loss = 0.412
2023-04-27 00:00:08,551 - INFO - Epoch 545 training loss = 0.4135
2023-04-27 00:00:12,553 - INFO - Epoch 546 training loss = 0.3181
2023-04-27 00:00:16,556 - INFO - Epoch 547 training loss = 0.4111
2023-04-27 00:00:20,556 - INFO - Epoch 548 training loss = 0.4075
2023-04-27 00:00:24,554 - INFO - Epoch 549 training loss = 0.3661
2023-04-27 00:00:28,553 - INFO - Epoch 550 training loss = 0.3717
2023-04-27 00:00:28,866 - INFO - Validation loss = 0.3783
2023-04-27 00:00:28,866 - INFO - best model
2023-04-27 00:00:32,887 - INFO - Epoch 551 training loss = 0.3713
2023-04-27 00:00:36,886 - INFO - Epoch 552 training loss = 0.3569
2023-04-27 00:00:40,886 - INFO - Epoch 553 training loss = 0.3693
2023-04-27 00:00:44,888 - INFO - Epoch 554 training loss = 0.3623
2023-04-27 00:00:48,887 - INFO - Epoch 555 training loss = 0.3892
2023-04-27 00:00:52,886 - INFO - Epoch 556 training loss = 0.3925
2023-04-27 00:00:56,889 - INFO - Epoch 557 training loss = 0.3533
2023-04-27 00:01:00,900 - INFO - Epoch 558 training loss = 0.3577
2023-04-27 00:01:04,911 - INFO - Epoch 559 training loss = 0.3852
2023-04-27 00:01:08,920 - INFO - Epoch 560 training loss = 0.4012
2023-04-27 00:01:09,235 - INFO - Validation loss = 0.3337
2023-04-27 00:01:09,235 - INFO - best model
2023-04-27 00:01:13,272 - INFO - Epoch 561 training loss = 0.3817
2023-04-27 00:01:17,285 - INFO - Epoch 562 training loss = 0.3494
2023-04-27 00:01:21,296 - INFO - Epoch 563 training loss = 0.3395
2023-04-27 00:01:25,306 - INFO - Epoch 564 training loss = 0.3541
2023-04-27 00:01:29,317 - INFO - Epoch 565 training loss = 0.3535
2023-04-27 00:01:33,328 - INFO - Epoch 566 training loss = 0.3329
2023-04-27 00:01:37,338 - INFO - Epoch 567 training loss = 0.3273
2023-04-27 00:01:41,350 - INFO - Epoch 568 training loss = 0.3729
2023-04-27 00:01:45,364 - INFO - Epoch 569 training loss = 0.3666
2023-04-27 00:01:49,376 - INFO - Epoch 570 training loss = 0.3349
2023-04-27 00:01:49,689 - INFO - Validation loss = 0.4465
2023-04-27 00:01:53,700 - INFO - Epoch 571 training loss = 0.352
2023-04-27 00:01:57,712 - INFO - Epoch 572 training loss = 0.3675
2023-04-27 00:02:01,722 - INFO - Epoch 573 training loss = 0.3118
2023-04-27 00:02:05,732 - INFO - Epoch 574 training loss = 0.332
2023-04-27 00:02:09,742 - INFO - Epoch 575 training loss = 0.3286
2023-04-27 00:02:13,755 - INFO - Epoch 576 training loss = 0.334
2023-04-27 00:02:17,767 - INFO - Epoch 577 training loss = 0.3421
2023-04-27 00:02:21,778 - INFO - Epoch 578 training loss = 0.344
2023-04-27 00:02:25,788 - INFO - Epoch 579 training loss = 0.3328
2023-04-27 00:02:29,798 - INFO - Epoch 580 training loss = 0.3263
2023-04-27 00:02:30,112 - INFO - Validation loss = 0.2729
2023-04-27 00:02:30,112 - INFO - best model
2023-04-27 00:02:34,146 - INFO - Epoch 581 training loss = 0.3057
2023-04-27 00:02:38,156 - INFO - Epoch 582 training loss = 0.3416
2023-04-27 00:02:42,169 - INFO - Epoch 583 training loss = 0.3209
2023-04-27 00:02:46,184 - INFO - Epoch 584 training loss = 0.3036
2023-04-27 00:02:50,187 - INFO - Epoch 585 training loss = 0.3424
2023-04-27 00:02:54,187 - INFO - Epoch 586 training loss = 0.2875
2023-04-27 00:02:58,187 - INFO - Epoch 587 training loss = 0.2827
2023-04-27 00:03:02,186 - INFO - Epoch 588 training loss = 0.3796
2023-04-27 00:03:06,184 - INFO - Epoch 589 training loss = 0.2857
2023-04-27 00:03:10,183 - INFO - Epoch 590 training loss = 0.2821
2023-04-27 00:03:10,496 - INFO - Validation loss = 0.3377
2023-04-27 00:03:14,499 - INFO - Epoch 591 training loss = 0.2907
2023-04-27 00:03:18,500 - INFO - Epoch 592 training loss = 0.2933
2023-04-27 00:03:22,498 - INFO - Epoch 593 training loss = 0.3468
2023-04-27 00:03:26,496 - INFO - Epoch 594 training loss = 0.3387
2023-04-27 00:03:30,495 - INFO - Epoch 595 training loss = 0.2624
2023-04-27 00:03:34,493 - INFO - Epoch 596 training loss = 0.2747
2023-04-27 00:03:38,492 - INFO - Epoch 597 training loss = 0.2608
2023-04-27 00:03:42,492 - INFO - Epoch 598 training loss = 0.3108
2023-04-27 00:03:46,493 - INFO - Epoch 599 training loss = 0.2512
2023-04-27 00:03:50,494 - INFO - Epoch 600 training loss = 0.2848
2023-04-27 00:03:50,807 - INFO - Validation loss = 0.3956
2023-04-27 00:03:54,806 - INFO - Epoch 601 training loss = 0.2881
2023-04-27 00:03:58,805 - INFO - Epoch 602 training loss = 0.272
2023-04-27 00:04:02,804 - INFO - Epoch 603 training loss = 0.2863
2023-04-27 00:04:06,803 - INFO - Epoch 604 training loss = 0.2636
2023-04-27 00:04:10,803 - INFO - Epoch 605 training loss = 0.2707
2023-04-27 00:04:14,816 - INFO - Epoch 606 training loss = 0.2755
2023-04-27 00:04:18,827 - INFO - Epoch 607 training loss = 0.2601
2023-04-27 00:04:22,836 - INFO - Epoch 608 training loss = 0.2898
2023-04-27 00:04:26,845 - INFO - Epoch 609 training loss = 0.2611
2023-04-27 00:04:30,854 - INFO - Epoch 610 training loss = 0.2873
2023-04-27 00:04:31,168 - INFO - Validation loss = 0.3232
2023-04-27 00:04:35,179 - INFO - Epoch 611 training loss = 0.2391
2023-04-27 00:04:39,189 - INFO - Epoch 612 training loss = 0.2686
2023-04-27 00:04:43,196 - INFO - Epoch 613 training loss = 0.2663
2023-04-27 00:04:47,197 - INFO - Epoch 614 training loss = 0.2304
2023-04-27 00:04:51,205 - INFO - Epoch 615 training loss = 0.2455
2023-04-27 00:04:55,215 - INFO - Epoch 616 training loss = 0.247
2023-04-27 00:04:59,225 - INFO - Epoch 617 training loss = 0.2331
2023-04-27 00:05:03,236 - INFO - Epoch 618 training loss = 0.2413
2023-04-27 00:05:07,245 - INFO - Epoch 619 training loss = 0.2482
2023-04-27 00:05:11,248 - INFO - Epoch 620 training loss = 0.2738
2023-04-27 00:05:11,561 - INFO - Validation loss = 0.3746
2023-04-27 00:05:15,574 - INFO - Epoch 621 training loss = 0.2319
2023-04-27 00:05:19,586 - INFO - Epoch 622 training loss = 0.2373
2023-04-27 00:05:23,594 - INFO - Epoch 623 training loss = 0.2379
2023-04-27 00:05:27,605 - INFO - Epoch 624 training loss = 0.241
2023-04-27 00:05:31,614 - INFO - Epoch 625 training loss = 0.2243
2023-04-27 00:05:35,622 - INFO - Epoch 626 training loss = 0.2417
2023-04-27 00:05:39,620 - INFO - Epoch 627 training loss = 0.223
2023-04-27 00:05:43,622 - INFO - Epoch 628 training loss = 0.2282
2023-04-27 00:05:47,624 - INFO - Epoch 629 training loss = 0.2373
2023-04-27 00:05:51,621 - INFO - Epoch 630 training loss = 0.223
2023-04-27 00:05:51,933 - INFO - Validation loss = 0.3814
2023-04-27 00:05:55,932 - INFO - Epoch 631 training loss = 0.2137
2023-04-27 00:05:59,930 - INFO - Epoch 632 training loss = 0.2301
2023-04-27 00:06:03,929 - INFO - Epoch 633 training loss = 0.2384
2023-04-27 00:06:07,927 - INFO - Epoch 634 training loss = 0.2169
2023-04-27 00:06:11,928 - INFO - Epoch 635 training loss = 0.2278
2023-04-27 00:06:15,929 - INFO - Epoch 636 training loss = 0.1952
2023-04-27 00:06:19,928 - INFO - Epoch 637 training loss = 0.214
2023-04-27 00:06:23,926 - INFO - Epoch 638 training loss = 0.2154
2023-04-27 00:06:27,926 - INFO - Epoch 639 training loss = 0.2329
2023-04-27 00:06:31,936 - INFO - Epoch 640 training loss = 0.1851
2023-04-27 00:06:32,249 - INFO - Validation loss = 0.342
2023-04-27 00:06:36,259 - INFO - Epoch 641 training loss = 0.1998
2023-04-27 00:06:40,270 - INFO - Epoch 642 training loss = 0.2175
2023-04-27 00:06:44,284 - INFO - Epoch 643 training loss = 0.1916
2023-04-27 00:06:48,297 - INFO - Epoch 644 training loss = 0.1946
2023-04-27 00:06:52,307 - INFO - Epoch 645 training loss = 0.2037
2023-04-27 00:06:56,318 - INFO - Epoch 646 training loss = 0.2081
2023-04-27 00:07:00,329 - INFO - Epoch 647 training loss = 0.202
2023-04-27 00:07:04,340 - INFO - Epoch 648 training loss = 0.1965
2023-04-27 00:07:08,350 - INFO - Epoch 649 training loss = 0.1999
2023-04-27 00:07:12,363 - INFO - Epoch 650 training loss = 0.1948
2023-04-27 00:07:12,677 - INFO - Validation loss = 0.1588
2023-04-27 00:07:12,677 - INFO - best model
2023-04-27 00:07:16,712 - INFO - Epoch 651 training loss = 0.1761
2023-04-27 00:07:20,723 - INFO - Epoch 652 training loss = 0.1939
2023-04-27 00:07:24,733 - INFO - Epoch 653 training loss = 0.1896
2023-04-27 00:07:28,743 - INFO - Epoch 654 training loss = 0.1803
2023-04-27 00:07:32,753 - INFO - Epoch 655 training loss = 0.1777
2023-04-27 00:07:36,763 - INFO - Epoch 656 training loss = 0.2244
2023-04-27 00:07:40,774 - INFO - Epoch 657 training loss = 0.1779
2023-04-27 00:07:44,786 - INFO - Epoch 658 training loss = 0.1755
2023-04-27 00:07:48,799 - INFO - Epoch 659 training loss = 0.1875
2023-04-27 00:07:52,808 - INFO - Epoch 660 training loss = 0.1683
2023-04-27 00:07:53,122 - INFO - Validation loss = 0.1679
2023-04-27 00:07:57,127 - INFO - Epoch 661 training loss = 0.1604
2023-04-27 00:08:01,126 - INFO - Epoch 662 training loss = 0.1745
2023-04-27 00:08:05,124 - INFO - Epoch 663 training loss = 0.171
2023-04-27 00:08:09,121 - INFO - Epoch 664 training loss = 0.1674
2023-04-27 00:08:13,123 - INFO - Epoch 665 training loss = 0.1815
2023-04-27 00:08:17,124 - INFO - Epoch 666 training loss = 0.1826
2023-04-27 00:08:21,121 - INFO - Epoch 667 training loss = 0.1661
2023-04-27 00:08:25,118 - INFO - Epoch 668 training loss = 0.1639
2023-04-27 00:08:29,115 - INFO - Epoch 669 training loss = 0.1765
2023-04-27 00:08:33,112 - INFO - Epoch 670 training loss = 0.1767
2023-04-27 00:08:33,425 - INFO - Validation loss = 0.1622
2023-04-27 00:08:37,424 - INFO - Epoch 671 training loss = 0.1454
2023-04-27 00:08:41,424 - INFO - Epoch 672 training loss = 0.1699
2023-04-27 00:08:45,426 - INFO - Epoch 673 training loss = 0.1655
2023-04-27 00:08:49,427 - INFO - Epoch 674 training loss = 0.1665
2023-04-27 00:08:53,426 - INFO - Epoch 675 training loss = 0.162
2023-04-27 00:08:57,424 - INFO - Epoch 676 training loss = 0.1851
2023-04-27 00:09:01,423 - INFO - Epoch 677 training loss = 0.1475
2023-04-27 00:09:05,422 - INFO - Epoch 678 training loss = 0.1547
2023-04-27 00:09:09,420 - INFO - Epoch 679 training loss = 0.1579
2023-04-27 00:09:13,422 - INFO - Epoch 680 training loss = 0.1519
2023-04-27 00:09:13,735 - INFO - Validation loss = 0.2131
2023-04-27 00:09:17,736 - INFO - Epoch 681 training loss = 0.1536
2023-04-27 00:09:21,735 - INFO - Epoch 682 training loss = 0.1544
2023-04-27 00:09:25,741 - INFO - Epoch 683 training loss = 0.1387
2023-04-27 00:09:29,751 - INFO - Epoch 684 training loss = 0.1299
2023-04-27 00:09:33,760 - INFO - Epoch 685 training loss = 0.1502
2023-04-27 00:09:37,769 - INFO - Epoch 686 training loss = 0.1397
2023-04-27 00:09:41,780 - INFO - Epoch 687 training loss = 0.1384
2023-04-27 00:09:45,792 - INFO - Epoch 688 training loss = 0.1483
2023-04-27 00:09:49,803 - INFO - Epoch 689 training loss = 0.1387
2023-04-27 00:09:53,812 - INFO - Epoch 690 training loss = 0.1219
2023-04-27 00:09:54,126 - INFO - Validation loss = 0.2458
2023-04-27 00:09:58,136 - INFO - Epoch 691 training loss = 0.1342
2023-04-27 00:10:02,148 - INFO - Epoch 692 training loss = 0.1409
2023-04-27 00:10:06,157 - INFO - Epoch 693 training loss = 0.1295
2023-04-27 00:10:10,167 - INFO - Epoch 694 training loss = 0.1335
2023-04-27 00:10:14,180 - INFO - Epoch 695 training loss = 0.1414
2023-04-27 00:10:18,192 - INFO - Epoch 696 training loss = 0.1377
2023-04-27 00:10:22,191 - INFO - Epoch 697 training loss = 0.1483
2023-04-27 00:10:26,190 - INFO - Epoch 698 training loss = 0.1477
2023-04-27 00:10:30,191 - INFO - Epoch 699 training loss = 0.1234
2023-04-27 00:10:34,200 - INFO - Epoch 700 training loss = 0.1169
2023-04-27 00:10:34,514 - INFO - Validation loss = 0.1589
2023-04-27 00:10:38,523 - INFO - Epoch 701 training loss = 0.1362
2023-04-27 00:10:42,533 - INFO - Epoch 702 training loss = 0.1308
2023-04-27 00:10:46,546 - INFO - Epoch 703 training loss = 0.1382
2023-04-27 00:10:50,556 - INFO - Epoch 704 training loss = 0.1204
2023-04-27 00:10:54,565 - INFO - Epoch 705 training loss = 0.1205
2023-04-27 00:10:58,576 - INFO - Epoch 706 training loss = 0.1099
2023-04-27 00:11:02,586 - INFO - Epoch 707 training loss = 0.1186
2023-04-27 00:11:06,595 - INFO - Epoch 708 training loss = 0.1129
2023-04-27 00:11:10,605 - INFO - Epoch 709 training loss = 0.1062
2023-04-27 00:11:14,617 - INFO - Epoch 710 training loss = 0.09925
2023-04-27 00:11:14,930 - INFO - Validation loss = 0.149
2023-04-27 00:11:14,931 - INFO - best model
2023-04-27 00:11:18,966 - INFO - Epoch 711 training loss = 0.1319
2023-04-27 00:11:22,975 - INFO - Epoch 712 training loss = 0.1098
2023-04-27 00:11:26,983 - INFO - Epoch 713 training loss = 0.1012
2023-04-27 00:11:30,992 - INFO - Epoch 714 training loss = 0.1159
2023-04-27 00:11:35,000 - INFO - Epoch 715 training loss = 0.1121
2023-04-27 00:11:39,010 - INFO - Epoch 716 training loss = 0.1005
2023-04-27 00:11:43,021 - INFO - Epoch 717 training loss = 0.1057
2023-04-27 00:11:47,033 - INFO - Epoch 718 training loss = 0.1037
2023-04-27 00:11:51,043 - INFO - Epoch 719 training loss = 0.1085
2023-04-27 00:11:55,052 - INFO - Epoch 720 training loss = 0.1037
2023-04-27 00:11:55,366 - INFO - Validation loss = 0.1532
2023-04-27 00:11:59,377 - INFO - Epoch 721 training loss = 0.1169
2023-04-27 00:12:03,388 - INFO - Epoch 722 training loss = 0.1016
2023-04-27 00:12:07,398 - INFO - Epoch 723 training loss = 0.1111
2023-04-27 00:12:11,410 - INFO - Epoch 724 training loss = 0.1017
2023-04-27 00:12:15,422 - INFO - Epoch 725 training loss = 0.1017
2023-04-27 00:12:19,434 - INFO - Epoch 726 training loss = 0.08988
2023-04-27 00:12:23,443 - INFO - Epoch 727 training loss = 0.08685
2023-04-27 00:12:27,453 - INFO - Epoch 728 training loss = 0.1045
2023-04-27 00:12:31,462 - INFO - Epoch 729 training loss = 0.09672
2023-04-27 00:12:35,471 - INFO - Epoch 730 training loss = 0.09051
2023-04-27 00:12:35,784 - INFO - Validation loss = 0.1253
2023-04-27 00:12:35,785 - INFO - best model
2023-04-27 00:12:39,815 - INFO - Epoch 731 training loss = 0.08899
2023-04-27 00:12:43,827 - INFO - Epoch 732 training loss = 0.08766
2023-04-27 00:12:47,839 - INFO - Epoch 733 training loss = 0.09297
2023-04-27 00:12:51,847 - INFO - Epoch 734 training loss = 0.09008
2023-04-27 00:12:55,856 - INFO - Epoch 735 training loss = 0.08742
2023-04-27 00:12:59,866 - INFO - Epoch 736 training loss = 0.08751
2023-04-27 00:13:03,877 - INFO - Epoch 737 training loss = 0.09019
2023-04-27 00:13:07,887 - INFO - Epoch 738 training loss = 0.093
2023-04-27 00:13:11,899 - INFO - Epoch 739 training loss = 0.08168
2023-04-27 00:13:15,910 - INFO - Epoch 740 training loss = 0.08625
2023-04-27 00:13:16,224 - INFO - Validation loss = 0.1175
2023-04-27 00:13:16,225 - INFO - best model
2023-04-27 00:13:20,257 - INFO - Epoch 741 training loss = 0.08625
2023-04-27 00:13:24,266 - INFO - Epoch 742 training loss = 0.0922
2023-04-27 00:13:28,276 - INFO - Epoch 743 training loss = 0.08634
2023-04-27 00:13:32,286 - INFO - Epoch 744 training loss = 0.08723
2023-04-27 00:13:36,295 - INFO - Epoch 745 training loss = 0.08532
2023-04-27 00:13:40,305 - INFO - Epoch 746 training loss = 0.07725
2023-04-27 00:13:44,318 - INFO - Epoch 747 training loss = 0.08544
2023-04-27 00:13:48,331 - INFO - Epoch 748 training loss = 0.07616
2023-04-27 00:13:52,341 - INFO - Epoch 749 training loss = 0.08044
2023-04-27 00:13:56,351 - INFO - Epoch 750 training loss = 0.07851
2023-04-27 00:13:56,664 - INFO - Validation loss = 0.1539
2023-04-27 00:14:00,675 - INFO - Epoch 751 training loss = 0.07408
2023-04-27 00:14:04,685 - INFO - Epoch 752 training loss = 0.06768
2023-04-27 00:14:08,696 - INFO - Epoch 753 training loss = 0.06531
2023-04-27 00:14:12,709 - INFO - Epoch 754 training loss = 0.07306
2023-04-27 00:14:16,722 - INFO - Epoch 755 training loss = 0.07023
2023-04-27 00:14:20,732 - INFO - Epoch 756 training loss = 0.07491
2023-04-27 00:14:24,740 - INFO - Epoch 757 training loss = 0.06757
2023-04-27 00:14:28,750 - INFO - Epoch 758 training loss = 0.07302
2023-04-27 00:14:32,758 - INFO - Epoch 759 training loss = 0.06778
2023-04-27 00:14:36,767 - INFO - Epoch 760 training loss = 0.06736
2023-04-27 00:14:37,080 - INFO - Validation loss = 0.1318
2023-04-27 00:14:41,092 - INFO - Epoch 761 training loss = 0.07143
2023-04-27 00:14:45,106 - INFO - Epoch 762 training loss = 0.0681
2023-04-27 00:14:49,118 - INFO - Epoch 763 training loss = 0.0624
2023-04-27 00:14:53,127 - INFO - Epoch 764 training loss = 0.06323
2023-04-27 00:14:57,138 - INFO - Epoch 765 training loss = 0.06185
2023-04-27 00:15:01,149 - INFO - Epoch 766 training loss = 0.06452
2023-04-27 00:15:05,160 - INFO - Epoch 767 training loss = 0.05968
2023-04-27 00:15:09,172 - INFO - Epoch 768 training loss = 0.06678
2023-04-27 00:15:13,184 - INFO - Epoch 769 training loss = 0.0666
2023-04-27 00:15:17,197 - INFO - Epoch 770 training loss = 0.05785
2023-04-27 00:15:17,511 - INFO - Validation loss = 0.08345
2023-04-27 00:15:17,511 - INFO - best model
2023-04-27 00:15:21,544 - INFO - Epoch 771 training loss = 0.05873
2023-04-27 00:15:25,554 - INFO - Epoch 772 training loss = 0.05953
2023-04-27 00:15:29,564 - INFO - Epoch 773 training loss = 0.05886
2023-04-27 00:15:33,574 - INFO - Epoch 774 training loss = 0.06385
2023-04-27 00:15:37,584 - INFO - Epoch 775 training loss = 0.05473
2023-04-27 00:15:41,595 - INFO - Epoch 776 training loss = 0.05652
2023-04-27 00:15:45,608 - INFO - Epoch 777 training loss = 0.05634
2023-04-27 00:15:49,621 - INFO - Epoch 778 training loss = 0.05359
2023-04-27 00:15:53,631 - INFO - Epoch 779 training loss = 0.05705
2023-04-27 00:15:57,695 - INFO - Epoch 780 training loss = 0.05369
2023-04-27 00:15:58,010 - INFO - Validation loss = 0.112
2023-04-27 00:16:02,104 - INFO - Epoch 781 training loss = 0.0516
2023-04-27 00:16:06,183 - INFO - Epoch 782 training loss = 0.05367
2023-04-27 00:16:10,194 - INFO - Epoch 783 training loss = 0.05344
2023-04-27 00:16:14,207 - INFO - Epoch 784 training loss = 0.05085
2023-04-27 00:16:18,220 - INFO - Epoch 785 training loss = 0.04864
2023-04-27 00:16:22,228 - INFO - Epoch 786 training loss = 0.04729
2023-04-27 00:16:26,237 - INFO - Epoch 787 training loss = 0.04519
2023-04-27 00:16:30,247 - INFO - Epoch 788 training loss = 0.04815
2023-04-27 00:16:34,258 - INFO - Epoch 789 training loss = 0.0497
2023-04-27 00:16:38,262 - INFO - Epoch 790 training loss = 0.04941
2023-04-27 00:16:38,575 - INFO - Validation loss = 0.07655
2023-04-27 00:16:38,575 - INFO - best model
2023-04-27 00:16:42,596 - INFO - Epoch 791 training loss = 0.04584
2023-04-27 00:16:46,597 - INFO - Epoch 792 training loss = 0.05207
2023-04-27 00:16:50,596 - INFO - Epoch 793 training loss = 0.04904
2023-04-27 00:16:54,593 - INFO - Epoch 794 training loss = 0.04895
2023-04-27 00:16:58,591 - INFO - Epoch 795 training loss = 0.04756
2023-04-27 00:17:02,591 - INFO - Epoch 796 training loss = 0.04373
2023-04-27 00:17:06,589 - INFO - Epoch 797 training loss = 0.04366
2023-04-27 00:17:10,589 - INFO - Epoch 798 training loss = 0.04596
2023-04-27 00:17:14,601 - INFO - Epoch 799 training loss = 0.04043
2023-04-27 00:17:18,614 - INFO - Epoch 800 training loss = 0.04411
2023-04-27 00:17:18,927 - INFO - Validation loss = 0.07122
2023-04-27 00:17:18,928 - INFO - best model
2023-04-27 00:17:22,959 - INFO - Epoch 801 training loss = 0.04029
2023-04-27 00:17:26,969 - INFO - Epoch 802 training loss = 0.04473
2023-04-27 00:17:30,978 - INFO - Epoch 803 training loss = 0.04121
2023-04-27 00:17:34,988 - INFO - Epoch 804 training loss = 0.04125
2023-04-27 00:17:38,998 - INFO - Epoch 805 training loss = 0.03679
2023-04-27 00:17:43,010 - INFO - Epoch 806 training loss = 0.03826
2023-04-27 00:17:47,023 - INFO - Epoch 807 training loss = 0.04246
2023-04-27 00:17:51,033 - INFO - Epoch 808 training loss = 0.03852
2023-04-27 00:17:55,043 - INFO - Epoch 809 training loss = 0.03996
2023-04-27 00:17:59,054 - INFO - Epoch 810 training loss = 0.03559
2023-04-27 00:17:59,368 - INFO - Validation loss = 0.08255
2023-04-27 00:18:03,379 - INFO - Epoch 811 training loss = 0.03486
2023-04-27 00:18:07,389 - INFO - Epoch 812 training loss = 0.03572
2023-04-27 00:18:11,402 - INFO - Epoch 813 training loss = 0.03538
2023-04-27 00:18:15,416 - INFO - Epoch 814 training loss = 0.03528
2023-04-27 00:18:19,428 - INFO - Epoch 815 training loss = 0.03389
2023-04-27 00:18:23,437 - INFO - Epoch 816 training loss = 0.03526
2023-04-27 00:18:27,447 - INFO - Epoch 817 training loss = 0.03309
2023-04-27 00:18:31,456 - INFO - Epoch 818 training loss = 0.03364
2023-04-27 00:18:35,466 - INFO - Epoch 819 training loss = 0.03623
2023-04-27 00:18:39,476 - INFO - Epoch 820 training loss = 0.03614
2023-04-27 00:18:39,790 - INFO - Validation loss = 0.06692
2023-04-27 00:18:39,790 - INFO - best model
2023-04-27 00:18:43,826 - INFO - Epoch 821 training loss = 0.03297
2023-04-27 00:18:47,829 - INFO - Epoch 822 training loss = 0.03377
2023-04-27 00:18:51,827 - INFO - Epoch 823 training loss = 0.03359
2023-04-27 00:18:55,827 - INFO - Epoch 824 training loss = 0.03123
2023-04-27 00:18:59,837 - INFO - Epoch 825 training loss = 0.03117
2023-04-27 00:19:03,849 - INFO - Epoch 826 training loss = 0.03136
2023-04-27 00:19:07,859 - INFO - Epoch 827 training loss = 0.0295
2023-04-27 00:19:11,872 - INFO - Epoch 828 training loss = 0.03032
2023-04-27 00:19:15,885 - INFO - Epoch 829 training loss = 0.02801
2023-04-27 00:19:19,898 - INFO - Epoch 830 training loss = 0.02768
2023-04-27 00:19:20,211 - INFO - Validation loss = 0.07414
2023-04-27 00:19:24,222 - INFO - Epoch 831 training loss = 0.02907
2023-04-27 00:19:28,232 - INFO - Epoch 832 training loss = 0.02719
2023-04-27 00:19:32,242 - INFO - Epoch 833 training loss = 0.02843
2023-04-27 00:19:36,252 - INFO - Epoch 834 training loss = 0.03013
2023-04-27 00:19:40,262 - INFO - Epoch 835 training loss = 0.02731
2023-04-27 00:19:44,276 - INFO - Epoch 836 training loss = 0.02692
2023-04-27 00:19:48,290 - INFO - Epoch 837 training loss = 0.02654
2023-04-27 00:19:52,299 - INFO - Epoch 838 training loss = 0.026
2023-04-27 00:19:56,308 - INFO - Epoch 839 training loss = 0.02564
2023-04-27 00:20:00,307 - INFO - Epoch 840 training loss = 0.02818
2023-04-27 00:20:00,620 - INFO - Validation loss = 0.06781
2023-04-27 00:20:04,623 - INFO - Epoch 841 training loss = 0.02619
2023-04-27 00:20:08,621 - INFO - Epoch 842 training loss = 0.02489
2023-04-27 00:20:12,622 - INFO - Epoch 843 training loss = 0.02507
2023-04-27 00:20:16,635 - INFO - Epoch 844 training loss = 0.02459
2023-04-27 00:20:20,647 - INFO - Epoch 845 training loss = 0.02346
2023-04-27 00:20:24,657 - INFO - Epoch 846 training loss = 0.02428
2023-04-27 00:20:28,668 - INFO - Epoch 847 training loss = 0.02404
2023-04-27 00:20:32,678 - INFO - Epoch 848 training loss = 0.02344
2023-04-27 00:20:36,689 - INFO - Epoch 849 training loss = 0.02363
2023-04-27 00:20:40,699 - INFO - Epoch 850 training loss = 0.02345
2023-04-27 00:20:41,012 - INFO - Validation loss = 0.05481
2023-04-27 00:20:41,013 - INFO - best model
2023-04-27 00:20:45,060 - INFO - Epoch 851 training loss = 0.02281
2023-04-27 00:20:49,073 - INFO - Epoch 852 training loss = 0.02205
2023-04-27 00:20:53,084 - INFO - Epoch 853 training loss = 0.02222
2023-04-27 00:20:57,096 - INFO - Epoch 854 training loss = 0.02168
2023-04-27 00:21:01,108 - INFO - Epoch 855 training loss = 0.0218
2023-04-27 00:21:05,122 - INFO - Epoch 856 training loss = 0.0216
2023-04-27 00:21:09,133 - INFO - Epoch 857 training loss = 0.02187
2023-04-27 00:21:13,148 - INFO - Epoch 858 training loss = 0.02129
2023-04-27 00:21:17,162 - INFO - Epoch 859 training loss = 0.02162
2023-04-27 00:21:21,164 - INFO - Epoch 860 training loss = 0.02141
2023-04-27 00:21:21,476 - INFO - Validation loss = 0.05647
2023-04-27 00:21:25,477 - INFO - Epoch 861 training loss = 0.02076
2023-04-27 00:21:29,489 - INFO - Epoch 862 training loss = 0.0211
2023-04-27 00:21:33,499 - INFO - Epoch 863 training loss = 0.02061
2023-04-27 00:21:37,509 - INFO - Epoch 864 training loss = 0.02079
2023-04-27 00:21:41,521 - INFO - Epoch 865 training loss = 0.0194
2023-04-27 00:21:45,534 - INFO - Epoch 866 training loss = 0.0203
2023-04-27 00:21:49,547 - INFO - Epoch 867 training loss = 0.01893
2023-04-27 00:21:53,557 - INFO - Epoch 868 training loss = 0.01906
2023-04-27 00:21:57,567 - INFO - Epoch 869 training loss = 0.01941
2023-04-27 00:22:01,577 - INFO - Epoch 870 training loss = 0.01842
2023-04-27 00:22:01,890 - INFO - Validation loss = 0.05281
2023-04-27 00:22:01,890 - INFO - best model
2023-04-27 00:22:05,923 - INFO - Epoch 871 training loss = 0.01884
2023-04-27 00:22:09,933 - INFO - Epoch 872 training loss = 0.01853
2023-04-27 00:22:13,947 - INFO - Epoch 873 training loss = 0.01849
2023-04-27 00:22:17,959 - INFO - Epoch 874 training loss = 0.01786
2023-04-27 00:22:21,968 - INFO - Epoch 875 training loss = 0.01775
2023-04-27 00:22:25,977 - INFO - Epoch 876 training loss = 0.01772
2023-04-27 00:22:29,987 - INFO - Epoch 877 training loss = 0.01816
2023-04-27 00:22:33,997 - INFO - Epoch 878 training loss = 0.01711
2023-04-27 00:22:38,007 - INFO - Epoch 879 training loss = 0.01748
2023-04-27 00:22:42,019 - INFO - Epoch 880 training loss = 0.01798
2023-04-27 00:22:42,333 - INFO - Validation loss = 0.05004
2023-04-27 00:22:42,334 - INFO - best model
2023-04-27 00:22:46,369 - INFO - Epoch 881 training loss = 0.01711
2023-04-27 00:22:50,381 - INFO - Epoch 882 training loss = 0.01742
2023-04-27 00:22:54,393 - INFO - Epoch 883 training loss = 0.01728
2023-04-27 00:22:58,396 - INFO - Epoch 884 training loss = 0.01681
2023-04-27 00:23:02,396 - INFO - Epoch 885 training loss = 0.01638
2023-04-27 00:23:06,395 - INFO - Epoch 886 training loss = 0.01655
2023-04-27 00:23:10,402 - INFO - Epoch 887 training loss = 0.01662
2023-04-27 00:23:14,416 - INFO - Epoch 888 training loss = 0.01592
2023-04-27 00:23:18,430 - INFO - Epoch 889 training loss = 0.01584
2023-04-27 00:23:22,440 - INFO - Epoch 890 training loss = 0.01576
2023-04-27 00:23:22,753 - INFO - Validation loss = 0.04948
2023-04-27 00:23:22,753 - INFO - best model
2023-04-27 00:23:26,786 - INFO - Epoch 891 training loss = 0.01561
2023-04-27 00:23:30,796 - INFO - Epoch 892 training loss = 0.01558
2023-04-27 00:23:34,806 - INFO - Epoch 893 training loss = 0.01562
2023-04-27 00:23:38,816 - INFO - Epoch 894 training loss = 0.0153
2023-04-27 00:23:42,828 - INFO - Epoch 895 training loss = 0.01579
2023-04-27 00:23:46,842 - INFO - Epoch 896 training loss = 0.01488
2023-04-27 00:23:50,848 - INFO - Epoch 897 training loss = 0.01549
2023-04-27 00:23:54,847 - INFO - Epoch 898 training loss = 0.0148
2023-04-27 00:23:58,845 - INFO - Epoch 899 training loss = 0.01458
2023-04-27 00:24:02,845 - INFO - Epoch 900 training loss = 0.01466
2023-04-27 00:24:03,159 - INFO - Validation loss = 0.04961
2023-04-27 00:24:07,157 - INFO - Epoch 901 training loss = 0.01457
2023-04-27 00:24:11,161 - INFO - Epoch 902 training loss = 0.01462
2023-04-27 00:24:15,173 - INFO - Epoch 903 training loss = 0.01408
2023-04-27 00:24:19,185 - INFO - Epoch 904 training loss = 0.01413
2023-04-27 00:24:23,194 - INFO - Epoch 905 training loss = 0.01412
2023-04-27 00:24:27,204 - INFO - Epoch 906 training loss = 0.01426
2023-04-27 00:24:31,213 - INFO - Epoch 907 training loss = 0.01414
2023-04-27 00:24:35,222 - INFO - Epoch 908 training loss = 0.01391
2023-04-27 00:24:39,232 - INFO - Epoch 909 training loss = 0.01374
2023-04-27 00:24:43,245 - INFO - Epoch 910 training loss = 0.01361
2023-04-27 00:24:43,558 - INFO - Validation loss = 0.04676
2023-04-27 00:24:43,558 - INFO - best model
2023-04-27 00:24:47,595 - INFO - Epoch 911 training loss = 0.01363
2023-04-27 00:24:51,605 - INFO - Epoch 912 training loss = 0.01334
2023-04-27 00:24:55,616 - INFO - Epoch 913 training loss = 0.01363
2023-04-27 00:24:59,619 - INFO - Epoch 914 training loss = 0.01335
2023-04-27 00:25:03,619 - INFO - Epoch 915 training loss = 0.01332
2023-04-27 00:25:07,617 - INFO - Epoch 916 training loss = 0.01327
2023-04-27 00:25:11,618 - INFO - Epoch 917 training loss = 0.01312
2023-04-27 00:25:15,619 - INFO - Epoch 918 training loss = 0.01311
2023-04-27 00:25:19,623 - INFO - Epoch 919 training loss = 0.01301
2023-04-27 00:25:23,633 - INFO - Epoch 920 training loss = 0.01295
2023-04-27 00:25:23,947 - INFO - Validation loss = 0.04606
2023-04-27 00:25:23,947 - INFO - best model
2023-04-27 00:25:27,986 - INFO - Epoch 921 training loss = 0.01285
2023-04-27 00:25:32,010 - INFO - Epoch 922 training loss = 0.01277
2023-04-27 00:25:36,033 - INFO - Epoch 923 training loss = 0.01286
2023-04-27 00:25:40,058 - INFO - Epoch 924 training loss = 0.01248
2023-04-27 00:25:44,084 - INFO - Epoch 925 training loss = 0.01271
2023-04-27 00:25:48,112 - INFO - Epoch 926 training loss = 0.01243
2023-04-27 00:25:52,136 - INFO - Epoch 927 training loss = 0.01257
2023-04-27 00:25:56,161 - INFO - Epoch 928 training loss = 0.01231
2023-04-27 00:26:00,184 - INFO - Epoch 929 training loss = 0.01233
2023-04-27 00:26:04,208 - INFO - Epoch 930 training loss = 0.01231
2023-04-27 00:26:04,523 - INFO - Validation loss = 0.04602
2023-04-27 00:26:04,523 - INFO - best model
2023-04-27 00:26:08,570 - INFO - Epoch 931 training loss = 0.01224
2023-04-27 00:26:12,598 - INFO - Epoch 932 training loss = 0.01223
2023-04-27 00:26:16,624 - INFO - Epoch 933 training loss = 0.012
2023-04-27 00:26:20,650 - INFO - Epoch 934 training loss = 0.01211
2023-04-27 00:26:24,674 - INFO - Epoch 935 training loss = 0.01195
2023-04-27 00:26:28,698 - INFO - Epoch 936 training loss = 0.01202
2023-04-27 00:26:32,722 - INFO - Epoch 937 training loss = 0.01195
2023-04-27 00:26:36,747 - INFO - Epoch 938 training loss = 0.01188
2023-04-27 00:26:40,773 - INFO - Epoch 939 training loss = 0.01182
2023-04-27 00:26:44,802 - INFO - Epoch 940 training loss = 0.01176
2023-04-27 00:26:45,116 - INFO - Validation loss = 0.04452
2023-04-27 00:26:45,117 - INFO - best model
2023-04-27 00:26:49,166 - INFO - Epoch 941 training loss = 0.01169
2023-04-27 00:26:53,191 - INFO - Epoch 942 training loss = 0.01163
2023-04-27 00:26:57,216 - INFO - Epoch 943 training loss = 0.01164
2023-04-27 00:27:01,242 - INFO - Epoch 944 training loss = 0.0116
2023-04-27 00:27:05,268 - INFO - Epoch 945 training loss = 0.01156
2023-04-27 00:27:09,292 - INFO - Epoch 946 training loss = 0.01148
2023-04-27 00:27:13,321 - INFO - Epoch 947 training loss = 0.01146
2023-04-27 00:27:17,348 - INFO - Epoch 948 training loss = 0.01147
2023-04-27 00:27:21,374 - INFO - Epoch 949 training loss = 0.0114
2023-04-27 00:27:25,399 - INFO - Epoch 950 training loss = 0.01134
2023-04-27 00:27:25,713 - INFO - Validation loss = 0.04352
2023-04-27 00:27:25,714 - INFO - best model
2023-04-27 00:27:29,761 - INFO - Epoch 951 training loss = 0.01135
2023-04-27 00:27:33,786 - INFO - Epoch 952 training loss = 0.01128
2023-04-27 00:27:37,810 - INFO - Epoch 953 training loss = 0.01123
2023-04-27 00:27:41,837 - INFO - Epoch 954 training loss = 0.01125
2023-04-27 00:27:45,865 - INFO - Epoch 955 training loss = 0.01117
2023-04-27 00:27:49,892 - INFO - Epoch 956 training loss = 0.01114
2023-04-27 00:27:53,916 - INFO - Epoch 957 training loss = 0.01114
2023-04-27 00:27:57,941 - INFO - Epoch 958 training loss = 0.01108
2023-04-27 00:28:01,966 - INFO - Epoch 959 training loss = 0.01108
2023-04-27 00:28:05,991 - INFO - Epoch 960 training loss = 0.01105
2023-04-27 00:28:06,305 - INFO - Validation loss = 0.04344
2023-04-27 00:28:06,305 - INFO - best model
2023-04-27 00:28:10,353 - INFO - Epoch 961 training loss = 0.01104
2023-04-27 00:28:14,366 - INFO - Epoch 962 training loss = 0.01102
2023-04-27 00:28:18,378 - INFO - Epoch 963 training loss = 0.01097
2023-04-27 00:28:22,388 - INFO - Epoch 964 training loss = 0.01096
2023-04-27 00:28:26,398 - INFO - Epoch 965 training loss = 0.01093
2023-04-27 00:28:30,408 - INFO - Epoch 966 training loss = 0.0109
2023-04-27 00:28:34,418 - INFO - Epoch 967 training loss = 0.01086
2023-04-27 00:28:38,428 - INFO - Epoch 968 training loss = 0.01086
2023-04-27 00:28:42,440 - INFO - Epoch 969 training loss = 0.01082
2023-04-27 00:28:46,454 - INFO - Epoch 970 training loss = 0.01084
2023-04-27 00:28:46,768 - INFO - Validation loss = 0.04307
2023-04-27 00:28:46,768 - INFO - best model
2023-04-27 00:28:50,815 - INFO - Epoch 971 training loss = 0.01078
2023-04-27 00:28:54,840 - INFO - Epoch 972 training loss = 0.01077
2023-04-27 00:28:58,864 - INFO - Epoch 973 training loss = 0.01075
2023-04-27 00:29:02,889 - INFO - Epoch 974 training loss = 0.01074
2023-04-27 00:29:06,914 - INFO - Epoch 975 training loss = 0.01073
2023-04-27 00:29:10,939 - INFO - Epoch 976 training loss = 0.01071
2023-04-27 00:29:14,966 - INFO - Epoch 977 training loss = 0.01068
2023-04-27 00:29:18,993 - INFO - Epoch 978 training loss = 0.01067
2023-04-27 00:29:23,017 - INFO - Epoch 979 training loss = 0.01066
2023-04-27 00:29:27,042 - INFO - Epoch 980 training loss = 0.01064
2023-04-27 00:29:27,356 - INFO - Validation loss = 0.04279
2023-04-27 00:29:27,356 - INFO - best model
2023-04-27 00:29:31,402 - INFO - Epoch 981 training loss = 0.01065
2023-04-27 00:29:35,425 - INFO - Epoch 982 training loss = 0.01063
2023-04-27 00:29:39,449 - INFO - Epoch 983 training loss = 0.0106
2023-04-27 00:29:43,475 - INFO - Epoch 984 training loss = 0.0106
2023-04-27 00:29:47,501 - INFO - Epoch 985 training loss = 0.01059
2023-04-27 00:29:51,526 - INFO - Epoch 986 training loss = 0.01057
2023-04-27 00:29:55,550 - INFO - Epoch 987 training loss = 0.01057
2023-04-27 00:29:59,573 - INFO - Epoch 988 training loss = 0.01056
2023-04-27 00:30:03,601 - INFO - Epoch 989 training loss = 0.01055
2023-04-27 00:30:07,624 - INFO - Epoch 990 training loss = 0.01054
2023-04-27 00:30:07,939 - INFO - Validation loss = 0.04271
2023-04-27 00:30:07,939 - INFO - best model
2023-04-27 00:30:11,989 - INFO - Epoch 991 training loss = 0.01053
2023-04-27 00:30:16,017 - INFO - Epoch 992 training loss = 0.01052
2023-04-27 00:30:20,044 - INFO - Epoch 993 training loss = 0.01052
2023-04-27 00:30:24,069 - INFO - Epoch 994 training loss = 0.01051
2023-04-27 00:30:28,093 - INFO - Epoch 995 training loss = 0.01051
2023-04-27 00:30:32,118 - INFO - Epoch 996 training loss = 0.01051
2023-04-27 00:30:36,142 - INFO - Epoch 997 training loss = 0.0105
2023-04-27 00:30:40,167 - INFO - Epoch 998 training loss = 0.0105
2023-04-27 00:30:44,195 - INFO - Epoch 999 training loss = 0.01049
2023-04-27 00:30:44,487 - INFO - Validation loss = 0.04264
