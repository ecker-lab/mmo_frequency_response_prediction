2023-04-26 19:12:10,441 - INFO - Config:
Namespace(config='configs/implicit_transformer.yaml', dir='/user/delden/acoustics/spp_ai_acoustics/acousticNN/experiments/arch/no_encoding/implicit_transformer', epochs=100, device='cuda', seed=0, parameter_list='configs/data.yaml', encoding='none', dir_name='arch/no_encoding/implicit_transformer')
2023-04-26 19:12:10,441 - INFO - Config:
Munch({'optimizer': Munch({'type': 'AdamW', 'kwargs': Munch({'lr': 0.0025, 'weight_decay': 0.005, 'betas': [0.9, 0.95]})}), 'scheduler': Munch({'type': 'CosLR', 'kwargs': Munch({'epochs': 1000, 'initial_epochs': 25})}), 'model': Munch({'name': 'ImplicitTransformer', 'input_encoding': True, 'encoding_dim': 16, 'encoder': Munch({'embed_dim': 66, 'num_heads': 3, 'depth': 4}), 'decoder': Munch({'embed_dim': 99, 'num_heads': 3, 'depth': 3})}), 'generation_frequency': 5000, 'validation_frequency': 10, 'batch_size': 16, 'epochs': 1000, 'gradient_clip': 10})
2023-04-26 19:12:24,498 - INFO - ==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ImplicitTransformer                      [16, 200, 4]              --
├─PositionalEncoding: 1-1                [16, 14, 66]              --
│    └─SinosoidalEncoding: 2-1           [16, 14, 66]              --
├─GroupwiseProjection: 1-2               [16, 14, 66]              --
│    └─ModuleList: 2-2                   --                        --
│    │    └─Linear: 3-1                  [16, 4, 66]               132
│    │    └─Linear: 3-2                  [16, 5, 66]               132
│    │    └─Linear: 3-3                  [16, 5, 66]               132
├─TransformerEncoder: 1-3                [16, 14, 66]              --
│    └─ModuleList: 2-3                   --                        --
│    │    └─Block: 3-4                   [16, 14, 66]              52,932
│    │    └─Block: 3-5                   [16, 14, 66]              52,932
│    │    └─Block: 3-6                   [16, 14, 66]              52,932
│    │    └─Block: 3-7                   [16, 14, 66]              52,932
├─Linear: 1-4                            [3200, 4, 99]             6,633
├─Linear: 1-5                            [16, 200, 99]             198
├─TransformerEncoder: 1-6                [3200, 4, 99]             --
│    └─ModuleList: 2-4                   --                        --
│    │    └─Block: 3-8                   [3200, 4, 99]             118,602
│    │    └─Block: 3-9                   [3200, 4, 99]             118,602
│    │    └─Block: 3-10                  [3200, 4, 99]             118,602
├─Linear: 1-7                            [3200, 4, 1]              100
==========================================================================================
Total params: 574,861
Trainable params: 574,861
Non-trainable params: 0
Total mult-adds (G): 1.16
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 352.64
Params size (MB): 2.30
Estimated Total Size (MB): 354.95
==========================================================================================
2023-04-26 19:12:28,541 - INFO - Epoch 0 training loss = 1.265e+04
2023-04-26 19:12:28,856 - INFO - Validation loss = 9.947e+03
2023-04-26 19:12:28,857 - INFO - best model
2023-04-26 19:12:32,894 - INFO - Epoch 1 training loss = 7.612e+03
2023-04-26 19:12:36,897 - INFO - Epoch 2 training loss = 461.8
2023-04-26 19:12:40,902 - INFO - Epoch 3 training loss = 272.4
2023-04-26 19:12:44,908 - INFO - Epoch 4 training loss = 141.2
2023-04-26 19:12:48,913 - INFO - Epoch 5 training loss = 103.0
2023-04-26 19:12:52,914 - INFO - Epoch 6 training loss = 87.22
2023-04-26 19:12:56,917 - INFO - Epoch 7 training loss = 72.83
2023-04-26 19:13:00,921 - INFO - Epoch 8 training loss = 57.54
2023-04-26 19:13:04,924 - INFO - Epoch 9 training loss = 52.05
2023-04-26 19:13:08,926 - INFO - Epoch 10 training loss = 45.58
2023-04-26 19:13:09,239 - INFO - Validation loss = 37.42
2023-04-26 19:13:09,239 - INFO - best model
2023-04-26 19:13:13,268 - INFO - Epoch 11 training loss = 40.17
2023-04-26 19:13:17,273 - INFO - Epoch 12 training loss = 37.04
2023-04-26 19:13:21,276 - INFO - Epoch 13 training loss = 34.13
2023-04-26 19:13:25,278 - INFO - Epoch 14 training loss = 31.2
2023-04-26 19:13:29,287 - INFO - Epoch 15 training loss = 28.63
2023-04-26 19:13:33,294 - INFO - Epoch 16 training loss = 28.41
2023-04-26 19:13:37,302 - INFO - Epoch 17 training loss = 28.6
2023-04-26 19:13:41,316 - INFO - Epoch 18 training loss = 24.63
2023-04-26 19:13:45,334 - INFO - Epoch 19 training loss = 25.27
2023-04-26 19:13:49,349 - INFO - Epoch 20 training loss = 23.53
2023-04-26 19:13:49,663 - INFO - Validation loss = 17.89
2023-04-26 19:13:49,663 - INFO - best model
2023-04-26 19:13:53,699 - INFO - Epoch 21 training loss = 21.5
2023-04-26 19:13:57,712 - INFO - Epoch 22 training loss = 22.26
2023-04-26 19:14:01,723 - INFO - Epoch 23 training loss = 21.35
2023-04-26 19:14:05,727 - INFO - Epoch 24 training loss = 20.89
2023-04-26 19:14:09,730 - INFO - Epoch 25 training loss = 18.79
2023-04-26 19:14:13,736 - INFO - Epoch 26 training loss = 18.39
2023-04-26 19:14:17,742 - INFO - Epoch 27 training loss = 17.37
2023-04-26 19:14:21,746 - INFO - Epoch 28 training loss = 15.61
2023-04-26 19:14:25,747 - INFO - Epoch 29 training loss = 15.53
2023-04-26 19:14:29,750 - INFO - Epoch 30 training loss = 12.79
2023-04-26 19:14:30,063 - INFO - Validation loss = 12.26
2023-04-26 19:14:30,064 - INFO - best model
2023-04-26 19:14:34,087 - INFO - Epoch 31 training loss = 13.1
2023-04-26 19:14:38,089 - INFO - Epoch 32 training loss = 12.33
2023-04-26 19:14:42,093 - INFO - Epoch 33 training loss = 11.35
2023-04-26 19:14:46,100 - INFO - Epoch 34 training loss = 11.13
2023-04-26 19:14:50,104 - INFO - Epoch 35 training loss = 10.17
2023-04-26 19:14:54,107 - INFO - Epoch 36 training loss = 9.896
2023-04-26 19:14:58,110 - INFO - Epoch 37 training loss = 9.764
2023-04-26 19:15:02,114 - INFO - Epoch 38 training loss = 9.076
2023-04-26 19:15:06,117 - INFO - Epoch 39 training loss = 8.856
2023-04-26 19:15:10,119 - INFO - Epoch 40 training loss = 8.026
2023-04-26 19:15:10,433 - INFO - Validation loss = 13.2
2023-04-26 19:15:14,442 - INFO - Epoch 41 training loss = 8.269
2023-04-26 19:15:18,446 - INFO - Epoch 42 training loss = 7.393
2023-04-26 19:15:22,448 - INFO - Epoch 43 training loss = 7.255
2023-04-26 19:15:26,450 - INFO - Epoch 44 training loss = 7.291
2023-04-26 19:15:30,452 - INFO - Epoch 45 training loss = 6.894
2023-04-26 19:15:34,454 - INFO - Epoch 46 training loss = 6.383
2023-04-26 19:15:38,455 - INFO - Epoch 47 training loss = 6.533
2023-04-26 19:15:42,460 - INFO - Epoch 48 training loss = 6.317
2023-04-26 19:15:46,465 - INFO - Epoch 49 training loss = 6.01
2023-04-26 19:15:50,467 - INFO - Epoch 50 training loss = 6.484
2023-04-26 19:15:50,780 - INFO - Validation loss = 4.637
2023-04-26 19:15:50,780 - INFO - best model
2023-04-26 19:15:54,805 - INFO - Epoch 51 training loss = 5.848
2023-04-26 19:15:58,806 - INFO - Epoch 52 training loss = 5.914
2023-04-26 19:16:02,809 - INFO - Epoch 53 training loss = 5.305
2023-04-26 19:16:06,810 - INFO - Epoch 54 training loss = 5.143
2023-04-26 19:16:10,814 - INFO - Epoch 55 training loss = 5.244
2023-04-26 19:16:14,823 - INFO - Epoch 56 training loss = 5.601
2023-04-26 19:16:18,828 - INFO - Epoch 57 training loss = 5.132
2023-04-26 19:16:22,830 - INFO - Epoch 58 training loss = 4.779
2023-04-26 19:16:26,832 - INFO - Epoch 59 training loss = 4.87
2023-04-26 19:16:30,835 - INFO - Epoch 60 training loss = 5.395
2023-04-26 19:16:31,159 - INFO - Validation loss = 4.955
2023-04-26 19:16:35,162 - INFO - Epoch 61 training loss = 4.595
2023-04-26 19:16:39,166 - INFO - Epoch 62 training loss = 4.048
2023-04-26 19:16:43,172 - INFO - Epoch 63 training loss = 4.335
2023-04-26 19:16:47,178 - INFO - Epoch 64 training loss = 4.41
2023-04-26 19:16:51,181 - INFO - Epoch 65 training loss = 4.355
2023-04-26 19:16:55,185 - INFO - Epoch 66 training loss = 4.527
2023-04-26 19:16:59,189 - INFO - Epoch 67 training loss = 4.11
2023-04-26 19:17:03,193 - INFO - Epoch 68 training loss = 4.224
2023-04-26 19:17:07,197 - INFO - Epoch 69 training loss = 4.066
2023-04-26 19:17:11,201 - INFO - Epoch 70 training loss = 3.837
2023-04-26 19:17:11,514 - INFO - Validation loss = 3.213
2023-04-26 19:17:11,514 - INFO - best model
2023-04-26 19:17:15,543 - INFO - Epoch 71 training loss = 3.747
2023-04-26 19:17:19,546 - INFO - Epoch 72 training loss = 3.554
2023-04-26 19:17:23,549 - INFO - Epoch 73 training loss = 3.907
2023-04-26 19:17:27,551 - INFO - Epoch 74 training loss = 3.52
2023-04-26 19:17:31,554 - INFO - Epoch 75 training loss = 3.712
2023-04-26 19:17:35,556 - INFO - Epoch 76 training loss = 3.489
2023-04-26 19:17:39,558 - INFO - Epoch 77 training loss = 3.537
2023-04-26 19:17:43,564 - INFO - Epoch 78 training loss = 2.953
2023-04-26 19:17:47,570 - INFO - Epoch 79 training loss = 3.392
2023-04-26 19:17:51,572 - INFO - Epoch 80 training loss = 3.556
2023-04-26 19:17:51,885 - INFO - Validation loss = 3.016
2023-04-26 19:17:51,886 - INFO - best model
2023-04-26 19:17:55,910 - INFO - Epoch 81 training loss = 3.079
2023-04-26 19:17:59,915 - INFO - Epoch 82 training loss = 3.465
2023-04-26 19:18:03,925 - INFO - Epoch 83 training loss = 3.254
2023-04-26 19:18:07,930 - INFO - Epoch 84 training loss = 3.443
2023-04-26 19:18:11,934 - INFO - Epoch 85 training loss = 3.443
2023-04-26 19:18:15,941 - INFO - Epoch 86 training loss = 3.136
2023-04-26 19:18:19,944 - INFO - Epoch 87 training loss = 2.855
2023-04-26 19:18:23,946 - INFO - Epoch 88 training loss = 3.045
2023-04-26 19:18:27,950 - INFO - Epoch 89 training loss = 2.988
2023-04-26 19:18:31,953 - INFO - Epoch 90 training loss = 2.883
2023-04-26 19:18:32,265 - INFO - Validation loss = 4.192
2023-04-26 19:18:36,267 - INFO - Epoch 91 training loss = 2.971
2023-04-26 19:18:40,270 - INFO - Epoch 92 training loss = 2.981
2023-04-26 19:18:44,276 - INFO - Epoch 93 training loss = 2.704
2023-04-26 19:18:48,281 - INFO - Epoch 94 training loss = 2.872
2023-04-26 19:18:52,284 - INFO - Epoch 95 training loss = 2.666
2023-04-26 19:18:56,287 - INFO - Epoch 96 training loss = 2.73
2023-04-26 19:19:00,290 - INFO - Epoch 97 training loss = 2.757
2023-04-26 19:19:04,293 - INFO - Epoch 98 training loss = 2.578
2023-04-26 19:19:08,295 - INFO - Epoch 99 training loss = 2.587
2023-04-26 19:19:12,300 - INFO - Epoch 100 training loss = 2.409
2023-04-26 19:19:12,613 - INFO - Validation loss = 1.957
2023-04-26 19:19:12,613 - INFO - best model
2023-04-26 19:19:16,640 - INFO - Epoch 101 training loss = 2.65
2023-04-26 19:19:20,644 - INFO - Epoch 102 training loss = 2.417
2023-04-26 19:19:24,647 - INFO - Epoch 103 training loss = 2.584
2023-04-26 19:19:28,651 - INFO - Epoch 104 training loss = 2.708
2023-04-26 19:19:32,654 - INFO - Epoch 105 training loss = 2.731
2023-04-26 19:19:36,657 - INFO - Epoch 106 training loss = 2.602
2023-04-26 19:19:40,661 - INFO - Epoch 107 training loss = 2.374
2023-04-26 19:19:44,669 - INFO - Epoch 108 training loss = 2.495
2023-04-26 19:19:48,675 - INFO - Epoch 109 training loss = 2.481
2023-04-26 19:19:52,678 - INFO - Epoch 110 training loss = 2.482
2023-04-26 19:19:52,991 - INFO - Validation loss = 1.73
2023-04-26 19:19:52,991 - INFO - best model
2023-04-26 19:19:57,017 - INFO - Epoch 111 training loss = 2.398
2023-04-26 19:20:01,020 - INFO - Epoch 112 training loss = 2.313
2023-04-26 19:20:05,027 - INFO - Epoch 113 training loss = 2.542
2023-04-26 19:20:09,032 - INFO - Epoch 114 training loss = 2.491
2023-04-26 19:20:13,041 - INFO - Epoch 115 training loss = 2.521
2023-04-26 19:20:17,058 - INFO - Epoch 116 training loss =  2.3
2023-04-26 19:20:21,074 - INFO - Epoch 117 training loss = 2.257
2023-04-26 19:20:25,088 - INFO - Epoch 118 training loss = 2.279
2023-04-26 19:20:29,103 - INFO - Epoch 119 training loss = 2.305
2023-04-26 19:20:33,117 - INFO - Epoch 120 training loss = 2.217
2023-04-26 19:20:33,431 - INFO - Validation loss = 1.477
2023-04-26 19:20:33,431 - INFO - best model
2023-04-26 19:20:37,465 - INFO - Epoch 121 training loss = 2.19
2023-04-26 19:20:41,478 - INFO - Epoch 122 training loss = 2.216
2023-04-26 19:20:45,496 - INFO - Epoch 123 training loss = 2.322
2023-04-26 19:20:49,510 - INFO - Epoch 124 training loss = 2.271
2023-04-26 19:20:53,523 - INFO - Epoch 125 training loss = 2.071
2023-04-26 19:20:57,538 - INFO - Epoch 126 training loss = 2.153
2023-04-26 19:21:01,552 - INFO - Epoch 127 training loss = 2.101
2023-04-26 19:21:05,565 - INFO - Epoch 128 training loss = 2.227
2023-04-26 19:21:09,580 - INFO - Epoch 129 training loss = 1.957
2023-04-26 19:21:13,598 - INFO - Epoch 130 training loss = 2.078
2023-04-26 19:21:13,911 - INFO - Validation loss = 2.485
2023-04-26 19:21:17,928 - INFO - Epoch 131 training loss = 2.239
2023-04-26 19:21:21,941 - INFO - Epoch 132 training loss = 2.139
2023-04-26 19:21:25,955 - INFO - Epoch 133 training loss = 1.893
2023-04-26 19:21:29,968 - INFO - Epoch 134 training loss = 1.904
2023-04-26 19:21:33,981 - INFO - Epoch 135 training loss = 2.014
2023-04-26 19:21:37,994 - INFO - Epoch 136 training loss = 1.915
2023-04-26 19:21:41,998 - INFO - Epoch 137 training loss = 1.933
2023-04-26 19:21:46,018 - INFO - Epoch 138 training loss = 1.887
2023-04-26 19:21:50,048 - INFO - Epoch 139 training loss = 2.121
2023-04-26 19:21:54,063 - INFO - Epoch 140 training loss = 1.884
2023-04-26 19:21:54,376 - INFO - Validation loss = 1.981
2023-04-26 19:21:58,380 - INFO - Epoch 141 training loss = 1.811
2023-04-26 19:22:02,384 - INFO - Epoch 142 training loss = 1.825
2023-04-26 19:22:06,388 - INFO - Epoch 143 training loss = 2.029
2023-04-26 19:22:10,391 - INFO - Epoch 144 training loss = 1.854
2023-04-26 19:22:14,398 - INFO - Epoch 145 training loss = 1.982
2023-04-26 19:22:18,402 - INFO - Epoch 146 training loss = 1.994
2023-04-26 19:22:22,405 - INFO - Epoch 147 training loss = 1.728
2023-04-26 19:22:26,407 - INFO - Epoch 148 training loss = 1.745
2023-04-26 19:22:30,410 - INFO - Epoch 149 training loss = 1.835
2023-04-26 19:22:34,412 - INFO - Epoch 150 training loss = 1.683
2023-04-26 19:22:34,725 - INFO - Validation loss = 1.619
2023-04-26 19:22:38,728 - INFO - Epoch 151 training loss = 1.893
2023-04-26 19:22:42,733 - INFO - Epoch 152 training loss = 1.766
2023-04-26 19:22:46,738 - INFO - Epoch 153 training loss = 1.664
2023-04-26 19:22:50,741 - INFO - Epoch 154 training loss = 1.872
2023-04-26 19:22:54,742 - INFO - Epoch 155 training loss = 1.774
2023-04-26 19:22:58,745 - INFO - Epoch 156 training loss = 1.73
2023-04-26 19:23:02,747 - INFO - Epoch 157 training loss = 1.756
2023-04-26 19:23:06,749 - INFO - Epoch 158 training loss = 1.659
2023-04-26 19:23:10,753 - INFO - Epoch 159 training loss = 1.737
2023-04-26 19:23:14,759 - INFO - Epoch 160 training loss = 1.692
2023-04-26 19:23:15,072 - INFO - Validation loss = 1.65
2023-04-26 19:23:19,078 - INFO - Epoch 161 training loss = 1.769
2023-04-26 19:23:23,080 - INFO - Epoch 162 training loss = 1.74
2023-04-26 19:23:27,083 - INFO - Epoch 163 training loss = 1.742
2023-04-26 19:23:31,086 - INFO - Epoch 164 training loss = 1.811
2023-04-26 19:23:35,088 - INFO - Epoch 165 training loss = 1.927
2023-04-26 19:23:39,090 - INFO - Epoch 166 training loss = 1.78
2023-04-26 19:23:43,095 - INFO - Epoch 167 training loss = 1.949
2023-04-26 19:23:47,101 - INFO - Epoch 168 training loss = 1.542
2023-04-26 19:23:51,105 - INFO - Epoch 169 training loss = 1.608
2023-04-26 19:23:55,109 - INFO - Epoch 170 training loss = 1.876
2023-04-26 19:23:55,421 - INFO - Validation loss = 1.812
2023-04-26 19:23:59,426 - INFO - Epoch 171 training loss = 1.545
2023-04-26 19:24:03,430 - INFO - Epoch 172 training loss = 1.629
2023-04-26 19:24:07,434 - INFO - Epoch 173 training loss = 1.532
2023-04-26 19:24:11,438 - INFO - Epoch 174 training loss = 1.709
2023-04-26 19:24:15,446 - INFO - Epoch 175 training loss = 1.557
2023-04-26 19:24:19,452 - INFO - Epoch 176 training loss = 1.717
2023-04-26 19:24:23,455 - INFO - Epoch 177 training loss = 1.497
2023-04-26 19:24:27,459 - INFO - Epoch 178 training loss = 1.467
2023-04-26 19:24:31,462 - INFO - Epoch 179 training loss = 1.519
2023-04-26 19:24:35,465 - INFO - Epoch 180 training loss = 1.632
2023-04-26 19:24:35,778 - INFO - Validation loss = 1.481
2023-04-26 19:24:39,782 - INFO - Epoch 181 training loss = 1.592
2023-04-26 19:24:43,788 - INFO - Epoch 182 training loss = 1.421
2023-04-26 19:24:47,794 - INFO - Epoch 183 training loss = 1.694
2023-04-26 19:24:51,797 - INFO - Epoch 184 training loss = 1.536
2023-04-26 19:24:55,800 - INFO - Epoch 185 training loss = 1.593
2023-04-26 19:24:59,804 - INFO - Epoch 186 training loss = 1.794
2023-04-26 19:25:03,808 - INFO - Epoch 187 training loss = 1.61
2023-04-26 19:25:07,811 - INFO - Epoch 188 training loss = 1.442
2023-04-26 19:25:11,816 - INFO - Epoch 189 training loss = 1.526
2023-04-26 19:25:15,824 - INFO - Epoch 190 training loss = 1.527
2023-04-26 19:25:16,136 - INFO - Validation loss = 1.538
2023-04-26 19:25:20,143 - INFO - Epoch 191 training loss = 1.459
2023-04-26 19:25:24,146 - INFO - Epoch 192 training loss = 1.49
2023-04-26 19:25:28,150 - INFO - Epoch 193 training loss = 1.581
2023-04-26 19:25:32,153 - INFO - Epoch 194 training loss = 1.471
2023-04-26 19:25:36,157 - INFO - Epoch 195 training loss = 1.678
2023-04-26 19:25:40,160 - INFO - Epoch 196 training loss = 1.307
2023-04-26 19:25:44,166 - INFO - Epoch 197 training loss = 1.428
2023-04-26 19:25:48,173 - INFO - Epoch 198 training loss = 1.499
2023-04-26 19:25:52,177 - INFO - Epoch 199 training loss = 1.42
2023-04-26 19:25:56,180 - INFO - Epoch 200 training loss = 1.435
2023-04-26 19:25:56,493 - INFO - Validation loss = 1.713
2023-04-26 19:26:00,498 - INFO - Epoch 201 training loss = 1.382
2023-04-26 19:26:04,502 - INFO - Epoch 202 training loss = 1.409
2023-04-26 19:26:08,505 - INFO - Epoch 203 training loss = 1.553
2023-04-26 19:26:12,511 - INFO - Epoch 204 training loss = 1.42
2023-04-26 19:26:16,517 - INFO - Epoch 205 training loss = 1.376
2023-04-26 19:26:20,522 - INFO - Epoch 206 training loss = 1.281
2023-04-26 19:26:24,525 - INFO - Epoch 207 training loss = 1.429
2023-04-26 19:26:28,528 - INFO - Epoch 208 training loss = 1.375
2023-04-26 19:26:32,531 - INFO - Epoch 209 training loss = 1.429
2023-04-26 19:26:36,534 - INFO - Epoch 210 training loss = 1.36
2023-04-26 19:26:36,847 - INFO - Validation loss = 1.179
2023-04-26 19:26:36,847 - INFO - best model
2023-04-26 19:26:40,873 - INFO - Epoch 211 training loss = 1.344
2023-04-26 19:26:44,881 - INFO - Epoch 212 training loss = 1.362
2023-04-26 19:26:48,886 - INFO - Epoch 213 training loss = 1.376
2023-04-26 19:26:52,889 - INFO - Epoch 214 training loss = 1.31
2023-04-26 19:26:56,892 - INFO - Epoch 215 training loss = 1.36
2023-04-26 19:27:00,898 - INFO - Epoch 216 training loss = 1.359
2023-04-26 19:27:04,901 - INFO - Epoch 217 training loss = 1.387
2023-04-26 19:27:08,904 - INFO - Epoch 218 training loss = 1.306
2023-04-26 19:27:12,909 - INFO - Epoch 219 training loss = 1.251
2023-04-26 19:27:16,915 - INFO - Epoch 220 training loss = 1.331
2023-04-26 19:27:17,227 - INFO - Validation loss = 1.599
2023-04-26 19:27:21,230 - INFO - Epoch 221 training loss = 1.408
2023-04-26 19:27:25,232 - INFO - Epoch 222 training loss = 1.342
2023-04-26 19:27:29,236 - INFO - Epoch 223 training loss = 1.275
2023-04-26 19:27:33,237 - INFO - Epoch 224 training loss = 1.316
2023-04-26 19:27:37,238 - INFO - Epoch 225 training loss = 1.531
2023-04-26 19:27:41,240 - INFO - Epoch 226 training loss = 1.307
2023-04-26 19:27:45,247 - INFO - Epoch 227 training loss = 1.415
2023-04-26 19:27:49,250 - INFO - Epoch 228 training loss = 1.346
2023-04-26 19:27:53,253 - INFO - Epoch 229 training loss = 1.339
2023-04-26 19:27:57,255 - INFO - Epoch 230 training loss = 1.19
2023-04-26 19:27:57,567 - INFO - Validation loss = 1.194
2023-04-26 19:28:01,572 - INFO - Epoch 231 training loss = 1.309
2023-04-26 19:28:05,575 - INFO - Epoch 232 training loss = 1.283
2023-04-26 19:28:09,579 - INFO - Epoch 233 training loss = 1.352
2023-04-26 19:28:13,585 - INFO - Epoch 234 training loss = 1.292
2023-04-26 19:28:17,591 - INFO - Epoch 235 training loss = 1.318
2023-04-26 19:28:21,594 - INFO - Epoch 236 training loss = 1.252
2023-04-26 19:28:25,597 - INFO - Epoch 237 training loss = 1.261
2023-04-26 19:28:29,600 - INFO - Epoch 238 training loss = 1.236
2023-04-26 19:28:33,603 - INFO - Epoch 239 training loss = 1.295
2023-04-26 19:28:37,606 - INFO - Epoch 240 training loss = 1.307
2023-04-26 19:28:37,919 - INFO - Validation loss = 1.199
2023-04-26 19:28:41,923 - INFO - Epoch 241 training loss = 1.237
2023-04-26 19:28:45,948 - INFO - Epoch 242 training loss = 1.156
2023-04-26 19:28:49,979 - INFO - Epoch 243 training loss = 1.186
2023-04-26 19:28:54,009 - INFO - Epoch 244 training loss = 1.303
2023-04-26 19:28:58,033 - INFO - Epoch 245 training loss = 1.327
2023-04-26 19:29:02,036 - INFO - Epoch 246 training loss = 1.204
2023-04-26 19:29:06,041 - INFO - Epoch 247 training loss = 1.227
2023-04-26 19:29:10,044 - INFO - Epoch 248 training loss = 1.091
2023-04-26 19:29:14,051 - INFO - Epoch 249 training loss = 1.309
2023-04-26 19:29:18,057 - INFO - Epoch 250 training loss =  1.2
2023-04-26 19:29:18,370 - INFO - Validation loss = 1.894
2023-04-26 19:29:22,374 - INFO - Epoch 251 training loss = 1.156
2023-04-26 19:29:26,377 - INFO - Epoch 252 training loss = 1.223
2023-04-26 19:29:30,381 - INFO - Epoch 253 training loss = 1.14
2023-04-26 19:29:34,386 - INFO - Epoch 254 training loss = 1.136
2023-04-26 19:29:38,390 - INFO - Epoch 255 training loss = 1.066
2023-04-26 19:29:42,396 - INFO - Epoch 256 training loss = 1.09
2023-04-26 19:29:46,404 - INFO - Epoch 257 training loss = 1.09
2023-04-26 19:29:50,410 - INFO - Epoch 258 training loss = 1.205
2023-04-26 19:29:54,413 - INFO - Epoch 259 training loss = 1.234
2023-04-26 19:29:58,418 - INFO - Epoch 260 training loss = 1.252
2023-04-26 19:29:58,731 - INFO - Validation loss = 1.222
2023-04-26 19:30:02,744 - INFO - Epoch 261 training loss = 1.112
2023-04-26 19:30:06,774 - INFO - Epoch 262 training loss = 1.193
2023-04-26 19:30:10,804 - INFO - Epoch 263 training loss = 1.238
2023-04-26 19:30:14,838 - INFO - Epoch 264 training loss = 1.229
2023-04-26 19:30:18,869 - INFO - Epoch 265 training loss = 1.066
2023-04-26 19:30:22,898 - INFO - Epoch 266 training loss = 1.165
2023-04-26 19:30:26,927 - INFO - Epoch 267 training loss = 1.107
2023-04-26 19:30:30,955 - INFO - Epoch 268 training loss = 1.096
2023-04-26 19:30:34,985 - INFO - Epoch 269 training loss =  1.1
2023-04-26 19:30:39,014 - INFO - Epoch 270 training loss = 1.101
2023-04-26 19:30:39,328 - INFO - Validation loss = 1.186
2023-04-26 19:30:43,360 - INFO - Epoch 271 training loss = 1.073
2023-04-26 19:30:47,393 - INFO - Epoch 272 training loss = 0.982
2023-04-26 19:30:51,422 - INFO - Epoch 273 training loss = 1.08
2023-04-26 19:30:55,451 - INFO - Epoch 274 training loss = 1.15
2023-04-26 19:30:59,481 - INFO - Epoch 275 training loss = 1.167
2023-04-26 19:31:03,512 - INFO - Epoch 276 training loss = 1.132
2023-04-26 19:31:07,541 - INFO - Epoch 277 training loss = 1.078
2023-04-26 19:31:11,572 - INFO - Epoch 278 training loss = 1.119
2023-04-26 19:31:15,605 - INFO - Epoch 279 training loss = 1.195
2023-04-26 19:31:19,636 - INFO - Epoch 280 training loss = 1.175
2023-04-26 19:31:19,950 - INFO - Validation loss = 1.181
2023-04-26 19:31:23,979 - INFO - Epoch 281 training loss = 1.224
2023-04-26 19:31:28,010 - INFO - Epoch 282 training loss = 1.226
2023-04-26 19:31:32,038 - INFO - Epoch 283 training loss = 1.066
2023-04-26 19:31:36,067 - INFO - Epoch 284 training loss = 1.221
2023-04-26 19:31:40,097 - INFO - Epoch 285 training loss = 1.151
2023-04-26 19:31:44,129 - INFO - Epoch 286 training loss = 1.155
2023-04-26 19:31:48,160 - INFO - Epoch 287 training loss = 1.098
2023-04-26 19:31:52,188 - INFO - Epoch 288 training loss = 0.9994
2023-04-26 19:31:56,216 - INFO - Epoch 289 training loss = 1.077
2023-04-26 19:32:00,245 - INFO - Epoch 290 training loss = 1.023
2023-04-26 19:32:00,559 - INFO - Validation loss = 2.371
2023-04-26 19:32:04,590 - INFO - Epoch 291 training loss = 1.162
2023-04-26 19:32:08,619 - INFO - Epoch 292 training loss = 1.128
2023-04-26 19:32:12,651 - INFO - Epoch 293 training loss = 1.136
2023-04-26 19:32:16,683 - INFO - Epoch 294 training loss = 1.038
2023-04-26 19:32:20,712 - INFO - Epoch 295 training loss = 1.042
2023-04-26 19:32:24,741 - INFO - Epoch 296 training loss = 1.108
2023-04-26 19:32:28,769 - INFO - Epoch 297 training loss = 0.9693
2023-04-26 19:32:32,797 - INFO - Epoch 298 training loss = 1.24
2023-04-26 19:32:36,826 - INFO - Epoch 299 training loss = 1.051
2023-04-26 19:32:40,854 - INFO - Epoch 300 training loss = 0.9509
2023-04-26 19:32:41,168 - INFO - Validation loss = 1.209
2023-04-26 19:32:45,202 - INFO - Epoch 301 training loss = 1.044
2023-04-26 19:32:49,231 - INFO - Epoch 302 training loss = 0.9767
2023-04-26 19:32:53,259 - INFO - Epoch 303 training loss = 0.9378
2023-04-26 19:32:57,288 - INFO - Epoch 304 training loss = 1.022
2023-04-26 19:33:01,318 - INFO - Epoch 305 training loss = 1.057
2023-04-26 19:33:05,346 - INFO - Epoch 306 training loss = 1.077
2023-04-26 19:33:09,374 - INFO - Epoch 307 training loss = 0.998
2023-04-26 19:33:13,405 - INFO - Epoch 308 training loss = 1.091
2023-04-26 19:33:17,436 - INFO - Epoch 309 training loss = 0.9657
2023-04-26 19:33:21,465 - INFO - Epoch 310 training loss = 0.9231
2023-04-26 19:33:21,779 - INFO - Validation loss = 0.8832
2023-04-26 19:33:21,779 - INFO - best model
2023-04-26 19:33:25,830 - INFO - Epoch 311 training loss = 0.9773
2023-04-26 19:33:29,859 - INFO - Epoch 312 training loss = 0.964
2023-04-26 19:33:33,887 - INFO - Epoch 313 training loss = 0.9122
2023-04-26 19:33:37,916 - INFO - Epoch 314 training loss = 0.9655
2023-04-26 19:33:41,946 - INFO - Epoch 315 training loss = 0.9587
2023-04-26 19:33:45,979 - INFO - Epoch 316 training loss = 0.853
2023-04-26 19:33:50,011 - INFO - Epoch 317 training loss = 0.9311
2023-04-26 19:33:54,039 - INFO - Epoch 318 training loss = 0.8807
2023-04-26 19:33:58,069 - INFO - Epoch 319 training loss = 0.9926
2023-04-26 19:34:02,098 - INFO - Epoch 320 training loss = 1.027
2023-04-26 19:34:02,412 - INFO - Validation loss = 0.9868
2023-04-26 19:34:06,442 - INFO - Epoch 321 training loss = 1.066
2023-04-26 19:34:10,472 - INFO - Epoch 322 training loss = 0.9657
2023-04-26 19:34:14,505 - INFO - Epoch 323 training loss = 0.9132
2023-04-26 19:34:18,536 - INFO - Epoch 324 training loss = 0.9831
2023-04-26 19:34:22,565 - INFO - Epoch 325 training loss = 0.9935
2023-04-26 19:34:26,593 - INFO - Epoch 326 training loss = 0.8898
2023-04-26 19:34:30,621 - INFO - Epoch 327 training loss = 0.9019
2023-04-26 19:34:34,650 - INFO - Epoch 328 training loss = 0.844
2023-04-26 19:34:38,679 - INFO - Epoch 329 training loss = 0.8568
2023-04-26 19:34:42,710 - INFO - Epoch 330 training loss = 0.9468
2023-04-26 19:34:43,024 - INFO - Validation loss = 1.737
2023-04-26 19:34:47,059 - INFO - Epoch 331 training loss = 0.8819
2023-04-26 19:34:51,089 - INFO - Epoch 332 training loss = 0.9121
2023-04-26 19:34:55,119 - INFO - Epoch 333 training loss = 0.8382
2023-04-26 19:34:59,149 - INFO - Epoch 334 training loss = 0.9158
2023-04-26 19:35:03,179 - INFO - Epoch 335 training loss = 0.9354
2023-04-26 19:35:07,208 - INFO - Epoch 336 training loss = 0.8668
2023-04-26 19:35:11,237 - INFO - Epoch 337 training loss = 0.8199
2023-04-26 19:35:15,270 - INFO - Epoch 338 training loss = 0.9185
2023-04-26 19:35:19,301 - INFO - Epoch 339 training loss = 0.8265
2023-04-26 19:35:23,329 - INFO - Epoch 340 training loss = 0.8989
2023-04-26 19:35:23,644 - INFO - Validation loss = 0.6174
2023-04-26 19:35:23,644 - INFO - best model
2023-04-26 19:35:27,695 - INFO - Epoch 341 training loss = 0.8313
2023-04-26 19:35:31,723 - INFO - Epoch 342 training loss = 0.987
2023-04-26 19:35:35,752 - INFO - Epoch 343 training loss = 0.926
2023-04-26 19:35:39,781 - INFO - Epoch 344 training loss = 0.8904
2023-04-26 19:35:43,813 - INFO - Epoch 345 training loss = 0.983
2023-04-26 19:35:47,845 - INFO - Epoch 346 training loss = 0.8103
2023-04-26 19:35:51,875 - INFO - Epoch 347 training loss = 0.9146
2023-04-26 19:35:55,903 - INFO - Epoch 348 training loss = 0.8461
2023-04-26 19:35:59,933 - INFO - Epoch 349 training loss = 0.8268
2023-04-26 19:36:03,963 - INFO - Epoch 350 training loss = 0.7892
2023-04-26 19:36:04,278 - INFO - Validation loss = 0.5434
2023-04-26 19:36:04,278 - INFO - best model
2023-04-26 19:36:08,329 - INFO - Epoch 351 training loss = 0.8533
2023-04-26 19:36:12,360 - INFO - Epoch 352 training loss = 0.8537
2023-04-26 19:36:16,393 - INFO - Epoch 353 training loss = 0.8882
2023-04-26 19:36:20,422 - INFO - Epoch 354 training loss = 0.8024
2023-04-26 19:36:24,451 - INFO - Epoch 355 training loss = 0.8809
2023-04-26 19:36:28,480 - INFO - Epoch 356 training loss = 0.8148
2023-04-26 19:36:32,509 - INFO - Epoch 357 training loss = 0.7925
2023-04-26 19:36:36,540 - INFO - Epoch 358 training loss = 0.7805
2023-04-26 19:36:40,569 - INFO - Epoch 359 training loss = 0.9329
2023-04-26 19:36:44,604 - INFO - Epoch 360 training loss = 0.8259
2023-04-26 19:36:44,918 - INFO - Validation loss = 0.8446
2023-04-26 19:36:48,950 - INFO - Epoch 361 training loss = 0.7842
2023-04-26 19:36:52,978 - INFO - Epoch 362 training loss = 0.9459
2023-04-26 19:36:57,033 - INFO - Epoch 363 training loss = 0.8122
2023-04-26 19:37:01,064 - INFO - Epoch 364 training loss = 0.8327
2023-04-26 19:37:05,093 - INFO - Epoch 365 training loss = 0.893
2023-04-26 19:37:09,123 - INFO - Epoch 366 training loss = 0.7827
2023-04-26 19:37:13,144 - INFO - Epoch 367 training loss = 0.7107
2023-04-26 19:37:17,163 - INFO - Epoch 368 training loss = 0.7832
2023-04-26 19:37:21,177 - INFO - Epoch 369 training loss = 0.9375
2023-04-26 19:37:25,192 - INFO - Epoch 370 training loss = 0.7623
2023-04-26 19:37:25,506 - INFO - Validation loss = 0.6953
2023-04-26 19:37:29,522 - INFO - Epoch 371 training loss = 0.8139
2023-04-26 19:37:33,535 - INFO - Epoch 372 training loss = 0.9049
2023-04-26 19:37:37,549 - INFO - Epoch 373 training loss = 0.733
2023-04-26 19:37:41,563 - INFO - Epoch 374 training loss = 0.8181
2023-04-26 19:37:45,583 - INFO - Epoch 375 training loss = 0.785
2023-04-26 19:37:49,599 - INFO - Epoch 376 training loss = 0.8012
2023-04-26 19:37:53,614 - INFO - Epoch 377 training loss = 0.7569
2023-04-26 19:37:57,630 - INFO - Epoch 378 training loss = 0.8226
2023-04-26 19:38:01,646 - INFO - Epoch 379 training loss = 0.7013
2023-04-26 19:38:05,661 - INFO - Epoch 380 training loss = 0.7224
2023-04-26 19:38:05,974 - INFO - Validation loss = 0.7417
2023-04-26 19:38:09,995 - INFO - Epoch 381 training loss = 0.7439
2023-04-26 19:38:14,013 - INFO - Epoch 382 training loss = 0.7732
2023-04-26 19:38:18,031 - INFO - Epoch 383 training loss = 0.6785
2023-04-26 19:38:22,046 - INFO - Epoch 384 training loss = 0.7333
2023-04-26 19:38:26,061 - INFO - Epoch 385 training loss = 0.7575
2023-04-26 19:38:30,077 - INFO - Epoch 386 training loss = 0.662
2023-04-26 19:38:34,092 - INFO - Epoch 387 training loss = 0.698
2023-04-26 19:38:38,106 - INFO - Epoch 388 training loss = 0.7196
2023-04-26 19:38:42,122 - INFO - Epoch 389 training loss = 0.7664
2023-04-26 19:38:46,141 - INFO - Epoch 390 training loss = 0.7044
2023-04-26 19:38:46,455 - INFO - Validation loss = 0.5334
2023-04-26 19:38:46,455 - INFO - best model
2023-04-26 19:38:50,493 - INFO - Epoch 391 training loss = 0.7764
2023-04-26 19:38:54,509 - INFO - Epoch 392 training loss = 0.7454
2023-04-26 19:38:58,526 - INFO - Epoch 393 training loss = 0.7455
2023-04-26 19:39:02,542 - INFO - Epoch 394 training loss = 0.6911
2023-04-26 19:39:06,558 - INFO - Epoch 395 training loss = 0.8043
2023-04-26 19:39:10,574 - INFO - Epoch 396 training loss = 0.7107
2023-04-26 19:39:14,584 - INFO - Epoch 397 training loss = 0.7731
2023-04-26 19:39:18,589 - INFO - Epoch 398 training loss = 0.7749
2023-04-26 19:39:22,592 - INFO - Epoch 399 training loss = 0.7662
2023-04-26 19:39:26,594 - INFO - Epoch 400 training loss = 0.6851
2023-04-26 19:39:26,908 - INFO - Validation loss = 0.6972
2023-04-26 19:39:30,911 - INFO - Epoch 401 training loss = 0.772
2023-04-26 19:39:34,913 - INFO - Epoch 402 training loss = 0.7869
2023-04-26 19:39:38,914 - INFO - Epoch 403 training loss = 0.7313
2023-04-26 19:39:42,919 - INFO - Epoch 404 training loss = 0.7065
2023-04-26 19:39:46,924 - INFO - Epoch 405 training loss = 0.6951
2023-04-26 19:39:50,926 - INFO - Epoch 406 training loss = 0.6828
2023-04-26 19:39:54,929 - INFO - Epoch 407 training loss = 0.7101
2023-04-26 19:39:58,931 - INFO - Epoch 408 training loss = 0.783
2023-04-26 19:40:02,938 - INFO - Epoch 409 training loss = 0.7777
2023-04-26 19:40:06,940 - INFO - Epoch 410 training loss = 0.7115
2023-04-26 19:40:07,265 - INFO - Validation loss = 0.6935
2023-04-26 19:40:11,270 - INFO - Epoch 411 training loss = 0.6392
2023-04-26 19:40:15,277 - INFO - Epoch 412 training loss = 0.7492
2023-04-26 19:40:19,280 - INFO - Epoch 413 training loss = 0.6512
2023-04-26 19:40:23,282 - INFO - Epoch 414 training loss = 0.7639
2023-04-26 19:40:27,288 - INFO - Epoch 415 training loss = 0.6472
2023-04-26 19:40:31,302 - INFO - Epoch 416 training loss = 0.6491
2023-04-26 19:40:35,315 - INFO - Epoch 417 training loss = 0.6279
2023-04-26 19:40:39,329 - INFO - Epoch 418 training loss = 0.6709
2023-04-26 19:40:43,345 - INFO - Epoch 419 training loss = 0.6861
2023-04-26 19:40:47,363 - INFO - Epoch 420 training loss = 0.6967
2023-04-26 19:40:47,677 - INFO - Validation loss = 0.9561
2023-04-26 19:40:51,695 - INFO - Epoch 421 training loss = 0.5746
2023-04-26 19:40:55,708 - INFO - Epoch 422 training loss = 0.6989
2023-04-26 19:40:59,724 - INFO - Epoch 423 training loss = 0.6268
2023-04-26 19:41:03,740 - INFO - Epoch 424 training loss = 0.6212
2023-04-26 19:41:07,754 - INFO - Epoch 425 training loss = 0.7183
2023-04-26 19:41:11,770 - INFO - Epoch 426 training loss = 0.6407
2023-04-26 19:41:15,788 - INFO - Epoch 427 training loss = 0.6632
2023-04-26 19:41:19,805 - INFO - Epoch 428 training loss = 0.6864
2023-04-26 19:41:23,819 - INFO - Epoch 429 training loss = 0.6127
2023-04-26 19:41:27,834 - INFO - Epoch 430 training loss = 0.634
2023-04-26 19:41:28,147 - INFO - Validation loss = 1.12
2023-04-26 19:41:32,162 - INFO - Epoch 431 training loss = 0.666
2023-04-26 19:41:36,176 - INFO - Epoch 432 training loss = 0.6357
2023-04-26 19:41:40,191 - INFO - Epoch 433 training loss = 0.6357
2023-04-26 19:41:44,208 - INFO - Epoch 434 training loss = 0.7269
2023-04-26 19:41:48,224 - INFO - Epoch 435 training loss = 0.6008
2023-04-26 19:41:52,238 - INFO - Epoch 436 training loss = 0.596
2023-04-26 19:41:56,252 - INFO - Epoch 437 training loss = 0.6715
2023-04-26 19:42:00,275 - INFO - Epoch 438 training loss = 0.5955
2023-04-26 19:42:04,304 - INFO - Epoch 439 training loss = 0.6855
2023-04-26 19:42:08,333 - INFO - Epoch 440 training loss = 0.5712
2023-04-26 19:42:08,647 - INFO - Validation loss = 0.5317
2023-04-26 19:42:08,647 - INFO - best model
2023-04-26 19:42:12,702 - INFO - Epoch 441 training loss = 0.584
2023-04-26 19:42:16,733 - INFO - Epoch 442 training loss = 0.6455
2023-04-26 19:42:20,762 - INFO - Epoch 443 training loss = 0.6794
2023-04-26 19:42:24,789 - INFO - Epoch 444 training loss = 0.5962
2023-04-26 19:42:28,818 - INFO - Epoch 445 training loss = 0.6541
2023-04-26 19:42:32,847 - INFO - Epoch 446 training loss = 0.5986
2023-04-26 19:42:36,876 - INFO - Epoch 447 training loss = 0.5353
2023-04-26 19:42:40,904 - INFO - Epoch 448 training loss = 0.5574
2023-04-26 19:42:44,936 - INFO - Epoch 449 training loss = 0.5211
2023-04-26 19:42:48,968 - INFO - Epoch 450 training loss = 0.6444
2023-04-26 19:42:49,282 - INFO - Validation loss = 0.554
2023-04-26 19:42:53,311 - INFO - Epoch 451 training loss = 0.5547
2023-04-26 19:42:57,340 - INFO - Epoch 452 training loss = 0.6004
2023-04-26 19:43:01,370 - INFO - Epoch 453 training loss = 0.5572
2023-04-26 19:43:05,400 - INFO - Epoch 454 training loss = 0.5767
2023-04-26 19:43:09,429 - INFO - Epoch 455 training loss = 0.6139
2023-04-26 19:43:13,462 - INFO - Epoch 456 training loss = 0.5434
2023-04-26 19:43:17,494 - INFO - Epoch 457 training loss = 0.5188
2023-04-26 19:43:21,523 - INFO - Epoch 458 training loss = 0.5623
2023-04-26 19:43:25,552 - INFO - Epoch 459 training loss = 0.5279
2023-04-26 19:43:29,582 - INFO - Epoch 460 training loss = 0.5644
2023-04-26 19:43:29,895 - INFO - Validation loss = 0.4209
2023-04-26 19:43:29,896 - INFO - best model
2023-04-26 19:43:33,946 - INFO - Epoch 461 training loss = 0.5478
2023-04-26 19:43:37,975 - INFO - Epoch 462 training loss = 0.5928
2023-04-26 19:43:42,005 - INFO - Epoch 463 training loss = 0.5335
2023-04-26 19:43:46,039 - INFO - Epoch 464 training loss = 0.5403
2023-04-26 19:43:50,069 - INFO - Epoch 465 training loss = 0.5778
2023-04-26 19:43:54,098 - INFO - Epoch 466 training loss = 0.4862
2023-04-26 19:43:58,128 - INFO - Epoch 467 training loss = 0.5305
2023-04-26 19:44:02,158 - INFO - Epoch 468 training loss = 0.5918
2023-04-26 19:44:06,187 - INFO - Epoch 469 training loss = 0.5574
2023-04-26 19:44:10,216 - INFO - Epoch 470 training loss = 0.5234
2023-04-26 19:44:10,529 - INFO - Validation loss = 0.6353
2023-04-26 19:44:14,562 - INFO - Epoch 471 training loss = 0.5053
2023-04-26 19:44:18,593 - INFO - Epoch 472 training loss = 0.5008
2023-04-26 19:44:22,622 - INFO - Epoch 473 training loss = 0.5698
2023-04-26 19:44:26,652 - INFO - Epoch 474 training loss = 0.4738
2023-04-26 19:44:30,681 - INFO - Epoch 475 training loss = 0.5608
2023-04-26 19:44:34,709 - INFO - Epoch 476 training loss = 0.4942
2023-04-26 19:44:38,711 - INFO - Epoch 477 training loss = 0.5282
2023-04-26 19:44:42,716 - INFO - Epoch 478 training loss = 0.4911
2023-04-26 19:44:46,722 - INFO - Epoch 479 training loss = 0.5549
2023-04-26 19:44:50,725 - INFO - Epoch 480 training loss = 0.487
2023-04-26 19:44:51,038 - INFO - Validation loss = 0.5449
2023-04-26 19:44:55,041 - INFO - Epoch 481 training loss = 0.5536
2023-04-26 19:44:59,043 - INFO - Epoch 482 training loss = 0.4902
2023-04-26 19:45:03,046 - INFO - Epoch 483 training loss = 0.532
2023-04-26 19:45:07,048 - INFO - Epoch 484 training loss = 0.5224
2023-04-26 19:45:11,051 - INFO - Epoch 485 training loss = 0.4956
2023-04-26 19:45:15,056 - INFO - Epoch 486 training loss = 0.5101
2023-04-26 19:45:19,061 - INFO - Epoch 487 training loss = 0.4578
2023-04-26 19:45:23,063 - INFO - Epoch 488 training loss = 0.503
2023-04-26 19:45:27,065 - INFO - Epoch 489 training loss = 0.5064
2023-04-26 19:45:31,079 - INFO - Epoch 490 training loss = 0.5638
2023-04-26 19:45:31,392 - INFO - Validation loss = 0.6129
2023-04-26 19:45:35,402 - INFO - Epoch 491 training loss = 0.4835
2023-04-26 19:45:39,404 - INFO - Epoch 492 training loss = 0.4903
2023-04-26 19:45:43,408 - INFO - Epoch 493 training loss = 0.483
2023-04-26 19:45:47,413 - INFO - Epoch 494 training loss = 0.4313
2023-04-26 19:45:51,415 - INFO - Epoch 495 training loss = 0.5278
2023-04-26 19:45:55,417 - INFO - Epoch 496 training loss = 0.4741
2023-04-26 19:45:59,424 - INFO - Epoch 497 training loss = 0.5046
2023-04-26 19:46:03,440 - INFO - Epoch 498 training loss = 0.4939
2023-04-26 19:46:07,453 - INFO - Epoch 499 training loss = 0.4915
2023-04-26 19:46:11,458 - INFO - Epoch 500 training loss = 0.4519
2023-04-26 19:46:11,771 - INFO - Validation loss = 0.5391
2023-04-26 19:46:15,787 - INFO - Epoch 501 training loss = 0.5129
2023-04-26 19:46:19,802 - INFO - Epoch 502 training loss = 0.4827
2023-04-26 19:46:23,816 - INFO - Epoch 503 training loss = 0.5076
2023-04-26 19:46:27,828 - INFO - Epoch 504 training loss = 0.4707
2023-04-26 19:46:31,830 - INFO - Epoch 505 training loss = 0.4946
2023-04-26 19:46:35,834 - INFO - Epoch 506 training loss = 0.4463
2023-04-26 19:46:39,837 - INFO - Epoch 507 training loss = 0.4337
2023-04-26 19:46:43,847 - INFO - Epoch 508 training loss = 0.4575
2023-04-26 19:46:47,864 - INFO - Epoch 509 training loss = 0.4513
2023-04-26 19:46:51,879 - INFO - Epoch 510 training loss = 0.4106
2023-04-26 19:46:52,192 - INFO - Validation loss = 0.525
2023-04-26 19:46:56,206 - INFO - Epoch 511 training loss = 0.4549
2023-04-26 19:47:00,220 - INFO - Epoch 512 training loss = 0.4269
2023-04-26 19:47:04,234 - INFO - Epoch 513 training loss = 0.431
2023-04-26 19:47:08,247 - INFO - Epoch 514 training loss = 0.4403
2023-04-26 19:47:12,262 - INFO - Epoch 515 training loss = 0.4302
2023-04-26 19:47:16,279 - INFO - Epoch 516 training loss = 0.4736
2023-04-26 19:47:20,292 - INFO - Epoch 517 training loss = 0.4885
2023-04-26 19:47:24,306 - INFO - Epoch 518 training loss = 0.3967
2023-04-26 19:47:28,319 - INFO - Epoch 519 training loss = 0.3963
2023-04-26 19:47:32,333 - INFO - Epoch 520 training loss = 0.409
2023-04-26 19:47:32,647 - INFO - Validation loss = 0.8289
2023-04-26 19:47:36,661 - INFO - Epoch 521 training loss = 0.4531
2023-04-26 19:47:40,676 - INFO - Epoch 522 training loss = 0.4148
2023-04-26 19:47:44,693 - INFO - Epoch 523 training loss = 0.4404
2023-04-26 19:47:48,709 - INFO - Epoch 524 training loss = 0.3929
2023-04-26 19:47:52,722 - INFO - Epoch 525 training loss = 0.3844
2023-04-26 19:47:56,736 - INFO - Epoch 526 training loss = 0.4169
2023-04-26 19:48:00,765 - INFO - Epoch 527 training loss = 0.3787
2023-04-26 19:48:04,790 - INFO - Epoch 528 training loss = 0.4175
2023-04-26 19:48:08,804 - INFO - Epoch 529 training loss = 0.3961
2023-04-26 19:48:12,821 - INFO - Epoch 530 training loss = 0.3882
2023-04-26 19:48:13,134 - INFO - Validation loss = 0.4402
2023-04-26 19:48:17,151 - INFO - Epoch 531 training loss = 0.4286
2023-04-26 19:48:21,165 - INFO - Epoch 532 training loss = 0.4013
2023-04-26 19:48:25,178 - INFO - Epoch 533 training loss = 0.4116
2023-04-26 19:48:29,203 - INFO - Epoch 534 training loss = 0.455
2023-04-26 19:48:33,232 - INFO - Epoch 535 training loss = 0.4087
2023-04-26 19:48:37,261 - INFO - Epoch 536 training loss = 0.3818
2023-04-26 19:48:41,278 - INFO - Epoch 537 training loss = 0.3558
2023-04-26 19:48:45,295 - INFO - Epoch 538 training loss = 0.3881
2023-04-26 19:48:49,323 - INFO - Epoch 539 training loss = 0.4261
2023-04-26 19:48:53,352 - INFO - Epoch 540 training loss = 0.3592
2023-04-26 19:48:53,665 - INFO - Validation loss = 0.4399
2023-04-26 19:48:57,694 - INFO - Epoch 541 training loss = 0.4324
2023-04-26 19:49:01,724 - INFO - Epoch 542 training loss = 0.3524
2023-04-26 19:49:05,752 - INFO - Epoch 543 training loss = 0.4324
2023-04-26 19:49:09,782 - INFO - Epoch 544 training loss = 0.3488
2023-04-26 19:49:13,813 - INFO - Epoch 545 training loss = 0.3483
2023-04-26 19:49:17,843 - INFO - Epoch 546 training loss = 0.3505
2023-04-26 19:49:21,864 - INFO - Epoch 547 training loss = 0.3654
2023-04-26 19:49:25,876 - INFO - Epoch 548 training loss = 0.3446
2023-04-26 19:49:29,891 - INFO - Epoch 549 training loss = 0.3418
2023-04-26 19:49:33,905 - INFO - Epoch 550 training loss = 0.3682
2023-04-26 19:49:34,218 - INFO - Validation loss = 0.4054
2023-04-26 19:49:34,218 - INFO - best model
2023-04-26 19:49:38,253 - INFO - Epoch 551 training loss = 0.3579
2023-04-26 19:49:42,268 - INFO - Epoch 552 training loss = 0.3688
2023-04-26 19:49:46,285 - INFO - Epoch 553 training loss = 0.369
2023-04-26 19:49:50,300 - INFO - Epoch 554 training loss = 0.3532
2023-04-26 19:49:54,329 - INFO - Epoch 555 training loss = 0.3298
2023-04-26 19:49:58,359 - INFO - Epoch 556 training loss = 0.3235
2023-04-26 19:50:02,383 - INFO - Epoch 557 training loss = 0.3471
2023-04-26 19:50:06,399 - INFO - Epoch 558 training loss = 0.359
2023-04-26 19:50:10,414 - INFO - Epoch 559 training loss = 0.3448
2023-04-26 19:50:14,432 - INFO - Epoch 560 training loss = 0.3467
2023-04-26 19:50:14,745 - INFO - Validation loss = 0.4371
2023-04-26 19:50:18,763 - INFO - Epoch 561 training loss = 0.3301
2023-04-26 19:50:22,777 - INFO - Epoch 562 training loss = 0.3869
2023-04-26 19:50:26,791 - INFO - Epoch 563 training loss = 0.3652
2023-04-26 19:50:30,817 - INFO - Epoch 564 training loss = 0.3413
2023-04-26 19:50:34,846 - INFO - Epoch 565 training loss = 0.3157
2023-04-26 19:50:38,873 - INFO - Epoch 566 training loss = 0.3284
2023-04-26 19:50:42,901 - INFO - Epoch 567 training loss = 0.306
2023-04-26 19:50:46,917 - INFO - Epoch 568 training loss = 0.2973
2023-04-26 19:50:50,932 - INFO - Epoch 569 training loss = 0.3485
2023-04-26 19:50:54,946 - INFO - Epoch 570 training loss =  0.3
2023-04-26 19:50:55,259 - INFO - Validation loss = 0.3915
2023-04-26 19:50:55,259 - INFO - best model
2023-04-26 19:50:59,296 - INFO - Epoch 571 training loss = 0.3195
2023-04-26 19:51:03,311 - INFO - Epoch 572 training loss = 0.3339
2023-04-26 19:51:07,324 - INFO - Epoch 573 training loss = 0.2752
2023-04-26 19:51:11,351 - INFO - Epoch 574 training loss = 0.3256
2023-04-26 19:51:15,383 - INFO - Epoch 575 training loss = 0.3321
2023-04-26 19:51:19,414 - INFO - Epoch 576 training loss = 0.3418
2023-04-26 19:51:23,443 - INFO - Epoch 577 training loss = 0.2901
2023-04-26 19:51:27,472 - INFO - Epoch 578 training loss = 0.297
2023-04-26 19:51:31,500 - INFO - Epoch 579 training loss = 0.3028
2023-04-26 19:51:35,528 - INFO - Epoch 580 training loss = 0.2775
2023-04-26 19:51:35,843 - INFO - Validation loss = 0.398
2023-04-26 19:51:39,873 - INFO - Epoch 581 training loss = 0.3126
2023-04-26 19:51:43,903 - INFO - Epoch 582 training loss = 0.2988
2023-04-26 19:51:47,935 - INFO - Epoch 583 training loss = 0.2804
2023-04-26 19:51:51,963 - INFO - Epoch 584 training loss = 0.3572
2023-04-26 19:51:55,993 - INFO - Epoch 585 training loss = 0.3009
2023-04-26 19:52:00,024 - INFO - Epoch 586 training loss = 0.3134
2023-04-26 19:52:04,053 - INFO - Epoch 587 training loss = 0.2998
2023-04-26 19:52:08,082 - INFO - Epoch 588 training loss = 0.276
2023-04-26 19:52:12,114 - INFO - Epoch 589 training loss = 0.2649
2023-04-26 19:52:16,140 - INFO - Epoch 590 training loss = 0.2728
2023-04-26 19:52:16,455 - INFO - Validation loss = 0.2081
2023-04-26 19:52:16,455 - INFO - best model
2023-04-26 19:52:20,492 - INFO - Epoch 591 training loss = 0.2616
2023-04-26 19:52:24,506 - INFO - Epoch 592 training loss = 0.2751
2023-04-26 19:52:28,521 - INFO - Epoch 593 training loss = 0.2953
2023-04-26 19:52:32,534 - INFO - Epoch 594 training loss = 0.2748
2023-04-26 19:52:36,552 - INFO - Epoch 595 training loss = 0.2788
2023-04-26 19:52:40,581 - INFO - Epoch 596 training loss = 0.3033
2023-04-26 19:52:44,612 - INFO - Epoch 597 training loss = 0.2775
2023-04-26 19:52:48,643 - INFO - Epoch 598 training loss = 0.2481
2023-04-26 19:52:52,670 - INFO - Epoch 599 training loss = 0.2825
2023-04-26 19:52:56,699 - INFO - Epoch 600 training loss = 0.2753
2023-04-26 19:52:57,012 - INFO - Validation loss = 0.4378
2023-04-26 19:53:01,042 - INFO - Epoch 601 training loss = 0.2752
2023-04-26 19:53:05,069 - INFO - Epoch 602 training loss = 0.2636
2023-04-26 19:53:09,097 - INFO - Epoch 603 training loss = 0.2351
2023-04-26 19:53:13,126 - INFO - Epoch 604 training loss = 0.2755
2023-04-26 19:53:17,157 - INFO - Epoch 605 training loss = 0.2512
2023-04-26 19:53:21,184 - INFO - Epoch 606 training loss = 0.2398
2023-04-26 19:53:25,211 - INFO - Epoch 607 training loss = 0.2578
2023-04-26 19:53:29,238 - INFO - Epoch 608 training loss = 0.2606
2023-04-26 19:53:33,264 - INFO - Epoch 609 training loss = 0.2265
2023-04-26 19:53:37,292 - INFO - Epoch 610 training loss = 0.2499
2023-04-26 19:53:37,606 - INFO - Validation loss = 0.2612
2023-04-26 19:53:41,635 - INFO - Epoch 611 training loss = 0.2222
2023-04-26 19:53:45,667 - INFO - Epoch 612 training loss = 0.2469
2023-04-26 19:53:49,697 - INFO - Epoch 613 training loss = 0.2384
2023-04-26 19:53:53,726 - INFO - Epoch 614 training loss = 0.2503
2023-04-26 19:53:57,754 - INFO - Epoch 615 training loss = 0.2436
2023-04-26 19:54:01,778 - INFO - Epoch 616 training loss = 0.231
2023-04-26 19:54:05,793 - INFO - Epoch 617 training loss = 0.2209
2023-04-26 19:54:09,809 - INFO - Epoch 618 training loss = 0.2293
2023-04-26 19:54:13,826 - INFO - Epoch 619 training loss = 0.2276
2023-04-26 19:54:17,844 - INFO - Epoch 620 training loss = 0.2154
2023-04-26 19:54:18,157 - INFO - Validation loss = 0.235
2023-04-26 19:54:22,173 - INFO - Epoch 621 training loss = 0.2637
2023-04-26 19:54:26,187 - INFO - Epoch 622 training loss = 0.2269
2023-04-26 19:54:30,201 - INFO - Epoch 623 training loss = 0.244
2023-04-26 19:54:34,216 - INFO - Epoch 624 training loss = 0.2352
2023-04-26 19:54:38,230 - INFO - Epoch 625 training loss = 0.2216
2023-04-26 19:54:42,246 - INFO - Epoch 626 training loss = 0.2226
2023-04-26 19:54:46,263 - INFO - Epoch 627 training loss = 0.2274
2023-04-26 19:54:50,280 - INFO - Epoch 628 training loss = 0.23
2023-04-26 19:54:54,294 - INFO - Epoch 629 training loss = 0.1899
2023-04-26 19:54:58,309 - INFO - Epoch 630 training loss = 0.2092
2023-04-26 19:54:58,623 - INFO - Validation loss = 0.3226
2023-04-26 19:55:02,639 - INFO - Epoch 631 training loss = 0.1956
2023-04-26 19:55:06,654 - INFO - Epoch 632 training loss = 0.2013
2023-04-26 19:55:10,672 - INFO - Epoch 633 training loss = 0.2016
2023-04-26 19:55:14,703 - INFO - Epoch 634 training loss = 0.2123
2023-04-26 19:55:18,723 - INFO - Epoch 635 training loss = 0.2077
2023-04-26 19:55:22,736 - INFO - Epoch 636 training loss = 0.2085
2023-04-26 19:55:26,750 - INFO - Epoch 637 training loss = 0.2148
2023-04-26 19:55:30,765 - INFO - Epoch 638 training loss = 0.1958
2023-04-26 19:55:34,780 - INFO - Epoch 639 training loss = 0.1986
2023-04-26 19:55:38,794 - INFO - Epoch 640 training loss = 0.1901
2023-04-26 19:55:39,108 - INFO - Validation loss = 0.2388
2023-04-26 19:55:43,125 - INFO - Epoch 641 training loss = 0.1973
2023-04-26 19:55:47,144 - INFO - Epoch 642 training loss = 0.1927
2023-04-26 19:55:51,159 - INFO - Epoch 643 training loss = 0.1931
2023-04-26 19:55:55,174 - INFO - Epoch 644 training loss = 0.1871
2023-04-26 19:55:59,188 - INFO - Epoch 645 training loss = 0.1875
2023-04-26 19:56:03,204 - INFO - Epoch 646 training loss = 0.1888
2023-04-26 19:56:07,223 - INFO - Epoch 647 training loss = 0.1809
2023-04-26 19:56:11,253 - INFO - Epoch 648 training loss = 0.1915
2023-04-26 19:56:15,284 - INFO - Epoch 649 training loss = 0.17
2023-04-26 19:56:19,313 - INFO - Epoch 650 training loss = 0.1945
2023-04-26 19:56:19,628 - INFO - Validation loss = 0.1961
2023-04-26 19:56:19,628 - INFO - best model
2023-04-26 19:56:23,678 - INFO - Epoch 651 training loss = 0.19
2023-04-26 19:56:27,703 - INFO - Epoch 652 training loss = 0.1729
2023-04-26 19:56:31,718 - INFO - Epoch 653 training loss = 0.1483
2023-04-26 19:56:35,733 - INFO - Epoch 654 training loss = 0.1868
2023-04-26 19:56:39,747 - INFO - Epoch 655 training loss = 0.1964
2023-04-26 19:56:43,765 - INFO - Epoch 656 training loss = 0.1621
2023-04-26 19:56:47,783 - INFO - Epoch 657 training loss = 0.1873
2023-04-26 19:56:51,798 - INFO - Epoch 658 training loss = 0.1828
2023-04-26 19:56:55,812 - INFO - Epoch 659 training loss = 0.1502
2023-04-26 19:56:59,827 - INFO - Epoch 660 training loss = 0.1738
2023-04-26 19:57:00,141 - INFO - Validation loss = 0.1735
2023-04-26 19:57:00,141 - INFO - best model
2023-04-26 19:57:04,178 - INFO - Epoch 661 training loss = 0.1606
2023-04-26 19:57:08,192 - INFO - Epoch 662 training loss = 0.1677
2023-04-26 19:57:12,209 - INFO - Epoch 663 training loss = 0.1873
2023-04-26 19:57:16,227 - INFO - Epoch 664 training loss = 0.154
2023-04-26 19:57:20,244 - INFO - Epoch 665 training loss = 0.1698
2023-04-26 19:57:24,258 - INFO - Epoch 666 training loss = 0.1534
2023-04-26 19:57:28,273 - INFO - Epoch 667 training loss = 0.1571
2023-04-26 19:57:32,288 - INFO - Epoch 668 training loss = 0.1869
2023-04-26 19:57:36,303 - INFO - Epoch 669 training loss = 0.1665
2023-04-26 19:57:40,318 - INFO - Epoch 670 training loss = 0.1365
2023-04-26 19:57:40,631 - INFO - Validation loss = 0.1494
2023-04-26 19:57:40,631 - INFO - best model
2023-04-26 19:57:44,672 - INFO - Epoch 671 training loss = 0.174
2023-04-26 19:57:48,689 - INFO - Epoch 672 training loss = 0.1425
2023-04-26 19:57:52,703 - INFO - Epoch 673 training loss = 0.1539
2023-04-26 19:57:56,718 - INFO - Epoch 674 training loss = 0.1497
2023-04-26 19:58:00,734 - INFO - Epoch 675 training loss = 0.1529
2023-04-26 19:58:04,750 - INFO - Epoch 676 training loss = 0.1634
2023-04-26 19:58:08,764 - INFO - Epoch 677 training loss = 0.1661
2023-04-26 19:58:12,782 - INFO - Epoch 678 training loss = 0.1528
2023-04-26 19:58:16,800 - INFO - Epoch 679 training loss = 0.1464
2023-04-26 19:58:20,816 - INFO - Epoch 680 training loss = 0.137
2023-04-26 19:58:21,129 - INFO - Validation loss = 0.1512
2023-04-26 19:58:25,144 - INFO - Epoch 681 training loss = 0.1426
2023-04-26 19:58:29,158 - INFO - Epoch 682 training loss = 0.1496
2023-04-26 19:58:33,172 - INFO - Epoch 683 training loss = 0.1345
2023-04-26 19:58:37,186 - INFO - Epoch 684 training loss = 0.1383
2023-04-26 19:58:41,201 - INFO - Epoch 685 training loss = 0.1206
2023-04-26 19:58:45,232 - INFO - Epoch 686 training loss = 0.1416
2023-04-26 19:58:49,264 - INFO - Epoch 687 training loss = 0.1333
2023-04-26 19:58:53,292 - INFO - Epoch 688 training loss = 0.1444
2023-04-26 19:58:57,315 - INFO - Epoch 689 training loss = 0.1341
2023-04-26 19:59:01,330 - INFO - Epoch 690 training loss = 0.128
2023-04-26 19:59:01,644 - INFO - Validation loss = 0.11
2023-04-26 19:59:01,644 - INFO - best model
2023-04-26 19:59:05,681 - INFO - Epoch 691 training loss = 0.129
2023-04-26 19:59:09,697 - INFO - Epoch 692 training loss = 0.1432
2023-04-26 19:59:13,715 - INFO - Epoch 693 training loss = 0.1372
2023-04-26 19:59:17,733 - INFO - Epoch 694 training loss = 0.1361
2023-04-26 19:59:21,748 - INFO - Epoch 695 training loss = 0.1268
2023-04-26 19:59:25,763 - INFO - Epoch 696 training loss = 0.1337
2023-04-26 19:59:29,778 - INFO - Epoch 697 training loss = 0.123
2023-04-26 19:59:33,794 - INFO - Epoch 698 training loss = 0.13
2023-04-26 19:59:37,810 - INFO - Epoch 699 training loss = 0.1297
2023-04-26 19:59:41,826 - INFO - Epoch 700 training loss = 0.1257
2023-04-26 19:59:42,139 - INFO - Validation loss = 0.1752
2023-04-26 19:59:46,159 - INFO - Epoch 701 training loss = 0.1214
2023-04-26 19:59:50,177 - INFO - Epoch 702 training loss = 0.115
2023-04-26 19:59:54,192 - INFO - Epoch 703 training loss = 0.1067
2023-04-26 19:59:58,207 - INFO - Epoch 704 training loss = 0.1354
2023-04-26 20:00:02,227 - INFO - Epoch 705 training loss = 0.1003
2023-04-26 20:00:06,243 - INFO - Epoch 706 training loss = 0.1115
2023-04-26 20:00:10,259 - INFO - Epoch 707 training loss = 0.1177
2023-04-26 20:00:14,277 - INFO - Epoch 708 training loss = 0.1196
2023-04-26 20:00:18,295 - INFO - Epoch 709 training loss = 0.09895
2023-04-26 20:00:22,310 - INFO - Epoch 710 training loss = 0.1145
2023-04-26 20:00:22,624 - INFO - Validation loss = 0.1814
2023-04-26 20:00:26,640 - INFO - Epoch 711 training loss = 0.1047
2023-04-26 20:00:30,656 - INFO - Epoch 712 training loss = 0.1116
2023-04-26 20:00:34,672 - INFO - Epoch 713 training loss = 0.1025
2023-04-26 20:00:38,687 - INFO - Epoch 714 training loss = 0.1138
2023-04-26 20:00:42,705 - INFO - Epoch 715 training loss = 0.1137
2023-04-26 20:00:46,723 - INFO - Epoch 716 training loss = 0.09738
2023-04-26 20:00:50,740 - INFO - Epoch 717 training loss = 0.1005
2023-04-26 20:00:54,755 - INFO - Epoch 718 training loss = 0.1058
2023-04-26 20:00:58,771 - INFO - Epoch 719 training loss = 0.09711
2023-04-26 20:01:02,788 - INFO - Epoch 720 training loss = 0.1004
2023-04-26 20:01:03,101 - INFO - Validation loss = 0.09869
2023-04-26 20:01:03,101 - INFO - best model
2023-04-26 20:01:07,139 - INFO - Epoch 721 training loss = 0.08977
2023-04-26 20:01:11,153 - INFO - Epoch 722 training loss = 0.09746
2023-04-26 20:01:15,171 - INFO - Epoch 723 training loss = 0.08767
2023-04-26 20:01:19,188 - INFO - Epoch 724 training loss = 0.1082
2023-04-26 20:01:23,201 - INFO - Epoch 725 training loss = 0.1013
2023-04-26 20:01:27,215 - INFO - Epoch 726 training loss = 0.08583
2023-04-26 20:01:31,229 - INFO - Epoch 727 training loss = 0.08911
2023-04-26 20:01:35,244 - INFO - Epoch 728 training loss = 0.09593
2023-04-26 20:01:39,259 - INFO - Epoch 729 training loss = 0.07986
2023-04-26 20:01:43,275 - INFO - Epoch 730 training loss = 0.08742
2023-04-26 20:01:43,589 - INFO - Validation loss = 0.112
2023-04-26 20:01:47,607 - INFO - Epoch 731 training loss = 0.08999
2023-04-26 20:01:51,623 - INFO - Epoch 732 training loss = 0.09373
2023-04-26 20:01:55,637 - INFO - Epoch 733 training loss = 0.08058
2023-04-26 20:01:59,652 - INFO - Epoch 734 training loss = 0.07378
2023-04-26 20:02:03,668 - INFO - Epoch 735 training loss = 0.08324
2023-04-26 20:02:07,682 - INFO - Epoch 736 training loss = 0.08772
2023-04-26 20:02:11,698 - INFO - Epoch 737 training loss = 0.0792
2023-04-26 20:02:15,715 - INFO - Epoch 738 training loss = 0.08604
2023-04-26 20:02:19,731 - INFO - Epoch 739 training loss = 0.07648
2023-04-26 20:02:23,745 - INFO - Epoch 740 training loss = 0.07849
2023-04-26 20:02:24,058 - INFO - Validation loss = 0.1316
2023-04-26 20:02:28,073 - INFO - Epoch 741 training loss = 0.07933
2023-04-26 20:02:32,087 - INFO - Epoch 742 training loss = 0.08219
2023-04-26 20:02:36,102 - INFO - Epoch 743 training loss = 0.08382
2023-04-26 20:02:40,117 - INFO - Epoch 744 training loss = 0.07461
2023-04-26 20:02:44,140 - INFO - Epoch 745 training loss = 0.07255
2023-04-26 20:02:48,168 - INFO - Epoch 746 training loss = 0.06449
2023-04-26 20:02:52,183 - INFO - Epoch 747 training loss = 0.07704
2023-04-26 20:02:56,197 - INFO - Epoch 748 training loss = 0.07305
2023-04-26 20:03:00,223 - INFO - Epoch 749 training loss = 0.073
2023-04-26 20:03:04,251 - INFO - Epoch 750 training loss = 0.06894
2023-04-26 20:03:04,565 - INFO - Validation loss = 0.09785
2023-04-26 20:03:04,565 - INFO - best model
2023-04-26 20:03:08,616 - INFO - Epoch 751 training loss = 0.07111
2023-04-26 20:03:12,646 - INFO - Epoch 752 training loss = 0.07109
2023-04-26 20:03:16,678 - INFO - Epoch 753 training loss = 0.06436
2023-04-26 20:03:20,708 - INFO - Epoch 754 training loss = 0.07087
2023-04-26 20:03:24,735 - INFO - Epoch 755 training loss = 0.06799
2023-04-26 20:03:28,763 - INFO - Epoch 756 training loss = 0.06349
2023-04-26 20:03:32,790 - INFO - Epoch 757 training loss = 0.06571
2023-04-26 20:03:36,818 - INFO - Epoch 758 training loss = 0.06135
2023-04-26 20:03:40,846 - INFO - Epoch 759 training loss = 0.06767
2023-04-26 20:03:44,878 - INFO - Epoch 760 training loss = 0.06288
2023-04-26 20:03:45,193 - INFO - Validation loss = 0.1157
2023-04-26 20:03:49,225 - INFO - Epoch 761 training loss = 0.06355
2023-04-26 20:03:53,252 - INFO - Epoch 762 training loss = 0.06312
2023-04-26 20:03:57,281 - INFO - Epoch 763 training loss = 0.05932
2023-04-26 20:04:01,309 - INFO - Epoch 764 training loss = 0.05966
2023-04-26 20:04:05,338 - INFO - Epoch 765 training loss = 0.05713
2023-04-26 20:04:09,366 - INFO - Epoch 766 training loss = 0.06493
2023-04-26 20:04:13,397 - INFO - Epoch 767 training loss = 0.05788
2023-04-26 20:04:17,428 - INFO - Epoch 768 training loss = 0.05693
2023-04-26 20:04:21,456 - INFO - Epoch 769 training loss = 0.06353
2023-04-26 20:04:25,483 - INFO - Epoch 770 training loss = 0.05882
2023-04-26 20:04:25,809 - INFO - Validation loss = 0.1258
2023-04-26 20:04:29,837 - INFO - Epoch 771 training loss = 0.05423
2023-04-26 20:04:33,865 - INFO - Epoch 772 training loss = 0.0506
2023-04-26 20:04:37,892 - INFO - Epoch 773 training loss = 0.05747
2023-04-26 20:04:41,921 - INFO - Epoch 774 training loss = 0.05283
2023-04-26 20:04:45,953 - INFO - Epoch 775 training loss = 0.04997
2023-04-26 20:04:49,983 - INFO - Epoch 776 training loss = 0.05183
2023-04-26 20:04:54,010 - INFO - Epoch 777 training loss = 0.05252
2023-04-26 20:04:58,039 - INFO - Epoch 778 training loss = 0.05116
2023-04-26 20:05:02,066 - INFO - Epoch 779 training loss = 0.05388
2023-04-26 20:05:06,095 - INFO - Epoch 780 training loss = 0.05413
2023-04-26 20:05:06,409 - INFO - Validation loss = 0.08713
2023-04-26 20:05:06,409 - INFO - best model
2023-04-26 20:05:10,461 - INFO - Epoch 781 training loss = 0.04499
2023-04-26 20:05:14,491 - INFO - Epoch 782 training loss = 0.04634
2023-04-26 20:05:18,522 - INFO - Epoch 783 training loss = 0.05139
2023-04-26 20:05:22,550 - INFO - Epoch 784 training loss = 0.05109
2023-04-26 20:05:26,577 - INFO - Epoch 785 training loss = 0.05274
2023-04-26 20:05:30,605 - INFO - Epoch 786 training loss = 0.04927
2023-04-26 20:05:34,623 - INFO - Epoch 787 training loss = 0.046
2023-04-26 20:05:38,638 - INFO - Epoch 788 training loss = 0.04419
2023-04-26 20:05:42,654 - INFO - Epoch 789 training loss = 0.04965
2023-04-26 20:05:46,681 - INFO - Epoch 790 training loss = 0.04262
2023-04-26 20:05:46,995 - INFO - Validation loss = 0.07245
2023-04-26 20:05:46,996 - INFO - best model
2023-04-26 20:05:51,048 - INFO - Epoch 791 training loss = 0.0452
2023-04-26 20:05:55,077 - INFO - Epoch 792 training loss = 0.04423
2023-04-26 20:05:59,106 - INFO - Epoch 793 training loss = 0.04322
2023-04-26 20:06:03,121 - INFO - Epoch 794 training loss = 0.04469
2023-04-26 20:06:07,136 - INFO - Epoch 795 training loss = 0.03815
2023-04-26 20:06:11,151 - INFO - Epoch 796 training loss = 0.04206
2023-04-26 20:06:15,168 - INFO - Epoch 797 training loss = 0.03783
2023-04-26 20:06:19,184 - INFO - Epoch 798 training loss = 0.04465
2023-04-26 20:06:23,187 - INFO - Epoch 799 training loss = 0.03976
2023-04-26 20:06:27,189 - INFO - Epoch 800 training loss = 0.03962
2023-04-26 20:06:27,502 - INFO - Validation loss = 0.07177
2023-04-26 20:06:27,503 - INFO - best model
2023-04-26 20:06:31,539 - INFO - Epoch 801 training loss = 0.03666
2023-04-26 20:06:35,567 - INFO - Epoch 802 training loss = 0.04393
2023-04-26 20:06:39,596 - INFO - Epoch 803 training loss = 0.03751
2023-04-26 20:06:43,626 - INFO - Epoch 804 training loss = 0.03581
2023-04-26 20:06:47,657 - INFO - Epoch 805 training loss = 0.03968
2023-04-26 20:06:51,686 - INFO - Epoch 806 training loss = 0.03674
2023-04-26 20:06:55,714 - INFO - Epoch 807 training loss = 0.03591
2023-04-26 20:06:59,742 - INFO - Epoch 808 training loss = 0.03798
2023-04-26 20:07:03,771 - INFO - Epoch 809 training loss = 0.03898
2023-04-26 20:07:07,801 - INFO - Epoch 810 training loss = 0.03624
2023-04-26 20:07:08,116 - INFO - Validation loss = 0.06614
2023-04-26 20:07:08,116 - INFO - best model
2023-04-26 20:07:12,169 - INFO - Epoch 811 training loss = 0.03527
2023-04-26 20:07:16,201 - INFO - Epoch 812 training loss = 0.03472
2023-04-26 20:07:20,231 - INFO - Epoch 813 training loss = 0.03537
2023-04-26 20:07:24,260 - INFO - Epoch 814 training loss = 0.03302
2023-04-26 20:07:28,287 - INFO - Epoch 815 training loss = 0.03303
2023-04-26 20:07:32,315 - INFO - Epoch 816 training loss = 0.03232
2023-04-26 20:07:36,342 - INFO - Epoch 817 training loss = 0.03376
2023-04-26 20:07:40,371 - INFO - Epoch 818 training loss = 0.03302
2023-04-26 20:07:44,403 - INFO - Epoch 819 training loss = 0.02962
2023-04-26 20:07:48,434 - INFO - Epoch 820 training loss = 0.03151
2023-04-26 20:07:48,748 - INFO - Validation loss = 0.06573
2023-04-26 20:07:48,749 - INFO - best model
2023-04-26 20:07:52,800 - INFO - Epoch 821 training loss = 0.02983
2023-04-26 20:07:56,828 - INFO - Epoch 822 training loss = 0.03061
2023-04-26 20:08:00,858 - INFO - Epoch 823 training loss = 0.03116
2023-04-26 20:08:04,888 - INFO - Epoch 824 training loss = 0.02782
2023-04-26 20:08:08,916 - INFO - Epoch 825 training loss = 0.02762
2023-04-26 20:08:12,948 - INFO - Epoch 826 training loss = 0.03199
2023-04-26 20:08:16,980 - INFO - Epoch 827 training loss = 0.02659
2023-04-26 20:08:21,009 - INFO - Epoch 828 training loss = 0.02905
2023-04-26 20:08:25,038 - INFO - Epoch 829 training loss = 0.02958
2023-04-26 20:08:29,066 - INFO - Epoch 830 training loss = 0.02757
2023-04-26 20:08:29,380 - INFO - Validation loss = 0.07248
2023-04-26 20:08:33,408 - INFO - Epoch 831 training loss = 0.02853
2023-04-26 20:08:37,436 - INFO - Epoch 832 training loss = 0.02759
2023-04-26 20:08:41,464 - INFO - Epoch 833 training loss = 0.02615
2023-04-26 20:08:45,496 - INFO - Epoch 834 training loss = 0.02587
2023-04-26 20:08:49,527 - INFO - Epoch 835 training loss = 0.0261
2023-04-26 20:08:53,555 - INFO - Epoch 836 training loss = 0.0266
2023-04-26 20:08:57,583 - INFO - Epoch 837 training loss = 0.02647
2023-04-26 20:09:01,612 - INFO - Epoch 838 training loss = 0.02551
2023-04-26 20:09:05,642 - INFO - Epoch 839 training loss = 0.02478
2023-04-26 20:09:09,669 - INFO - Epoch 840 training loss = 0.02457
2023-04-26 20:09:09,984 - INFO - Validation loss = 0.05846
2023-04-26 20:09:09,984 - INFO - best model
2023-04-26 20:09:14,039 - INFO - Epoch 841 training loss = 0.0265
2023-04-26 20:09:18,070 - INFO - Epoch 842 training loss = 0.02421
2023-04-26 20:09:22,098 - INFO - Epoch 843 training loss = 0.02619
2023-04-26 20:09:26,126 - INFO - Epoch 844 training loss = 0.02468
2023-04-26 20:09:30,155 - INFO - Epoch 845 training loss = 0.02335
2023-04-26 20:09:34,183 - INFO - Epoch 846 training loss = 0.02182
2023-04-26 20:09:38,212 - INFO - Epoch 847 training loss = 0.02243
2023-04-26 20:09:42,242 - INFO - Epoch 848 training loss = 0.02235
2023-04-26 20:09:46,273 - INFO - Epoch 849 training loss = 0.02288
2023-04-26 20:09:50,303 - INFO - Epoch 850 training loss = 0.0209
2023-04-26 20:09:50,618 - INFO - Validation loss = 0.05089
2023-04-26 20:09:50,618 - INFO - best model
2023-04-26 20:09:54,669 - INFO - Epoch 851 training loss = 0.02061
2023-04-26 20:09:58,698 - INFO - Epoch 852 training loss = 0.0209
2023-04-26 20:10:02,730 - INFO - Epoch 853 training loss = 0.02108
2023-04-26 20:10:06,759 - INFO - Epoch 854 training loss = 0.02117
2023-04-26 20:10:10,789 - INFO - Epoch 855 training loss = 0.02191
2023-04-26 20:10:14,821 - INFO - Epoch 856 training loss = 0.02003
2023-04-26 20:10:18,852 - INFO - Epoch 857 training loss = 0.0208
2023-04-26 20:10:22,881 - INFO - Epoch 858 training loss = 0.01992
2023-04-26 20:10:26,908 - INFO - Epoch 859 training loss = 0.02154
2023-04-26 20:10:30,937 - INFO - Epoch 860 training loss = 0.02012
2023-04-26 20:10:31,251 - INFO - Validation loss = 0.05216
2023-04-26 20:10:35,280 - INFO - Epoch 861 training loss = 0.01976
2023-04-26 20:10:39,308 - INFO - Epoch 862 training loss = 0.02018
2023-04-26 20:10:43,339 - INFO - Epoch 863 training loss = 0.01873
2023-04-26 20:10:47,369 - INFO - Epoch 864 training loss = 0.01872
2023-04-26 20:10:51,399 - INFO - Epoch 865 training loss = 0.01869
2023-04-26 20:10:55,427 - INFO - Epoch 866 training loss = 0.01907
2023-04-26 20:10:59,456 - INFO - Epoch 867 training loss = 0.02009
2023-04-26 20:11:03,485 - INFO - Epoch 868 training loss = 0.01788
2023-04-26 20:11:07,514 - INFO - Epoch 869 training loss = 0.01836
2023-04-26 20:11:11,545 - INFO - Epoch 870 training loss = 0.01736
2023-04-26 20:11:11,859 - INFO - Validation loss = 0.0537
2023-04-26 20:11:15,892 - INFO - Epoch 871 training loss = 0.01787
2023-04-26 20:11:19,922 - INFO - Epoch 872 training loss = 0.01787
2023-04-26 20:11:23,949 - INFO - Epoch 873 training loss = 0.01774
2023-04-26 20:11:27,978 - INFO - Epoch 874 training loss = 0.01747
2023-04-26 20:11:32,006 - INFO - Epoch 875 training loss = 0.01674
2023-04-26 20:11:36,036 - INFO - Epoch 876 training loss = 0.0165
2023-04-26 20:11:40,064 - INFO - Epoch 877 training loss = 0.01664
2023-04-26 20:11:44,096 - INFO - Epoch 878 training loss = 0.0166
2023-04-26 20:11:48,127 - INFO - Epoch 879 training loss = 0.01687
2023-04-26 20:11:52,156 - INFO - Epoch 880 training loss = 0.01643
2023-04-26 20:11:52,470 - INFO - Validation loss = 0.04738
2023-04-26 20:11:52,471 - INFO - best model
2023-04-26 20:11:56,520 - INFO - Epoch 881 training loss = 0.01628
2023-04-26 20:12:00,550 - INFO - Epoch 882 training loss = 0.01621
2023-04-26 20:12:04,579 - INFO - Epoch 883 training loss = 0.01621
2023-04-26 20:12:08,607 - INFO - Epoch 884 training loss = 0.01583
2023-04-26 20:12:12,639 - INFO - Epoch 885 training loss = 0.01571
2023-04-26 20:12:16,669 - INFO - Epoch 886 training loss = 0.01594
2023-04-26 20:12:20,698 - INFO - Epoch 887 training loss = 0.01605
2023-04-26 20:12:24,726 - INFO - Epoch 888 training loss = 0.01552
2023-04-26 20:12:28,755 - INFO - Epoch 889 training loss = 0.01519
2023-04-26 20:12:32,783 - INFO - Epoch 890 training loss = 0.01502
2023-04-26 20:12:33,097 - INFO - Validation loss = 0.04631
2023-04-26 20:12:33,097 - INFO - best model
2023-04-26 20:12:37,148 - INFO - Epoch 891 training loss = 0.01475
2023-04-26 20:12:41,176 - INFO - Epoch 892 training loss = 0.01488
2023-04-26 20:12:45,207 - INFO - Epoch 893 training loss = 0.01494
2023-04-26 20:12:49,238 - INFO - Epoch 894 training loss = 0.01467
2023-04-26 20:12:53,265 - INFO - Epoch 895 training loss = 0.01449
2023-04-26 20:12:57,295 - INFO - Epoch 896 training loss = 0.01476
2023-04-26 20:13:01,324 - INFO - Epoch 897 training loss = 0.01496
2023-04-26 20:13:05,352 - INFO - Epoch 898 training loss = 0.01412
2023-04-26 20:13:09,380 - INFO - Epoch 899 training loss = 0.01413
2023-04-26 20:13:13,413 - INFO - Epoch 900 training loss = 0.0137
2023-04-26 20:13:13,727 - INFO - Validation loss = 0.04556
2023-04-26 20:13:13,727 - INFO - best model
2023-04-26 20:13:17,781 - INFO - Epoch 901 training loss = 0.01397
2023-04-26 20:13:21,809 - INFO - Epoch 902 training loss = 0.01381
2023-04-26 20:13:25,837 - INFO - Epoch 903 training loss = 0.01362
2023-04-26 20:13:29,865 - INFO - Epoch 904 training loss = 0.01348
2023-04-26 20:13:33,892 - INFO - Epoch 905 training loss = 0.01368
2023-04-26 20:13:37,921 - INFO - Epoch 906 training loss = 0.01355
2023-04-26 20:13:41,950 - INFO - Epoch 907 training loss = 0.01302
2023-04-26 20:13:45,981 - INFO - Epoch 908 training loss = 0.01337
2023-04-26 20:13:50,013 - INFO - Epoch 909 training loss = 0.01333
2023-04-26 20:13:54,042 - INFO - Epoch 910 training loss = 0.01326
2023-04-26 20:13:54,356 - INFO - Validation loss = 0.04474
2023-04-26 20:13:54,356 - INFO - best model
2023-04-26 20:13:58,408 - INFO - Epoch 911 training loss = 0.01273
2023-04-26 20:14:02,438 - INFO - Epoch 912 training loss = 0.01299
2023-04-26 20:14:06,467 - INFO - Epoch 913 training loss = 0.01283
2023-04-26 20:14:10,497 - INFO - Epoch 914 training loss = 0.01273
2023-04-26 20:14:14,528 - INFO - Epoch 915 training loss = 0.01265
2023-04-26 20:14:18,560 - INFO - Epoch 916 training loss = 0.01259
2023-04-26 20:14:22,588 - INFO - Epoch 917 training loss = 0.01247
2023-04-26 20:14:26,617 - INFO - Epoch 918 training loss = 0.01238
2023-04-26 20:14:30,645 - INFO - Epoch 919 training loss = 0.01244
2023-04-26 20:14:34,672 - INFO - Epoch 920 training loss = 0.01226
2023-04-26 20:14:34,987 - INFO - Validation loss = 0.04485
2023-04-26 20:14:39,015 - INFO - Epoch 921 training loss = 0.0123
2023-04-26 20:14:43,044 - INFO - Epoch 922 training loss = 0.01202
2023-04-26 20:14:47,074 - INFO - Epoch 923 training loss = 0.0121
2023-04-26 20:14:51,102 - INFO - Epoch 924 training loss = 0.0121
2023-04-26 20:14:55,130 - INFO - Epoch 925 training loss = 0.01195
2023-04-26 20:14:59,157 - INFO - Epoch 926 training loss = 0.01182
2023-04-26 20:15:03,186 - INFO - Epoch 927 training loss = 0.01173
2023-04-26 20:15:07,213 - INFO - Epoch 928 training loss = 0.01171
2023-04-26 20:15:11,241 - INFO - Epoch 929 training loss = 0.01178
2023-04-26 20:15:15,272 - INFO - Epoch 930 training loss = 0.01158
2023-04-26 20:15:15,586 - INFO - Validation loss = 0.04296
2023-04-26 20:15:15,586 - INFO - best model
2023-04-26 20:15:19,638 - INFO - Epoch 931 training loss = 0.01164
2023-04-26 20:15:23,665 - INFO - Epoch 932 training loss = 0.01153
2023-04-26 20:15:27,692 - INFO - Epoch 933 training loss = 0.01148
2023-04-26 20:15:31,719 - INFO - Epoch 934 training loss = 0.01148
2023-04-26 20:15:35,746 - INFO - Epoch 935 training loss = 0.01141
2023-04-26 20:15:39,773 - INFO - Epoch 936 training loss = 0.01134
2023-04-26 20:15:43,803 - INFO - Epoch 937 training loss = 0.01129
2023-04-26 20:15:47,834 - INFO - Epoch 938 training loss = 0.01126
2023-04-26 20:15:51,861 - INFO - Epoch 939 training loss = 0.01118
2023-04-26 20:15:55,889 - INFO - Epoch 940 training loss = 0.01118
2023-04-26 20:15:56,203 - INFO - Validation loss = 0.04239
2023-04-26 20:15:56,204 - INFO - best model
2023-04-26 20:16:00,254 - INFO - Epoch 941 training loss = 0.01108
2023-04-26 20:16:04,281 - INFO - Epoch 942 training loss = 0.01106
2023-04-26 20:16:08,308 - INFO - Epoch 943 training loss = 0.011
2023-04-26 20:16:12,337 - INFO - Epoch 944 training loss = 0.01095
2023-04-26 20:16:16,367 - INFO - Epoch 945 training loss = 0.01099
2023-04-26 20:16:20,396 - INFO - Epoch 946 training loss = 0.01101
2023-04-26 20:16:24,422 - INFO - Epoch 947 training loss = 0.01086
2023-04-26 20:16:28,449 - INFO - Epoch 948 training loss = 0.01082
2023-04-26 20:16:32,475 - INFO - Epoch 949 training loss = 0.01082
2023-04-26 20:16:36,502 - INFO - Epoch 950 training loss = 0.01074
2023-04-26 20:16:36,816 - INFO - Validation loss = 0.04175
2023-04-26 20:16:36,816 - INFO - best model
2023-04-26 20:16:40,866 - INFO - Epoch 951 training loss = 0.01076
2023-04-26 20:16:44,897 - INFO - Epoch 952 training loss = 0.01068
2023-04-26 20:16:48,927 - INFO - Epoch 953 training loss = 0.01068
2023-04-26 20:16:52,954 - INFO - Epoch 954 training loss = 0.01065
2023-04-26 20:16:56,982 - INFO - Epoch 955 training loss = 0.01061
2023-04-26 20:17:01,011 - INFO - Epoch 956 training loss = 0.01058
2023-04-26 20:17:05,040 - INFO - Epoch 957 training loss = 0.01056
2023-04-26 20:17:09,068 - INFO - Epoch 958 training loss = 0.01052
2023-04-26 20:17:13,099 - INFO - Epoch 959 training loss = 0.01049
2023-04-26 20:17:17,130 - INFO - Epoch 960 training loss = 0.01046
2023-04-26 20:17:17,444 - INFO - Validation loss = 0.04157
2023-04-26 20:17:17,445 - INFO - best model
2023-04-26 20:17:21,495 - INFO - Epoch 961 training loss = 0.01044
2023-04-26 20:17:25,522 - INFO - Epoch 962 training loss = 0.01042
2023-04-26 20:17:29,550 - INFO - Epoch 963 training loss = 0.0104
2023-04-26 20:17:33,577 - INFO - Epoch 964 training loss = 0.01038
2023-04-26 20:17:37,604 - INFO - Epoch 965 training loss = 0.01034
2023-04-26 20:17:41,632 - INFO - Epoch 966 training loss = 0.01032
2023-04-26 20:17:45,664 - INFO - Epoch 967 training loss = 0.01034
2023-04-26 20:17:49,693 - INFO - Epoch 968 training loss = 0.01028
2023-04-26 20:17:53,720 - INFO - Epoch 969 training loss = 0.01026
2023-04-26 20:17:57,748 - INFO - Epoch 970 training loss = 0.01024
2023-04-26 20:17:58,063 - INFO - Validation loss = 0.0411
2023-04-26 20:17:58,063 - INFO - best model
2023-04-26 20:18:02,115 - INFO - Epoch 971 training loss = 0.01022
2023-04-26 20:18:06,143 - INFO - Epoch 972 training loss = 0.01021
2023-04-26 20:18:10,171 - INFO - Epoch 973 training loss = 0.01018
2023-04-26 20:18:14,202 - INFO - Epoch 974 training loss = 0.01017
2023-04-26 20:18:18,232 - INFO - Epoch 975 training loss = 0.01015
2023-04-26 20:18:22,260 - INFO - Epoch 976 training loss = 0.01015
2023-04-26 20:18:26,287 - INFO - Epoch 977 training loss = 0.01012
2023-04-26 20:18:30,315 - INFO - Epoch 978 training loss = 0.0101
2023-04-26 20:18:34,343 - INFO - Epoch 979 training loss = 0.01009
2023-04-26 20:18:38,370 - INFO - Epoch 980 training loss = 0.01008
2023-04-26 20:18:38,685 - INFO - Validation loss = 0.04075
2023-04-26 20:18:38,685 - INFO - best model
2023-04-26 20:18:42,736 - INFO - Epoch 981 training loss = 0.01006
2023-04-26 20:18:46,766 - INFO - Epoch 982 training loss = 0.01006
2023-04-26 20:18:50,796 - INFO - Epoch 983 training loss = 0.01004
2023-04-26 20:18:54,824 - INFO - Epoch 984 training loss = 0.01003
2023-04-26 20:18:58,852 - INFO - Epoch 985 training loss = 0.01004
2023-04-26 20:19:02,880 - INFO - Epoch 986 training loss = 0.01001
2023-04-26 20:19:06,908 - INFO - Epoch 987 training loss = 0.009998
2023-04-26 20:19:10,937 - INFO - Epoch 988 training loss = 0.009995
2023-04-26 20:19:14,968 - INFO - Epoch 989 training loss = 0.009991
2023-04-26 20:19:18,998 - INFO - Epoch 990 training loss = 0.009973
2023-04-26 20:19:19,312 - INFO - Validation loss = 0.04076
2023-04-26 20:19:23,340 - INFO - Epoch 991 training loss = 0.009969
2023-04-26 20:19:27,369 - INFO - Epoch 992 training loss = 0.009966
2023-04-26 20:19:31,396 - INFO - Epoch 993 training loss = 0.009962
2023-04-26 20:19:35,423 - INFO - Epoch 994 training loss = 0.009952
2023-04-26 20:19:39,451 - INFO - Epoch 995 training loss = 0.009952
2023-04-26 20:19:43,481 - INFO - Epoch 996 training loss = 0.009943
2023-04-26 20:19:47,511 - INFO - Epoch 997 training loss = 0.009943
2023-04-26 20:19:51,540 - INFO - Epoch 998 training loss = 0.009939
2023-04-26 20:19:55,567 - INFO - Epoch 999 training loss = 0.00994
2023-04-26 20:19:55,859 - INFO - Validation loss = 0.04066
