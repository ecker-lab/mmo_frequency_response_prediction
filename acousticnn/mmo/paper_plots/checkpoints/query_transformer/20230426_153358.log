2023-04-26 15:33:58,570 - INFO - Config:
Namespace(config='configs/implicit_transformer.yaml', dir='/user/delden/acoustics/spp_ai_acoustics/acousticNN/experiments/arch/no_encoding/implicit_transformer', epochs=100, device='cuda', seed=0, parameter_list='configs/data.yaml', encoding='none', dir_name='arch/no_encoding/implicit_transformer')
2023-04-26 15:33:58,570 - INFO - Config:
Munch({'optimizer': Munch({'type': 'AdamW', 'kwargs': Munch({'lr': 0.001, 'weight_decay': 0.005, 'betas': [0.9, 0.95]})}), 'scheduler': Munch({'type': 'CosLR', 'kwargs': Munch({'epochs': 500, 'initial_epochs': 25})}), 'model': Munch({'name': 'ImplicitTransformer', 'input_encoding': True, 'encoding_dim': 16, 'encoder': Munch({'embed_dim': 66, 'num_heads': 3, 'depth': 4}), 'decoder': Munch({'embed_dim': 99, 'num_heads': 3, 'depth': 3})}), 'generation_frequency': 5000, 'validation_frequency': 10, 'batch_size': 16, 'epochs': 500, 'gradient_clip': 10})
2023-04-26 15:34:13,885 - INFO - ==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ImplicitTransformer                      [16, 200, 4]              --
├─PositionalEncoding: 1-1                [16, 14, 66]              --
│    └─SinosoidalEncoding: 2-1           [16, 14, 66]              --
├─GroupwiseProjection: 1-2               [16, 14, 66]              --
│    └─ModuleList: 2-2                   --                        --
│    │    └─Linear: 3-1                  [16, 4, 66]               132
│    │    └─Linear: 3-2                  [16, 5, 66]               132
│    │    └─Linear: 3-3                  [16, 5, 66]               132
├─TransformerEncoder: 1-3                [16, 14, 66]              --
│    └─ModuleList: 2-3                   --                        --
│    │    └─Block: 3-4                   [16, 14, 66]              52,932
│    │    └─Block: 3-5                   [16, 14, 66]              52,932
│    │    └─Block: 3-6                   [16, 14, 66]              52,932
│    │    └─Block: 3-7                   [16, 14, 66]              52,932
├─Linear: 1-4                            [3200, 4, 99]             6,633
├─Linear: 1-5                            [16, 200, 99]             198
├─TransformerEncoder: 1-6                [3200, 4, 99]             --
│    └─ModuleList: 2-4                   --                        --
│    │    └─Block: 3-8                   [3200, 4, 99]             118,602
│    │    └─Block: 3-9                   [3200, 4, 99]             118,602
│    │    └─Block: 3-10                  [3200, 4, 99]             118,602
├─Linear: 1-7                            [3200, 4, 1]              100
==========================================================================================
Total params: 574,861
Trainable params: 574,861
Non-trainable params: 0
Total mult-adds (G): 1.16
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 352.64
Params size (MB): 2.30
Estimated Total Size (MB): 354.95
==========================================================================================
2023-04-26 15:34:21,325 - INFO - Epoch 0 training loss = 1.265e+04
2023-04-26 15:34:21,834 - INFO - Validation loss = 9.947e+03
2023-04-26 15:34:21,834 - INFO - best model
2023-04-26 15:34:29,127 - INFO - Epoch 1 training loss = 7.612e+03
2023-04-26 15:34:36,581 - INFO - Epoch 2 training loss = 505.5
2023-04-26 15:34:44,014 - INFO - Epoch 3 training loss = 209.5
2023-04-26 15:34:51,436 - INFO - Epoch 4 training loss = 193.0
