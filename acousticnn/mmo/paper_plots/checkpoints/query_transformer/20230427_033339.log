2023-04-27 03:33:39,535 - INFO - Config:
Namespace(config='configs/implicit_transformer.yaml', dir='/user/delden/acoustics/spp_ai_acoustics/acousticNN/experiments/arch/no_encoding/implicit_transformer', epochs=100, device='cuda', seed=2, parameter_list='configs/data.yaml', encoding='none', dir_name='arch/no_encoding/implicit_transformer')
2023-04-27 03:33:39,535 - INFO - Config:
Munch({'optimizer': Munch({'type': 'AdamW', 'kwargs': Munch({'lr': 0.0025, 'weight_decay': 0.005, 'betas': [0.9, 0.95]})}), 'scheduler': Munch({'type': 'CosLR', 'kwargs': Munch({'epochs': 1000, 'initial_epochs': 25})}), 'model': Munch({'name': 'ImplicitTransformer', 'input_encoding': True, 'encoding_dim': 16, 'encoder': Munch({'embed_dim': 66, 'num_heads': 3, 'depth': 4}), 'decoder': Munch({'embed_dim': 99, 'num_heads': 3, 'depth': 3})}), 'generation_frequency': 5000, 'validation_frequency': 10, 'batch_size': 16, 'epochs': 1000, 'gradient_clip': 10})
2023-04-27 03:33:53,632 - INFO - ==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ImplicitTransformer                      [16, 200, 4]              --
├─PositionalEncoding: 1-1                [16, 14, 66]              --
│    └─SinosoidalEncoding: 2-1           [16, 14, 66]              --
├─GroupwiseProjection: 1-2               [16, 14, 66]              --
│    └─ModuleList: 2-2                   --                        --
│    │    └─Linear: 3-1                  [16, 4, 66]               132
│    │    └─Linear: 3-2                  [16, 5, 66]               132
│    │    └─Linear: 3-3                  [16, 5, 66]               132
├─TransformerEncoder: 1-3                [16, 14, 66]              --
│    └─ModuleList: 2-3                   --                        --
│    │    └─Block: 3-4                   [16, 14, 66]              52,932
│    │    └─Block: 3-5                   [16, 14, 66]              52,932
│    │    └─Block: 3-6                   [16, 14, 66]              52,932
│    │    └─Block: 3-7                   [16, 14, 66]              52,932
├─Linear: 1-4                            [3200, 4, 99]             6,633
├─Linear: 1-5                            [16, 200, 99]             198
├─TransformerEncoder: 1-6                [3200, 4, 99]             --
│    └─ModuleList: 2-4                   --                        --
│    │    └─Block: 3-8                   [3200, 4, 99]             118,602
│    │    └─Block: 3-9                   [3200, 4, 99]             118,602
│    │    └─Block: 3-10                  [3200, 4, 99]             118,602
├─Linear: 1-7                            [3200, 4, 1]              100
==========================================================================================
Total params: 574,861
Trainable params: 574,861
Non-trainable params: 0
Total mult-adds (G): 1.16
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 352.64
Params size (MB): 2.30
Estimated Total Size (MB): 354.95
==========================================================================================
2023-04-27 03:33:57,691 - INFO - Epoch 0 training loss = 5.023e+03
2023-04-27 03:33:58,005 - INFO - Validation loss = 3.112e+03
2023-04-27 03:33:58,006 - INFO - best model
2023-04-27 03:34:02,055 - INFO - Epoch 1 training loss = 2.21e+03
2023-04-27 03:34:06,082 - INFO - Epoch 2 training loss = 469.0
2023-04-27 03:34:10,109 - INFO - Epoch 3 training loss = 267.4
2023-04-27 03:34:14,131 - INFO - Epoch 4 training loss = 137.2
2023-04-27 03:34:18,146 - INFO - Epoch 5 training loss = 104.5
2023-04-27 03:34:22,157 - INFO - Epoch 6 training loss = 89.63
2023-04-27 03:34:26,169 - INFO - Epoch 7 training loss = 70.31
2023-04-27 03:34:30,194 - INFO - Epoch 8 training loss = 57.44
2023-04-27 03:34:34,218 - INFO - Epoch 9 training loss = 52.09
2023-04-27 03:34:38,243 - INFO - Epoch 10 training loss = 47.19
2023-04-27 03:34:38,557 - INFO - Validation loss = 48.41
2023-04-27 03:34:38,557 - INFO - best model
2023-04-27 03:34:42,607 - INFO - Epoch 11 training loss = 42.31
2023-04-27 03:34:46,636 - INFO - Epoch 12 training loss = 38.35
2023-04-27 03:34:50,663 - INFO - Epoch 13 training loss = 34.32
2023-04-27 03:34:54,690 - INFO - Epoch 14 training loss = 32.48
2023-04-27 03:34:58,717 - INFO - Epoch 15 training loss = 29.53
2023-04-27 03:35:02,744 - INFO - Epoch 16 training loss = 26.45
2023-04-27 03:35:06,770 - INFO - Epoch 17 training loss = 28.07
2023-04-27 03:35:10,797 - INFO - Epoch 18 training loss = 22.26
2023-04-27 03:35:14,827 - INFO - Epoch 19 training loss = 23.52
2023-04-27 03:35:18,855 - INFO - Epoch 20 training loss = 22.52
2023-04-27 03:35:19,169 - INFO - Validation loss = 20.4
2023-04-27 03:35:19,169 - INFO - best model
2023-04-27 03:35:23,218 - INFO - Epoch 21 training loss = 20.7
2023-04-27 03:35:27,244 - INFO - Epoch 22 training loss = 20.52
2023-04-27 03:35:31,270 - INFO - Epoch 23 training loss = 18.52
2023-04-27 03:35:35,295 - INFO - Epoch 24 training loss = 18.91
2023-04-27 03:35:39,321 - INFO - Epoch 25 training loss = 17.79
2023-04-27 03:35:43,350 - INFO - Epoch 26 training loss = 19.5
2023-04-27 03:35:47,381 - INFO - Epoch 27 training loss = 16.37
2023-04-27 03:35:51,410 - INFO - Epoch 28 training loss = 15.33
2023-04-27 03:35:55,437 - INFO - Epoch 29 training loss = 13.92
2023-04-27 03:35:59,464 - INFO - Epoch 30 training loss = 12.78
2023-04-27 03:35:59,777 - INFO - Validation loss = 12.09
2023-04-27 03:35:59,777 - INFO - best model
2023-04-27 03:36:03,823 - INFO - Epoch 31 training loss = 13.09
2023-04-27 03:36:07,848 - INFO - Epoch 32 training loss = 12.14
2023-04-27 03:36:11,876 - INFO - Epoch 33 training loss = 11.21
2023-04-27 03:36:15,906 - INFO - Epoch 34 training loss = 11.64
2023-04-27 03:36:19,934 - INFO - Epoch 35 training loss = 9.757
2023-04-27 03:36:23,960 - INFO - Epoch 36 training loss = 9.314
2023-04-27 03:36:27,986 - INFO - Epoch 37 training loss = 9.177
2023-04-27 03:36:32,012 - INFO - Epoch 38 training loss = 9.61
2023-04-27 03:36:36,038 - INFO - Epoch 39 training loss = 7.811
2023-04-27 03:36:40,064 - INFO - Epoch 40 training loss = 7.659
2023-04-27 03:36:40,378 - INFO - Validation loss = 6.181
2023-04-27 03:36:40,378 - INFO - best model
2023-04-27 03:36:44,432 - INFO - Epoch 41 training loss = 7.767
2023-04-27 03:36:48,463 - INFO - Epoch 42 training loss = 7.72
2023-04-27 03:36:52,491 - INFO - Epoch 43 training loss = 7.141
2023-04-27 03:36:56,519 - INFO - Epoch 44 training loss = 6.158
2023-04-27 03:37:00,547 - INFO - Epoch 45 training loss = 6.285
2023-04-27 03:37:04,575 - INFO - Epoch 46 training loss = 6.181
2023-04-27 03:37:08,602 - INFO - Epoch 47 training loss = 6.455
2023-04-27 03:37:12,633 - INFO - Epoch 48 training loss = 5.999
2023-04-27 03:37:16,662 - INFO - Epoch 49 training loss = 6.384
2023-04-27 03:37:20,690 - INFO - Epoch 50 training loss = 5.562
2023-04-27 03:37:21,004 - INFO - Validation loss = 4.577
2023-04-27 03:37:21,004 - INFO - best model
2023-04-27 03:37:25,052 - INFO - Epoch 51 training loss = 5.187
2023-04-27 03:37:29,069 - INFO - Epoch 52 training loss = 5.498
2023-04-27 03:37:33,083 - INFO - Epoch 53 training loss = 6.167
2023-04-27 03:37:37,102 - INFO - Epoch 54 training loss = 5.285
2023-04-27 03:37:41,130 - INFO - Epoch 55 training loss = 5.186
2023-04-27 03:37:45,161 - INFO - Epoch 56 training loss = 4.991
2023-04-27 03:37:49,191 - INFO - Epoch 57 training loss = 4.682
2023-04-27 03:37:53,218 - INFO - Epoch 58 training loss = 4.991
2023-04-27 03:37:57,245 - INFO - Epoch 59 training loss = 4.46
2023-04-27 03:38:01,263 - INFO - Epoch 60 training loss = 4.636
2023-04-27 03:38:01,576 - INFO - Validation loss = 5.242
2023-04-27 03:38:05,591 - INFO - Epoch 61 training loss = 4.536
2023-04-27 03:38:09,605 - INFO - Epoch 62 training loss = 4.958
2023-04-27 03:38:13,622 - INFO - Epoch 63 training loss = 4.497
2023-04-27 03:38:17,638 - INFO - Epoch 64 training loss = 4.498
2023-04-27 03:38:21,652 - INFO - Epoch 65 training loss = 4.108
2023-04-27 03:38:25,665 - INFO - Epoch 66 training loss = 4.262
2023-04-27 03:38:29,680 - INFO - Epoch 67 training loss =  4.2
2023-04-27 03:38:33,694 - INFO - Epoch 68 training loss = 4.196
2023-04-27 03:38:37,707 - INFO - Epoch 69 training loss = 4.015
2023-04-27 03:38:41,711 - INFO - Epoch 70 training loss = 3.732
2023-04-27 03:38:42,024 - INFO - Validation loss = 2.731
2023-04-27 03:38:42,024 - INFO - best model
2023-04-27 03:38:46,050 - INFO - Epoch 71 training loss = 4.467
2023-04-27 03:38:50,054 - INFO - Epoch 72 training loss = 4.084
2023-04-27 03:38:54,056 - INFO - Epoch 73 training loss = 3.612
2023-04-27 03:38:58,058 - INFO - Epoch 74 training loss = 3.651
2023-04-27 03:39:02,060 - INFO - Epoch 75 training loss = 3.42
2023-04-27 03:39:06,062 - INFO - Epoch 76 training loss = 3.411
2023-04-27 03:39:10,064 - INFO - Epoch 77 training loss = 3.878
2023-04-27 03:39:14,068 - INFO - Epoch 78 training loss = 3.538
2023-04-27 03:39:18,070 - INFO - Epoch 79 training loss = 3.759
2023-04-27 03:39:22,070 - INFO - Epoch 80 training loss = 3.184
2023-04-27 03:39:22,395 - INFO - Validation loss =  2.4
2023-04-27 03:39:22,395 - INFO - best model
2023-04-27 03:39:26,417 - INFO - Epoch 81 training loss = 3.379
2023-04-27 03:39:30,416 - INFO - Epoch 82 training loss = 3.272
2023-04-27 03:39:34,416 - INFO - Epoch 83 training loss = 3.414
2023-04-27 03:39:38,416 - INFO - Epoch 84 training loss = 3.259
2023-04-27 03:39:42,418 - INFO - Epoch 85 training loss = 3.252
2023-04-27 03:39:46,421 - INFO - Epoch 86 training loss = 3.106
2023-04-27 03:39:50,423 - INFO - Epoch 87 training loss = 3.019
2023-04-27 03:39:54,424 - INFO - Epoch 88 training loss = 3.075
2023-04-27 03:39:58,425 - INFO - Epoch 89 training loss = 3.045
2023-04-27 03:40:02,429 - INFO - Epoch 90 training loss = 3.342
2023-04-27 03:40:02,742 - INFO - Validation loss = 2.858
2023-04-27 03:40:06,743 - INFO - Epoch 91 training loss = 3.368
2023-04-27 03:40:10,745 - INFO - Epoch 92 training loss = 2.849
2023-04-27 03:40:14,750 - INFO - Epoch 93 training loss = 3.239
2023-04-27 03:40:18,754 - INFO - Epoch 94 training loss = 3.214
2023-04-27 03:40:22,755 - INFO - Epoch 95 training loss = 3.065
2023-04-27 03:40:26,756 - INFO - Epoch 96 training loss = 2.785
2023-04-27 03:40:30,757 - INFO - Epoch 97 training loss = 2.912
2023-04-27 03:40:34,758 - INFO - Epoch 98 training loss = 2.82
2023-04-27 03:40:38,759 - INFO - Epoch 99 training loss = 2.708
2023-04-27 03:40:42,762 - INFO - Epoch 100 training loss = 2.624
2023-04-27 03:40:43,075 - INFO - Validation loss = 2.094
2023-04-27 03:40:43,076 - INFO - best model
2023-04-27 03:40:47,102 - INFO - Epoch 101 training loss = 2.814
2023-04-27 03:40:51,105 - INFO - Epoch 102 training loss = 2.737
2023-04-27 03:40:55,106 - INFO - Epoch 103 training loss = 2.616
2023-04-27 03:40:59,108 - INFO - Epoch 104 training loss = 2.678
2023-04-27 03:41:03,110 - INFO - Epoch 105 training loss = 2.55
2023-04-27 03:41:07,112 - INFO - Epoch 106 training loss = 2.664
2023-04-27 03:41:11,114 - INFO - Epoch 107 training loss = 2.502
2023-04-27 03:41:15,121 - INFO - Epoch 108 training loss = 2.672
2023-04-27 03:41:19,124 - INFO - Epoch 109 training loss = 2.603
2023-04-27 03:41:23,126 - INFO - Epoch 110 training loss = 2.487
2023-04-27 03:41:23,438 - INFO - Validation loss = 2.112
2023-04-27 03:41:27,441 - INFO - Epoch 111 training loss = 2.554
2023-04-27 03:41:31,442 - INFO - Epoch 112 training loss = 2.529
2023-04-27 03:41:35,442 - INFO - Epoch 113 training loss = 2.469
2023-04-27 03:41:39,443 - INFO - Epoch 114 training loss = 2.564
2023-04-27 03:41:43,451 - INFO - Epoch 115 training loss = 2.355
2023-04-27 03:41:47,456 - INFO - Epoch 116 training loss = 2.285
2023-04-27 03:41:51,458 - INFO - Epoch 117 training loss = 2.583
2023-04-27 03:41:55,459 - INFO - Epoch 118 training loss = 2.498
2023-04-27 03:41:59,461 - INFO - Epoch 119 training loss = 2.521
2023-04-27 03:42:03,464 - INFO - Epoch 120 training loss = 2.351
2023-04-27 03:42:03,778 - INFO - Validation loss = 2.298
2023-04-27 03:42:07,780 - INFO - Epoch 121 training loss = 2.438
2023-04-27 03:42:11,784 - INFO - Epoch 122 training loss = 2.436
2023-04-27 03:42:15,788 - INFO - Epoch 123 training loss = 2.165
2023-04-27 03:42:19,791 - INFO - Epoch 124 training loss = 2.325
2023-04-27 03:42:23,792 - INFO - Epoch 125 training loss = 2.457
2023-04-27 03:42:27,792 - INFO - Epoch 126 training loss = 2.144
2023-04-27 03:42:31,793 - INFO - Epoch 127 training loss = 2.211
2023-04-27 03:42:35,792 - INFO - Epoch 128 training loss = 2.052
2023-04-27 03:42:39,792 - INFO - Epoch 129 training loss = 2.201
2023-04-27 03:42:43,803 - INFO - Epoch 130 training loss = 2.027
2023-04-27 03:42:44,117 - INFO - Validation loss = 2.096
2023-04-27 03:42:48,133 - INFO - Epoch 131 training loss = 2.208
2023-04-27 03:42:52,147 - INFO - Epoch 132 training loss = 2.215
2023-04-27 03:42:56,160 - INFO - Epoch 133 training loss = 2.117
2023-04-27 03:43:00,174 - INFO - Epoch 134 training loss = 2.268
2023-04-27 03:43:04,189 - INFO - Epoch 135 training loss = 2.145
2023-04-27 03:43:08,202 - INFO - Epoch 136 training loss = 2.132
2023-04-27 03:43:12,218 - INFO - Epoch 137 training loss = 2.12
2023-04-27 03:43:16,228 - INFO - Epoch 138 training loss = 2.042
2023-04-27 03:43:20,229 - INFO - Epoch 139 training loss = 2.29
2023-04-27 03:43:24,229 - INFO - Epoch 140 training loss = 1.785
2023-04-27 03:43:24,542 - INFO - Validation loss = 3.045
2023-04-27 03:43:28,545 - INFO - Epoch 141 training loss = 1.79
2023-04-27 03:43:32,545 - INFO - Epoch 142 training loss = 2.028
2023-04-27 03:43:36,546 - INFO - Epoch 143 training loss = 1.816
2023-04-27 03:43:40,545 - INFO - Epoch 144 training loss = 2.072
2023-04-27 03:43:44,550 - INFO - Epoch 145 training loss = 1.859
2023-04-27 03:43:48,553 - INFO - Epoch 146 training loss = 2.061
2023-04-27 03:43:52,554 - INFO - Epoch 147 training loss = 1.932
2023-04-27 03:43:56,554 - INFO - Epoch 148 training loss = 2.099
2023-04-27 03:44:00,556 - INFO - Epoch 149 training loss = 1.772
2023-04-27 03:44:04,570 - INFO - Epoch 150 training loss = 1.909
2023-04-27 03:44:04,884 - INFO - Validation loss = 3.632
2023-04-27 03:44:08,899 - INFO - Epoch 151 training loss = 1.84
2023-04-27 03:44:12,907 - INFO - Epoch 152 training loss = 1.927
2023-04-27 03:44:16,912 - INFO - Epoch 153 training loss = 1.771
2023-04-27 03:44:20,914 - INFO - Epoch 154 training loss = 1.829
2023-04-27 03:44:24,927 - INFO - Epoch 155 training loss = 1.97
2023-04-27 03:44:28,941 - INFO - Epoch 156 training loss = 1.986
2023-04-27 03:44:32,954 - INFO - Epoch 157 training loss = 2.123
2023-04-27 03:44:36,967 - INFO - Epoch 158 training loss = 1.993
2023-04-27 03:44:40,981 - INFO - Epoch 159 training loss = 1.828
2023-04-27 03:44:44,999 - INFO - Epoch 160 training loss = 1.944
2023-04-27 03:44:45,312 - INFO - Validation loss = 1.204
2023-04-27 03:44:45,313 - INFO - best model
2023-04-27 03:44:49,350 - INFO - Epoch 161 training loss = 1.844
2023-04-27 03:44:53,364 - INFO - Epoch 162 training loss = 1.842
2023-04-27 03:44:57,378 - INFO - Epoch 163 training loss = 1.673
2023-04-27 03:45:01,392 - INFO - Epoch 164 training loss = 1.872
2023-04-27 03:45:05,406 - INFO - Epoch 165 training loss = 1.706
2023-04-27 03:45:09,421 - INFO - Epoch 166 training loss = 1.881
2023-04-27 03:45:13,439 - INFO - Epoch 167 training loss = 1.64
2023-04-27 03:45:17,454 - INFO - Epoch 168 training loss = 1.916
2023-04-27 03:45:21,468 - INFO - Epoch 169 training loss = 1.749
2023-04-27 03:45:25,482 - INFO - Epoch 170 training loss = 1.659
2023-04-27 03:45:25,795 - INFO - Validation loss = 1.942
2023-04-27 03:45:29,810 - INFO - Epoch 171 training loss = 1.857
2023-04-27 03:45:33,823 - INFO - Epoch 172 training loss = 1.687
2023-04-27 03:45:37,836 - INFO - Epoch 173 training loss = 1.797
2023-04-27 03:45:41,850 - INFO - Epoch 174 training loss = 1.673
2023-04-27 03:45:45,867 - INFO - Epoch 175 training loss = 1.696
2023-04-27 03:45:49,882 - INFO - Epoch 176 training loss = 1.921
2023-04-27 03:45:53,895 - INFO - Epoch 177 training loss = 1.687
2023-04-27 03:45:57,909 - INFO - Epoch 178 training loss = 1.518
2023-04-27 03:46:01,924 - INFO - Epoch 179 training loss = 1.757
2023-04-27 03:46:05,937 - INFO - Epoch 180 training loss = 1.717
2023-04-27 03:46:06,251 - INFO - Validation loss = 2.073
2023-04-27 03:46:10,266 - INFO - Epoch 181 training loss = 1.766
2023-04-27 03:46:14,284 - INFO - Epoch 182 training loss = 1.78
2023-04-27 03:46:18,291 - INFO - Epoch 183 training loss = 1.464
2023-04-27 03:46:22,292 - INFO - Epoch 184 training loss = 1.639
2023-04-27 03:46:26,294 - INFO - Epoch 185 training loss = 1.626
2023-04-27 03:46:30,297 - INFO - Epoch 186 training loss = 1.633
2023-04-27 03:46:34,298 - INFO - Epoch 187 training loss = 1.794
2023-04-27 03:46:38,300 - INFO - Epoch 188 training loss = 1.661
2023-04-27 03:46:42,303 - INFO - Epoch 189 training loss = 1.643
2023-04-27 03:46:46,308 - INFO - Epoch 190 training loss = 1.729
2023-04-27 03:46:46,622 - INFO - Validation loss = 1.484
2023-04-27 03:46:50,626 - INFO - Epoch 191 training loss = 1.693
2023-04-27 03:46:54,628 - INFO - Epoch 192 training loss = 1.534
2023-04-27 03:46:58,637 - INFO - Epoch 193 training loss = 1.675
2023-04-27 03:47:02,652 - INFO - Epoch 194 training loss = 1.432
2023-04-27 03:47:06,658 - INFO - Epoch 195 training loss = 1.695
2023-04-27 03:47:10,659 - INFO - Epoch 196 training loss = 1.443
2023-04-27 03:47:14,665 - INFO - Epoch 197 training loss = 1.431
2023-04-27 03:47:18,681 - INFO - Epoch 198 training loss = 1.471
2023-04-27 03:47:22,688 - INFO - Epoch 199 training loss = 1.649
2023-04-27 03:47:26,699 - INFO - Epoch 200 training loss = 1.549
2023-04-27 03:47:27,012 - INFO - Validation loss = 1.141
2023-04-27 03:47:27,012 - INFO - best model
2023-04-27 03:47:31,049 - INFO - Epoch 201 training loss = 1.454
2023-04-27 03:47:35,063 - INFO - Epoch 202 training loss = 1.449
2023-04-27 03:47:39,077 - INFO - Epoch 203 training loss = 1.458
2023-04-27 03:47:43,094 - INFO - Epoch 204 training loss = 1.591
2023-04-27 03:47:47,111 - INFO - Epoch 205 training loss = 1.495
2023-04-27 03:47:51,126 - INFO - Epoch 206 training loss = 1.517
2023-04-27 03:47:55,139 - INFO - Epoch 207 training loss = 1.573
2023-04-27 03:47:59,154 - INFO - Epoch 208 training loss = 1.503
2023-04-27 03:48:03,171 - INFO - Epoch 209 training loss = 1.536
2023-04-27 03:48:07,185 - INFO - Epoch 210 training loss = 1.31
2023-04-27 03:48:07,499 - INFO - Validation loss = 1.178
2023-04-27 03:48:11,514 - INFO - Epoch 211 training loss = 1.484
2023-04-27 03:48:15,533 - INFO - Epoch 212 training loss = 1.627
2023-04-27 03:48:19,549 - INFO - Epoch 213 training loss = 1.39
2023-04-27 03:48:23,563 - INFO - Epoch 214 training loss = 1.324
2023-04-27 03:48:27,578 - INFO - Epoch 215 training loss = 1.384
2023-04-27 03:48:31,591 - INFO - Epoch 216 training loss = 1.403
2023-04-27 03:48:35,605 - INFO - Epoch 217 training loss = 1.308
2023-04-27 03:48:39,620 - INFO - Epoch 218 training loss = 1.474
2023-04-27 03:48:43,638 - INFO - Epoch 219 training loss = 1.31
2023-04-27 03:48:47,655 - INFO - Epoch 220 training loss = 1.462
2023-04-27 03:48:47,969 - INFO - Validation loss = 1.261
2023-04-27 03:48:51,985 - INFO - Epoch 221 training loss = 1.295
2023-04-27 03:48:56,000 - INFO - Epoch 222 training loss = 1.551
2023-04-27 03:49:00,015 - INFO - Epoch 223 training loss = 1.385
2023-04-27 03:49:04,031 - INFO - Epoch 224 training loss = 1.354
2023-04-27 03:49:08,046 - INFO - Epoch 225 training loss = 1.324
2023-04-27 03:49:12,063 - INFO - Epoch 226 training loss = 1.382
2023-04-27 03:49:16,079 - INFO - Epoch 227 training loss = 1.389
2023-04-27 03:49:20,093 - INFO - Epoch 228 training loss = 1.351
2023-04-27 03:49:24,096 - INFO - Epoch 229 training loss = 1.36
2023-04-27 03:49:28,098 - INFO - Epoch 230 training loss = 1.344
2023-04-27 03:49:28,410 - INFO - Validation loss = 1.753
2023-04-27 03:49:32,438 - INFO - Epoch 231 training loss = 1.46
2023-04-27 03:49:36,465 - INFO - Epoch 232 training loss = 1.249
2023-04-27 03:49:40,492 - INFO - Epoch 233 training loss = 1.34
2023-04-27 03:49:44,522 - INFO - Epoch 234 training loss = 1.453
2023-04-27 03:49:48,551 - INFO - Epoch 235 training loss = 1.216
2023-04-27 03:49:52,578 - INFO - Epoch 236 training loss = 1.318
2023-04-27 03:49:56,605 - INFO - Epoch 237 training loss = 1.339
2023-04-27 03:50:00,632 - INFO - Epoch 238 training loss = 1.432
2023-04-27 03:50:04,662 - INFO - Epoch 239 training loss = 1.416
2023-04-27 03:50:08,688 - INFO - Epoch 240 training loss = 1.354
2023-04-27 03:50:09,003 - INFO - Validation loss = 1.461
2023-04-27 03:50:13,033 - INFO - Epoch 241 training loss = 1.266
2023-04-27 03:50:17,062 - INFO - Epoch 242 training loss = 1.356
2023-04-27 03:50:21,089 - INFO - Epoch 243 training loss = 1.329
2023-04-27 03:50:25,116 - INFO - Epoch 244 training loss = 1.361
2023-04-27 03:50:29,142 - INFO - Epoch 245 training loss = 1.277
2023-04-27 03:50:33,168 - INFO - Epoch 246 training loss = 1.26
2023-04-27 03:50:37,196 - INFO - Epoch 247 training loss = 1.175
2023-04-27 03:50:41,224 - INFO - Epoch 248 training loss = 1.39
2023-04-27 03:50:45,241 - INFO - Epoch 249 training loss = 1.19
2023-04-27 03:50:49,256 - INFO - Epoch 250 training loss = 1.253
2023-04-27 03:50:49,569 - INFO - Validation loss = 1.04
2023-04-27 03:50:49,570 - INFO - best model
2023-04-27 03:50:53,602 - INFO - Epoch 251 training loss = 1.232
2023-04-27 03:50:57,616 - INFO - Epoch 252 training loss = 1.295
2023-04-27 03:51:01,630 - INFO - Epoch 253 training loss = 1.194
2023-04-27 03:51:05,642 - INFO - Epoch 254 training loss = 1.266
2023-04-27 03:51:09,655 - INFO - Epoch 255 training loss = 1.283
2023-04-27 03:51:13,671 - INFO - Epoch 256 training loss = 1.374
2023-04-27 03:51:17,686 - INFO - Epoch 257 training loss = 1.309
2023-04-27 03:51:21,699 - INFO - Epoch 258 training loss = 1.129
2023-04-27 03:51:25,711 - INFO - Epoch 259 training loss = 1.226
2023-04-27 03:51:29,725 - INFO - Epoch 260 training loss = 1.295
2023-04-27 03:51:30,038 - INFO - Validation loss = 1.244
2023-04-27 03:51:34,051 - INFO - Epoch 261 training loss = 1.191
2023-04-27 03:51:38,065 - INFO - Epoch 262 training loss = 1.152
2023-04-27 03:51:42,079 - INFO - Epoch 263 training loss = 1.233
2023-04-27 03:51:46,096 - INFO - Epoch 264 training loss = 1.192
2023-04-27 03:51:50,111 - INFO - Epoch 265 training loss = 1.193
2023-04-27 03:51:54,125 - INFO - Epoch 266 training loss = 1.236
2023-04-27 03:51:58,139 - INFO - Epoch 267 training loss = 1.281
2023-04-27 03:52:02,153 - INFO - Epoch 268 training loss = 1.135
2023-04-27 03:52:06,167 - INFO - Epoch 269 training loss = 1.338
2023-04-27 03:52:10,182 - INFO - Epoch 270 training loss = 1.289
2023-04-27 03:52:10,495 - INFO - Validation loss = 1.718
2023-04-27 03:52:14,524 - INFO - Epoch 271 training loss = 1.314
2023-04-27 03:52:18,555 - INFO - Epoch 272 training loss = 1.255
2023-04-27 03:52:22,581 - INFO - Epoch 273 training loss = 1.134
2023-04-27 03:52:26,610 - INFO - Epoch 274 training loss = 1.24
2023-04-27 03:52:30,635 - INFO - Epoch 275 training loss = 1.13
2023-04-27 03:52:34,663 - INFO - Epoch 276 training loss = 1.205
2023-04-27 03:52:38,690 - INFO - Epoch 277 training loss = 1.226
2023-04-27 03:52:42,718 - INFO - Epoch 278 training loss = 1.149
2023-04-27 03:52:46,749 - INFO - Epoch 279 training loss = 1.179
2023-04-27 03:52:50,777 - INFO - Epoch 280 training loss = 1.061
2023-04-27 03:52:51,091 - INFO - Validation loss = 1.761
2023-04-27 03:52:55,118 - INFO - Epoch 281 training loss = 1.068
2023-04-27 03:52:59,146 - INFO - Epoch 282 training loss = 1.007
2023-04-27 03:53:03,175 - INFO - Epoch 283 training loss = 1.182
2023-04-27 03:53:07,202 - INFO - Epoch 284 training loss = 1.034
2023-04-27 03:53:11,231 - INFO - Epoch 285 training loss = 1.24
2023-04-27 03:53:15,261 - INFO - Epoch 286 training loss = 1.17
2023-04-27 03:53:19,290 - INFO - Epoch 287 training loss = 1.188
2023-04-27 03:53:23,317 - INFO - Epoch 288 training loss = 1.028
2023-04-27 03:53:27,344 - INFO - Epoch 289 training loss = 0.9857
2023-04-27 03:53:31,363 - INFO - Epoch 290 training loss = 1.297
2023-04-27 03:53:31,676 - INFO - Validation loss = 1.361
2023-04-27 03:53:35,679 - INFO - Epoch 291 training loss = 1.149
2023-04-27 03:53:39,680 - INFO - Epoch 292 training loss = 1.04
2023-04-27 03:53:43,684 - INFO - Epoch 293 training loss = 1.06
2023-04-27 03:53:47,689 - INFO - Epoch 294 training loss = 1.083
2023-04-27 03:53:51,690 - INFO - Epoch 295 training loss = 1.272
2023-04-27 03:53:55,690 - INFO - Epoch 296 training loss = 1.09
2023-04-27 03:53:59,691 - INFO - Epoch 297 training loss = 0.9825
2023-04-27 03:54:03,693 - INFO - Epoch 298 training loss = 1.14
2023-04-27 03:54:07,712 - INFO - Epoch 299 training loss = 1.111
2023-04-27 03:54:11,740 - INFO - Epoch 300 training loss = 1.033
2023-04-27 03:54:12,054 - INFO - Validation loss = 1.052
2023-04-27 03:54:16,067 - INFO - Epoch 301 training loss = 0.9629
2023-04-27 03:54:20,069 - INFO - Epoch 302 training loss = 1.024
2023-04-27 03:54:24,071 - INFO - Epoch 303 training loss = 1.168
2023-04-27 03:54:28,072 - INFO - Epoch 304 training loss = 1.019
2023-04-27 03:54:32,073 - INFO - Epoch 305 training loss = 0.9559
2023-04-27 03:54:36,074 - INFO - Epoch 306 training loss = 1.129
2023-04-27 03:54:40,077 - INFO - Epoch 307 training loss = 1.136
2023-04-27 03:54:44,081 - INFO - Epoch 308 training loss = 0.9934
2023-04-27 03:54:48,084 - INFO - Epoch 309 training loss = 0.9875
2023-04-27 03:54:52,085 - INFO - Epoch 310 training loss = 1.037
2023-04-27 03:54:52,398 - INFO - Validation loss = 2.915
2023-04-27 03:54:56,399 - INFO - Epoch 311 training loss = 1.019
2023-04-27 03:55:00,401 - INFO - Epoch 312 training loss = 1.013
2023-04-27 03:55:04,403 - INFO - Epoch 313 training loss = 0.9825
2023-04-27 03:55:08,404 - INFO - Epoch 314 training loss = 0.9877
2023-04-27 03:55:12,408 - INFO - Epoch 315 training loss = 1.062
2023-04-27 03:55:16,413 - INFO - Epoch 316 training loss = 1.01
2023-04-27 03:55:20,437 - INFO - Epoch 317 training loss = 1.153
2023-04-27 03:55:24,464 - INFO - Epoch 318 training loss = 1.027
2023-04-27 03:55:28,490 - INFO - Epoch 319 training loss = 1.084
2023-04-27 03:55:32,516 - INFO - Epoch 320 training loss = 1.135
2023-04-27 03:55:32,829 - INFO - Validation loss = 0.7794
2023-04-27 03:55:32,829 - INFO - best model
2023-04-27 03:55:36,874 - INFO - Epoch 321 training loss = 0.9909
2023-04-27 03:55:40,901 - INFO - Epoch 322 training loss = 1.008
2023-04-27 03:55:44,931 - INFO - Epoch 323 training loss = 0.9939
2023-04-27 03:55:48,960 - INFO - Epoch 324 training loss = 0.973
2023-04-27 03:55:52,987 - INFO - Epoch 325 training loss = 1.001
2023-04-27 03:55:57,014 - INFO - Epoch 326 training loss = 0.9401
2023-04-27 03:56:01,041 - INFO - Epoch 327 training loss = 0.969
2023-04-27 03:56:05,068 - INFO - Epoch 328 training loss = 1.065
2023-04-27 03:56:09,094 - INFO - Epoch 329 training loss = 0.9641
2023-04-27 03:56:13,123 - INFO - Epoch 330 training loss = 0.9795
2023-04-27 03:56:13,437 - INFO - Validation loss = 1.197
2023-04-27 03:56:17,469 - INFO - Epoch 331 training loss = 0.8793
2023-04-27 03:56:21,497 - INFO - Epoch 332 training loss = 0.9696
2023-04-27 03:56:25,524 - INFO - Epoch 333 training loss = 0.931
2023-04-27 03:56:29,552 - INFO - Epoch 334 training loss = 0.9158
2023-04-27 03:56:33,578 - INFO - Epoch 335 training loss = 1.077
2023-04-27 03:56:37,604 - INFO - Epoch 336 training loss = 0.9331
2023-04-27 03:56:41,613 - INFO - Epoch 337 training loss = 1.027
2023-04-27 03:56:45,617 - INFO - Epoch 338 training loss = 0.9736
2023-04-27 03:56:49,619 - INFO - Epoch 339 training loss = 1.001
2023-04-27 03:56:53,620 - INFO - Epoch 340 training loss = 0.9285
2023-04-27 03:56:53,933 - INFO - Validation loss = 0.7706
2023-04-27 03:56:53,934 - INFO - best model
2023-04-27 03:56:57,958 - INFO - Epoch 341 training loss = 0.8994
2023-04-27 03:57:01,959 - INFO - Epoch 342 training loss = 0.8947
2023-04-27 03:57:05,960 - INFO - Epoch 343 training loss = 0.9558
2023-04-27 03:57:09,962 - INFO - Epoch 344 training loss = 0.9704
2023-04-27 03:57:13,968 - INFO - Epoch 345 training loss = 0.8547
2023-04-27 03:57:17,971 - INFO - Epoch 346 training loss = 0.9832
2023-04-27 03:57:21,972 - INFO - Epoch 347 training loss = 0.9279
2023-04-27 03:57:25,973 - INFO - Epoch 348 training loss = 0.9344
2023-04-27 03:57:29,979 - INFO - Epoch 349 training loss = 0.9177
2023-04-27 03:57:33,992 - INFO - Epoch 350 training loss = 1.031
2023-04-27 03:57:34,306 - INFO - Validation loss = 0.8911
2023-04-27 03:57:38,320 - INFO - Epoch 351 training loss = 0.9219
2023-04-27 03:57:42,335 - INFO - Epoch 352 training loss = 0.8799
2023-04-27 03:57:46,353 - INFO - Epoch 353 training loss = 0.9014
2023-04-27 03:57:50,368 - INFO - Epoch 354 training loss = 0.9817
2023-04-27 03:57:54,381 - INFO - Epoch 355 training loss = 0.9316
2023-04-27 03:57:58,394 - INFO - Epoch 356 training loss = 0.9018
2023-04-27 03:58:02,408 - INFO - Epoch 357 training loss = 0.9292
2023-04-27 03:58:06,422 - INFO - Epoch 358 training loss = 0.8525
2023-04-27 03:58:10,436 - INFO - Epoch 359 training loss = 0.9437
2023-04-27 03:58:14,453 - INFO - Epoch 360 training loss = 0.8861
2023-04-27 03:58:14,766 - INFO - Validation loss = 0.6455
2023-04-27 03:58:14,767 - INFO - best model
2023-04-27 03:58:18,803 - INFO - Epoch 361 training loss = 0.9628
2023-04-27 03:58:22,815 - INFO - Epoch 362 training loss = 0.8785
2023-04-27 03:58:26,829 - INFO - Epoch 363 training loss = 0.9068
2023-04-27 03:58:30,841 - INFO - Epoch 364 training loss = 0.8505
2023-04-27 03:58:34,853 - INFO - Epoch 365 training loss = 0.8228
2023-04-27 03:58:38,866 - INFO - Epoch 366 training loss = 0.7842
2023-04-27 03:58:42,881 - INFO - Epoch 367 training loss = 0.9101
2023-04-27 03:58:46,898 - INFO - Epoch 368 training loss = 0.8575
2023-04-27 03:58:50,913 - INFO - Epoch 369 training loss = 0.8561
2023-04-27 03:58:54,926 - INFO - Epoch 370 training loss = 0.8474
2023-04-27 03:58:55,239 - INFO - Validation loss = 1.191
2023-04-27 03:58:59,252 - INFO - Epoch 371 training loss = 0.8853
2023-04-27 03:59:03,265 - INFO - Epoch 372 training loss = 0.8471
2023-04-27 03:59:07,278 - INFO - Epoch 373 training loss = 0.7931
2023-04-27 03:59:11,293 - INFO - Epoch 374 training loss = 0.8334
2023-04-27 03:59:15,308 - INFO - Epoch 375 training loss = 0.8221
2023-04-27 03:59:19,321 - INFO - Epoch 376 training loss = 0.7546
2023-04-27 03:59:23,334 - INFO - Epoch 377 training loss = 0.8168
2023-04-27 03:59:27,346 - INFO - Epoch 378 training loss = 0.8317
2023-04-27 03:59:31,358 - INFO - Epoch 379 training loss = 0.673
2023-04-27 03:59:35,369 - INFO - Epoch 380 training loss = 0.798
2023-04-27 03:59:35,683 - INFO - Validation loss = 1.417
2023-04-27 03:59:39,697 - INFO - Epoch 381 training loss = 0.8934
2023-04-27 03:59:43,712 - INFO - Epoch 382 training loss = 0.8388
2023-04-27 03:59:47,728 - INFO - Epoch 383 training loss = 0.7743
2023-04-27 03:59:51,742 - INFO - Epoch 384 training loss = 0.7238
2023-04-27 03:59:55,754 - INFO - Epoch 385 training loss = 0.7613
2023-04-27 03:59:59,767 - INFO - Epoch 386 training loss = 0.787
2023-04-27 04:00:03,791 - INFO - Epoch 387 training loss = 0.8488
2023-04-27 04:00:07,819 - INFO - Epoch 388 training loss = 0.7193
2023-04-27 04:00:11,849 - INFO - Epoch 389 training loss = 0.8333
2023-04-27 04:00:15,879 - INFO - Epoch 390 training loss = 0.8587
2023-04-27 04:00:16,194 - INFO - Validation loss = 0.9847
2023-04-27 04:00:20,222 - INFO - Epoch 391 training loss = 0.7845
2023-04-27 04:00:24,249 - INFO - Epoch 392 training loss = 0.7288
2023-04-27 04:00:28,277 - INFO - Epoch 393 training loss = 0.8034
2023-04-27 04:00:32,304 - INFO - Epoch 394 training loss = 0.7577
2023-04-27 04:00:36,331 - INFO - Epoch 395 training loss = 0.776
2023-04-27 04:00:40,358 - INFO - Epoch 396 training loss = 0.8063
2023-04-27 04:00:44,388 - INFO - Epoch 397 training loss = 0.7367
2023-04-27 04:00:48,418 - INFO - Epoch 398 training loss = 0.7468
2023-04-27 04:00:52,445 - INFO - Epoch 399 training loss = 0.7722
2023-04-27 04:00:56,473 - INFO - Epoch 400 training loss = 0.7393
2023-04-27 04:00:56,787 - INFO - Validation loss = 0.5508
2023-04-27 04:00:56,787 - INFO - best model
2023-04-27 04:01:00,837 - INFO - Epoch 401 training loss = 0.7101
2023-04-27 04:01:04,865 - INFO - Epoch 402 training loss = 0.7174
2023-04-27 04:01:08,893 - INFO - Epoch 403 training loss = 0.7572
2023-04-27 04:01:12,923 - INFO - Epoch 404 training loss = 0.7178
2023-04-27 04:01:16,954 - INFO - Epoch 405 training loss = 0.7923
2023-04-27 04:01:20,969 - INFO - Epoch 406 training loss = 0.7053
2023-04-27 04:01:24,982 - INFO - Epoch 407 training loss = 0.7562
2023-04-27 04:01:28,994 - INFO - Epoch 408 training loss = 0.6287
2023-04-27 04:01:33,006 - INFO - Epoch 409 training loss = 0.6935
2023-04-27 04:01:37,018 - INFO - Epoch 410 training loss = 0.6576
2023-04-27 04:01:37,332 - INFO - Validation loss = 0.4861
2023-04-27 04:01:37,332 - INFO - best model
2023-04-27 04:01:41,375 - INFO - Epoch 411 training loss = 0.7663
2023-04-27 04:01:45,391 - INFO - Epoch 412 training loss = 0.696
2023-04-27 04:01:49,407 - INFO - Epoch 413 training loss = 0.761
2023-04-27 04:01:53,420 - INFO - Epoch 414 training loss = 0.737
2023-04-27 04:01:57,435 - INFO - Epoch 415 training loss = 0.7836
2023-04-27 04:02:01,448 - INFO - Epoch 416 training loss = 0.6382
2023-04-27 04:02:05,462 - INFO - Epoch 417 training loss = 0.7128
2023-04-27 04:02:09,475 - INFO - Epoch 418 training loss = 0.7162
2023-04-27 04:02:13,491 - INFO - Epoch 419 training loss = 0.7609
2023-04-27 04:02:17,507 - INFO - Epoch 420 training loss = 0.659
2023-04-27 04:02:17,821 - INFO - Validation loss = 0.8323
2023-04-27 04:02:21,834 - INFO - Epoch 421 training loss = 0.6963
2023-04-27 04:02:25,847 - INFO - Epoch 422 training loss = 0.6883
2023-04-27 04:02:29,860 - INFO - Epoch 423 training loss = 0.7184
2023-04-27 04:02:33,873 - INFO - Epoch 424 training loss = 0.7048
2023-04-27 04:02:37,886 - INFO - Epoch 425 training loss = 0.6511
2023-04-27 04:02:41,901 - INFO - Epoch 426 training loss = 0.7002
2023-04-27 04:02:45,917 - INFO - Epoch 427 training loss = 0.698
2023-04-27 04:02:49,932 - INFO - Epoch 428 training loss = 0.6643
2023-04-27 04:02:53,946 - INFO - Epoch 429 training loss = 0.7769
2023-04-27 04:02:57,959 - INFO - Epoch 430 training loss = 0.6402
2023-04-27 04:02:58,272 - INFO - Validation loss = 0.4361
2023-04-27 04:02:58,273 - INFO - best model
2023-04-27 04:03:02,329 - INFO - Epoch 431 training loss = 0.7372
2023-04-27 04:03:06,342 - INFO - Epoch 432 training loss = 0.6558
2023-04-27 04:03:10,356 - INFO - Epoch 433 training loss = 0.6608
2023-04-27 04:03:14,372 - INFO - Epoch 434 training loss = 0.6494
2023-04-27 04:03:18,388 - INFO - Epoch 435 training loss = 0.6741
2023-04-27 04:03:22,400 - INFO - Epoch 436 training loss = 0.6357
2023-04-27 04:03:26,413 - INFO - Epoch 437 training loss = 0.7209
2023-04-27 04:03:30,425 - INFO - Epoch 438 training loss = 0.6804
2023-04-27 04:03:34,438 - INFO - Epoch 439 training loss = 0.624
2023-04-27 04:03:38,450 - INFO - Epoch 440 training loss = 0.6052
2023-04-27 04:03:38,764 - INFO - Validation loss = 0.7114
2023-04-27 04:03:42,779 - INFO - Epoch 441 training loss = 0.6333
2023-04-27 04:03:46,795 - INFO - Epoch 442 training loss = 0.562
2023-04-27 04:03:50,809 - INFO - Epoch 443 training loss = 0.6833
2023-04-27 04:03:54,822 - INFO - Epoch 444 training loss = 0.6799
2023-04-27 04:03:58,835 - INFO - Epoch 445 training loss = 0.6593
2023-04-27 04:04:02,849 - INFO - Epoch 446 training loss = 0.562
2023-04-27 04:04:06,863 - INFO - Epoch 447 training loss = 0.6263
2023-04-27 04:04:10,876 - INFO - Epoch 448 training loss = 0.6029
2023-04-27 04:04:14,893 - INFO - Epoch 449 training loss = 0.6829
2023-04-27 04:04:18,908 - INFO - Epoch 450 training loss = 0.6578
2023-04-27 04:04:19,220 - INFO - Validation loss = 0.8285
2023-04-27 04:04:23,234 - INFO - Epoch 451 training loss = 0.6672
2023-04-27 04:04:27,246 - INFO - Epoch 452 training loss = 0.6233
2023-04-27 04:04:31,258 - INFO - Epoch 453 training loss = 0.6821
2023-04-27 04:04:35,271 - INFO - Epoch 454 training loss = 0.5994
2023-04-27 04:04:39,283 - INFO - Epoch 455 training loss = 0.563
2023-04-27 04:04:43,298 - INFO - Epoch 456 training loss = 0.5789
2023-04-27 04:04:47,314 - INFO - Epoch 457 training loss = 0.5796
2023-04-27 04:04:51,328 - INFO - Epoch 458 training loss = 0.5739
2023-04-27 04:04:55,340 - INFO - Epoch 459 training loss = 0.617
2023-04-27 04:04:59,354 - INFO - Epoch 460 training loss = 0.5592
2023-04-27 04:04:59,667 - INFO - Validation loss = 0.4556
2023-04-27 04:05:03,681 - INFO - Epoch 461 training loss = 0.5931
2023-04-27 04:05:07,696 - INFO - Epoch 462 training loss = 0.5787
2023-04-27 04:05:11,711 - INFO - Epoch 463 training loss = 0.5551
2023-04-27 04:05:15,728 - INFO - Epoch 464 training loss = 0.6038
2023-04-27 04:05:19,743 - INFO - Epoch 465 training loss = 0.5776
2023-04-27 04:05:23,756 - INFO - Epoch 466 training loss = 0.5449
2023-04-27 04:05:27,769 - INFO - Epoch 467 training loss = 0.6182
2023-04-27 04:05:31,782 - INFO - Epoch 468 training loss = 0.5736
2023-04-27 04:05:35,795 - INFO - Epoch 469 training loss = 0.538
2023-04-27 04:05:39,807 - INFO - Epoch 470 training loss = 0.5645
2023-04-27 04:05:40,120 - INFO - Validation loss = 0.571
2023-04-27 04:05:44,137 - INFO - Epoch 471 training loss = 0.5557
2023-04-27 04:05:48,153 - INFO - Epoch 472 training loss = 0.5605
2023-04-27 04:05:52,166 - INFO - Epoch 473 training loss = 0.547
2023-04-27 04:05:56,179 - INFO - Epoch 474 training loss = 0.5516
2023-04-27 04:06:00,193 - INFO - Epoch 475 training loss = 0.5279
2023-04-27 04:06:04,208 - INFO - Epoch 476 training loss = 0.5339
2023-04-27 04:06:08,221 - INFO - Epoch 477 training loss = 0.5573
2023-04-27 04:06:12,238 - INFO - Epoch 478 training loss = 0.548
2023-04-27 04:06:16,254 - INFO - Epoch 479 training loss = 0.5846
2023-04-27 04:06:20,268 - INFO - Epoch 480 training loss = 0.5381
2023-04-27 04:06:20,581 - INFO - Validation loss = 0.6443
2023-04-27 04:06:24,595 - INFO - Epoch 481 training loss = 0.5559
2023-04-27 04:06:28,608 - INFO - Epoch 482 training loss = 0.5246
2023-04-27 04:06:32,621 - INFO - Epoch 483 training loss = 0.5214
2023-04-27 04:06:36,635 - INFO - Epoch 484 training loss = 0.528
2023-04-27 04:06:40,649 - INFO - Epoch 485 training loss = 0.5585
2023-04-27 04:06:44,665 - INFO - Epoch 486 training loss = 0.5058
2023-04-27 04:06:48,681 - INFO - Epoch 487 training loss = 0.6019
2023-04-27 04:06:52,693 - INFO - Epoch 488 training loss = 0.533
2023-04-27 04:06:56,706 - INFO - Epoch 489 training loss = 0.5148
2023-04-27 04:07:00,719 - INFO - Epoch 490 training loss = 0.5465
2023-04-27 04:07:01,033 - INFO - Validation loss = 0.5253
2023-04-27 04:07:05,047 - INFO - Epoch 491 training loss = 0.5261
2023-04-27 04:07:09,060 - INFO - Epoch 492 training loss = 0.6362
2023-04-27 04:07:13,073 - INFO - Epoch 493 training loss = 0.5412
2023-04-27 04:07:17,078 - INFO - Epoch 494 training loss = 0.4877
2023-04-27 04:07:21,080 - INFO - Epoch 495 training loss = 0.5224
2023-04-27 04:07:25,081 - INFO - Epoch 496 training loss = 0.4822
2023-04-27 04:07:29,088 - INFO - Epoch 497 training loss = 0.5428
2023-04-27 04:07:33,100 - INFO - Epoch 498 training loss = 0.4957
2023-04-27 04:07:37,113 - INFO - Epoch 499 training loss = 0.5237
2023-04-27 04:07:41,126 - INFO - Epoch 500 training loss = 0.4764
2023-04-27 04:07:41,440 - INFO - Validation loss = 0.4289
2023-04-27 04:07:41,441 - INFO - best model
2023-04-27 04:07:45,476 - INFO - Epoch 501 training loss = 0.4891
2023-04-27 04:07:49,490 - INFO - Epoch 502 training loss = 0.4472
2023-04-27 04:07:53,502 - INFO - Epoch 503 training loss = 0.5074
2023-04-27 04:07:57,515 - INFO - Epoch 504 training loss = 0.5508
2023-04-27 04:08:01,529 - INFO - Epoch 505 training loss = 0.4701
2023-04-27 04:08:05,542 - INFO - Epoch 506 training loss = 0.5284
2023-04-27 04:08:09,554 - INFO - Epoch 507 training loss = 0.5589
2023-04-27 04:08:13,569 - INFO - Epoch 508 training loss = 0.4594
2023-04-27 04:08:17,585 - INFO - Epoch 509 training loss = 0.486
2023-04-27 04:08:21,598 - INFO - Epoch 510 training loss = 0.4541
2023-04-27 04:08:21,911 - INFO - Validation loss = 0.6645
2023-04-27 04:08:25,923 - INFO - Epoch 511 training loss = 0.4016
2023-04-27 04:08:29,935 - INFO - Epoch 512 training loss = 0.443
2023-04-27 04:08:33,946 - INFO - Epoch 513 training loss = 0.5136
2023-04-27 04:08:37,958 - INFO - Epoch 514 training loss = 0.5415
2023-04-27 04:08:41,971 - INFO - Epoch 515 training loss = 0.4585
2023-04-27 04:08:45,987 - INFO - Epoch 516 training loss = 0.496
2023-04-27 04:08:50,000 - INFO - Epoch 517 training loss = 0.511
2023-04-27 04:08:54,011 - INFO - Epoch 518 training loss = 0.5078
2023-04-27 04:08:58,024 - INFO - Epoch 519 training loss = 0.4535
2023-04-27 04:09:02,037 - INFO - Epoch 520 training loss = 0.4284
2023-04-27 04:09:02,350 - INFO - Validation loss = 0.6967
2023-04-27 04:09:06,364 - INFO - Epoch 521 training loss = 0.4286
2023-04-27 04:09:10,377 - INFO - Epoch 522 training loss = 0.4025
2023-04-27 04:09:14,391 - INFO - Epoch 523 training loss = 0.4298
2023-04-27 04:09:18,407 - INFO - Epoch 524 training loss = 0.4145
2023-04-27 04:09:22,419 - INFO - Epoch 525 training loss = 0.437
2023-04-27 04:09:26,432 - INFO - Epoch 526 training loss = 0.4053
2023-04-27 04:09:30,444 - INFO - Epoch 527 training loss = 0.5227
2023-04-27 04:09:34,456 - INFO - Epoch 528 training loss = 0.4709
2023-04-27 04:09:38,470 - INFO - Epoch 529 training loss = 0.3801
2023-04-27 04:09:42,484 - INFO - Epoch 530 training loss = 0.4815
2023-04-27 04:09:42,798 - INFO - Validation loss = 0.4338
2023-04-27 04:09:46,815 - INFO - Epoch 531 training loss = 0.409
2023-04-27 04:09:50,827 - INFO - Epoch 532 training loss = 0.3712
2023-04-27 04:09:54,840 - INFO - Epoch 533 training loss = 0.3697
2023-04-27 04:09:58,854 - INFO - Epoch 534 training loss = 0.3652
2023-04-27 04:10:02,870 - INFO - Epoch 535 training loss = 0.4441
2023-04-27 04:10:06,883 - INFO - Epoch 536 training loss = 0.3863
2023-04-27 04:10:10,896 - INFO - Epoch 537 training loss = 0.4557
2023-04-27 04:10:14,911 - INFO - Epoch 538 training loss = 0.4509
2023-04-27 04:10:18,927 - INFO - Epoch 539 training loss = 0.3831
2023-04-27 04:10:22,940 - INFO - Epoch 540 training loss = 0.4487
2023-04-27 04:10:23,253 - INFO - Validation loss = 0.4763
2023-04-27 04:10:27,267 - INFO - Epoch 541 training loss = 0.4314
2023-04-27 04:10:31,279 - INFO - Epoch 542 training loss = 0.3834
2023-04-27 04:10:35,291 - INFO - Epoch 543 training loss = 0.4226
2023-04-27 04:10:39,303 - INFO - Epoch 544 training loss = 0.3622
2023-04-27 04:10:43,318 - INFO - Epoch 545 training loss = 0.3772
2023-04-27 04:10:47,334 - INFO - Epoch 546 training loss = 0.3929
2023-04-27 04:10:51,348 - INFO - Epoch 547 training loss = 0.3412
2023-04-27 04:10:55,360 - INFO - Epoch 548 training loss = 0.3639
2023-04-27 04:10:59,374 - INFO - Epoch 549 training loss = 0.3419
2023-04-27 04:11:03,387 - INFO - Epoch 550 training loss = 0.397
2023-04-27 04:11:03,701 - INFO - Validation loss = 0.414
2023-04-27 04:11:03,701 - INFO - best model
2023-04-27 04:11:07,736 - INFO - Epoch 551 training loss = 0.3611
2023-04-27 04:11:11,749 - INFO - Epoch 552 training loss = 0.3936
2023-04-27 04:11:15,766 - INFO - Epoch 553 training loss = 0.3938
2023-04-27 04:11:19,779 - INFO - Epoch 554 training loss = 0.39
2023-04-27 04:11:23,792 - INFO - Epoch 555 training loss = 0.3291
2023-04-27 04:11:27,805 - INFO - Epoch 556 training loss = 0.3711
2023-04-27 04:11:31,817 - INFO - Epoch 557 training loss = 0.3526
2023-04-27 04:11:35,831 - INFO - Epoch 558 training loss = 0.3399
2023-04-27 04:11:39,843 - INFO - Epoch 559 training loss = 0.3618
2023-04-27 04:11:43,859 - INFO - Epoch 560 training loss = 0.3462
2023-04-27 04:11:44,173 - INFO - Validation loss = 0.3361
2023-04-27 04:11:44,173 - INFO - best model
2023-04-27 04:11:48,211 - INFO - Epoch 561 training loss = 0.3667
2023-04-27 04:11:52,225 - INFO - Epoch 562 training loss = 0.3365
2023-04-27 04:11:56,239 - INFO - Epoch 563 training loss = 0.3378
2023-04-27 04:12:00,253 - INFO - Epoch 564 training loss = 0.3602
2023-04-27 04:12:04,267 - INFO - Epoch 565 training loss = 0.387
2023-04-27 04:12:08,280 - INFO - Epoch 566 training loss = 0.3472
2023-04-27 04:12:12,295 - INFO - Epoch 567 training loss = 0.361
2023-04-27 04:12:16,311 - INFO - Epoch 568 training loss = 0.3291
2023-04-27 04:12:20,325 - INFO - Epoch 569 training loss = 0.3267
2023-04-27 04:12:24,337 - INFO - Epoch 570 training loss = 0.3671
2023-04-27 04:12:24,651 - INFO - Validation loss = 0.352
2023-04-27 04:12:28,665 - INFO - Epoch 571 training loss = 0.3266
2023-04-27 04:12:32,678 - INFO - Epoch 572 training loss = 0.3709
2023-04-27 04:12:36,692 - INFO - Epoch 573 training loss = 0.3694
2023-04-27 04:12:40,705 - INFO - Epoch 574 training loss = 0.3295
2023-04-27 04:12:44,721 - INFO - Epoch 575 training loss = 0.3146
2023-04-27 04:12:48,738 - INFO - Epoch 576 training loss = 0.3367
2023-04-27 04:12:52,752 - INFO - Epoch 577 training loss = 0.3212
2023-04-27 04:12:56,766 - INFO - Epoch 578 training loss = 0.2784
2023-04-27 04:13:00,780 - INFO - Epoch 579 training loss = 0.3144
2023-04-27 04:13:04,794 - INFO - Epoch 580 training loss = 0.2932
2023-04-27 04:13:05,108 - INFO - Validation loss = 0.3381
2023-04-27 04:13:09,121 - INFO - Epoch 581 training loss = 0.3212
2023-04-27 04:13:13,137 - INFO - Epoch 582 training loss = 0.2939
2023-04-27 04:13:17,154 - INFO - Epoch 583 training loss = 0.2884
2023-04-27 04:13:21,169 - INFO - Epoch 584 training loss = 0.3174
2023-04-27 04:13:25,182 - INFO - Epoch 585 training loss = 0.3263
2023-04-27 04:13:29,196 - INFO - Epoch 586 training loss = 0.2795
2023-04-27 04:13:33,209 - INFO - Epoch 587 training loss = 0.2963
2023-04-27 04:13:37,223 - INFO - Epoch 588 training loss = 0.2874
2023-04-27 04:13:41,238 - INFO - Epoch 589 training loss = 0.252
2023-04-27 04:13:45,253 - INFO - Epoch 590 training loss = 0.3131
2023-04-27 04:13:45,567 - INFO - Validation loss = 0.2708
2023-04-27 04:13:45,568 - INFO - best model
2023-04-27 04:13:49,604 - INFO - Epoch 591 training loss = 0.3118
2023-04-27 04:13:53,617 - INFO - Epoch 592 training loss = 0.2981
2023-04-27 04:13:57,631 - INFO - Epoch 593 training loss = 0.3469
2023-04-27 04:14:01,646 - INFO - Epoch 594 training loss = 0.3144
2023-04-27 04:14:05,662 - INFO - Epoch 595 training loss = 0.2863
2023-04-27 04:14:09,676 - INFO - Epoch 596 training loss = 0.2936
2023-04-27 04:14:13,694 - INFO - Epoch 597 training loss = 0.2805
2023-04-27 04:14:17,711 - INFO - Epoch 598 training loss = 0.2747
2023-04-27 04:14:21,725 - INFO - Epoch 599 training loss = 0.2638
2023-04-27 04:14:25,739 - INFO - Epoch 600 training loss = 0.2791
2023-04-27 04:14:26,052 - INFO - Validation loss = 0.3288
2023-04-27 04:14:30,067 - INFO - Epoch 601 training loss = 0.2736
2023-04-27 04:14:34,081 - INFO - Epoch 602 training loss = 0.2496
2023-04-27 04:14:38,095 - INFO - Epoch 603 training loss = 0.2646
2023-04-27 04:14:42,112 - INFO - Epoch 604 training loss = 0.2445
2023-04-27 04:14:46,130 - INFO - Epoch 605 training loss = 0.2887
2023-04-27 04:14:50,146 - INFO - Epoch 606 training loss = 0.2445
2023-04-27 04:14:54,160 - INFO - Epoch 607 training loss = 0.2914
2023-04-27 04:14:58,175 - INFO - Epoch 608 training loss = 0.2549
2023-04-27 04:15:02,189 - INFO - Epoch 609 training loss = 0.2576
2023-04-27 04:15:06,204 - INFO - Epoch 610 training loss = 0.2499
2023-04-27 04:15:06,518 - INFO - Validation loss = 0.3289
2023-04-27 04:15:10,533 - INFO - Epoch 611 training loss = 0.2645
2023-04-27 04:15:14,550 - INFO - Epoch 612 training loss = 0.2423
2023-04-27 04:15:18,567 - INFO - Epoch 613 training loss = 0.2343
2023-04-27 04:15:22,581 - INFO - Epoch 614 training loss = 0.2644
2023-04-27 04:15:26,595 - INFO - Epoch 615 training loss = 0.2215
2023-04-27 04:15:30,609 - INFO - Epoch 616 training loss = 0.2784
2023-04-27 04:15:34,623 - INFO - Epoch 617 training loss = 0.2452
2023-04-27 04:15:38,637 - INFO - Epoch 618 training loss = 0.2583
2023-04-27 04:15:42,653 - INFO - Epoch 619 training loss = 0.2219
2023-04-27 04:15:46,672 - INFO - Epoch 620 training loss = 0.2497
2023-04-27 04:15:46,984 - INFO - Validation loss = 0.2135
2023-04-27 04:15:46,984 - INFO - best model
2023-04-27 04:15:51,019 - INFO - Epoch 621 training loss = 0.2159
2023-04-27 04:15:55,033 - INFO - Epoch 622 training loss = 0.2268
2023-04-27 04:15:59,046 - INFO - Epoch 623 training loss = 0.2751
2023-04-27 04:16:03,060 - INFO - Epoch 624 training loss = 0.2434
2023-04-27 04:16:07,075 - INFO - Epoch 625 training loss = 0.2402
2023-04-27 04:16:11,090 - INFO - Epoch 626 training loss = 0.2264
2023-04-27 04:16:15,107 - INFO - Epoch 627 training loss = 0.2246
2023-04-27 04:16:19,123 - INFO - Epoch 628 training loss = 0.2516
2023-04-27 04:16:23,138 - INFO - Epoch 629 training loss = 0.2073
2023-04-27 04:16:27,153 - INFO - Epoch 630 training loss = 0.2591
2023-04-27 04:16:27,467 - INFO - Validation loss = 0.3402
2023-04-27 04:16:31,481 - INFO - Epoch 631 training loss = 0.2334
2023-04-27 04:16:35,495 - INFO - Epoch 632 training loss = 0.2401
2023-04-27 04:16:39,510 - INFO - Epoch 633 training loss = 0.2153
2023-04-27 04:16:43,526 - INFO - Epoch 634 training loss = 0.2303
2023-04-27 04:16:47,543 - INFO - Epoch 635 training loss = 0.2406
2023-04-27 04:16:51,558 - INFO - Epoch 636 training loss = 0.2004
2023-04-27 04:16:55,573 - INFO - Epoch 637 training loss = 0.2302
2023-04-27 04:16:59,588 - INFO - Epoch 638 training loss = 0.2268
2023-04-27 04:17:03,604 - INFO - Epoch 639 training loss = 0.2415
2023-04-27 04:17:07,619 - INFO - Epoch 640 training loss = 0.223
2023-04-27 04:17:07,932 - INFO - Validation loss = 0.186
2023-04-27 04:17:07,932 - INFO - best model
2023-04-27 04:17:11,969 - INFO - Epoch 641 training loss = 0.2001
2023-04-27 04:17:15,985 - INFO - Epoch 642 training loss = 0.2014
2023-04-27 04:17:19,999 - INFO - Epoch 643 training loss = 0.1981
2023-04-27 04:17:24,011 - INFO - Epoch 644 training loss = 0.1837
2023-04-27 04:17:28,024 - INFO - Epoch 645 training loss = 0.1819
2023-04-27 04:17:32,036 - INFO - Epoch 646 training loss = 0.1858
2023-04-27 04:17:36,049 - INFO - Epoch 647 training loss = 0.2016
2023-04-27 04:17:40,062 - INFO - Epoch 648 training loss = 0.1973
2023-04-27 04:17:44,078 - INFO - Epoch 649 training loss = 0.1973
2023-04-27 04:17:48,094 - INFO - Epoch 650 training loss = 0.2147
2023-04-27 04:17:48,408 - INFO - Validation loss = 0.2159
2023-04-27 04:17:52,422 - INFO - Epoch 651 training loss = 0.2011
2023-04-27 04:17:56,435 - INFO - Epoch 652 training loss = 0.1936
2023-04-27 04:18:00,448 - INFO - Epoch 653 training loss = 0.2181
2023-04-27 04:18:04,462 - INFO - Epoch 654 training loss = 0.1651
2023-04-27 04:18:08,474 - INFO - Epoch 655 training loss = 0.1868
2023-04-27 04:18:12,490 - INFO - Epoch 656 training loss = 0.1929
2023-04-27 04:18:16,506 - INFO - Epoch 657 training loss = 0.1917
2023-04-27 04:18:20,520 - INFO - Epoch 658 training loss = 0.1734
2023-04-27 04:18:24,533 - INFO - Epoch 659 training loss = 0.1622
2023-04-27 04:18:28,547 - INFO - Epoch 660 training loss = 0.171
2023-04-27 04:18:28,860 - INFO - Validation loss = 0.2285
2023-04-27 04:18:32,873 - INFO - Epoch 661 training loss = 0.1893
2023-04-27 04:18:36,886 - INFO - Epoch 662 training loss = 0.2033
2023-04-27 04:18:40,899 - INFO - Epoch 663 training loss = 0.1903
2023-04-27 04:18:44,915 - INFO - Epoch 664 training loss = 0.1573
2023-04-27 04:18:48,932 - INFO - Epoch 665 training loss = 0.1763
2023-04-27 04:18:52,946 - INFO - Epoch 666 training loss = 0.1607
2023-04-27 04:18:56,959 - INFO - Epoch 667 training loss = 0.167
2023-04-27 04:19:00,973 - INFO - Epoch 668 training loss = 0.1897
2023-04-27 04:19:04,986 - INFO - Epoch 669 training loss = 0.1677
2023-04-27 04:19:08,999 - INFO - Epoch 670 training loss = 0.1589
2023-04-27 04:19:09,313 - INFO - Validation loss = 0.207
2023-04-27 04:19:13,330 - INFO - Epoch 671 training loss = 0.1592
2023-04-27 04:19:17,347 - INFO - Epoch 672 training loss = 0.1716
2023-04-27 04:19:21,361 - INFO - Epoch 673 training loss = 0.1507
2023-04-27 04:19:25,374 - INFO - Epoch 674 training loss = 0.1733
2023-04-27 04:19:29,388 - INFO - Epoch 675 training loss = 0.158
2023-04-27 04:19:33,402 - INFO - Epoch 676 training loss = 0.158
2023-04-27 04:19:37,415 - INFO - Epoch 677 training loss = 0.158
2023-04-27 04:19:41,429 - INFO - Epoch 678 training loss = 0.1504
2023-04-27 04:19:45,446 - INFO - Epoch 679 training loss = 0.1566
2023-04-27 04:19:49,463 - INFO - Epoch 680 training loss = 0.1561
2023-04-27 04:19:49,777 - INFO - Validation loss = 0.2366
2023-04-27 04:19:53,791 - INFO - Epoch 681 training loss = 0.1462
2023-04-27 04:19:57,807 - INFO - Epoch 682 training loss = 0.1552
2023-04-27 04:20:01,824 - INFO - Epoch 683 training loss = 0.1672
2023-04-27 04:20:05,840 - INFO - Epoch 684 training loss = 0.1421
2023-04-27 04:20:09,855 - INFO - Epoch 685 training loss = 0.139
2023-04-27 04:20:13,871 - INFO - Epoch 686 training loss = 0.1334
2023-04-27 04:20:17,887 - INFO - Epoch 687 training loss = 0.1377
2023-04-27 04:20:21,900 - INFO - Epoch 688 training loss = 0.1515
2023-04-27 04:20:25,913 - INFO - Epoch 689 training loss = 0.127
2023-04-27 04:20:29,926 - INFO - Epoch 690 training loss = 0.1375
2023-04-27 04:20:30,240 - INFO - Validation loss = 0.2394
2023-04-27 04:20:34,253 - INFO - Epoch 691 training loss = 0.1413
2023-04-27 04:20:38,267 - INFO - Epoch 692 training loss = 0.1457
2023-04-27 04:20:42,281 - INFO - Epoch 693 training loss = 0.1404
2023-04-27 04:20:46,297 - INFO - Epoch 694 training loss = 0.1434
2023-04-27 04:20:50,311 - INFO - Epoch 695 training loss = 0.1459
2023-04-27 04:20:54,325 - INFO - Epoch 696 training loss = 0.1254
2023-04-27 04:20:58,338 - INFO - Epoch 697 training loss = 0.1458
2023-04-27 04:21:02,353 - INFO - Epoch 698 training loss = 0.1375
2023-04-27 04:21:06,367 - INFO - Epoch 699 training loss = 0.1267
2023-04-27 04:21:10,382 - INFO - Epoch 700 training loss = 0.11
2023-04-27 04:21:10,695 - INFO - Validation loss = 0.1597
2023-04-27 04:21:10,695 - INFO - best model
2023-04-27 04:21:14,735 - INFO - Epoch 701 training loss = 0.1489
2023-04-27 04:21:18,752 - INFO - Epoch 702 training loss = 0.1223
2023-04-27 04:21:22,767 - INFO - Epoch 703 training loss = 0.1112
2023-04-27 04:21:26,781 - INFO - Epoch 704 training loss = 0.1364
2023-04-27 04:21:30,796 - INFO - Epoch 705 training loss = 0.1206
2023-04-27 04:21:34,811 - INFO - Epoch 706 training loss = 0.132
2023-04-27 04:21:38,826 - INFO - Epoch 707 training loss = 0.1279
2023-04-27 04:21:42,842 - INFO - Epoch 708 training loss = 0.1125
2023-04-27 04:21:46,860 - INFO - Epoch 709 training loss = 0.1207
2023-04-27 04:21:50,877 - INFO - Epoch 710 training loss = 0.1133
2023-04-27 04:21:51,190 - INFO - Validation loss = 0.1608
2023-04-27 04:21:55,205 - INFO - Epoch 711 training loss = 0.1177
2023-04-27 04:21:59,220 - INFO - Epoch 712 training loss = 0.1039
2023-04-27 04:22:03,234 - INFO - Epoch 713 training loss = 0.111
2023-04-27 04:22:07,248 - INFO - Epoch 714 training loss = 0.1135
2023-04-27 04:22:11,265 - INFO - Epoch 715 training loss = 0.09905
2023-04-27 04:22:15,281 - INFO - Epoch 716 training loss = 0.1096
2023-04-27 04:22:19,297 - INFO - Epoch 717 training loss = 0.1214
2023-04-27 04:22:23,309 - INFO - Epoch 718 training loss = 0.1057
2023-04-27 04:22:27,323 - INFO - Epoch 719 training loss = 0.09638
2023-04-27 04:22:31,336 - INFO - Epoch 720 training loss = 0.1128
2023-04-27 04:22:31,649 - INFO - Validation loss = 0.1739
2023-04-27 04:22:35,663 - INFO - Epoch 721 training loss = 0.1053
2023-04-27 04:22:39,676 - INFO - Epoch 722 training loss = 0.101
2023-04-27 04:22:43,693 - INFO - Epoch 723 training loss = 0.09104
2023-04-27 04:22:47,709 - INFO - Epoch 724 training loss = 0.09491
2023-04-27 04:22:51,722 - INFO - Epoch 725 training loss = 0.1096
2023-04-27 04:22:55,736 - INFO - Epoch 726 training loss = 0.09116
2023-04-27 04:22:59,751 - INFO - Epoch 727 training loss = 0.09953
2023-04-27 04:23:03,766 - INFO - Epoch 728 training loss = 0.1016
2023-04-27 04:23:07,779 - INFO - Epoch 729 training loss = 0.09936
2023-04-27 04:23:11,795 - INFO - Epoch 730 training loss = 0.09419
2023-04-27 04:23:12,109 - INFO - Validation loss = 0.1882
2023-04-27 04:23:16,126 - INFO - Epoch 731 training loss = 0.09092
2023-04-27 04:23:20,141 - INFO - Epoch 732 training loss = 0.09056
2023-04-27 04:23:24,154 - INFO - Epoch 733 training loss = 0.09242
2023-04-27 04:23:28,166 - INFO - Epoch 734 training loss = 0.08249
2023-04-27 04:23:32,179 - INFO - Epoch 735 training loss = 0.08365
2023-04-27 04:23:36,192 - INFO - Epoch 736 training loss = 0.09089
2023-04-27 04:23:40,206 - INFO - Epoch 737 training loss = 0.08227
2023-04-27 04:23:44,223 - INFO - Epoch 738 training loss = 0.08536
2023-04-27 04:23:48,240 - INFO - Epoch 739 training loss = 0.08357
2023-04-27 04:23:52,253 - INFO - Epoch 740 training loss = 0.08849
2023-04-27 04:23:52,567 - INFO - Validation loss = 0.1168
2023-04-27 04:23:52,567 - INFO - best model
2023-04-27 04:23:56,603 - INFO - Epoch 741 training loss = 0.07887
2023-04-27 04:24:00,618 - INFO - Epoch 742 training loss = 0.08307
2023-04-27 04:24:04,633 - INFO - Epoch 743 training loss = 0.09066
2023-04-27 04:24:08,647 - INFO - Epoch 744 training loss = 0.07468
2023-04-27 04:24:12,664 - INFO - Epoch 745 training loss = 0.07991
2023-04-27 04:24:16,682 - INFO - Epoch 746 training loss = 0.0748
2023-04-27 04:24:20,697 - INFO - Epoch 747 training loss = 0.07191
2023-04-27 04:24:24,710 - INFO - Epoch 748 training loss = 0.07688
2023-04-27 04:24:28,724 - INFO - Epoch 749 training loss = 0.07574
2023-04-27 04:24:32,738 - INFO - Epoch 750 training loss = 0.07531
2023-04-27 04:24:33,052 - INFO - Validation loss = 0.1224
2023-04-27 04:24:37,065 - INFO - Epoch 751 training loss = 0.07334
2023-04-27 04:24:41,079 - INFO - Epoch 752 training loss = 0.06872
2023-04-27 04:24:45,095 - INFO - Epoch 753 training loss = 0.06728
2023-04-27 04:24:49,111 - INFO - Epoch 754 training loss = 0.07528
2023-04-27 04:24:53,123 - INFO - Epoch 755 training loss = 0.07151
2023-04-27 04:24:57,137 - INFO - Epoch 756 training loss = 0.06909
2023-04-27 04:25:01,151 - INFO - Epoch 757 training loss = 0.06706
2023-04-27 04:25:05,176 - INFO - Epoch 758 training loss = 0.06871
2023-04-27 04:25:09,203 - INFO - Epoch 759 training loss = 0.06901
2023-04-27 04:25:13,220 - INFO - Epoch 760 training loss = 0.06898
2023-04-27 04:25:13,534 - INFO - Validation loss = 0.08789
2023-04-27 04:25:13,534 - INFO - best model
2023-04-27 04:25:17,573 - INFO - Epoch 761 training loss = 0.0741
2023-04-27 04:25:21,586 - INFO - Epoch 762 training loss = 0.06384
2023-04-27 04:25:25,598 - INFO - Epoch 763 training loss = 0.05976
2023-04-27 04:25:29,612 - INFO - Epoch 764 training loss = 0.06608
2023-04-27 04:25:33,625 - INFO - Epoch 765 training loss = 0.06318
2023-04-27 04:25:37,638 - INFO - Epoch 766 training loss = 0.05755
2023-04-27 04:25:41,653 - INFO - Epoch 767 training loss = 0.059
2023-04-27 04:25:45,668 - INFO - Epoch 768 training loss = 0.06004
2023-04-27 04:25:49,683 - INFO - Epoch 769 training loss = 0.06431
2023-04-27 04:25:53,695 - INFO - Epoch 770 training loss = 0.06041
2023-04-27 04:25:54,009 - INFO - Validation loss = 0.1094
2023-04-27 04:25:58,022 - INFO - Epoch 771 training loss = 0.06076
2023-04-27 04:26:02,036 - INFO - Epoch 772 training loss = 0.06074
2023-04-27 04:26:06,049 - INFO - Epoch 773 training loss = 0.0551
2023-04-27 04:26:10,063 - INFO - Epoch 774 training loss = 0.0635
2023-04-27 04:26:14,078 - INFO - Epoch 775 training loss = 0.05784
2023-04-27 04:26:18,094 - INFO - Epoch 776 training loss = 0.05561
2023-04-27 04:26:22,106 - INFO - Epoch 777 training loss = 0.05353
2023-04-27 04:26:26,118 - INFO - Epoch 778 training loss = 0.05613
2023-04-27 04:26:30,130 - INFO - Epoch 779 training loss = 0.05422
2023-04-27 04:26:34,142 - INFO - Epoch 780 training loss = 0.05565
2023-04-27 04:26:34,456 - INFO - Validation loss = 0.09753
2023-04-27 04:26:38,469 - INFO - Epoch 781 training loss = 0.05318
2023-04-27 04:26:42,484 - INFO - Epoch 782 training loss = 0.0509
2023-04-27 04:26:46,499 - INFO - Epoch 783 training loss = 0.0522
2023-04-27 04:26:50,502 - INFO - Epoch 784 training loss = 0.0486
2023-04-27 04:26:54,505 - INFO - Epoch 785 training loss = 0.05148
2023-04-27 04:26:58,506 - INFO - Epoch 786 training loss = 0.04863
2023-04-27 04:27:02,509 - INFO - Epoch 787 training loss = 0.04838
2023-04-27 04:27:06,510 - INFO - Epoch 788 training loss = 0.04673
2023-04-27 04:27:10,513 - INFO - Epoch 789 training loss = 0.04628
2023-04-27 04:27:14,518 - INFO - Epoch 790 training loss = 0.04456
2023-04-27 04:27:14,831 - INFO - Validation loss = 0.07957
2023-04-27 04:27:14,832 - INFO - best model
2023-04-27 04:27:18,857 - INFO - Epoch 791 training loss = 0.04786
2023-04-27 04:27:22,858 - INFO - Epoch 792 training loss = 0.04659
2023-04-27 04:27:26,860 - INFO - Epoch 793 training loss = 0.04791
2023-04-27 04:27:30,861 - INFO - Epoch 794 training loss = 0.04381
2023-04-27 04:27:34,863 - INFO - Epoch 795 training loss = 0.04446
2023-04-27 04:27:38,864 - INFO - Epoch 796 training loss = 0.04461
2023-04-27 04:27:42,868 - INFO - Epoch 797 training loss = 0.04265
2023-04-27 04:27:46,874 - INFO - Epoch 798 training loss = 0.0415
2023-04-27 04:27:50,877 - INFO - Epoch 799 training loss = 0.04242
2023-04-27 04:27:54,878 - INFO - Epoch 800 training loss = 0.04607
2023-04-27 04:27:55,205 - INFO - Validation loss = 0.07665
2023-04-27 04:27:55,205 - INFO - best model
2023-04-27 04:27:59,231 - INFO - Epoch 801 training loss = 0.0422
2023-04-27 04:28:03,234 - INFO - Epoch 802 training loss = 0.04159
2023-04-27 04:28:07,236 - INFO - Epoch 803 training loss = 0.04297
2023-04-27 04:28:11,240 - INFO - Epoch 804 training loss = 0.0435
2023-04-27 04:28:15,246 - INFO - Epoch 805 training loss = 0.0386
2023-04-27 04:28:19,250 - INFO - Epoch 806 training loss = 0.0392
2023-04-27 04:28:23,252 - INFO - Epoch 807 training loss = 0.03748
2023-04-27 04:28:27,255 - INFO - Epoch 808 training loss = 0.04098
2023-04-27 04:28:31,258 - INFO - Epoch 809 training loss = 0.04164
2023-04-27 04:28:35,259 - INFO - Epoch 810 training loss = 0.03672
2023-04-27 04:28:35,572 - INFO - Validation loss = 0.07236
2023-04-27 04:28:35,572 - INFO - best model
2023-04-27 04:28:39,597 - INFO - Epoch 811 training loss = 0.03759
2023-04-27 04:28:43,602 - INFO - Epoch 812 training loss = 0.03625
2023-04-27 04:28:47,608 - INFO - Epoch 813 training loss = 0.03641
2023-04-27 04:28:51,610 - INFO - Epoch 814 training loss = 0.03492
2023-04-27 04:28:55,613 - INFO - Epoch 815 training loss = 0.036
2023-04-27 04:28:59,615 - INFO - Epoch 816 training loss = 0.03384
2023-04-27 04:29:03,618 - INFO - Epoch 817 training loss = 0.0359
2023-04-27 04:29:07,620 - INFO - Epoch 818 training loss = 0.03377
2023-04-27 04:29:11,622 - INFO - Epoch 819 training loss = 0.03796
2023-04-27 04:29:15,628 - INFO - Epoch 820 training loss = 0.03362
2023-04-27 04:29:15,942 - INFO - Validation loss = 0.06922
2023-04-27 04:29:15,942 - INFO - best model
2023-04-27 04:29:19,969 - INFO - Epoch 821 training loss = 0.03098
2023-04-27 04:29:23,970 - INFO - Epoch 822 training loss = 0.03229
2023-04-27 04:29:27,972 - INFO - Epoch 823 training loss = 0.0324
2023-04-27 04:29:31,973 - INFO - Epoch 824 training loss = 0.03398
2023-04-27 04:29:35,975 - INFO - Epoch 825 training loss = 0.03102
2023-04-27 04:29:39,977 - INFO - Epoch 826 training loss = 0.03126
2023-04-27 04:29:43,988 - INFO - Epoch 827 training loss = 0.02958
2023-04-27 04:29:48,005 - INFO - Epoch 828 training loss = 0.02864
2023-04-27 04:29:52,019 - INFO - Epoch 829 training loss = 0.03008
2023-04-27 04:29:56,033 - INFO - Epoch 830 training loss = 0.03045
2023-04-27 04:29:56,347 - INFO - Validation loss = 0.08566
2023-04-27 04:30:00,362 - INFO - Epoch 831 training loss = 0.03154
2023-04-27 04:30:04,378 - INFO - Epoch 832 training loss = 0.02975
2023-04-27 04:30:08,392 - INFO - Epoch 833 training loss = 0.03094
2023-04-27 04:30:12,407 - INFO - Epoch 834 training loss = 0.02728
2023-04-27 04:30:16,422 - INFO - Epoch 835 training loss = 0.02763
2023-04-27 04:30:20,438 - INFO - Epoch 836 training loss = 0.02732
2023-04-27 04:30:24,451 - INFO - Epoch 837 training loss = 0.02891
2023-04-27 04:30:28,464 - INFO - Epoch 838 training loss = 0.02899
2023-04-27 04:30:32,476 - INFO - Epoch 839 training loss = 0.02777
2023-04-27 04:30:36,488 - INFO - Epoch 840 training loss = 0.02654
2023-04-27 04:30:36,802 - INFO - Validation loss = 0.06172
2023-04-27 04:30:36,802 - INFO - best model
2023-04-27 04:30:40,838 - INFO - Epoch 841 training loss = 0.0267
2023-04-27 04:30:44,855 - INFO - Epoch 842 training loss = 0.02687
2023-04-27 04:30:48,873 - INFO - Epoch 843 training loss = 0.02628
2023-04-27 04:30:52,888 - INFO - Epoch 844 training loss = 0.02486
2023-04-27 04:30:56,904 - INFO - Epoch 845 training loss = 0.02492
2023-04-27 04:31:00,918 - INFO - Epoch 846 training loss = 0.02576
2023-04-27 04:31:04,934 - INFO - Epoch 847 training loss = 0.02354
2023-04-27 04:31:08,948 - INFO - Epoch 848 training loss = 0.02461
2023-04-27 04:31:12,957 - INFO - Epoch 849 training loss = 0.02493
2023-04-27 04:31:16,974 - INFO - Epoch 850 training loss = 0.02441
2023-04-27 04:31:17,288 - INFO - Validation loss = 0.06016
2023-04-27 04:31:17,288 - INFO - best model
2023-04-27 04:31:21,326 - INFO - Epoch 851 training loss = 0.02438
2023-04-27 04:31:25,340 - INFO - Epoch 852 training loss = 0.02379
2023-04-27 04:31:29,347 - INFO - Epoch 853 training loss = 0.02255
2023-04-27 04:31:33,349 - INFO - Epoch 854 training loss = 0.02333
2023-04-27 04:31:37,351 - INFO - Epoch 855 training loss = 0.023
2023-04-27 04:31:41,354 - INFO - Epoch 856 training loss = 0.02228
2023-04-27 04:31:45,359 - INFO - Epoch 857 training loss = 0.02271
2023-04-27 04:31:49,363 - INFO - Epoch 858 training loss = 0.02177
2023-04-27 04:31:53,365 - INFO - Epoch 859 training loss = 0.02167
2023-04-27 04:31:57,368 - INFO - Epoch 860 training loss = 0.02206
2023-04-27 04:31:57,681 - INFO - Validation loss = 0.06392
2023-04-27 04:32:01,684 - INFO - Epoch 861 training loss = 0.02117
2023-04-27 04:32:05,686 - INFO - Epoch 862 training loss = 0.02016
2023-04-27 04:32:09,689 - INFO - Epoch 863 training loss = 0.0207
2023-04-27 04:32:13,693 - INFO - Epoch 864 training loss = 0.02164
2023-04-27 04:32:17,698 - INFO - Epoch 865 training loss = 0.02037
2023-04-27 04:32:21,700 - INFO - Epoch 866 training loss = 0.02071
2023-04-27 04:32:25,701 - INFO - Epoch 867 training loss = 0.02022
2023-04-27 04:32:29,703 - INFO - Epoch 868 training loss = 0.01959
2023-04-27 04:32:33,703 - INFO - Epoch 869 training loss = 0.01998
2023-04-27 04:32:37,705 - INFO - Epoch 870 training loss = 0.01951
2023-04-27 04:32:38,018 - INFO - Validation loss = 0.05542
2023-04-27 04:32:38,018 - INFO - best model
2023-04-27 04:32:42,043 - INFO - Epoch 871 training loss = 0.01868
2023-04-27 04:32:46,048 - INFO - Epoch 872 training loss = 0.01892
2023-04-27 04:32:50,051 - INFO - Epoch 873 training loss = 0.01911
2023-04-27 04:32:54,054 - INFO - Epoch 874 training loss = 0.01877
2023-04-27 04:32:58,056 - INFO - Epoch 875 training loss = 0.01862
2023-04-27 04:33:02,059 - INFO - Epoch 876 training loss = 0.01783
2023-04-27 04:33:06,061 - INFO - Epoch 877 training loss = 0.01825
2023-04-27 04:33:10,063 - INFO - Epoch 878 training loss = 0.01871
2023-04-27 04:33:14,067 - INFO - Epoch 879 training loss = 0.01802
2023-04-27 04:33:18,071 - INFO - Epoch 880 training loss = 0.01822
2023-04-27 04:33:18,384 - INFO - Validation loss = 0.05207
2023-04-27 04:33:18,385 - INFO - best model
2023-04-27 04:33:22,408 - INFO - Epoch 881 training loss = 0.01713
2023-04-27 04:33:26,410 - INFO - Epoch 882 training loss = 0.01705
2023-04-27 04:33:30,411 - INFO - Epoch 883 training loss = 0.01771
2023-04-27 04:33:34,413 - INFO - Epoch 884 training loss = 0.01699
2023-04-27 04:33:38,415 - INFO - Epoch 885 training loss = 0.01684
2023-04-27 04:33:42,419 - INFO - Epoch 886 training loss = 0.01661
2023-04-27 04:33:46,424 - INFO - Epoch 887 training loss = 0.01691
2023-04-27 04:33:50,429 - INFO - Epoch 888 training loss = 0.01654
2023-04-27 04:33:54,431 - INFO - Epoch 889 training loss = 0.01637
2023-04-27 04:33:58,433 - INFO - Epoch 890 training loss = 0.01604
2023-04-27 04:33:58,747 - INFO - Validation loss = 0.05138
2023-04-27 04:33:58,747 - INFO - best model
2023-04-27 04:34:02,771 - INFO - Epoch 891 training loss = 0.01626
2023-04-27 04:34:06,773 - INFO - Epoch 892 training loss = 0.01602
2023-04-27 04:34:10,777 - INFO - Epoch 893 training loss = 0.01591
2023-04-27 04:34:14,782 - INFO - Epoch 894 training loss = 0.01551
2023-04-27 04:34:18,786 - INFO - Epoch 895 training loss = 0.0157
2023-04-27 04:34:22,793 - INFO - Epoch 896 training loss = 0.01525
2023-04-27 04:34:26,807 - INFO - Epoch 897 training loss = 0.01539
2023-04-27 04:34:30,822 - INFO - Epoch 898 training loss = 0.01528
2023-04-27 04:34:34,834 - INFO - Epoch 899 training loss = 0.01504
2023-04-27 04:34:38,835 - INFO - Epoch 900 training loss = 0.01487
2023-04-27 04:34:39,148 - INFO - Validation loss = 0.04957
2023-04-27 04:34:39,148 - INFO - best model
2023-04-27 04:34:43,173 - INFO - Epoch 901 training loss = 0.01471
2023-04-27 04:34:47,176 - INFO - Epoch 902 training loss = 0.01475
2023-04-27 04:34:51,179 - INFO - Epoch 903 training loss = 0.01472
2023-04-27 04:34:55,180 - INFO - Epoch 904 training loss = 0.01471
2023-04-27 04:34:59,181 - INFO - Epoch 905 training loss = 0.0147
2023-04-27 04:35:03,183 - INFO - Epoch 906 training loss = 0.01467
2023-04-27 04:35:07,184 - INFO - Epoch 907 training loss = 0.01427
2023-04-27 04:35:11,186 - INFO - Epoch 908 training loss = 0.01416
2023-04-27 04:35:15,189 - INFO - Epoch 909 training loss = 0.01433
2023-04-27 04:35:19,193 - INFO - Epoch 910 training loss = 0.01404
2023-04-27 04:35:19,505 - INFO - Validation loss = 0.04787
2023-04-27 04:35:19,505 - INFO - best model
2023-04-27 04:35:23,530 - INFO - Epoch 911 training loss = 0.01398
2023-04-27 04:35:27,532 - INFO - Epoch 912 training loss = 0.01394
2023-04-27 04:35:31,533 - INFO - Epoch 913 training loss = 0.01363
2023-04-27 04:35:35,536 - INFO - Epoch 914 training loss = 0.01371
2023-04-27 04:35:39,538 - INFO - Epoch 915 training loss = 0.01364
2023-04-27 04:35:43,543 - INFO - Epoch 916 training loss = 0.01365
2023-04-27 04:35:47,548 - INFO - Epoch 917 training loss = 0.01355
2023-04-27 04:35:51,550 - INFO - Epoch 918 training loss = 0.01348
2023-04-27 04:35:55,552 - INFO - Epoch 919 training loss = 0.01335
2023-04-27 04:35:59,554 - INFO - Epoch 920 training loss = 0.01316
2023-04-27 04:35:59,867 - INFO - Validation loss = 0.04771
2023-04-27 04:35:59,867 - INFO - best model
2023-04-27 04:36:03,893 - INFO - Epoch 921 training loss = 0.01322
2023-04-27 04:36:07,894 - INFO - Epoch 922 training loss = 0.01299
2023-04-27 04:36:11,898 - INFO - Epoch 923 training loss = 0.01294
2023-04-27 04:36:15,903 - INFO - Epoch 924 training loss = 0.01304
2023-04-27 04:36:19,908 - INFO - Epoch 925 training loss = 0.01309
2023-04-27 04:36:23,909 - INFO - Epoch 926 training loss = 0.01289
2023-04-27 04:36:27,911 - INFO - Epoch 927 training loss = 0.01277
2023-04-27 04:36:31,912 - INFO - Epoch 928 training loss = 0.01271
2023-04-27 04:36:35,914 - INFO - Epoch 929 training loss = 0.01275
2023-04-27 04:36:39,915 - INFO - Epoch 930 training loss = 0.01259
2023-04-27 04:36:40,228 - INFO - Validation loss = 0.0466
2023-04-27 04:36:40,228 - INFO - best model
2023-04-27 04:36:44,255 - INFO - Epoch 931 training loss = 0.01261
2023-04-27 04:36:48,260 - INFO - Epoch 932 training loss = 0.01252
2023-04-27 04:36:52,262 - INFO - Epoch 933 training loss = 0.01245
2023-04-27 04:36:56,263 - INFO - Epoch 934 training loss = 0.01236
2023-04-27 04:37:00,266 - INFO - Epoch 935 training loss = 0.01231
2023-04-27 04:37:04,267 - INFO - Epoch 936 training loss = 0.01231
2023-04-27 04:37:08,269 - INFO - Epoch 937 training loss = 0.01224
2023-04-27 04:37:12,274 - INFO - Epoch 938 training loss = 0.01219
2023-04-27 04:37:16,279 - INFO - Epoch 939 training loss = 0.01211
2023-04-27 04:37:20,281 - INFO - Epoch 940 training loss = 0.01211
2023-04-27 04:37:20,594 - INFO - Validation loss = 0.04605
2023-04-27 04:37:20,594 - INFO - best model
2023-04-27 04:37:24,617 - INFO - Epoch 941 training loss = 0.01211
2023-04-27 04:37:28,618 - INFO - Epoch 942 training loss = 0.01208
2023-04-27 04:37:32,620 - INFO - Epoch 943 training loss = 0.01199
2023-04-27 04:37:36,622 - INFO - Epoch 944 training loss = 0.01191
2023-04-27 04:37:40,624 - INFO - Epoch 945 training loss = 0.01197
2023-04-27 04:37:44,630 - INFO - Epoch 946 training loss = 0.01187
2023-04-27 04:37:48,635 - INFO - Epoch 947 training loss = 0.0118
2023-04-27 04:37:52,637 - INFO - Epoch 948 training loss = 0.01175
2023-04-27 04:37:56,639 - INFO - Epoch 949 training loss = 0.0117
2023-04-27 04:38:00,641 - INFO - Epoch 950 training loss = 0.01172
2023-04-27 04:38:00,954 - INFO - Validation loss = 0.0458
2023-04-27 04:38:00,954 - INFO - best model
2023-04-27 04:38:04,978 - INFO - Epoch 951 training loss = 0.01168
2023-04-27 04:38:08,980 - INFO - Epoch 952 training loss = 0.01164
2023-04-27 04:38:12,984 - INFO - Epoch 953 training loss = 0.01164
2023-04-27 04:38:16,987 - INFO - Epoch 954 training loss = 0.01154
2023-04-27 04:38:20,990 - INFO - Epoch 955 training loss = 0.01154
2023-04-27 04:38:24,991 - INFO - Epoch 956 training loss = 0.01149
2023-04-27 04:38:28,992 - INFO - Epoch 957 training loss = 0.01144
2023-04-27 04:38:32,993 - INFO - Epoch 958 training loss = 0.01143
2023-04-27 04:38:36,995 - INFO - Epoch 959 training loss = 0.01141
2023-04-27 04:38:40,997 - INFO - Epoch 960 training loss = 0.01136
2023-04-27 04:38:41,310 - INFO - Validation loss = 0.04479
2023-04-27 04:38:41,311 - INFO - best model
2023-04-27 04:38:45,337 - INFO - Epoch 961 training loss = 0.01134
2023-04-27 04:38:49,341 - INFO - Epoch 962 training loss = 0.01132
2023-04-27 04:38:53,343 - INFO - Epoch 963 training loss = 0.01129
2023-04-27 04:38:57,346 - INFO - Epoch 964 training loss = 0.01125
2023-04-27 04:39:01,349 - INFO - Epoch 965 training loss = 0.01125
2023-04-27 04:39:05,351 - INFO - Epoch 966 training loss = 0.01123
2023-04-27 04:39:09,353 - INFO - Epoch 967 training loss = 0.0112
2023-04-27 04:39:13,359 - INFO - Epoch 968 training loss = 0.01116
2023-04-27 04:39:17,364 - INFO - Epoch 969 training loss = 0.01115
2023-04-27 04:39:21,366 - INFO - Epoch 970 training loss = 0.01114
2023-04-27 04:39:21,679 - INFO - Validation loss = 0.04472
2023-04-27 04:39:21,679 - INFO - best model
2023-04-27 04:39:25,703 - INFO - Epoch 971 training loss = 0.01111
2023-04-27 04:39:29,705 - INFO - Epoch 972 training loss = 0.01111
2023-04-27 04:39:33,707 - INFO - Epoch 973 training loss = 0.01108
2023-04-27 04:39:37,709 - INFO - Epoch 974 training loss = 0.01105
2023-04-27 04:39:41,712 - INFO - Epoch 975 training loss = 0.01104
2023-04-27 04:39:45,718 - INFO - Epoch 976 training loss = 0.01104
2023-04-27 04:39:49,722 - INFO - Epoch 977 training loss = 0.01101
2023-04-27 04:39:53,725 - INFO - Epoch 978 training loss = 0.01099
2023-04-27 04:39:57,727 - INFO - Epoch 979 training loss = 0.01099
2023-04-27 04:40:01,732 - INFO - Epoch 980 training loss = 0.01096
2023-04-27 04:40:02,046 - INFO - Validation loss = 0.04426
2023-04-27 04:40:02,047 - INFO - best model
2023-04-27 04:40:06,070 - INFO - Epoch 981 training loss = 0.01096
2023-04-27 04:40:10,074 - INFO - Epoch 982 training loss = 0.01093
2023-04-27 04:40:14,079 - INFO - Epoch 983 training loss = 0.01092
2023-04-27 04:40:18,083 - INFO - Epoch 984 training loss = 0.0109
2023-04-27 04:40:22,085 - INFO - Epoch 985 training loss = 0.0109
2023-04-27 04:40:26,086 - INFO - Epoch 986 training loss = 0.01089
2023-04-27 04:40:30,088 - INFO - Epoch 987 training loss = 0.01089
2023-04-27 04:40:34,090 - INFO - Epoch 988 training loss = 0.01088
2023-04-27 04:40:38,092 - INFO - Epoch 989 training loss = 0.01087
2023-04-27 04:40:42,095 - INFO - Epoch 990 training loss = 0.01085
2023-04-27 04:40:42,408 - INFO - Validation loss = 0.04429
2023-04-27 04:40:46,414 - INFO - Epoch 991 training loss = 0.01085
2023-04-27 04:40:50,419 - INFO - Epoch 992 training loss = 0.01084
2023-04-27 04:40:54,421 - INFO - Epoch 993 training loss = 0.01084
2023-04-27 04:40:58,424 - INFO - Epoch 994 training loss = 0.01083
2023-04-27 04:41:02,426 - INFO - Epoch 995 training loss = 0.01083
2023-04-27 04:41:06,429 - INFO - Epoch 996 training loss = 0.01082
2023-04-27 04:41:10,431 - INFO - Epoch 997 training loss = 0.01082
2023-04-27 04:41:14,436 - INFO - Epoch 998 training loss = 0.01082
2023-04-27 04:41:18,441 - INFO - Epoch 999 training loss = 0.01081
2023-04-27 04:41:18,733 - INFO - Validation loss = 0.04422
