2023-05-15 18:15:04,433 - INFO - Config:
Namespace(config='configs/implicit_transformer.yaml', dir='experiments/mass_variation', epochs=100, device='cuda', seed=0, parameter_list='configs/data.yaml', wildcard=0, encoding='none', dir_name='mass_variation')
2023-05-15 18:15:04,434 - INFO - Config:
Munch({'optimizer': Munch({'type': 'AdamW', 'kwargs': Munch({'lr': 0.0025, 'weight_decay': 0.005, 'betas': [0.9, 0.95]})}), 'scheduler': Munch({'type': 'CosLR', 'kwargs': Munch({'epochs': 1500, 'initial_epochs': 50})}), 'model': Munch({'name': 'ImplicitTransformer', 'input_encoding': 'none', 'encoding_dim': 16, 'encoder': Munch({'embed_dim': 66, 'num_heads': 3, 'depth': 4}), 'decoder': Munch({'embed_dim': 99, 'num_heads': 3, 'depth': 3})}), 'generation_frequency': 5000, 'validation_frequency': 10, 'batch_size': 16, 'epochs': 1500, 'gradient_clip': 10})
2023-05-15 18:15:22,251 - INFO - Epoch 0 training loss = 2.816e+04
2023-05-15 18:15:22,638 - INFO - Validation loss = 2.042e+04
2023-05-15 18:15:22,638 - INFO - best model
2023-05-15 18:15:26,754 - INFO - Epoch 1 training loss = 1.453e+04
2023-05-15 18:15:30,877 - INFO - Epoch 2 training loss = 811.8
2023-05-15 18:15:34,987 - INFO - Epoch 3 training loss = 398.8
2023-05-15 18:15:39,096 - INFO - Epoch 4 training loss = 401.9
2023-05-15 18:15:43,221 - INFO - Epoch 5 training loss = 436.5
2023-05-15 18:15:47,304 - INFO - Epoch 6 training loss = 268.8
2023-05-15 18:15:51,398 - INFO - Epoch 7 training loss = 205.7
2023-05-15 18:15:55,500 - INFO - Epoch 8 training loss = 208.9
2023-05-15 18:15:59,598 - INFO - Epoch 9 training loss = 205.2
2023-05-15 18:16:03,701 - INFO - Epoch 10 training loss = 179.8
2023-05-15 18:16:04,086 - INFO - Validation loss = 179.6
2023-05-15 18:16:04,086 - INFO - best model
2023-05-15 18:16:08,210 - INFO - Epoch 11 training loss = 150.8
2023-05-15 18:16:12,327 - INFO - Epoch 12 training loss = 157.7
2023-05-15 18:16:16,454 - INFO - Epoch 13 training loss = 145.4
2023-05-15 18:16:20,577 - INFO - Epoch 14 training loss = 131.3
2023-05-15 18:16:24,715 - INFO - Epoch 15 training loss = 124.2
2023-05-15 18:16:28,860 - INFO - Epoch 16 training loss = 120.2
2023-05-15 18:16:32,967 - INFO - Epoch 17 training loss = 121.0
2023-05-15 18:16:37,064 - INFO - Epoch 18 training loss = 106.2
2023-05-15 18:16:41,141 - INFO - Epoch 19 training loss = 112.5
2023-05-15 18:16:45,217 - INFO - Epoch 20 training loss = 98.83
2023-05-15 18:16:45,599 - INFO - Validation loss = 114.5
2023-05-15 18:16:45,599 - INFO - best model
2023-05-15 18:16:49,719 - INFO - Epoch 21 training loss = 99.24
2023-05-15 18:16:53,804 - INFO - Epoch 22 training loss = 81.33
2023-05-15 18:16:57,877 - INFO - Epoch 23 training loss = 75.61
2023-05-15 18:17:01,952 - INFO - Epoch 24 training loss = 75.16
2023-05-15 18:17:06,035 - INFO - Epoch 25 training loss = 73.39
2023-05-15 18:17:10,114 - INFO - Epoch 26 training loss = 65.72
2023-05-15 18:17:14,179 - INFO - Epoch 27 training loss = 64.86
2023-05-15 18:17:18,258 - INFO - Epoch 28 training loss = 68.75
2023-05-15 18:17:22,327 - INFO - Epoch 29 training loss = 59.85
2023-05-15 18:17:26,400 - INFO - Epoch 30 training loss = 58.8
2023-05-15 18:17:26,781 - INFO - Validation loss = 120.8
2023-05-15 18:17:30,873 - INFO - Epoch 31 training loss = 49.41
2023-05-15 18:17:34,967 - INFO - Epoch 32 training loss = 56.7
2023-05-15 18:17:39,046 - INFO - Epoch 33 training loss = 52.74
2023-05-15 18:17:43,146 - INFO - Epoch 34 training loss = 49.64
2023-05-15 18:17:47,253 - INFO - Epoch 35 training loss = 47.34
2023-05-15 18:17:51,365 - INFO - Epoch 36 training loss = 47.16
2023-05-15 18:17:55,464 - INFO - Epoch 37 training loss = 44.94
2023-05-15 18:17:59,571 - INFO - Epoch 38 training loss = 45.61
2023-05-15 18:18:03,687 - INFO - Epoch 39 training loss = 43.28
2023-05-15 18:18:07,789 - INFO - Epoch 40 training loss = 45.23
2023-05-15 18:18:08,169 - INFO - Validation loss = 49.6
2023-05-15 18:18:08,169 - INFO - best model
2023-05-15 18:18:12,281 - INFO - Epoch 41 training loss = 40.77
2023-05-15 18:18:16,362 - INFO - Epoch 42 training loss = 37.4
2023-05-15 18:18:20,438 - INFO - Epoch 43 training loss = 42.23
2023-05-15 18:18:24,525 - INFO - Epoch 44 training loss = 38.87
2023-05-15 18:18:28,610 - INFO - Epoch 45 training loss = 39.11
2023-05-15 18:18:32,677 - INFO - Epoch 46 training loss = 36.33
2023-05-15 18:18:36,741 - INFO - Epoch 47 training loss = 40.53
2023-05-15 18:18:40,821 - INFO - Epoch 48 training loss = 37.21
2023-05-15 18:18:44,925 - INFO - Epoch 49 training loss = 35.96
2023-05-15 18:18:49,037 - INFO - Epoch 50 training loss = 34.88
2023-05-15 18:18:49,420 - INFO - Validation loss = 48.03
2023-05-15 18:18:49,420 - INFO - best model
2023-05-15 18:18:53,552 - INFO - Epoch 51 training loss = 32.4
2023-05-15 18:18:57,657 - INFO - Epoch 52 training loss = 32.75
2023-05-15 18:19:01,768 - INFO - Epoch 53 training loss = 32.86
2023-05-15 18:19:05,879 - INFO - Epoch 54 training loss = 29.44
2023-05-15 18:19:09,991 - INFO - Epoch 55 training loss = 30.11
2023-05-15 18:19:14,097 - INFO - Epoch 56 training loss = 27.37
2023-05-15 18:19:18,186 - INFO - Epoch 57 training loss = 27.09
2023-05-15 18:19:22,243 - INFO - Epoch 58 training loss = 27.72
2023-05-15 18:19:26,300 - INFO - Epoch 59 training loss = 24.75
2023-05-15 18:19:30,351 - INFO - Epoch 60 training loss = 28.53
2023-05-15 18:19:30,733 - INFO - Validation loss = 17.68
2023-05-15 18:19:30,733 - INFO - best model
2023-05-15 18:19:34,829 - INFO - Epoch 61 training loss = 26.49
2023-05-15 18:19:38,920 - INFO - Epoch 62 training loss = 24.25
2023-05-15 18:19:43,008 - INFO - Epoch 63 training loss = 27.12
2023-05-15 18:19:47,100 - INFO - Epoch 64 training loss = 22.47
2023-05-15 18:19:51,189 - INFO - Epoch 65 training loss = 22.98
2023-05-15 18:19:55,274 - INFO - Epoch 66 training loss = 23.33
2023-05-15 18:19:59,359 - INFO - Epoch 67 training loss = 21.66
2023-05-15 18:20:03,439 - INFO - Epoch 68 training loss = 25.36
2023-05-15 18:20:07,517 - INFO - Epoch 69 training loss = 22.34
2023-05-15 18:20:11,602 - INFO - Epoch 70 training loss = 18.23
2023-05-15 18:20:11,986 - INFO - Validation loss = 13.12
2023-05-15 18:20:11,986 - INFO - best model
2023-05-15 18:20:16,092 - INFO - Epoch 71 training loss = 20.05
2023-05-15 18:20:20,165 - INFO - Epoch 72 training loss = 20.14
2023-05-15 18:20:24,235 - INFO - Epoch 73 training loss = 18.51
2023-05-15 18:20:28,289 - INFO - Epoch 74 training loss = 19.45
2023-05-15 18:20:32,335 - INFO - Epoch 75 training loss = 19.49
2023-05-15 18:20:36,414 - INFO - Epoch 76 training loss = 15.65
2023-05-15 18:20:40,491 - INFO - Epoch 77 training loss = 19.59
2023-05-15 18:20:44,551 - INFO - Epoch 78 training loss = 17.6
2023-05-15 18:20:48,623 - INFO - Epoch 79 training loss = 19.31
2023-05-15 18:20:52,703 - INFO - Epoch 80 training loss = 14.81
2023-05-15 18:20:53,086 - INFO - Validation loss = 14.18
2023-05-15 18:20:57,195 - INFO - Epoch 81 training loss = 15.53
2023-05-15 18:21:01,330 - INFO - Epoch 82 training loss = 16.19
2023-05-15 18:21:05,447 - INFO - Epoch 83 training loss = 14.6
2023-05-15 18:21:09,571 - INFO - Epoch 84 training loss = 16.35
2023-05-15 18:21:13,697 - INFO - Epoch 85 training loss = 14.98
2023-05-15 18:21:17,827 - INFO - Epoch 86 training loss = 16.37
2023-05-15 18:21:21,912 - INFO - Epoch 87 training loss = 14.76
2023-05-15 18:21:26,003 - INFO - Epoch 88 training loss = 16.6
2023-05-15 18:21:30,110 - INFO - Epoch 89 training loss = 16.45
2023-05-15 18:21:34,238 - INFO - Epoch 90 training loss = 15.54
2023-05-15 18:21:34,620 - INFO - Validation loss = 15.87
2023-05-15 18:21:38,747 - INFO - Epoch 91 training loss = 13.92
2023-05-15 18:21:42,843 - INFO - Epoch 92 training loss = 15.74
2023-05-15 18:21:46,956 - INFO - Epoch 93 training loss = 14.46
2023-05-15 18:21:51,079 - INFO - Epoch 94 training loss = 14.65
2023-05-15 18:21:55,210 - INFO - Epoch 95 training loss = 15.88
2023-05-15 18:21:59,327 - INFO - Epoch 96 training loss = 14.28
2023-05-15 18:22:03,463 - INFO - Epoch 97 training loss = 12.98
2023-05-15 18:22:07,603 - INFO - Epoch 98 training loss = 13.54
2023-05-15 18:22:11,716 - INFO - Epoch 99 training loss = 12.54
2023-05-15 18:22:15,835 - INFO - Epoch 100 training loss = 13.11
2023-05-15 18:22:16,218 - INFO - Validation loss = 16.7
2023-05-15 18:22:20,346 - INFO - Epoch 101 training loss = 11.63
2023-05-15 18:22:24,466 - INFO - Epoch 102 training loss = 11.95
2023-05-15 18:22:28,590 - INFO - Epoch 103 training loss = 13.54
2023-05-15 18:22:32,705 - INFO - Epoch 104 training loss = 11.35
2023-05-15 18:22:36,828 - INFO - Epoch 105 training loss = 11.72
2023-05-15 18:22:40,948 - INFO - Epoch 106 training loss = 12.03
2023-05-15 18:22:45,055 - INFO - Epoch 107 training loss = 10.9
2023-05-15 18:22:49,148 - INFO - Epoch 108 training loss = 11.28
2023-05-15 18:22:53,230 - INFO - Epoch 109 training loss = 13.32
2023-05-15 18:22:57,270 - INFO - Epoch 110 training loss = 13.53
2023-05-15 18:22:57,649 - INFO - Validation loss = 17.3
2023-05-15 18:23:01,691 - INFO - Epoch 111 training loss = 10.78
2023-05-15 18:23:05,733 - INFO - Epoch 112 training loss = 11.14
2023-05-15 18:23:09,779 - INFO - Epoch 113 training loss = 11.98
2023-05-15 18:23:13,997 - INFO - Epoch 114 training loss = 11.37
2023-05-15 18:23:18,062 - INFO - Epoch 115 training loss = 8.91
2023-05-15 18:23:22,123 - INFO - Epoch 116 training loss = 14.76
2023-05-15 18:23:26,185 - INFO - Epoch 117 training loss = 10.23
2023-05-15 18:23:30,244 - INFO - Epoch 118 training loss = 10.97
2023-05-15 18:23:34,305 - INFO - Epoch 119 training loss = 11.79
2023-05-15 18:23:38,338 - INFO - Epoch 120 training loss = 10.39
2023-05-15 18:23:38,718 - INFO - Validation loss = 10.19
2023-05-15 18:23:38,718 - INFO - best model
2023-05-15 18:23:42,796 - INFO - Epoch 121 training loss = 8.984
2023-05-15 18:23:46,855 - INFO - Epoch 122 training loss = 10.93
2023-05-15 18:23:50,913 - INFO - Epoch 123 training loss = 12.12
2023-05-15 18:23:54,970 - INFO - Epoch 124 training loss = 11.47
2023-05-15 18:23:59,027 - INFO - Epoch 125 training loss = 12.19
2023-05-15 18:24:03,084 - INFO - Epoch 126 training loss = 11.04
2023-05-15 18:24:07,143 - INFO - Epoch 127 training loss = 8.994
2023-05-15 18:24:11,203 - INFO - Epoch 128 training loss = 9.869
2023-05-15 18:24:15,257 - INFO - Epoch 129 training loss = 12.11
2023-05-15 18:24:19,290 - INFO - Epoch 130 training loss = 9.675
2023-05-15 18:24:19,670 - INFO - Validation loss = 9.753
2023-05-15 18:24:19,670 - INFO - best model
2023-05-15 18:24:23,722 - INFO - Epoch 131 training loss =  9.3
2023-05-15 18:24:27,753 - INFO - Epoch 132 training loss = 10.43
2023-05-15 18:24:31,782 - INFO - Epoch 133 training loss = 8.796
2023-05-15 18:24:35,815 - INFO - Epoch 134 training loss =  9.6
2023-05-15 18:24:39,844 - INFO - Epoch 135 training loss = 10.46
2023-05-15 18:24:43,877 - INFO - Epoch 136 training loss = 9.934
2023-05-15 18:24:47,909 - INFO - Epoch 137 training loss = 10.49
2023-05-15 18:24:51,957 - INFO - Epoch 138 training loss = 8.941
2023-05-15 18:24:56,014 - INFO - Epoch 139 training loss = 11.04
2023-05-15 18:25:00,073 - INFO - Epoch 140 training loss = 10.25
2023-05-15 18:25:00,453 - INFO - Validation loss = 7.59
2023-05-15 18:25:00,453 - INFO - best model
2023-05-15 18:25:04,533 - INFO - Epoch 141 training loss = 9.01
2023-05-15 18:25:08,591 - INFO - Epoch 142 training loss = 8.609
2023-05-15 18:25:12,650 - INFO - Epoch 143 training loss = 9.651
2023-05-15 18:25:16,710 - INFO - Epoch 144 training loss = 9.017
2023-05-15 18:25:20,769 - INFO - Epoch 145 training loss = 9.064
2023-05-15 18:25:24,825 - INFO - Epoch 146 training loss = 9.607
2023-05-15 18:25:28,881 - INFO - Epoch 147 training loss = 9.243
2023-05-15 18:25:32,938 - INFO - Epoch 148 training loss = 8.57
2023-05-15 18:25:36,995 - INFO - Epoch 149 training loss = 8.309
2023-05-15 18:25:41,051 - INFO - Epoch 150 training loss = 8.394
2023-05-15 18:25:41,432 - INFO - Validation loss = 9.717
2023-05-15 18:25:45,494 - INFO - Epoch 151 training loss = 8.802
2023-05-15 18:25:49,554 - INFO - Epoch 152 training loss = 8.393
2023-05-15 18:25:53,612 - INFO - Epoch 153 training loss = 7.966
2023-05-15 18:25:57,669 - INFO - Epoch 154 training loss = 9.743
2023-05-15 18:26:01,728 - INFO - Epoch 155 training loss = 8.276
2023-05-15 18:26:05,787 - INFO - Epoch 156 training loss = 8.544
2023-05-15 18:26:09,846 - INFO - Epoch 157 training loss = 7.612
2023-05-15 18:26:13,906 - INFO - Epoch 158 training loss = 7.645
2023-05-15 18:26:17,966 - INFO - Epoch 159 training loss = 6.887
2023-05-15 18:26:22,015 - INFO - Epoch 160 training loss = 8.384
2023-05-15 18:26:22,395 - INFO - Validation loss = 7.597
2023-05-15 18:26:26,439 - INFO - Epoch 161 training loss = 6.442
2023-05-15 18:26:30,480 - INFO - Epoch 162 training loss = 7.636
2023-05-15 18:26:34,524 - INFO - Epoch 163 training loss = 8.214
2023-05-15 18:26:38,569 - INFO - Epoch 164 training loss = 8.219
2023-05-15 18:26:42,626 - INFO - Epoch 165 training loss = 7.736
2023-05-15 18:26:46,686 - INFO - Epoch 166 training loss = 8.02
2023-05-15 18:26:50,740 - INFO - Epoch 167 training loss = 7.478
2023-05-15 18:26:54,781 - INFO - Epoch 168 training loss = 8.474
2023-05-15 18:26:58,824 - INFO - Epoch 169 training loss = 9.185
2023-05-15 18:27:02,867 - INFO - Epoch 170 training loss = 8.218
2023-05-15 18:27:03,246 - INFO - Validation loss = 13.96
2023-05-15 18:27:07,291 - INFO - Epoch 171 training loss = 8.369
2023-05-15 18:27:11,337 - INFO - Epoch 172 training loss = 7.961
2023-05-15 18:27:15,384 - INFO - Epoch 173 training loss = 6.699
2023-05-15 18:27:19,428 - INFO - Epoch 174 training loss = 7.36
2023-05-15 18:27:23,472 - INFO - Epoch 175 training loss = 7.516
2023-05-15 18:27:27,515 - INFO - Epoch 176 training loss = 7.693
2023-05-15 18:27:31,568 - INFO - Epoch 177 training loss = 9.397
2023-05-15 18:27:35,625 - INFO - Epoch 178 training loss = 6.816
2023-05-15 18:27:39,683 - INFO - Epoch 179 training loss = 8.552
2023-05-15 18:27:43,742 - INFO - Epoch 180 training loss = 7.176
2023-05-15 18:27:44,123 - INFO - Validation loss = 7.237
2023-05-15 18:27:44,124 - INFO - best model
2023-05-15 18:27:48,205 - INFO - Epoch 181 training loss = 6.741
2023-05-15 18:27:52,262 - INFO - Epoch 182 training loss = 7.425
2023-05-15 18:27:56,318 - INFO - Epoch 183 training loss = 8.908
2023-05-15 18:28:00,375 - INFO - Epoch 184 training loss = 6.543
2023-05-15 18:28:04,433 - INFO - Epoch 185 training loss = 7.395
2023-05-15 18:28:08,491 - INFO - Epoch 186 training loss = 7.013
2023-05-15 18:28:12,549 - INFO - Epoch 187 training loss = 8.658
2023-05-15 18:28:16,609 - INFO - Epoch 188 training loss = 6.879
2023-05-15 18:28:20,667 - INFO - Epoch 189 training loss = 7.286
2023-05-15 18:28:25,002 - INFO - Epoch 190 training loss = 6.711
2023-05-15 18:28:25,400 - INFO - Validation loss = 8.079
2023-05-15 18:28:29,496 - INFO - Epoch 191 training loss = 6.119
2023-05-15 18:28:33,544 - INFO - Epoch 192 training loss = 7.34
2023-05-15 18:28:37,591 - INFO - Epoch 193 training loss = 6.166
2023-05-15 18:28:41,638 - INFO - Epoch 194 training loss = 7.387
2023-05-15 18:28:45,686 - INFO - Epoch 195 training loss = 6.862
2023-05-15 18:28:49,735 - INFO - Epoch 196 training loss = 7.861
2023-05-15 18:28:53,780 - INFO - Epoch 197 training loss = 6.781
2023-05-15 18:28:57,825 - INFO - Epoch 198 training loss = 7.278
2023-05-15 18:29:01,871 - INFO - Epoch 199 training loss = 6.894
2023-05-15 18:29:05,918 - INFO - Epoch 200 training loss = 6.464
2023-05-15 18:29:06,298 - INFO - Validation loss = 4.719
2023-05-15 18:29:06,299 - INFO - best model
2023-05-15 18:29:10,367 - INFO - Epoch 201 training loss = 6.416
2023-05-15 18:29:14,416 - INFO - Epoch 202 training loss = 6.77
2023-05-15 18:29:18,473 - INFO - Epoch 203 training loss = 6.758
2023-05-15 18:29:22,526 - INFO - Epoch 204 training loss = 7.896
2023-05-15 18:29:26,571 - INFO - Epoch 205 training loss = 7.197
2023-05-15 18:29:30,617 - INFO - Epoch 206 training loss = 7.36
2023-05-15 18:29:34,663 - INFO - Epoch 207 training loss = 7.171
2023-05-15 18:29:38,709 - INFO - Epoch 208 training loss = 7.667
2023-05-15 18:29:42,756 - INFO - Epoch 209 training loss = 8.301
2023-05-15 18:29:46,804 - INFO - Epoch 210 training loss = 7.112
2023-05-15 18:29:47,184 - INFO - Validation loss = 6.363
2023-05-15 18:29:51,232 - INFO - Epoch 211 training loss = 6.91
2023-05-15 18:29:55,277 - INFO - Epoch 212 training loss = 6.131
2023-05-15 18:29:59,323 - INFO - Epoch 213 training loss = 6.931
2023-05-15 18:30:03,372 - INFO - Epoch 214 training loss = 5.661
2023-05-15 18:30:07,430 - INFO - Epoch 215 training loss = 6.777
2023-05-15 18:30:11,491 - INFO - Epoch 216 training loss = 6.741
2023-05-15 18:30:15,554 - INFO - Epoch 217 training loss = 5.421
2023-05-15 18:30:19,617 - INFO - Epoch 218 training loss = 6.571
2023-05-15 18:30:23,676 - INFO - Epoch 219 training loss = 6.026
2023-05-15 18:30:27,734 - INFO - Epoch 220 training loss = 5.985
2023-05-15 18:30:28,114 - INFO - Validation loss = 10.53
2023-05-15 18:30:32,174 - INFO - Epoch 221 training loss = 6.229
2023-05-15 18:30:36,236 - INFO - Epoch 222 training loss = 5.781
2023-05-15 18:30:40,296 - INFO - Epoch 223 training loss = 7.059
2023-05-15 18:30:44,358 - INFO - Epoch 224 training loss = 5.597
2023-05-15 18:30:48,420 - INFO - Epoch 225 training loss = 5.939
2023-05-15 18:30:52,481 - INFO - Epoch 226 training loss = 6.421
2023-05-15 18:30:56,541 - INFO - Epoch 227 training loss = 7.211
2023-05-15 18:31:00,600 - INFO - Epoch 228 training loss = 6.389
2023-05-15 18:31:04,660 - INFO - Epoch 229 training loss = 6.431
2023-05-15 18:31:08,721 - INFO - Epoch 230 training loss = 5.622
2023-05-15 18:31:09,101 - INFO - Validation loss = 7.41
2023-05-15 18:31:13,155 - INFO - Epoch 231 training loss = 5.971
2023-05-15 18:31:17,204 - INFO - Epoch 232 training loss = 5.724
2023-05-15 18:31:21,250 - INFO - Epoch 233 training loss = 5.56
2023-05-15 18:31:25,296 - INFO - Epoch 234 training loss = 6.132
2023-05-15 18:31:29,350 - INFO - Epoch 235 training loss = 5.776
2023-05-15 18:31:33,410 - INFO - Epoch 236 training loss = 5.799
2023-05-15 18:31:37,471 - INFO - Epoch 237 training loss = 5.764
2023-05-15 18:31:41,532 - INFO - Epoch 238 training loss = 5.968
2023-05-15 18:31:45,593 - INFO - Epoch 239 training loss = 5.773
2023-05-15 18:31:49,656 - INFO - Epoch 240 training loss = 6.297
2023-05-15 18:31:50,036 - INFO - Validation loss = 7.97
2023-05-15 18:31:54,104 - INFO - Epoch 241 training loss = 6.895
2023-05-15 18:31:58,174 - INFO - Epoch 242 training loss = 5.665
2023-05-15 18:32:02,243 - INFO - Epoch 243 training loss = 5.941
2023-05-15 18:32:06,313 - INFO - Epoch 244 training loss = 5.275
2023-05-15 18:32:10,379 - INFO - Epoch 245 training loss = 5.459
2023-05-15 18:32:14,442 - INFO - Epoch 246 training loss = 7.431
2023-05-15 18:32:18,505 - INFO - Epoch 247 training loss = 5.63
2023-05-15 18:32:22,565 - INFO - Epoch 248 training loss = 7.145
2023-05-15 18:32:26,623 - INFO - Epoch 249 training loss = 5.988
2023-05-15 18:32:30,681 - INFO - Epoch 250 training loss = 6.419
2023-05-15 18:32:31,061 - INFO - Validation loss = 6.473
2023-05-15 18:32:35,121 - INFO - Epoch 251 training loss = 6.776
2023-05-15 18:32:39,180 - INFO - Epoch 252 training loss = 5.793
2023-05-15 18:32:43,242 - INFO - Epoch 253 training loss = 6.324
2023-05-15 18:32:47,304 - INFO - Epoch 254 training loss = 4.901
2023-05-15 18:32:51,366 - INFO - Epoch 255 training loss = 6.784
2023-05-15 18:32:55,425 - INFO - Epoch 256 training loss = 5.481
2023-05-15 18:32:59,484 - INFO - Epoch 257 training loss = 5.404
2023-05-15 18:33:03,545 - INFO - Epoch 258 training loss = 5.959
2023-05-15 18:33:07,597 - INFO - Epoch 259 training loss = 4.905
2023-05-15 18:33:11,643 - INFO - Epoch 260 training loss = 5.679
2023-05-15 18:33:12,023 - INFO - Validation loss = 6.958
2023-05-15 18:33:16,074 - INFO - Epoch 261 training loss =  5.8
2023-05-15 18:33:20,121 - INFO - Epoch 262 training loss = 5.068
2023-05-15 18:33:24,167 - INFO - Epoch 263 training loss = 5.521
2023-05-15 18:33:28,211 - INFO - Epoch 264 training loss = 4.669
2023-05-15 18:33:32,256 - INFO - Epoch 265 training loss = 6.032
2023-05-15 18:33:36,301 - INFO - Epoch 266 training loss = 7.13
2023-05-15 18:33:40,360 - INFO - Epoch 267 training loss = 5.092
2023-05-15 18:33:44,421 - INFO - Epoch 268 training loss = 6.096
2023-05-15 18:33:48,486 - INFO - Epoch 269 training loss = 6.337
2023-05-15 18:33:52,546 - INFO - Epoch 270 training loss = 5.738
2023-05-15 18:33:52,927 - INFO - Validation loss = 5.011
2023-05-15 18:33:56,988 - INFO - Epoch 271 training loss = 6.103
2023-05-15 18:34:01,049 - INFO - Epoch 272 training loss = 5.688
2023-05-15 18:34:05,110 - INFO - Epoch 273 training loss = 6.024
2023-05-15 18:34:09,165 - INFO - Epoch 274 training loss = 5.543
2023-05-15 18:34:13,228 - INFO - Epoch 275 training loss = 6.277
2023-05-15 18:34:17,292 - INFO - Epoch 276 training loss = 5.215
2023-05-15 18:34:21,354 - INFO - Epoch 277 training loss = 6.041
2023-05-15 18:34:25,414 - INFO - Epoch 278 training loss = 5.316
2023-05-15 18:34:29,474 - INFO - Epoch 279 training loss = 5.266
2023-05-15 18:34:33,534 - INFO - Epoch 280 training loss = 5.682
2023-05-15 18:34:33,915 - INFO - Validation loss = 7.721
2023-05-15 18:34:37,966 - INFO - Epoch 281 training loss = 6.05
2023-05-15 18:34:42,021 - INFO - Epoch 282 training loss = 5.473
2023-05-15 18:34:46,085 - INFO - Epoch 283 training loss = 4.999
2023-05-15 18:34:50,151 - INFO - Epoch 284 training loss = 5.966
2023-05-15 18:34:54,214 - INFO - Epoch 285 training loss = 4.784
2023-05-15 18:34:58,278 - INFO - Epoch 286 training loss = 5.155
2023-05-15 18:35:02,341 - INFO - Epoch 287 training loss = 5.748
2023-05-15 18:35:06,406 - INFO - Epoch 288 training loss = 7.064
2023-05-15 18:35:10,470 - INFO - Epoch 289 training loss = 5.825
2023-05-15 18:35:14,535 - INFO - Epoch 290 training loss = 4.452
2023-05-15 18:35:14,916 - INFO - Validation loss = 3.941
2023-05-15 18:35:14,916 - INFO - best model
2023-05-15 18:35:19,004 - INFO - Epoch 291 training loss = 5.519
2023-05-15 18:35:23,072 - INFO - Epoch 292 training loss = 5.506
2023-05-15 18:35:27,139 - INFO - Epoch 293 training loss = 5.702
2023-05-15 18:35:31,206 - INFO - Epoch 294 training loss = 7.338
2023-05-15 18:35:35,274 - INFO - Epoch 295 training loss = 5.035
2023-05-15 18:35:39,343 - INFO - Epoch 296 training loss = 5.202
2023-05-15 18:35:43,412 - INFO - Epoch 297 training loss = 5.047
2023-05-15 18:35:47,483 - INFO - Epoch 298 training loss = 5.189
2023-05-15 18:35:51,546 - INFO - Epoch 299 training loss = 5.474
2023-05-15 18:35:55,614 - INFO - Epoch 300 training loss = 5.238
2023-05-15 18:35:55,995 - INFO - Validation loss = 5.799
2023-05-15 18:36:00,062 - INFO - Epoch 301 training loss = 5.807
2023-05-15 18:36:04,126 - INFO - Epoch 302 training loss = 6.001
2023-05-15 18:36:08,189 - INFO - Epoch 303 training loss = 5.845
2023-05-15 18:36:12,254 - INFO - Epoch 304 training loss = 5.417
2023-05-15 18:36:16,320 - INFO - Epoch 305 training loss = 5.908
2023-05-15 18:36:20,387 - INFO - Epoch 306 training loss = 4.861
2023-05-15 18:36:24,450 - INFO - Epoch 307 training loss = 5.644
2023-05-15 18:36:28,512 - INFO - Epoch 308 training loss = 6.265
2023-05-15 18:36:32,574 - INFO - Epoch 309 training loss = 5.04
2023-05-15 18:36:36,637 - INFO - Epoch 310 training loss = 5.01
2023-05-15 18:36:37,018 - INFO - Validation loss = 7.911
2023-05-15 18:36:41,082 - INFO - Epoch 311 training loss = 4.699
2023-05-15 18:36:45,147 - INFO - Epoch 312 training loss = 5.082
2023-05-15 18:36:49,214 - INFO - Epoch 313 training loss = 4.535
2023-05-15 18:36:53,278 - INFO - Epoch 314 training loss = 6.063
2023-05-15 18:36:57,341 - INFO - Epoch 315 training loss = 4.554
2023-05-15 18:37:01,404 - INFO - Epoch 316 training loss = 5.243
2023-05-15 18:37:05,470 - INFO - Epoch 317 training loss = 5.772
2023-05-15 18:37:09,533 - INFO - Epoch 318 training loss = 5.247
2023-05-15 18:37:13,600 - INFO - Epoch 319 training loss = 5.411
2023-05-15 18:37:17,665 - INFO - Epoch 320 training loss = 5.076
2023-05-15 18:37:18,046 - INFO - Validation loss = 4.033
2023-05-15 18:37:22,110 - INFO - Epoch 321 training loss = 4.183
2023-05-15 18:37:26,172 - INFO - Epoch 322 training loss = 5.126
2023-05-15 18:37:30,234 - INFO - Epoch 323 training loss = 5.075
2023-05-15 18:37:34,296 - INFO - Epoch 324 training loss = 4.935
2023-05-15 18:37:38,361 - INFO - Epoch 325 training loss = 4.508
2023-05-15 18:37:42,424 - INFO - Epoch 326 training loss = 4.442
2023-05-15 18:37:46,491 - INFO - Epoch 327 training loss = 4.221
2023-05-15 18:37:50,555 - INFO - Epoch 328 training loss = 5.121
2023-05-15 18:37:54,619 - INFO - Epoch 329 training loss = 5.213
2023-05-15 18:37:58,683 - INFO - Epoch 330 training loss = 4.432
2023-05-15 18:37:59,064 - INFO - Validation loss = 3.803
2023-05-15 18:37:59,064 - INFO - best model
2023-05-15 18:38:03,149 - INFO - Epoch 331 training loss = 4.546
2023-05-15 18:38:07,213 - INFO - Epoch 332 training loss = 5.391
2023-05-15 18:38:11,278 - INFO - Epoch 333 training loss = 4.518
2023-05-15 18:38:15,344 - INFO - Epoch 334 training loss = 4.829
2023-05-15 18:38:19,409 - INFO - Epoch 335 training loss = 3.868
2023-05-15 18:38:23,473 - INFO - Epoch 336 training loss = 4.854
2023-05-15 18:38:27,535 - INFO - Epoch 337 training loss = 5.208
2023-05-15 18:38:31,600 - INFO - Epoch 338 training loss = 3.96
2023-05-15 18:38:35,664 - INFO - Epoch 339 training loss = 5.429
2023-05-15 18:38:39,728 - INFO - Epoch 340 training loss = 5.323
2023-05-15 18:38:40,108 - INFO - Validation loss = 5.641
2023-05-15 18:38:44,175 - INFO - Epoch 341 training loss = 4.255
2023-05-15 18:38:48,241 - INFO - Epoch 342 training loss = 4.51
2023-05-15 18:38:52,304 - INFO - Epoch 343 training loss = 4.358
2023-05-15 18:38:56,372 - INFO - Epoch 344 training loss = 4.815
2023-05-15 18:39:00,440 - INFO - Epoch 345 training loss = 4.305
2023-05-15 18:39:04,510 - INFO - Epoch 346 training loss = 5.05
2023-05-15 18:39:08,577 - INFO - Epoch 347 training loss = 4.261
2023-05-15 18:39:12,647 - INFO - Epoch 348 training loss = 5.673
2023-05-15 18:39:16,718 - INFO - Epoch 349 training loss = 5.94
2023-05-15 18:39:20,786 - INFO - Epoch 350 training loss = 5.387
2023-05-15 18:39:21,167 - INFO - Validation loss = 4.571
2023-05-15 18:39:25,235 - INFO - Epoch 351 training loss = 4.335
2023-05-15 18:39:29,303 - INFO - Epoch 352 training loss = 4.549
2023-05-15 18:39:33,370 - INFO - Epoch 353 training loss = 4.213
2023-05-15 18:39:37,440 - INFO - Epoch 354 training loss = 4.669
2023-05-15 18:39:41,508 - INFO - Epoch 355 training loss = 4.459
2023-05-15 18:39:45,579 - INFO - Epoch 356 training loss = 4.349
2023-05-15 18:39:49,647 - INFO - Epoch 357 training loss = 5.182
2023-05-15 18:39:53,715 - INFO - Epoch 358 training loss = 4.13
2023-05-15 18:39:57,782 - INFO - Epoch 359 training loss = 4.755
2023-05-15 18:40:01,851 - INFO - Epoch 360 training loss = 4.501
2023-05-15 18:40:02,233 - INFO - Validation loss = 5.161
2023-05-15 18:40:06,304 - INFO - Epoch 361 training loss = 4.51
2023-05-15 18:40:10,372 - INFO - Epoch 362 training loss = 3.952
2023-05-15 18:40:14,442 - INFO - Epoch 363 training loss = 4.405
2023-05-15 18:40:18,513 - INFO - Epoch 364 training loss = 4.087
2023-05-15 18:40:22,577 - INFO - Epoch 365 training loss = 3.554
2023-05-15 18:40:26,640 - INFO - Epoch 366 training loss = 4.534
2023-05-15 18:40:30,702 - INFO - Epoch 367 training loss = 4.455
2023-05-15 18:40:34,765 - INFO - Epoch 368 training loss = 4.455
2023-05-15 18:40:38,829 - INFO - Epoch 369 training loss = 3.65
2023-05-15 18:40:42,893 - INFO - Epoch 370 training loss = 4.861
2023-05-15 18:40:43,274 - INFO - Validation loss = 12.34
2023-05-15 18:40:47,341 - INFO - Epoch 371 training loss = 4.748
2023-05-15 18:40:51,405 - INFO - Epoch 372 training loss = 3.994
2023-05-15 18:40:55,468 - INFO - Epoch 373 training loss = 4.559
2023-05-15 18:40:59,531 - INFO - Epoch 374 training loss = 4.734
2023-05-15 18:41:03,596 - INFO - Epoch 375 training loss = 4.048
2023-05-15 18:41:07,661 - INFO - Epoch 376 training loss = 3.802
2023-05-15 18:41:11,726 - INFO - Epoch 377 training loss = 4.932
2023-05-15 18:41:15,792 - INFO - Epoch 378 training loss = 4.619
2023-05-15 18:41:19,857 - INFO - Epoch 379 training loss = 4.316
2023-05-15 18:41:23,919 - INFO - Epoch 380 training loss = 4.123
2023-05-15 18:41:24,300 - INFO - Validation loss = 6.539
2023-05-15 18:41:28,364 - INFO - Epoch 381 training loss = 4.902
2023-05-15 18:41:32,427 - INFO - Epoch 382 training loss = 4.285
2023-05-15 18:41:36,491 - INFO - Epoch 383 training loss = 4.135
2023-05-15 18:41:40,556 - INFO - Epoch 384 training loss = 3.779
2023-05-15 18:41:44,624 - INFO - Epoch 385 training loss = 4.472
2023-05-15 18:41:48,690 - INFO - Epoch 386 training loss = 4.272
2023-05-15 18:41:52,755 - INFO - Epoch 387 training loss = 3.801
2023-05-15 18:41:56,818 - INFO - Epoch 388 training loss = 3.827
2023-05-15 18:42:00,881 - INFO - Epoch 389 training loss = 4.228
2023-05-15 18:42:04,947 - INFO - Epoch 390 training loss = 4.31
2023-05-15 18:42:05,328 - INFO - Validation loss = 5.905
2023-05-15 18:42:09,392 - INFO - Epoch 391 training loss = 4.188
2023-05-15 18:42:13,459 - INFO - Epoch 392 training loss = 4.853
2023-05-15 18:42:17,527 - INFO - Epoch 393 training loss = 3.709
2023-05-15 18:42:21,597 - INFO - Epoch 394 training loss = 4.427
2023-05-15 18:42:25,664 - INFO - Epoch 395 training loss = 4.206
2023-05-15 18:42:29,733 - INFO - Epoch 396 training loss = 3.77
2023-05-15 18:42:33,800 - INFO - Epoch 397 training loss = 3.949
2023-05-15 18:42:37,858 - INFO - Epoch 398 training loss = 3.585
2023-05-15 18:42:41,919 - INFO - Epoch 399 training loss = 3.575
2023-05-15 18:42:45,987 - INFO - Epoch 400 training loss = 4.947
2023-05-15 18:42:46,369 - INFO - Validation loss = 3.368
2023-05-15 18:42:46,369 - INFO - best model
2023-05-15 18:42:50,457 - INFO - Epoch 401 training loss = 4.029
2023-05-15 18:42:54,522 - INFO - Epoch 402 training loss = 3.882
2023-05-15 18:42:58,586 - INFO - Epoch 403 training loss = 3.911
2023-05-15 18:43:02,651 - INFO - Epoch 404 training loss = 4.703
2023-05-15 18:43:06,715 - INFO - Epoch 405 training loss = 4.589
2023-05-15 18:43:10,780 - INFO - Epoch 406 training loss = 3.994
2023-05-15 18:43:14,846 - INFO - Epoch 407 training loss = 4.54
2023-05-15 18:43:18,911 - INFO - Epoch 408 training loss = 4.058
2023-05-15 18:43:22,974 - INFO - Epoch 409 training loss = 3.544
2023-05-15 18:43:27,036 - INFO - Epoch 410 training loss = 3.757
2023-05-15 18:43:27,417 - INFO - Validation loss = 3.664
2023-05-15 18:43:31,479 - INFO - Epoch 411 training loss = 3.824
2023-05-15 18:43:35,543 - INFO - Epoch 412 training loss = 3.561
2023-05-15 18:43:39,606 - INFO - Epoch 413 training loss = 3.874
2023-05-15 18:43:43,671 - INFO - Epoch 414 training loss = 4.076
2023-05-15 18:43:47,736 - INFO - Epoch 415 training loss = 4.941
2023-05-15 18:43:51,799 - INFO - Epoch 416 training loss = 3.401
2023-05-15 18:43:55,863 - INFO - Epoch 417 training loss = 3.748
2023-05-15 18:43:59,925 - INFO - Epoch 418 training loss = 3.958
2023-05-15 18:44:03,990 - INFO - Epoch 419 training loss = 4.403
2023-05-15 18:44:08,053 - INFO - Epoch 420 training loss = 4.317
2023-05-15 18:44:08,434 - INFO - Validation loss = 2.942
2023-05-15 18:44:08,434 - INFO - best model
2023-05-15 18:44:12,524 - INFO - Epoch 421 training loss = 3.483
2023-05-15 18:44:16,591 - INFO - Epoch 422 training loss = 4.205
2023-05-15 18:44:20,657 - INFO - Epoch 423 training loss = 3.753
2023-05-15 18:44:24,720 - INFO - Epoch 424 training loss = 3.969
2023-05-15 18:44:28,784 - INFO - Epoch 425 training loss = 3.38
2023-05-15 18:44:32,846 - INFO - Epoch 426 training loss = 4.449
2023-05-15 18:44:36,911 - INFO - Epoch 427 training loss = 3.785
2023-05-15 18:44:40,974 - INFO - Epoch 428 training loss = 3.481
2023-05-15 18:44:45,029 - INFO - Epoch 429 training loss = 3.619
2023-05-15 18:44:49,069 - INFO - Epoch 430 training loss = 4.946
2023-05-15 18:44:49,449 - INFO - Validation loss = 2.554
2023-05-15 18:44:49,449 - INFO - best model
2023-05-15 18:44:53,508 - INFO - Epoch 431 training loss = 3.65
2023-05-15 18:44:57,545 - INFO - Epoch 432 training loss = 3.398
2023-05-15 18:45:01,583 - INFO - Epoch 433 training loss = 4.001
2023-05-15 18:45:05,628 - INFO - Epoch 434 training loss = 4.083
2023-05-15 18:45:09,692 - INFO - Epoch 435 training loss = 3.929
2023-05-15 18:45:13,757 - INFO - Epoch 436 training loss = 3.762
2023-05-15 18:45:17,817 - INFO - Epoch 437 training loss = 5.037
2023-05-15 18:45:21,859 - INFO - Epoch 438 training loss = 3.227
2023-05-15 18:45:25,897 - INFO - Epoch 439 training loss = 3.578
2023-05-15 18:45:29,934 - INFO - Epoch 440 training loss = 3.844
2023-05-15 18:45:30,314 - INFO - Validation loss = 5.859
2023-05-15 18:45:34,351 - INFO - Epoch 441 training loss = 3.981
2023-05-15 18:45:38,389 - INFO - Epoch 442 training loss = 3.719
2023-05-15 18:45:42,426 - INFO - Epoch 443 training loss = 4.31
2023-05-15 18:45:46,481 - INFO - Epoch 444 training loss = 3.222
2023-05-15 18:45:50,550 - INFO - Epoch 445 training loss = 4.131
2023-05-15 18:45:54,612 - INFO - Epoch 446 training loss = 4.348
2023-05-15 18:45:58,674 - INFO - Epoch 447 training loss =  3.3
2023-05-15 18:46:02,737 - INFO - Epoch 448 training loss = 3.934
2023-05-15 18:46:06,800 - INFO - Epoch 449 training loss = 3.507
2023-05-15 18:46:10,865 - INFO - Epoch 450 training loss = 4.195
2023-05-15 18:46:11,246 - INFO - Validation loss = 3.294
2023-05-15 18:46:15,312 - INFO - Epoch 451 training loss = 4.192
2023-05-15 18:46:19,388 - INFO - Epoch 452 training loss = 3.62
2023-05-15 18:46:23,452 - INFO - Epoch 453 training loss = 4.001
2023-05-15 18:46:27,515 - INFO - Epoch 454 training loss = 3.313
2023-05-15 18:46:31,577 - INFO - Epoch 455 training loss = 4.203
2023-05-15 18:46:35,642 - INFO - Epoch 456 training loss = 5.082
2023-05-15 18:46:39,713 - INFO - Epoch 457 training loss = 2.86
2023-05-15 18:46:43,780 - INFO - Epoch 458 training loss = 3.479
2023-05-15 18:46:47,851 - INFO - Epoch 459 training loss = 4.083
2023-05-15 18:46:51,920 - INFO - Epoch 460 training loss = 4.795
2023-05-15 18:46:52,301 - INFO - Validation loss = 4.692
2023-05-15 18:46:56,368 - INFO - Epoch 461 training loss = 3.556
2023-05-15 18:47:00,415 - INFO - Epoch 462 training loss = 3.709
2023-05-15 18:47:04,458 - INFO - Epoch 463 training loss = 4.011
2023-05-15 18:47:08,508 - INFO - Epoch 464 training loss = 3.657
2023-05-15 18:47:12,559 - INFO - Epoch 465 training loss = 3.594
2023-05-15 18:47:16,610 - INFO - Epoch 466 training loss = 5.198
2023-05-15 18:47:20,660 - INFO - Epoch 467 training loss = 3.432
2023-05-15 18:47:24,709 - INFO - Epoch 468 training loss = 3.773
2023-05-15 18:47:28,757 - INFO - Epoch 469 training loss = 3.215
2023-05-15 18:47:32,807 - INFO - Epoch 470 training loss = 3.856
2023-05-15 18:47:33,187 - INFO - Validation loss = 9.937
2023-05-15 18:47:37,238 - INFO - Epoch 471 training loss = 3.525
2023-05-15 18:47:41,287 - INFO - Epoch 472 training loss = 3.928
2023-05-15 18:47:45,339 - INFO - Epoch 473 training loss = 4.629
2023-05-15 18:47:49,391 - INFO - Epoch 474 training loss = 3.451
2023-05-15 18:47:53,441 - INFO - Epoch 475 training loss = 4.507
2023-05-15 18:47:57,489 - INFO - Epoch 476 training loss = 3.276
2023-05-15 18:48:01,539 - INFO - Epoch 477 training loss = 3.332
2023-05-15 18:48:05,589 - INFO - Epoch 478 training loss = 4.503
2023-05-15 18:48:09,639 - INFO - Epoch 479 training loss = 3.199
2023-05-15 18:48:13,691 - INFO - Epoch 480 training loss = 3.368
2023-05-15 18:48:14,071 - INFO - Validation loss = 4.042
2023-05-15 18:48:18,122 - INFO - Epoch 481 training loss = 3.231
2023-05-15 18:48:22,171 - INFO - Epoch 482 training loss = 3.291
2023-05-15 18:48:26,220 - INFO - Epoch 483 training loss =  3.2
2023-05-15 18:48:30,269 - INFO - Epoch 484 training loss = 3.639
2023-05-15 18:48:34,317 - INFO - Epoch 485 training loss = 4.027
2023-05-15 18:48:38,366 - INFO - Epoch 486 training loss = 3.135
2023-05-15 18:48:42,417 - INFO - Epoch 487 training loss = 3.957
2023-05-15 18:48:46,468 - INFO - Epoch 488 training loss = 3.496
2023-05-15 18:48:50,519 - INFO - Epoch 489 training loss = 3.09
2023-05-15 18:48:54,567 - INFO - Epoch 490 training loss = 3.459
2023-05-15 18:48:54,947 - INFO - Validation loss = 4.318
2023-05-15 18:48:58,997 - INFO - Epoch 491 training loss = 4.134
2023-05-15 18:49:03,046 - INFO - Epoch 492 training loss = 3.736
2023-05-15 18:49:07,098 - INFO - Epoch 493 training loss = 3.16
2023-05-15 18:49:11,149 - INFO - Epoch 494 training loss = 3.254
2023-05-15 18:49:15,201 - INFO - Epoch 495 training loss = 3.454
2023-05-15 18:49:19,251 - INFO - Epoch 496 training loss = 3.332
2023-05-15 18:49:23,299 - INFO - Epoch 497 training loss = 3.206
2023-05-15 18:49:27,348 - INFO - Epoch 498 training loss = 4.791
2023-05-15 18:49:31,395 - INFO - Epoch 499 training loss = 3.565
2023-05-15 18:49:35,444 - INFO - Epoch 500 training loss = 3.112
2023-05-15 18:49:35,825 - INFO - Validation loss = 4.925
2023-05-15 18:49:39,874 - INFO - Epoch 501 training loss =  3.2
2023-05-15 18:49:43,924 - INFO - Epoch 502 training loss = 3.355
2023-05-15 18:49:47,976 - INFO - Epoch 503 training loss = 2.812
2023-05-15 18:49:52,026 - INFO - Epoch 504 training loss = 2.986
2023-05-15 18:49:56,074 - INFO - Epoch 505 training loss = 3.49
2023-05-15 18:50:00,124 - INFO - Epoch 506 training loss = 3.347
2023-05-15 18:50:04,176 - INFO - Epoch 507 training loss = 2.844
2023-05-15 18:50:08,226 - INFO - Epoch 508 training loss = 3.263
2023-05-15 18:50:12,277 - INFO - Epoch 509 training loss = 4.199
2023-05-15 18:50:16,329 - INFO - Epoch 510 training loss = 2.91
2023-05-15 18:50:16,709 - INFO - Validation loss = 3.052
2023-05-15 18:50:20,773 - INFO - Epoch 511 training loss = 3.722
2023-05-15 18:50:24,834 - INFO - Epoch 512 training loss = 3.026
2023-05-15 18:50:28,896 - INFO - Epoch 513 training loss = 2.894
2023-05-15 18:50:32,958 - INFO - Epoch 514 training loss = 3.475
2023-05-15 18:50:37,021 - INFO - Epoch 515 training loss = 2.944
2023-05-15 18:50:41,083 - INFO - Epoch 516 training loss = 3.423
2023-05-15 18:50:45,148 - INFO - Epoch 517 training loss = 2.941
2023-05-15 18:50:49,213 - INFO - Epoch 518 training loss = 3.583
2023-05-15 18:50:53,275 - INFO - Epoch 519 training loss = 3.203
2023-05-15 18:50:57,337 - INFO - Epoch 520 training loss = 3.548
2023-05-15 18:50:57,719 - INFO - Validation loss = 3.894
2023-05-15 18:51:01,797 - INFO - Epoch 521 training loss = 2.931
2023-05-15 18:51:05,877 - INFO - Epoch 522 training loss = 2.977
2023-05-15 18:51:09,955 - INFO - Epoch 523 training loss = 3.261
2023-05-15 18:51:14,032 - INFO - Epoch 524 training loss = 3.419
2023-05-15 18:51:18,112 - INFO - Epoch 525 training loss = 2.669
2023-05-15 18:51:22,188 - INFO - Epoch 526 training loss = 2.933
2023-05-15 18:51:26,263 - INFO - Epoch 527 training loss = 2.814
2023-05-15 18:51:30,338 - INFO - Epoch 528 training loss = 3.314
2023-05-15 18:51:34,414 - INFO - Epoch 529 training loss = 3.356
2023-05-15 18:51:38,491 - INFO - Epoch 530 training loss = 2.75
2023-05-15 18:51:38,873 - INFO - Validation loss = 2.728
2023-05-15 18:51:42,952 - INFO - Epoch 531 training loss = 2.876
2023-05-15 18:51:47,031 - INFO - Epoch 532 training loss = 3.279
2023-05-15 18:51:51,109 - INFO - Epoch 533 training loss = 3.035
2023-05-15 18:51:55,184 - INFO - Epoch 534 training loss = 3.181
2023-05-15 18:51:59,260 - INFO - Epoch 535 training loss = 3.051
2023-05-15 18:52:03,338 - INFO - Epoch 536 training loss = 3.265
2023-05-15 18:52:07,398 - INFO - Epoch 537 training loss = 2.711
2023-05-15 18:52:11,451 - INFO - Epoch 538 training loss = 2.788
2023-05-15 18:52:15,504 - INFO - Epoch 539 training loss = 3.356
2023-05-15 18:52:19,555 - INFO - Epoch 540 training loss = 3.328
2023-05-15 18:52:19,937 - INFO - Validation loss = 3.039
2023-05-15 18:52:23,986 - INFO - Epoch 541 training loss = 2.636
2023-05-15 18:52:28,050 - INFO - Epoch 542 training loss = 2.467
2023-05-15 18:52:32,125 - INFO - Epoch 543 training loss = 3.104
2023-05-15 18:52:36,201 - INFO - Epoch 544 training loss = 3.447
2023-05-15 18:52:40,278 - INFO - Epoch 545 training loss = 2.959
2023-05-15 18:52:44,356 - INFO - Epoch 546 training loss = 2.729
2023-05-15 18:52:48,435 - INFO - Epoch 547 training loss = 2.525
2023-05-15 18:52:52,512 - INFO - Epoch 548 training loss = 2.733
2023-05-15 18:52:56,587 - INFO - Epoch 549 training loss = 3.087
2023-05-15 18:53:00,663 - INFO - Epoch 550 training loss = 3.033
2023-05-15 18:53:01,045 - INFO - Validation loss = 3.302
2023-05-15 18:53:05,123 - INFO - Epoch 551 training loss = 3.315
2023-05-15 18:53:09,200 - INFO - Epoch 552 training loss = 2.87
2023-05-15 18:53:13,276 - INFO - Epoch 553 training loss = 3.181
2023-05-15 18:53:17,355 - INFO - Epoch 554 training loss = 3.023
2023-05-15 18:53:21,431 - INFO - Epoch 555 training loss = 2.811
2023-05-15 18:53:25,507 - INFO - Epoch 556 training loss = 3.204
2023-05-15 18:53:29,581 - INFO - Epoch 557 training loss = 3.267
2023-05-15 18:53:33,657 - INFO - Epoch 558 training loss = 2.583
2023-05-15 18:53:37,733 - INFO - Epoch 559 training loss = 2.776
2023-05-15 18:53:41,812 - INFO - Epoch 560 training loss = 3.541
2023-05-15 18:53:42,195 - INFO - Validation loss = 3.573
2023-05-15 18:53:46,275 - INFO - Epoch 561 training loss = 3.061
2023-05-15 18:53:50,353 - INFO - Epoch 562 training loss = 3.048
2023-05-15 18:53:54,428 - INFO - Epoch 563 training loss = 3.334
2023-05-15 18:53:58,482 - INFO - Epoch 564 training loss = 2.601
2023-05-15 18:54:02,533 - INFO - Epoch 565 training loss = 3.212
2023-05-15 18:54:06,585 - INFO - Epoch 566 training loss = 2.532
2023-05-15 18:54:10,637 - INFO - Epoch 567 training loss = 3.319
2023-05-15 18:54:14,691 - INFO - Epoch 568 training loss = 2.658
2023-05-15 18:54:18,743 - INFO - Epoch 569 training loss = 2.92
2023-05-15 18:54:22,792 - INFO - Epoch 570 training loss = 2.305
2023-05-15 18:54:23,173 - INFO - Validation loss = 3.031
2023-05-15 18:54:27,225 - INFO - Epoch 571 training loss = 2.538
2023-05-15 18:54:31,275 - INFO - Epoch 572 training loss = 2.866
2023-05-15 18:54:35,332 - INFO - Epoch 573 training loss = 2.923
2023-05-15 18:54:39,408 - INFO - Epoch 574 training loss = 2.545
2023-05-15 18:54:43,487 - INFO - Epoch 575 training loss = 2.534
2023-05-15 18:54:47,568 - INFO - Epoch 576 training loss = 3.409
2023-05-15 18:54:51,645 - INFO - Epoch 577 training loss = 3.041
2023-05-15 18:54:55,721 - INFO - Epoch 578 training loss = 3.191
2023-05-15 18:54:59,798 - INFO - Epoch 579 training loss = 3.051
2023-05-15 18:55:03,875 - INFO - Epoch 580 training loss = 2.799
2023-05-15 18:55:04,272 - INFO - Validation loss = 2.64
2023-05-15 18:55:08,350 - INFO - Epoch 581 training loss = 3.459
2023-05-15 18:55:12,428 - INFO - Epoch 582 training loss = 2.491
2023-05-15 18:55:16,509 - INFO - Epoch 583 training loss = 3.194
2023-05-15 18:55:20,587 - INFO - Epoch 584 training loss = 2.74
2023-05-15 18:55:24,664 - INFO - Epoch 585 training loss = 2.829
2023-05-15 18:55:28,740 - INFO - Epoch 586 training loss = 2.906
2023-05-15 18:55:32,817 - INFO - Epoch 587 training loss =  2.9
2023-05-15 18:55:36,894 - INFO - Epoch 588 training loss = 2.639
2023-05-15 18:55:40,971 - INFO - Epoch 589 training loss = 2.857
2023-05-15 18:55:45,050 - INFO - Epoch 590 training loss = 2.533
2023-05-15 18:55:45,432 - INFO - Validation loss = 2.702
2023-05-15 18:55:49,502 - INFO - Epoch 591 training loss = 2.563
2023-05-15 18:55:53,552 - INFO - Epoch 592 training loss = 2.85
2023-05-15 18:55:57,603 - INFO - Epoch 593 training loss = 3.022
2023-05-15 18:56:01,655 - INFO - Epoch 594 training loss = 3.38
2023-05-15 18:56:05,714 - INFO - Epoch 595 training loss = 2.544
2023-05-15 18:56:09,792 - INFO - Epoch 596 training loss = 2.894
2023-05-15 18:56:13,871 - INFO - Epoch 597 training loss = 2.436
2023-05-15 18:56:17,951 - INFO - Epoch 598 training loss = 2.829
2023-05-15 18:56:22,029 - INFO - Epoch 599 training loss = 2.848
2023-05-15 18:56:26,105 - INFO - Epoch 600 training loss = 2.688
2023-05-15 18:56:26,488 - INFO - Validation loss = 3.255
2023-05-15 18:56:30,564 - INFO - Epoch 601 training loss = 2.504
2023-05-15 18:56:34,641 - INFO - Epoch 602 training loss = 2.547
2023-05-15 18:56:38,718 - INFO - Epoch 603 training loss = 2.592
2023-05-15 18:56:42,796 - INFO - Epoch 604 training loss = 3.506
2023-05-15 18:56:46,876 - INFO - Epoch 605 training loss = 2.94
2023-05-15 18:56:50,947 - INFO - Epoch 606 training loss = 2.973
2023-05-15 18:56:54,995 - INFO - Epoch 607 training loss = 2.965
2023-05-15 18:56:59,045 - INFO - Epoch 608 training loss = 3.133
2023-05-15 18:57:03,096 - INFO - Epoch 609 training loss = 2.851
2023-05-15 18:57:07,148 - INFO - Epoch 610 training loss = 2.627
2023-05-15 18:57:07,529 - INFO - Validation loss = 3.129
2023-05-15 18:57:11,582 - INFO - Epoch 611 training loss = 2.373
2023-05-15 18:57:15,637 - INFO - Epoch 612 training loss = 3.158
2023-05-15 18:57:19,689 - INFO - Epoch 613 training loss = 2.33
2023-05-15 18:57:23,739 - INFO - Epoch 614 training loss = 2.712
2023-05-15 18:57:27,789 - INFO - Epoch 615 training loss = 2.536
2023-05-15 18:57:31,840 - INFO - Epoch 616 training loss = 3.207
2023-05-15 18:57:35,890 - INFO - Epoch 617 training loss = 2.744
2023-05-15 18:57:39,942 - INFO - Epoch 618 training loss = 2.065
2023-05-15 18:57:43,995 - INFO - Epoch 619 training loss = 2.549
2023-05-15 18:57:48,049 - INFO - Epoch 620 training loss = 2.615
2023-05-15 18:57:48,430 - INFO - Validation loss = 2.958
2023-05-15 18:57:52,482 - INFO - Epoch 621 training loss = 2.57
2023-05-15 18:57:56,549 - INFO - Epoch 622 training loss = 2.714
2023-05-15 18:58:00,626 - INFO - Epoch 623 training loss = 2.773
2023-05-15 18:58:04,704 - INFO - Epoch 624 training loss = 2.696
2023-05-15 18:58:08,781 - INFO - Epoch 625 training loss = 2.563
2023-05-15 18:58:12,860 - INFO - Epoch 626 training loss = 2.449
2023-05-15 18:58:16,941 - INFO - Epoch 627 training loss = 2.439
2023-05-15 18:58:21,019 - INFO - Epoch 628 training loss = 2.635
2023-05-15 18:58:25,095 - INFO - Epoch 629 training loss = 2.566
2023-05-15 18:58:29,170 - INFO - Epoch 630 training loss = 2.548
2023-05-15 18:58:29,551 - INFO - Validation loss = 4.405
2023-05-15 18:58:33,624 - INFO - Epoch 631 training loss = 2.341
2023-05-15 18:58:37,675 - INFO - Epoch 632 training loss = 2.552
2023-05-15 18:58:41,726 - INFO - Epoch 633 training loss = 2.666
2023-05-15 18:58:45,790 - INFO - Epoch 634 training loss = 2.476
2023-05-15 18:58:49,869 - INFO - Epoch 635 training loss = 2.452
2023-05-15 18:58:53,944 - INFO - Epoch 636 training loss = 2.766
2023-05-15 18:58:58,021 - INFO - Epoch 637 training loss = 2.159
2023-05-15 18:59:02,097 - INFO - Epoch 638 training loss = 2.802
2023-05-15 18:59:06,174 - INFO - Epoch 639 training loss = 2.117
2023-05-15 18:59:10,251 - INFO - Epoch 640 training loss = 2.292
2023-05-15 18:59:10,633 - INFO - Validation loss = 2.281
2023-05-15 18:59:10,633 - INFO - best model
2023-05-15 18:59:14,733 - INFO - Epoch 641 training loss = 2.832
2023-05-15 18:59:18,810 - INFO - Epoch 642 training loss = 2.319
2023-05-15 18:59:22,865 - INFO - Epoch 643 training loss = 2.597
2023-05-15 18:59:26,913 - INFO - Epoch 644 training loss = 2.191
2023-05-15 18:59:30,963 - INFO - Epoch 645 training loss = 2.451
2023-05-15 18:59:35,010 - INFO - Epoch 646 training loss = 2.084
2023-05-15 18:59:39,060 - INFO - Epoch 647 training loss = 2.451
2023-05-15 18:59:43,113 - INFO - Epoch 648 training loss = 2.099
2023-05-15 18:59:47,164 - INFO - Epoch 649 training loss = 2.468
2023-05-15 18:59:51,215 - INFO - Epoch 650 training loss = 2.084
2023-05-15 18:59:51,597 - INFO - Validation loss = 2.273
2023-05-15 18:59:51,597 - INFO - best model
2023-05-15 18:59:55,670 - INFO - Epoch 651 training loss = 2.667
2023-05-15 18:59:59,718 - INFO - Epoch 652 training loss = 2.804
2023-05-15 19:00:03,781 - INFO - Epoch 653 training loss = 2.149
2023-05-15 19:00:07,856 - INFO - Epoch 654 training loss = 2.381
2023-05-15 19:00:11,932 - INFO - Epoch 655 training loss = 2.324
2023-05-15 19:00:16,012 - INFO - Epoch 656 training loss = 2.456
2023-05-15 19:00:20,090 - INFO - Epoch 657 training loss = 2.317
2023-05-15 19:00:24,166 - INFO - Epoch 658 training loss = 2.096
2023-05-15 19:00:28,241 - INFO - Epoch 659 training loss = 2.168
2023-05-15 19:00:32,318 - INFO - Epoch 660 training loss = 2.304
2023-05-15 19:00:32,700 - INFO - Validation loss = 2.46
2023-05-15 19:00:36,778 - INFO - Epoch 661 training loss = 2.553
2023-05-15 19:00:40,856 - INFO - Epoch 662 training loss = 2.134
2023-05-15 19:00:44,935 - INFO - Epoch 663 training loss = 2.367
2023-05-15 19:00:49,015 - INFO - Epoch 664 training loss = 2.416
2023-05-15 19:00:53,092 - INFO - Epoch 665 training loss = 3.071
2023-05-15 19:00:57,169 - INFO - Epoch 666 training loss = 2.678
2023-05-15 19:01:01,246 - INFO - Epoch 667 training loss = 2.751
2023-05-15 19:01:05,324 - INFO - Epoch 668 training loss = 2.358
2023-05-15 19:01:09,402 - INFO - Epoch 669 training loss = 2.37
2023-05-15 19:01:13,466 - INFO - Epoch 670 training loss = 2.359
2023-05-15 19:01:13,848 - INFO - Validation loss = 2.719
2023-05-15 19:01:17,897 - INFO - Epoch 671 training loss = 1.993
2023-05-15 19:01:21,944 - INFO - Epoch 672 training loss = 2.176
2023-05-15 19:01:25,990 - INFO - Epoch 673 training loss = 2.913
2023-05-15 19:01:30,035 - INFO - Epoch 674 training loss = 2.436
2023-05-15 19:01:34,080 - INFO - Epoch 675 training loss = 2.404
2023-05-15 19:01:38,127 - INFO - Epoch 676 training loss = 2.834
2023-05-15 19:01:42,176 - INFO - Epoch 677 training loss = 2.377
2023-05-15 19:01:46,224 - INFO - Epoch 678 training loss = 2.222
2023-05-15 19:01:50,273 - INFO - Epoch 679 training loss = 2.005
2023-05-15 19:01:54,318 - INFO - Epoch 680 training loss = 2.247
2023-05-15 19:01:54,699 - INFO - Validation loss = 3.373
2023-05-15 19:01:58,746 - INFO - Epoch 681 training loss = 2.198
2023-05-15 19:02:02,793 - INFO - Epoch 682 training loss = 2.228
2023-05-15 19:02:06,842 - INFO - Epoch 683 training loss = 2.043
2023-05-15 19:02:10,890 - INFO - Epoch 684 training loss = 2.104
2023-05-15 19:02:14,941 - INFO - Epoch 685 training loss = 1.945
2023-05-15 19:02:18,990 - INFO - Epoch 686 training loss = 2.16
2023-05-15 19:02:23,059 - INFO - Epoch 687 training loss = 2.378
2023-05-15 19:02:27,131 - INFO - Epoch 688 training loss = 2.285
2023-05-15 19:02:31,203 - INFO - Epoch 689 training loss = 2.24
2023-05-15 19:02:35,279 - INFO - Epoch 690 training loss = 2.102
2023-05-15 19:02:35,661 - INFO - Validation loss = 3.471
2023-05-15 19:02:39,738 - INFO - Epoch 691 training loss = 2.063
2023-05-15 19:02:43,816 - INFO - Epoch 692 training loss = 2.062
2023-05-15 19:02:47,896 - INFO - Epoch 693 training loss = 1.775
2023-05-15 19:02:51,972 - INFO - Epoch 694 training loss = 2.142
2023-05-15 19:02:56,031 - INFO - Epoch 695 training loss = 2.056
2023-05-15 19:03:00,088 - INFO - Epoch 696 training loss = 2.104
2023-05-15 19:03:04,165 - INFO - Epoch 697 training loss = 1.566
2023-05-15 19:03:08,242 - INFO - Epoch 698 training loss = 2.15
2023-05-15 19:03:12,321 - INFO - Epoch 699 training loss = 2.426
2023-05-15 19:03:16,400 - INFO - Epoch 700 training loss = 1.596
2023-05-15 19:03:16,782 - INFO - Validation loss = 3.47
2023-05-15 19:03:20,860 - INFO - Epoch 701 training loss = 2.154
2023-05-15 19:03:24,937 - INFO - Epoch 702 training loss = 2.099
2023-05-15 19:03:29,010 - INFO - Epoch 703 training loss = 1.901
2023-05-15 19:03:33,082 - INFO - Epoch 704 training loss = 2.108
2023-05-15 19:03:37,155 - INFO - Epoch 705 training loss = 2.24
2023-05-15 19:03:41,227 - INFO - Epoch 706 training loss = 2.269
2023-05-15 19:03:45,302 - INFO - Epoch 707 training loss = 1.844
2023-05-15 19:03:49,376 - INFO - Epoch 708 training loss = 1.933
2023-05-15 19:03:53,448 - INFO - Epoch 709 training loss = 1.868
2023-05-15 19:03:57,520 - INFO - Epoch 710 training loss = 2.565
2023-05-15 19:03:57,902 - INFO - Validation loss = 2.27
2023-05-15 19:03:57,902 - INFO - best model
2023-05-15 19:04:01,997 - INFO - Epoch 711 training loss = 2.154
2023-05-15 19:04:06,071 - INFO - Epoch 712 training loss = 1.873
2023-05-15 19:04:10,144 - INFO - Epoch 713 training loss = 1.949
2023-05-15 19:04:14,220 - INFO - Epoch 714 training loss = 2.304
2023-05-15 19:04:18,294 - INFO - Epoch 715 training loss = 2.021
2023-05-15 19:04:22,368 - INFO - Epoch 716 training loss = 1.825
2023-05-15 19:04:26,439 - INFO - Epoch 717 training loss = 1.715
2023-05-15 19:04:30,511 - INFO - Epoch 718 training loss = 1.801
2023-05-15 19:04:34,582 - INFO - Epoch 719 training loss = 2.531
2023-05-15 19:04:38,656 - INFO - Epoch 720 training loss = 1.648
2023-05-15 19:04:39,038 - INFO - Validation loss = 2.478
2023-05-15 19:04:43,113 - INFO - Epoch 721 training loss = 2.07
2023-05-15 19:04:47,188 - INFO - Epoch 722 training loss = 1.747
2023-05-15 19:04:51,261 - INFO - Epoch 723 training loss = 2.135
2023-05-15 19:04:55,332 - INFO - Epoch 724 training loss = 2.012
2023-05-15 19:04:59,403 - INFO - Epoch 725 training loss = 2.081
2023-05-15 19:05:03,477 - INFO - Epoch 726 training loss = 1.585
2023-05-15 19:05:07,549 - INFO - Epoch 727 training loss = 1.954
2023-05-15 19:05:11,624 - INFO - Epoch 728 training loss = 1.976
2023-05-15 19:05:15,699 - INFO - Epoch 729 training loss = 1.607
2023-05-15 19:05:19,772 - INFO - Epoch 730 training loss = 2.068
2023-05-15 19:05:20,155 - INFO - Validation loss = 2.052
2023-05-15 19:05:20,155 - INFO - best model
2023-05-15 19:05:24,250 - INFO - Epoch 731 training loss = 1.611
2023-05-15 19:05:28,322 - INFO - Epoch 732 training loss = 1.896
2023-05-15 19:05:32,394 - INFO - Epoch 733 training loss = 2.021
2023-05-15 19:05:36,466 - INFO - Epoch 734 training loss = 2.096
2023-05-15 19:05:40,538 - INFO - Epoch 735 training loss = 1.799
2023-05-15 19:05:44,612 - INFO - Epoch 736 training loss = 1.782
2023-05-15 19:05:48,687 - INFO - Epoch 737 training loss = 2.108
2023-05-15 19:05:52,758 - INFO - Epoch 738 training loss = 1.915
2023-05-15 19:05:56,818 - INFO - Epoch 739 training loss = 1.619
2023-05-15 19:06:00,877 - INFO - Epoch 740 training loss = 2.304
2023-05-15 19:06:01,259 - INFO - Validation loss = 1.753
2023-05-15 19:06:01,259 - INFO - best model
2023-05-15 19:06:05,342 - INFO - Epoch 741 training loss = 2.237
2023-05-15 19:06:09,400 - INFO - Epoch 742 training loss = 1.447
2023-05-15 19:06:13,462 - INFO - Epoch 743 training loss = 1.806
2023-05-15 19:06:17,522 - INFO - Epoch 744 training loss = 1.689
2023-05-15 19:06:21,583 - INFO - Epoch 745 training loss = 1.705
2023-05-15 19:06:25,641 - INFO - Epoch 746 training loss = 1.701
2023-05-15 19:06:29,699 - INFO - Epoch 747 training loss = 1.728
2023-05-15 19:06:33,756 - INFO - Epoch 748 training loss = 1.82
2023-05-15 19:06:37,816 - INFO - Epoch 749 training loss = 1.691
2023-05-15 19:06:41,877 - INFO - Epoch 750 training loss = 1.627
2023-05-15 19:06:42,259 - INFO - Validation loss = 2.048
2023-05-15 19:06:46,320 - INFO - Epoch 751 training loss = 1.572
2023-05-15 19:06:50,380 - INFO - Epoch 752 training loss = 1.845
2023-05-15 19:06:54,438 - INFO - Epoch 753 training loss = 1.63
2023-05-15 19:06:58,495 - INFO - Epoch 754 training loss = 1.789
2023-05-15 19:07:02,554 - INFO - Epoch 755 training loss = 1.907
2023-05-15 19:07:06,615 - INFO - Epoch 756 training loss = 1.95
2023-05-15 19:07:10,674 - INFO - Epoch 757 training loss = 1.824
2023-05-15 19:07:14,735 - INFO - Epoch 758 training loss = 1.868
2023-05-15 19:07:18,796 - INFO - Epoch 759 training loss = 1.883
2023-05-15 19:07:22,858 - INFO - Epoch 760 training loss = 1.834
2023-05-15 19:07:23,239 - INFO - Validation loss = 2.817
2023-05-15 19:07:27,300 - INFO - Epoch 761 training loss = 1.801
2023-05-15 19:07:31,362 - INFO - Epoch 762 training loss = 1.851
2023-05-15 19:07:35,424 - INFO - Epoch 763 training loss = 1.642
2023-05-15 19:07:39,486 - INFO - Epoch 764 training loss = 1.402
2023-05-15 19:07:43,550 - INFO - Epoch 765 training loss = 1.646
2023-05-15 19:07:47,611 - INFO - Epoch 766 training loss = 1.966
2023-05-15 19:07:51,671 - INFO - Epoch 767 training loss = 1.575
2023-05-15 19:07:55,730 - INFO - Epoch 768 training loss = 1.495
2023-05-15 19:07:59,792 - INFO - Epoch 769 training loss = 1.573
2023-05-15 19:08:03,854 - INFO - Epoch 770 training loss = 1.802
2023-05-15 19:08:04,235 - INFO - Validation loss = 1.978
2023-05-15 19:08:08,295 - INFO - Epoch 771 training loss = 1.692
2023-05-15 19:08:12,360 - INFO - Epoch 772 training loss = 2.026
2023-05-15 19:08:16,424 - INFO - Epoch 773 training loss = 1.771
2023-05-15 19:08:20,488 - INFO - Epoch 774 training loss = 1.791
2023-05-15 19:08:24,550 - INFO - Epoch 775 training loss = 1.47
2023-05-15 19:08:28,612 - INFO - Epoch 776 training loss = 1.987
2023-05-15 19:08:32,673 - INFO - Epoch 777 training loss = 1.645
2023-05-15 19:08:36,737 - INFO - Epoch 778 training loss = 1.604
2023-05-15 19:08:40,800 - INFO - Epoch 779 training loss = 1.823
2023-05-15 19:08:44,865 - INFO - Epoch 780 training loss = 1.541
2023-05-15 19:08:45,247 - INFO - Validation loss = 1.456
2023-05-15 19:08:45,247 - INFO - best model
2023-05-15 19:08:49,334 - INFO - Epoch 781 training loss = 1.399
2023-05-15 19:08:53,396 - INFO - Epoch 782 training loss = 1.395
2023-05-15 19:08:57,457 - INFO - Epoch 783 training loss = 1.642
2023-05-15 19:09:01,520 - INFO - Epoch 784 training loss = 1.635
2023-05-15 19:09:05,583 - INFO - Epoch 785 training loss = 1.603
2023-05-15 19:09:09,645 - INFO - Epoch 786 training loss = 1.889
2023-05-15 19:09:13,710 - INFO - Epoch 787 training loss = 1.778
2023-05-15 19:09:17,775 - INFO - Epoch 788 training loss = 1.589
2023-05-15 19:09:21,838 - INFO - Epoch 789 training loss = 2.096
2023-05-15 19:09:25,899 - INFO - Epoch 790 training loss = 1.579
2023-05-15 19:09:26,280 - INFO - Validation loss = 2.532
2023-05-15 19:09:30,343 - INFO - Epoch 791 training loss = 1.628
2023-05-15 19:09:34,405 - INFO - Epoch 792 training loss = 1.986
2023-05-15 19:09:38,467 - INFO - Epoch 793 training loss = 1.827
2023-05-15 19:09:42,522 - INFO - Epoch 794 training loss = 1.446
2023-05-15 19:09:46,576 - INFO - Epoch 795 training loss = 1.302
2023-05-15 19:09:50,634 - INFO - Epoch 796 training loss = 1.385
2023-05-15 19:09:54,696 - INFO - Epoch 797 training loss =  1.2
2023-05-15 19:09:58,758 - INFO - Epoch 798 training loss = 1.56
2023-05-15 19:10:02,824 - INFO - Epoch 799 training loss = 1.615
2023-05-15 19:10:06,887 - INFO - Epoch 800 training loss = 1.419
2023-05-15 19:10:07,269 - INFO - Validation loss = 1.744
2023-05-15 19:10:11,335 - INFO - Epoch 801 training loss = 1.323
2023-05-15 19:10:15,400 - INFO - Epoch 802 training loss = 1.785
2023-05-15 19:10:19,465 - INFO - Epoch 803 training loss = 2.029
2023-05-15 19:10:23,527 - INFO - Epoch 804 training loss = 1.465
2023-05-15 19:10:27,588 - INFO - Epoch 805 training loss = 1.674
2023-05-15 19:10:31,647 - INFO - Epoch 806 training loss = 1.496
2023-05-15 19:10:35,706 - INFO - Epoch 807 training loss = 1.297
2023-05-15 19:10:39,764 - INFO - Epoch 808 training loss = 1.387
2023-05-15 19:10:43,826 - INFO - Epoch 809 training loss = 1.451
2023-05-15 19:10:47,889 - INFO - Epoch 810 training loss = 1.536
2023-05-15 19:10:48,270 - INFO - Validation loss = 1.694
2023-05-15 19:10:52,330 - INFO - Epoch 811 training loss = 1.455
2023-05-15 19:10:56,388 - INFO - Epoch 812 training loss = 1.787
2023-05-15 19:11:00,447 - INFO - Epoch 813 training loss = 1.201
2023-05-15 19:11:04,506 - INFO - Epoch 814 training loss = 1.517
2023-05-15 19:11:08,566 - INFO - Epoch 815 training loss = 1.586
2023-05-15 19:11:12,627 - INFO - Epoch 816 training loss = 1.193
2023-05-15 19:11:16,689 - INFO - Epoch 817 training loss = 1.791
2023-05-15 19:11:20,749 - INFO - Epoch 818 training loss = 1.388
2023-05-15 19:11:24,806 - INFO - Epoch 819 training loss = 1.346
2023-05-15 19:11:28,864 - INFO - Epoch 820 training loss = 1.436
2023-05-15 19:11:29,246 - INFO - Validation loss = 1.38
2023-05-15 19:11:29,246 - INFO - best model
2023-05-15 19:11:33,331 - INFO - Epoch 821 training loss = 1.371
2023-05-15 19:11:37,392 - INFO - Epoch 822 training loss = 1.461
2023-05-15 19:11:41,450 - INFO - Epoch 823 training loss = 1.643
2023-05-15 19:11:45,511 - INFO - Epoch 824 training loss = 1.241
2023-05-15 19:11:49,571 - INFO - Epoch 825 training loss = 1.412
2023-05-15 19:11:53,630 - INFO - Epoch 826 training loss = 1.236
2023-05-15 19:11:57,700 - INFO - Epoch 827 training loss = 1.246
2023-05-15 19:12:01,760 - INFO - Epoch 828 training loss = 1.505
2023-05-15 19:12:05,820 - INFO - Epoch 829 training loss = 1.439
2023-05-15 19:12:09,879 - INFO - Epoch 830 training loss = 1.196
2023-05-15 19:12:10,261 - INFO - Validation loss = 1.634
2023-05-15 19:12:14,324 - INFO - Epoch 831 training loss = 1.225
2023-05-15 19:12:18,385 - INFO - Epoch 832 training loss = 1.426
2023-05-15 19:12:22,435 - INFO - Epoch 833 training loss = 1.301
2023-05-15 19:12:26,485 - INFO - Epoch 834 training loss = 1.249
2023-05-15 19:12:30,535 - INFO - Epoch 835 training loss = 1.376
2023-05-15 19:12:34,586 - INFO - Epoch 836 training loss = 1.284
2023-05-15 19:12:38,638 - INFO - Epoch 837 training loss = 1.441
2023-05-15 19:12:42,690 - INFO - Epoch 838 training loss = 1.435
2023-05-15 19:12:46,744 - INFO - Epoch 839 training loss = 1.607
2023-05-15 19:12:50,800 - INFO - Epoch 840 training loss = 1.521
2023-05-15 19:12:51,182 - INFO - Validation loss = 2.731
2023-05-15 19:12:55,245 - INFO - Epoch 841 training loss = 1.581
2023-05-15 19:12:59,308 - INFO - Epoch 842 training loss = 1.671
2023-05-15 19:13:03,371 - INFO - Epoch 843 training loss = 1.237
2023-05-15 19:13:07,434 - INFO - Epoch 844 training loss = 1.508
2023-05-15 19:13:11,498 - INFO - Epoch 845 training loss = 1.322
2023-05-15 19:13:15,563 - INFO - Epoch 846 training loss = 1.143
2023-05-15 19:13:19,627 - INFO - Epoch 847 training loss = 1.399
2023-05-15 19:13:23,690 - INFO - Epoch 848 training loss = 1.164
2023-05-15 19:13:27,741 - INFO - Epoch 849 training loss = 1.766
2023-05-15 19:13:31,791 - INFO - Epoch 850 training loss = 1.359
2023-05-15 19:13:32,171 - INFO - Validation loss = 2.239
2023-05-15 19:13:36,226 - INFO - Epoch 851 training loss = 1.305
2023-05-15 19:13:40,290 - INFO - Epoch 852 training loss = 1.848
2023-05-15 19:13:44,356 - INFO - Epoch 853 training loss = 1.582
2023-05-15 19:13:48,421 - INFO - Epoch 854 training loss = 1.112
2023-05-15 19:13:52,486 - INFO - Epoch 855 training loss = 1.197
2023-05-15 19:13:56,548 - INFO - Epoch 856 training loss = 1.296
2023-05-15 19:14:00,610 - INFO - Epoch 857 training loss = 1.549
2023-05-15 19:14:04,674 - INFO - Epoch 858 training loss = 1.245
2023-05-15 19:14:08,736 - INFO - Epoch 859 training loss = 1.253
2023-05-15 19:14:12,801 - INFO - Epoch 860 training loss = 1.313
2023-05-15 19:14:13,184 - INFO - Validation loss = 1.545
2023-05-15 19:14:17,246 - INFO - Epoch 861 training loss = 1.221
2023-05-15 19:14:21,307 - INFO - Epoch 862 training loss = 1.255
2023-05-15 19:14:25,365 - INFO - Epoch 863 training loss = 1.269
2023-05-15 19:14:29,424 - INFO - Epoch 864 training loss = 1.361
2023-05-15 19:14:33,483 - INFO - Epoch 865 training loss = 1.702
2023-05-15 19:14:37,547 - INFO - Epoch 866 training loss = 1.261
2023-05-15 19:14:41,610 - INFO - Epoch 867 training loss = 1.425
2023-05-15 19:14:45,676 - INFO - Epoch 868 training loss = 1.541
2023-05-15 19:14:49,741 - INFO - Epoch 869 training loss = 1.191
2023-05-15 19:14:53,803 - INFO - Epoch 870 training loss = 1.15
2023-05-15 19:14:54,185 - INFO - Validation loss = 1.854
2023-05-15 19:14:58,246 - INFO - Epoch 871 training loss = 1.096
2023-05-15 19:15:02,309 - INFO - Epoch 872 training loss = 1.385
2023-05-15 19:15:06,372 - INFO - Epoch 873 training loss = 1.336
2023-05-15 19:15:10,435 - INFO - Epoch 874 training loss = 1.111
2023-05-15 19:15:14,490 - INFO - Epoch 875 training loss = 1.818
2023-05-15 19:15:18,544 - INFO - Epoch 876 training loss = 1.337
2023-05-15 19:15:22,592 - INFO - Epoch 877 training loss = 1.097
2023-05-15 19:15:26,639 - INFO - Epoch 878 training loss = 1.038
2023-05-15 19:15:30,685 - INFO - Epoch 879 training loss = 1.047
2023-05-15 19:15:34,732 - INFO - Epoch 880 training loss = 1.015
2023-05-15 19:15:35,114 - INFO - Validation loss = 2.046
2023-05-15 19:15:39,161 - INFO - Epoch 881 training loss = 1.163
2023-05-15 19:15:43,210 - INFO - Epoch 882 training loss = 1.289
2023-05-15 19:15:47,260 - INFO - Epoch 883 training loss = 1.069
2023-05-15 19:15:51,309 - INFO - Epoch 884 training loss = 1.098
2023-05-15 19:15:55,355 - INFO - Epoch 885 training loss = 1.155
2023-05-15 19:15:59,401 - INFO - Epoch 886 training loss = 1.011
2023-05-15 19:16:03,449 - INFO - Epoch 887 training loss = 1.612
2023-05-15 19:16:07,500 - INFO - Epoch 888 training loss = 1.166
2023-05-15 19:16:11,553 - INFO - Epoch 889 training loss = 0.9875
2023-05-15 19:16:15,606 - INFO - Epoch 890 training loss = 1.336
2023-05-15 19:16:15,986 - INFO - Validation loss = 2.189
2023-05-15 19:16:20,039 - INFO - Epoch 891 training loss = 1.279
2023-05-15 19:16:24,088 - INFO - Epoch 892 training loss = 0.9831
2023-05-15 19:16:28,138 - INFO - Epoch 893 training loss = 1.261
2023-05-15 19:16:32,187 - INFO - Epoch 894 training loss = 1.049
2023-05-15 19:16:36,239 - INFO - Epoch 895 training loss = 1.111
2023-05-15 19:16:40,288 - INFO - Epoch 896 training loss = 0.9579
2023-05-15 19:16:44,341 - INFO - Epoch 897 training loss = 0.9499
2023-05-15 19:16:48,394 - INFO - Epoch 898 training loss = 1.171
2023-05-15 19:16:52,446 - INFO - Epoch 899 training loss =  1.1
2023-05-15 19:16:56,495 - INFO - Epoch 900 training loss = 1.045
2023-05-15 19:16:56,876 - INFO - Validation loss = 1.641
2023-05-15 19:17:00,928 - INFO - Epoch 901 training loss = 0.9794
2023-05-15 19:17:04,980 - INFO - Epoch 902 training loss = 1.14
2023-05-15 19:17:09,030 - INFO - Epoch 903 training loss = 1.159
2023-05-15 19:17:13,084 - INFO - Epoch 904 training loss = 0.966
2023-05-15 19:17:17,139 - INFO - Epoch 905 training loss = 0.8649
2023-05-15 19:17:21,194 - INFO - Epoch 906 training loss = 1.356
2023-05-15 19:17:25,248 - INFO - Epoch 907 training loss = 1.108
2023-05-15 19:17:29,302 - INFO - Epoch 908 training loss = 1.083
2023-05-15 19:17:33,355 - INFO - Epoch 909 training loss = 1.17
2023-05-15 19:17:37,410 - INFO - Epoch 910 training loss = 1.019
2023-05-15 19:17:37,791 - INFO - Validation loss = 1.666
2023-05-15 19:17:41,846 - INFO - Epoch 911 training loss = 1.119
2023-05-15 19:17:45,904 - INFO - Epoch 912 training loss = 1.034
2023-05-15 19:17:49,960 - INFO - Epoch 913 training loss = 1.025
2023-05-15 19:17:54,014 - INFO - Epoch 914 training loss = 1.047
2023-05-15 19:17:58,068 - INFO - Epoch 915 training loss = 1.009
2023-05-15 19:18:02,123 - INFO - Epoch 916 training loss = 1.082
2023-05-15 19:18:06,174 - INFO - Epoch 917 training loss = 1.068
2023-05-15 19:18:10,226 - INFO - Epoch 918 training loss = 0.856
2023-05-15 19:18:14,278 - INFO - Epoch 919 training loss = 0.8342
2023-05-15 19:18:18,330 - INFO - Epoch 920 training loss = 1.036
2023-05-15 19:18:18,712 - INFO - Validation loss = 3.332
2023-05-15 19:18:22,763 - INFO - Epoch 921 training loss = 0.8982
2023-05-15 19:18:26,813 - INFO - Epoch 922 training loss = 0.8986
2023-05-15 19:18:30,862 - INFO - Epoch 923 training loss = 1.167
2023-05-15 19:18:34,915 - INFO - Epoch 924 training loss = 0.8801
2023-05-15 19:18:38,982 - INFO - Epoch 925 training loss = 1.129
2023-05-15 19:18:43,051 - INFO - Epoch 926 training loss = 1.059
2023-05-15 19:18:47,122 - INFO - Epoch 927 training loss = 0.8804
2023-05-15 19:18:51,186 - INFO - Epoch 928 training loss = 0.91
2023-05-15 19:18:55,242 - INFO - Epoch 929 training loss = 1.074
2023-05-15 19:18:59,296 - INFO - Epoch 930 training loss = 0.9948
2023-05-15 19:18:59,677 - INFO - Validation loss = 1.169
2023-05-15 19:18:59,677 - INFO - best model
2023-05-15 19:19:03,755 - INFO - Epoch 931 training loss = 0.994
2023-05-15 19:19:07,810 - INFO - Epoch 932 training loss = 1.278
2023-05-15 19:19:11,867 - INFO - Epoch 933 training loss = 0.7959
2023-05-15 19:19:15,926 - INFO - Epoch 934 training loss = 0.855
2023-05-15 19:19:19,983 - INFO - Epoch 935 training loss =  1.1
2023-05-15 19:19:24,038 - INFO - Epoch 936 training loss = 0.7823
2023-05-15 19:19:28,093 - INFO - Epoch 937 training loss = 0.8581
2023-05-15 19:19:32,147 - INFO - Epoch 938 training loss = 0.8411
2023-05-15 19:19:36,203 - INFO - Epoch 939 training loss = 0.8758
2023-05-15 19:19:40,257 - INFO - Epoch 940 training loss = 0.8489
2023-05-15 19:19:40,638 - INFO - Validation loss = 2.027
2023-05-15 19:19:44,698 - INFO - Epoch 941 training loss = 0.8679
2023-05-15 19:19:48,754 - INFO - Epoch 942 training loss = 0.879
2023-05-15 19:19:52,810 - INFO - Epoch 943 training loss = 0.8093
2023-05-15 19:19:56,864 - INFO - Epoch 944 training loss = 0.8339
2023-05-15 19:20:00,920 - INFO - Epoch 945 training loss = 1.142
2023-05-15 19:20:04,978 - INFO - Epoch 946 training loss = 0.7703
2023-05-15 19:20:09,045 - INFO - Epoch 947 training loss = 0.9892
2023-05-15 19:20:13,114 - INFO - Epoch 948 training loss = 0.9018
2023-05-15 19:20:17,183 - INFO - Epoch 949 training loss = 0.8721
2023-05-15 19:20:21,250 - INFO - Epoch 950 training loss = 0.8327
2023-05-15 19:20:21,632 - INFO - Validation loss = 1.205
2023-05-15 19:20:25,698 - INFO - Epoch 951 training loss = 0.9941
2023-05-15 19:20:29,751 - INFO - Epoch 952 training loss = 0.7093
2023-05-15 19:20:33,806 - INFO - Epoch 953 training loss = 0.91
2023-05-15 19:20:37,862 - INFO - Epoch 954 training loss = 0.8913
2023-05-15 19:20:41,917 - INFO - Epoch 955 training loss = 0.707
2023-05-15 19:20:45,975 - INFO - Epoch 956 training loss = 1.091
2023-05-15 19:20:50,033 - INFO - Epoch 957 training loss = 0.8772
2023-05-15 19:20:54,086 - INFO - Epoch 958 training loss = 0.6742
2023-05-15 19:20:58,140 - INFO - Epoch 959 training loss = 1.011
2023-05-15 19:21:02,197 - INFO - Epoch 960 training loss = 0.9166
2023-05-15 19:21:02,583 - INFO - Validation loss = 1.209
2023-05-15 19:21:06,639 - INFO - Epoch 961 training loss = 0.7782
2023-05-15 19:21:10,693 - INFO - Epoch 962 training loss = 0.8134
2023-05-15 19:21:14,749 - INFO - Epoch 963 training loss = 0.6474
2023-05-15 19:21:18,805 - INFO - Epoch 964 training loss = 0.778
2023-05-15 19:21:22,869 - INFO - Epoch 965 training loss = 0.8063
2023-05-15 19:21:26,934 - INFO - Epoch 966 training loss = 0.8354
2023-05-15 19:21:30,999 - INFO - Epoch 967 training loss = 0.9401
2023-05-15 19:21:35,066 - INFO - Epoch 968 training loss = 0.749
2023-05-15 19:21:39,131 - INFO - Epoch 969 training loss = 0.8272
2023-05-15 19:21:43,201 - INFO - Epoch 970 training loss = 0.7298
2023-05-15 19:21:43,583 - INFO - Validation loss = 1.835
2023-05-15 19:21:47,652 - INFO - Epoch 971 training loss = 0.9861
2023-05-15 19:21:51,720 - INFO - Epoch 972 training loss = 0.8559
2023-05-15 19:21:55,785 - INFO - Epoch 973 training loss = 0.6313
2023-05-15 19:21:59,853 - INFO - Epoch 974 training loss = 0.6934
2023-05-15 19:22:03,920 - INFO - Epoch 975 training loss = 0.6612
2023-05-15 19:22:07,989 - INFO - Epoch 976 training loss = 0.7312
2023-05-15 19:22:12,058 - INFO - Epoch 977 training loss = 1.003
2023-05-15 19:22:16,129 - INFO - Epoch 978 training loss = 0.7034
2023-05-15 19:22:20,189 - INFO - Epoch 979 training loss = 0.7968
2023-05-15 19:22:24,242 - INFO - Epoch 980 training loss = 0.6846
2023-05-15 19:22:24,623 - INFO - Validation loss = 1.664
2023-05-15 19:22:28,678 - INFO - Epoch 981 training loss = 0.7719
2023-05-15 19:22:32,733 - INFO - Epoch 982 training loss = 0.6786
2023-05-15 19:22:36,787 - INFO - Epoch 983 training loss = 0.7594
2023-05-15 19:22:40,841 - INFO - Epoch 984 training loss = 0.8587
2023-05-15 19:22:44,899 - INFO - Epoch 985 training loss = 0.9256
2023-05-15 19:22:48,956 - INFO - Epoch 986 training loss = 0.701
2023-05-15 19:22:53,022 - INFO - Epoch 987 training loss = 0.7295
2023-05-15 19:22:57,088 - INFO - Epoch 988 training loss = 0.8699
2023-05-15 19:23:01,154 - INFO - Epoch 989 training loss = 0.8468
2023-05-15 19:23:05,221 - INFO - Epoch 990 training loss = 0.6536
2023-05-15 19:23:05,603 - INFO - Validation loss = 3.171
2023-05-15 19:23:09,671 - INFO - Epoch 991 training loss = 0.6958
2023-05-15 19:23:13,742 - INFO - Epoch 992 training loss = 0.5437
2023-05-15 19:23:17,812 - INFO - Epoch 993 training loss = 0.6498
2023-05-15 19:23:21,879 - INFO - Epoch 994 training loss = 0.7925
2023-05-15 19:23:25,945 - INFO - Epoch 995 training loss = 0.6805
2023-05-15 19:23:30,011 - INFO - Epoch 996 training loss = 0.7169
2023-05-15 19:23:34,078 - INFO - Epoch 997 training loss = 0.9253
2023-05-15 19:23:38,145 - INFO - Epoch 998 training loss = 0.7795
2023-05-15 19:23:42,214 - INFO - Epoch 999 training loss = 0.6293
2023-05-15 19:23:46,285 - INFO - Epoch 1000 training loss = 0.5019
2023-05-15 19:23:46,667 - INFO - Validation loss = 1.026
2023-05-15 19:23:46,668 - INFO - best model
2023-05-15 19:23:50,759 - INFO - Epoch 1001 training loss = 0.5461
2023-05-15 19:23:54,825 - INFO - Epoch 1002 training loss = 0.7506
2023-05-15 19:23:58,893 - INFO - Epoch 1003 training loss = 0.633
2023-05-15 19:24:02,961 - INFO - Epoch 1004 training loss = 0.692
2023-05-15 19:24:07,029 - INFO - Epoch 1005 training loss = 0.6477
2023-05-15 19:24:11,098 - INFO - Epoch 1006 training loss = 0.633
2023-05-15 19:24:15,168 - INFO - Epoch 1007 training loss = 0.5357
2023-05-15 19:24:19,232 - INFO - Epoch 1008 training loss = 0.6086
2023-05-15 19:24:23,287 - INFO - Epoch 1009 training loss = 0.6312
2023-05-15 19:24:27,341 - INFO - Epoch 1010 training loss = 0.5746
2023-05-15 19:24:27,722 - INFO - Validation loss = 1.53
2023-05-15 19:24:31,775 - INFO - Epoch 1011 training loss = 0.5027
2023-05-15 19:24:35,829 - INFO - Epoch 1012 training loss = 0.6574
2023-05-15 19:24:39,883 - INFO - Epoch 1013 training loss = 0.6553
2023-05-15 19:24:43,940 - INFO - Epoch 1014 training loss = 0.5245
2023-05-15 19:24:47,998 - INFO - Epoch 1015 training loss = 0.6172
2023-05-15 19:24:52,059 - INFO - Epoch 1016 training loss = 0.5834
2023-05-15 19:24:56,125 - INFO - Epoch 1017 training loss = 0.5775
2023-05-15 19:25:00,188 - INFO - Epoch 1018 training loss = 0.5871
2023-05-15 19:25:04,244 - INFO - Epoch 1019 training loss = 0.6179
2023-05-15 19:25:08,310 - INFO - Epoch 1020 training loss = 0.5497
2023-05-15 19:25:08,692 - INFO - Validation loss = 1.251
2023-05-15 19:25:12,762 - INFO - Epoch 1021 training loss = 0.6381
2023-05-15 19:25:16,833 - INFO - Epoch 1022 training loss = 0.6012
2023-05-15 19:25:20,894 - INFO - Epoch 1023 training loss = 0.5981
2023-05-15 19:25:24,947 - INFO - Epoch 1024 training loss = 0.5104
2023-05-15 19:25:29,000 - INFO - Epoch 1025 training loss = 0.5847
2023-05-15 19:25:33,054 - INFO - Epoch 1026 training loss = 0.6494
2023-05-15 19:25:37,108 - INFO - Epoch 1027 training loss = 0.7008
2023-05-15 19:25:41,162 - INFO - Epoch 1028 training loss = 0.5403
2023-05-15 19:25:45,219 - INFO - Epoch 1029 training loss = 0.5422
2023-05-15 19:25:49,275 - INFO - Epoch 1030 training loss = 0.7177
2023-05-15 19:25:49,656 - INFO - Validation loss = 1.477
2023-05-15 19:25:53,711 - INFO - Epoch 1031 training loss = 0.6903
2023-05-15 19:25:57,764 - INFO - Epoch 1032 training loss = 0.5131
2023-05-15 19:26:01,819 - INFO - Epoch 1033 training loss = 0.5387
2023-05-15 19:26:05,874 - INFO - Epoch 1034 training loss = 0.5186
2023-05-15 19:26:09,928 - INFO - Epoch 1035 training loss = 0.4458
2023-05-15 19:26:13,986 - INFO - Epoch 1036 training loss = 0.5077
2023-05-15 19:26:18,043 - INFO - Epoch 1037 training loss = 0.494
2023-05-15 19:26:22,097 - INFO - Epoch 1038 training loss = 0.6378
2023-05-15 19:26:26,150 - INFO - Epoch 1039 training loss = 0.5372
2023-05-15 19:26:30,203 - INFO - Epoch 1040 training loss = 0.4646
2023-05-15 19:26:30,584 - INFO - Validation loss = 0.8937
2023-05-15 19:26:30,584 - INFO - best model
2023-05-15 19:26:34,660 - INFO - Epoch 1041 training loss = 0.5178
2023-05-15 19:26:38,725 - INFO - Epoch 1042 training loss = 0.5441
2023-05-15 19:26:42,793 - INFO - Epoch 1043 training loss = 0.4839
2023-05-15 19:26:46,863 - INFO - Epoch 1044 training loss = 0.5967
2023-05-15 19:26:50,932 - INFO - Epoch 1045 training loss = 0.537
2023-05-15 19:26:54,998 - INFO - Epoch 1046 training loss = 0.6572
2023-05-15 19:26:59,065 - INFO - Epoch 1047 training loss = 0.5329
2023-05-15 19:27:03,133 - INFO - Epoch 1048 training loss = 0.6597
2023-05-15 19:27:07,200 - INFO - Epoch 1049 training loss = 0.5329
2023-05-15 19:27:11,267 - INFO - Epoch 1050 training loss = 0.4598
2023-05-15 19:27:11,649 - INFO - Validation loss = 1.33
2023-05-15 19:27:15,704 - INFO - Epoch 1051 training loss = 0.5661
2023-05-15 19:27:19,753 - INFO - Epoch 1052 training loss = 0.4904
2023-05-15 19:27:23,805 - INFO - Epoch 1053 training loss = 0.5478
2023-05-15 19:27:27,858 - INFO - Epoch 1054 training loss = 0.4688
2023-05-15 19:27:31,913 - INFO - Epoch 1055 training loss = 0.497
2023-05-15 19:27:35,967 - INFO - Epoch 1056 training loss = 0.4266
2023-05-15 19:27:40,022 - INFO - Epoch 1057 training loss = 0.4022
2023-05-15 19:27:44,080 - INFO - Epoch 1058 training loss = 0.4467
2023-05-15 19:27:48,138 - INFO - Epoch 1059 training loss = 0.4977
2023-05-15 19:27:52,201 - INFO - Epoch 1060 training loss = 0.472
2023-05-15 19:27:52,583 - INFO - Validation loss = 0.7156
2023-05-15 19:27:52,583 - INFO - best model
2023-05-15 19:27:56,659 - INFO - Epoch 1061 training loss = 0.5733
2023-05-15 19:28:00,713 - INFO - Epoch 1062 training loss = 0.4989
2023-05-15 19:28:04,769 - INFO - Epoch 1063 training loss = 0.4817
2023-05-15 19:28:08,824 - INFO - Epoch 1064 training loss = 0.6032
2023-05-15 19:28:12,875 - INFO - Epoch 1065 training loss = 0.4268
2023-05-15 19:28:16,926 - INFO - Epoch 1066 training loss = 0.4166
2023-05-15 19:28:20,973 - INFO - Epoch 1067 training loss = 0.4613
2023-05-15 19:28:25,030 - INFO - Epoch 1068 training loss = 0.4933
2023-05-15 19:28:29,088 - INFO - Epoch 1069 training loss = 0.4287
2023-05-15 19:28:33,146 - INFO - Epoch 1070 training loss = 0.3945
2023-05-15 19:28:33,528 - INFO - Validation loss = 0.9548
2023-05-15 19:28:37,589 - INFO - Epoch 1071 training loss = 0.3628
2023-05-15 19:28:41,648 - INFO - Epoch 1072 training loss = 0.4786
2023-05-15 19:28:45,710 - INFO - Epoch 1073 training loss = 0.3662
2023-05-15 19:28:49,771 - INFO - Epoch 1074 training loss = 0.4159
2023-05-15 19:28:53,829 - INFO - Epoch 1075 training loss = 0.4344
2023-05-15 19:28:57,888 - INFO - Epoch 1076 training loss = 0.4482
2023-05-15 19:29:01,942 - INFO - Epoch 1077 training loss = 0.435
2023-05-15 19:29:05,994 - INFO - Epoch 1078 training loss = 0.5362
2023-05-15 19:29:10,053 - INFO - Epoch 1079 training loss = 0.436
2023-05-15 19:29:14,116 - INFO - Epoch 1080 training loss = 0.4523
2023-05-15 19:29:14,498 - INFO - Validation loss = 1.399
2023-05-15 19:29:18,559 - INFO - Epoch 1081 training loss = 0.4618
2023-05-15 19:29:22,610 - INFO - Epoch 1082 training loss = 0.4724
2023-05-15 19:29:26,657 - INFO - Epoch 1083 training loss = 0.5694
2023-05-15 19:29:30,703 - INFO - Epoch 1084 training loss = 0.4048
2023-05-15 19:29:34,750 - INFO - Epoch 1085 training loss = 0.4361
2023-05-15 19:29:38,800 - INFO - Epoch 1086 training loss = 0.3868
2023-05-15 19:29:42,851 - INFO - Epoch 1087 training loss = 0.4366
2023-05-15 19:29:46,904 - INFO - Epoch 1088 training loss = 0.3231
2023-05-15 19:29:50,956 - INFO - Epoch 1089 training loss = 0.351
2023-05-15 19:29:55,005 - INFO - Epoch 1090 training loss = 0.4076
2023-05-15 19:29:55,387 - INFO - Validation loss = 1.317
2023-05-15 19:29:59,435 - INFO - Epoch 1091 training loss = 0.3604
2023-05-15 19:30:03,488 - INFO - Epoch 1092 training loss = 0.4503
2023-05-15 19:30:07,538 - INFO - Epoch 1093 training loss = 0.4128
2023-05-15 19:30:11,588 - INFO - Epoch 1094 training loss = 0.324
2023-05-15 19:30:15,642 - INFO - Epoch 1095 training loss = 0.3389
2023-05-15 19:30:19,693 - INFO - Epoch 1096 training loss = 0.4322
2023-05-15 19:30:23,751 - INFO - Epoch 1097 training loss = 0.4421
2023-05-15 19:30:27,808 - INFO - Epoch 1098 training loss = 0.4197
2023-05-15 19:30:31,865 - INFO - Epoch 1099 training loss = 0.4892
2023-05-15 19:30:35,923 - INFO - Epoch 1100 training loss = 0.4303
2023-05-15 19:30:36,305 - INFO - Validation loss = 1.22
2023-05-15 19:30:40,364 - INFO - Epoch 1101 training loss = 0.423
2023-05-15 19:30:44,426 - INFO - Epoch 1102 training loss = 0.4699
2023-05-15 19:30:48,488 - INFO - Epoch 1103 training loss = 0.4024
2023-05-15 19:30:52,547 - INFO - Epoch 1104 training loss = 0.3524
2023-05-15 19:30:56,606 - INFO - Epoch 1105 training loss = 0.3912
2023-05-15 19:31:00,664 - INFO - Epoch 1106 training loss = 0.3344
2023-05-15 19:31:04,724 - INFO - Epoch 1107 training loss = 0.3724
2023-05-15 19:31:08,787 - INFO - Epoch 1108 training loss = 0.322
2023-05-15 19:31:12,856 - INFO - Epoch 1109 training loss = 0.3399
2023-05-15 19:31:16,921 - INFO - Epoch 1110 training loss = 0.3189
2023-05-15 19:31:17,303 - INFO - Validation loss = 0.7073
2023-05-15 19:31:17,304 - INFO - best model
2023-05-15 19:31:21,385 - INFO - Epoch 1111 training loss = 0.3625
2023-05-15 19:31:25,442 - INFO - Epoch 1112 training loss = 0.3115
2023-05-15 19:31:29,500 - INFO - Epoch 1113 training loss = 0.3052
2023-05-15 19:31:33,557 - INFO - Epoch 1114 training loss = 0.309
2023-05-15 19:31:37,617 - INFO - Epoch 1115 training loss = 0.3848
2023-05-15 19:31:41,676 - INFO - Epoch 1116 training loss = 0.3097
2023-05-15 19:31:45,739 - INFO - Epoch 1117 training loss = 0.331
2023-05-15 19:31:49,799 - INFO - Epoch 1118 training loss = 0.2576
2023-05-15 19:31:53,865 - INFO - Epoch 1119 training loss = 0.2736
2023-05-15 19:31:57,938 - INFO - Epoch 1120 training loss = 0.26
2023-05-15 19:31:58,320 - INFO - Validation loss = 0.9066
2023-05-15 19:32:02,394 - INFO - Epoch 1121 training loss = 0.3248
2023-05-15 19:32:06,468 - INFO - Epoch 1122 training loss = 0.2752
2023-05-15 19:32:10,542 - INFO - Epoch 1123 training loss = 0.3417
2023-05-15 19:32:14,611 - INFO - Epoch 1124 training loss = 0.3529
2023-05-15 19:32:18,672 - INFO - Epoch 1125 training loss = 0.2635
2023-05-15 19:32:22,730 - INFO - Epoch 1126 training loss = 0.3591
2023-05-15 19:32:26,788 - INFO - Epoch 1127 training loss = 0.3026
2023-05-15 19:32:30,846 - INFO - Epoch 1128 training loss = 0.339
2023-05-15 19:32:34,905 - INFO - Epoch 1129 training loss = 0.3163
2023-05-15 19:32:38,963 - INFO - Epoch 1130 training loss = 0.3672
2023-05-15 19:32:39,344 - INFO - Validation loss = 0.6682
2023-05-15 19:32:39,344 - INFO - best model
2023-05-15 19:32:43,426 - INFO - Epoch 1131 training loss = 0.3174
2023-05-15 19:32:47,488 - INFO - Epoch 1132 training loss = 0.2528
2023-05-15 19:32:51,554 - INFO - Epoch 1133 training loss = 0.2991
2023-05-15 19:32:55,630 - INFO - Epoch 1134 training loss = 0.2526
2023-05-15 19:32:59,707 - INFO - Epoch 1135 training loss = 0.2799
2023-05-15 19:33:03,783 - INFO - Epoch 1136 training loss = 0.2491
2023-05-15 19:33:07,859 - INFO - Epoch 1137 training loss = 0.3032
2023-05-15 19:33:11,937 - INFO - Epoch 1138 training loss = 0.2621
2023-05-15 19:33:16,017 - INFO - Epoch 1139 training loss = 0.2824
2023-05-15 19:33:20,095 - INFO - Epoch 1140 training loss = 0.2556
2023-05-15 19:33:20,477 - INFO - Validation loss = 0.6651
2023-05-15 19:33:20,477 - INFO - best model
2023-05-15 19:33:24,574 - INFO - Epoch 1141 training loss = 0.2641
2023-05-15 19:33:28,650 - INFO - Epoch 1142 training loss = 0.3019
2023-05-15 19:33:32,725 - INFO - Epoch 1143 training loss = 0.2519
2023-05-15 19:33:36,802 - INFO - Epoch 1144 training loss = 0.2982
2023-05-15 19:33:40,877 - INFO - Epoch 1145 training loss = 0.2698
2023-05-15 19:33:44,956 - INFO - Epoch 1146 training loss = 0.2447
2023-05-15 19:33:49,034 - INFO - Epoch 1147 training loss = 0.2616
2023-05-15 19:33:53,110 - INFO - Epoch 1148 training loss = 0.3065
2023-05-15 19:33:57,185 - INFO - Epoch 1149 training loss = 0.2749
2023-05-15 19:34:01,261 - INFO - Epoch 1150 training loss = 0.31
2023-05-15 19:34:01,643 - INFO - Validation loss = 0.9403
2023-05-15 19:34:05,721 - INFO - Epoch 1151 training loss = 0.2485
2023-05-15 19:34:09,793 - INFO - Epoch 1152 training loss = 0.2261
2023-05-15 19:34:13,871 - INFO - Epoch 1153 training loss = 0.268
2023-05-15 19:34:17,947 - INFO - Epoch 1154 training loss = 0.2781
2023-05-15 19:34:22,021 - INFO - Epoch 1155 training loss = 0.2261
2023-05-15 19:34:26,092 - INFO - Epoch 1156 training loss = 0.2482
2023-05-15 19:34:30,165 - INFO - Epoch 1157 training loss = 0.273
2023-05-15 19:34:34,236 - INFO - Epoch 1158 training loss = 0.2465
2023-05-15 19:34:38,314 - INFO - Epoch 1159 training loss = 0.2443
2023-05-15 19:34:42,391 - INFO - Epoch 1160 training loss = 0.2151
2023-05-15 19:34:42,774 - INFO - Validation loss = 0.823
2023-05-15 19:34:46,856 - INFO - Epoch 1161 training loss = 0.2207
2023-05-15 19:34:50,934 - INFO - Epoch 1162 training loss = 0.2033
2023-05-15 19:34:55,009 - INFO - Epoch 1163 training loss = 0.2326
2023-05-15 19:34:59,082 - INFO - Epoch 1164 training loss = 0.2263
2023-05-15 19:35:03,157 - INFO - Epoch 1165 training loss = 0.2551
2023-05-15 19:35:07,231 - INFO - Epoch 1166 training loss = 0.2275
2023-05-15 19:35:11,308 - INFO - Epoch 1167 training loss = 0.2245
2023-05-15 19:35:15,387 - INFO - Epoch 1168 training loss = 0.216
2023-05-15 19:35:19,462 - INFO - Epoch 1169 training loss = 0.2714
2023-05-15 19:35:23,535 - INFO - Epoch 1170 training loss = 0.2557
2023-05-15 19:35:23,918 - INFO - Validation loss = 0.6227
2023-05-15 19:35:23,918 - INFO - best model
2023-05-15 19:35:28,013 - INFO - Epoch 1171 training loss = 0.2113
2023-05-15 19:35:32,085 - INFO - Epoch 1172 training loss = 0.2175
2023-05-15 19:35:36,159 - INFO - Epoch 1173 training loss = 0.2151
2023-05-15 19:35:40,233 - INFO - Epoch 1174 training loss = 0.2025
2023-05-15 19:35:44,311 - INFO - Epoch 1175 training loss = 0.2144
2023-05-15 19:35:48,388 - INFO - Epoch 1176 training loss = 0.2215
2023-05-15 19:35:52,461 - INFO - Epoch 1177 training loss = 0.1991
2023-05-15 19:35:56,535 - INFO - Epoch 1178 training loss = 0.2035
2023-05-15 19:36:00,609 - INFO - Epoch 1179 training loss = 0.2383
2023-05-15 19:36:04,683 - INFO - Epoch 1180 training loss = 0.2178
2023-05-15 19:36:05,065 - INFO - Validation loss = 0.6482
2023-05-15 19:36:09,139 - INFO - Epoch 1181 training loss = 0.179
2023-05-15 19:36:13,214 - INFO - Epoch 1182 training loss = 0.2169
2023-05-15 19:36:17,294 - INFO - Epoch 1183 training loss = 0.2099
2023-05-15 19:36:21,371 - INFO - Epoch 1184 training loss = 0.2096
2023-05-15 19:36:25,445 - INFO - Epoch 1185 training loss = 0.1985
2023-05-15 19:36:29,521 - INFO - Epoch 1186 training loss = 0.2407
2023-05-15 19:36:33,596 - INFO - Epoch 1187 training loss = 0.1968
2023-05-15 19:36:37,670 - INFO - Epoch 1188 training loss = 0.2283
2023-05-15 19:36:41,743 - INFO - Epoch 1189 training loss = 0.1603
2023-05-15 19:36:45,821 - INFO - Epoch 1190 training loss = 0.1739
2023-05-15 19:36:46,203 - INFO - Validation loss = 0.6196
2023-05-15 19:36:46,204 - INFO - best model
2023-05-15 19:36:50,300 - INFO - Epoch 1191 training loss = 0.1726
2023-05-15 19:36:54,374 - INFO - Epoch 1192 training loss = 0.222
2023-05-15 19:36:58,446 - INFO - Epoch 1193 training loss = 0.1607
2023-05-15 19:37:02,520 - INFO - Epoch 1194 training loss = 0.2004
2023-05-15 19:37:06,587 - INFO - Epoch 1195 training loss = 0.2019
2023-05-15 19:37:10,649 - INFO - Epoch 1196 training loss = 0.1898
2023-05-15 19:37:14,724 - INFO - Epoch 1197 training loss = 0.1946
2023-05-15 19:37:18,799 - INFO - Epoch 1198 training loss = 0.1726
2023-05-15 19:37:22,872 - INFO - Epoch 1199 training loss = 0.1781
2023-05-15 19:37:26,945 - INFO - Epoch 1200 training loss = 0.1677
2023-05-15 19:37:27,327 - INFO - Validation loss = 0.6183
2023-05-15 19:37:27,328 - INFO - best model
2023-05-15 19:37:31,423 - INFO - Epoch 1201 training loss = 0.1845
2023-05-15 19:37:35,496 - INFO - Epoch 1202 training loss = 0.1513
2023-05-15 19:37:39,567 - INFO - Epoch 1203 training loss = 0.1712
2023-05-15 19:37:43,641 - INFO - Epoch 1204 training loss = 0.1745
2023-05-15 19:37:47,720 - INFO - Epoch 1205 training loss = 0.1693
2023-05-15 19:37:51,798 - INFO - Epoch 1206 training loss = 0.1639
2023-05-15 19:37:55,873 - INFO - Epoch 1207 training loss = 0.1394
2023-05-15 19:37:59,949 - INFO - Epoch 1208 training loss = 0.168
2023-05-15 19:38:04,023 - INFO - Epoch 1209 training loss = 0.1717
2023-05-15 19:38:08,096 - INFO - Epoch 1210 training loss = 0.1508
2023-05-15 19:38:08,479 - INFO - Validation loss = 0.6221
2023-05-15 19:38:12,558 - INFO - Epoch 1211 training loss = 0.1563
2023-05-15 19:38:16,638 - INFO - Epoch 1212 training loss = 0.1603
2023-05-15 19:38:20,716 - INFO - Epoch 1213 training loss = 0.1619
2023-05-15 19:38:24,792 - INFO - Epoch 1214 training loss = 0.2042
2023-05-15 19:38:28,868 - INFO - Epoch 1215 training loss = 0.1606
2023-05-15 19:38:32,944 - INFO - Epoch 1216 training loss = 0.143
2023-05-15 19:38:37,021 - INFO - Epoch 1217 training loss = 0.1552
2023-05-15 19:38:41,098 - INFO - Epoch 1218 training loss = 0.1979
2023-05-15 19:38:45,177 - INFO - Epoch 1219 training loss = 0.1543
2023-05-15 19:38:49,255 - INFO - Epoch 1220 training loss = 0.1754
2023-05-15 19:38:49,637 - INFO - Validation loss = 0.5453
2023-05-15 19:38:49,638 - INFO - best model
2023-05-15 19:38:53,739 - INFO - Epoch 1221 training loss = 0.1633
2023-05-15 19:38:57,815 - INFO - Epoch 1222 training loss = 0.1409
2023-05-15 19:39:01,892 - INFO - Epoch 1223 training loss = 0.1832
2023-05-15 19:39:05,970 - INFO - Epoch 1224 training loss = 0.1517
2023-05-15 19:39:10,047 - INFO - Epoch 1225 training loss = 0.1395
2023-05-15 19:39:14,128 - INFO - Epoch 1226 training loss = 0.1445
2023-05-15 19:39:18,209 - INFO - Epoch 1227 training loss = 0.1401
2023-05-15 19:39:22,286 - INFO - Epoch 1228 training loss = 0.1443
2023-05-15 19:39:26,363 - INFO - Epoch 1229 training loss = 0.139
2023-05-15 19:39:30,439 - INFO - Epoch 1230 training loss = 0.161
2023-05-15 19:39:30,821 - INFO - Validation loss = 0.5215
2023-05-15 19:39:30,822 - INFO - best model
2023-05-15 19:39:34,920 - INFO - Epoch 1231 training loss = 0.1436
2023-05-15 19:39:39,000 - INFO - Epoch 1232 training loss = 0.1269
2023-05-15 19:39:43,082 - INFO - Epoch 1233 training loss = 0.1294
2023-05-15 19:39:47,167 - INFO - Epoch 1234 training loss = 0.1387
2023-05-15 19:39:51,245 - INFO - Epoch 1235 training loss = 0.1471
2023-05-15 19:39:55,322 - INFO - Epoch 1236 training loss = 0.1295
2023-05-15 19:39:59,397 - INFO - Epoch 1237 training loss = 0.115
2023-05-15 19:40:03,477 - INFO - Epoch 1238 training loss = 0.1211
2023-05-15 19:40:07,554 - INFO - Epoch 1239 training loss = 0.1384
2023-05-15 19:40:11,631 - INFO - Epoch 1240 training loss = 0.1545
2023-05-15 19:40:12,014 - INFO - Validation loss = 0.6421
2023-05-15 19:40:16,093 - INFO - Epoch 1241 training loss = 0.1224
2023-05-15 19:40:20,172 - INFO - Epoch 1242 training loss = 0.1189
2023-05-15 19:40:24,247 - INFO - Epoch 1243 training loss = 0.1142
2023-05-15 19:40:28,323 - INFO - Epoch 1244 training loss = 0.1115
2023-05-15 19:40:32,398 - INFO - Epoch 1245 training loss = 0.1181
2023-05-15 19:40:36,476 - INFO - Epoch 1246 training loss = 0.128
2023-05-15 19:40:40,553 - INFO - Epoch 1247 training loss = 0.1348
2023-05-15 19:40:44,633 - INFO - Epoch 1248 training loss = 0.116
2023-05-15 19:40:48,712 - INFO - Epoch 1249 training loss = 0.1271
2023-05-15 19:40:52,792 - INFO - Epoch 1250 training loss = 0.1211
2023-05-15 19:40:53,174 - INFO - Validation loss = 0.5614
2023-05-15 19:40:57,252 - INFO - Epoch 1251 training loss = 0.1198
2023-05-15 19:41:01,329 - INFO - Epoch 1252 training loss = 0.1167
2023-05-15 19:41:05,406 - INFO - Epoch 1253 training loss = 0.1224
2023-05-15 19:41:09,483 - INFO - Epoch 1254 training loss = 0.1188
2023-05-15 19:41:13,559 - INFO - Epoch 1255 training loss = 0.1162
2023-05-15 19:41:17,636 - INFO - Epoch 1256 training loss = 0.1051
2023-05-15 19:41:21,710 - INFO - Epoch 1257 training loss = 0.1092
2023-05-15 19:41:25,782 - INFO - Epoch 1258 training loss = 0.1103
2023-05-15 19:41:29,855 - INFO - Epoch 1259 training loss = 0.115
2023-05-15 19:41:33,927 - INFO - Epoch 1260 training loss = 0.1045
2023-05-15 19:41:34,309 - INFO - Validation loss = 0.5215
2023-05-15 19:41:34,309 - INFO - best model
2023-05-15 19:41:38,405 - INFO - Epoch 1261 training loss = 0.11
2023-05-15 19:41:42,478 - INFO - Epoch 1262 training loss = 0.1272
2023-05-15 19:41:46,554 - INFO - Epoch 1263 training loss = 0.102
2023-05-15 19:41:50,630 - INFO - Epoch 1264 training loss = 0.108
2023-05-15 19:41:54,706 - INFO - Epoch 1265 training loss = 0.1027
2023-05-15 19:41:58,777 - INFO - Epoch 1266 training loss = 0.1063
2023-05-15 19:42:02,851 - INFO - Epoch 1267 training loss = 0.1066
2023-05-15 19:42:06,918 - INFO - Epoch 1268 training loss = 0.09585
2023-05-15 19:42:10,978 - INFO - Epoch 1269 training loss = 0.09902
2023-05-15 19:42:15,050 - INFO - Epoch 1270 training loss = 0.09927
2023-05-15 19:42:15,432 - INFO - Validation loss = 0.4912
2023-05-15 19:42:15,433 - INFO - best model
2023-05-15 19:42:19,532 - INFO - Epoch 1271 training loss = 0.09687
2023-05-15 19:42:23,612 - INFO - Epoch 1272 training loss = 0.1018
2023-05-15 19:42:27,692 - INFO - Epoch 1273 training loss = 0.1042
2023-05-15 19:42:31,767 - INFO - Epoch 1274 training loss = 0.09404
2023-05-15 19:42:35,842 - INFO - Epoch 1275 training loss = 0.1005
2023-05-15 19:42:39,914 - INFO - Epoch 1276 training loss = 0.1041
2023-05-15 19:42:43,989 - INFO - Epoch 1277 training loss = 0.09131
2023-05-15 19:42:48,066 - INFO - Epoch 1278 training loss = 0.09865
2023-05-15 19:42:52,139 - INFO - Epoch 1279 training loss = 0.0979
2023-05-15 19:42:56,211 - INFO - Epoch 1280 training loss = 0.09292
2023-05-15 19:42:56,593 - INFO - Validation loss = 0.4785
2023-05-15 19:42:56,594 - INFO - best model
2023-05-15 19:43:00,690 - INFO - Epoch 1281 training loss = 0.09124
2023-05-15 19:43:04,764 - INFO - Epoch 1282 training loss = 0.08573
2023-05-15 19:43:08,837 - INFO - Epoch 1283 training loss = 0.099
2023-05-15 19:43:12,912 - INFO - Epoch 1284 training loss = 0.1037
2023-05-15 19:43:16,989 - INFO - Epoch 1285 training loss = 0.09361
2023-05-15 19:43:21,062 - INFO - Epoch 1286 training loss = 0.09353
2023-05-15 19:43:25,134 - INFO - Epoch 1287 training loss = 0.08112
2023-05-15 19:43:29,206 - INFO - Epoch 1288 training loss = 0.09638
2023-05-15 19:43:33,278 - INFO - Epoch 1289 training loss = 0.08482
2023-05-15 19:43:37,358 - INFO - Epoch 1290 training loss = 0.1033
2023-05-15 19:43:37,752 - INFO - Validation loss = 0.4806
2023-05-15 19:43:41,833 - INFO - Epoch 1291 training loss = 0.08279
2023-05-15 19:43:45,909 - INFO - Epoch 1292 training loss = 0.08603
2023-05-15 19:43:49,983 - INFO - Epoch 1293 training loss = 0.08079
2023-05-15 19:43:54,055 - INFO - Epoch 1294 training loss = 0.07974
2023-05-15 19:43:58,118 - INFO - Epoch 1295 training loss = 0.08448
2023-05-15 19:44:02,178 - INFO - Epoch 1296 training loss = 0.08783
2023-05-15 19:44:06,238 - INFO - Epoch 1297 training loss = 0.08261
2023-05-15 19:44:10,297 - INFO - Epoch 1298 training loss = 0.08127
2023-05-15 19:44:14,358 - INFO - Epoch 1299 training loss = 0.0824
2023-05-15 19:44:18,427 - INFO - Epoch 1300 training loss = 0.07548
2023-05-15 19:44:18,808 - INFO - Validation loss = 0.4927
2023-05-15 19:44:22,868 - INFO - Epoch 1301 training loss = 0.08283
2023-05-15 19:44:26,932 - INFO - Epoch 1302 training loss = 0.07453
2023-05-15 19:44:30,993 - INFO - Epoch 1303 training loss = 0.076
2023-05-15 19:44:35,054 - INFO - Epoch 1304 training loss = 0.08255
2023-05-15 19:44:39,120 - INFO - Epoch 1305 training loss = 0.07677
2023-05-15 19:44:43,187 - INFO - Epoch 1306 training loss = 0.07894
2023-05-15 19:44:47,257 - INFO - Epoch 1307 training loss = 0.07227
2023-05-15 19:44:51,325 - INFO - Epoch 1308 training loss = 0.07504
2023-05-15 19:44:55,391 - INFO - Epoch 1309 training loss = 0.07035
2023-05-15 19:44:59,458 - INFO - Epoch 1310 training loss = 0.07334
2023-05-15 19:44:59,840 - INFO - Validation loss = 0.4661
2023-05-15 19:44:59,840 - INFO - best model
2023-05-15 19:45:03,930 - INFO - Epoch 1311 training loss = 0.07621
2023-05-15 19:45:07,998 - INFO - Epoch 1312 training loss = 0.07813
2023-05-15 19:45:12,073 - INFO - Epoch 1313 training loss = 0.07443
2023-05-15 19:45:16,157 - INFO - Epoch 1314 training loss = 0.0723
2023-05-15 19:45:20,239 - INFO - Epoch 1315 training loss = 0.07361
2023-05-15 19:45:24,320 - INFO - Epoch 1316 training loss = 0.07049
2023-05-15 19:45:28,400 - INFO - Epoch 1317 training loss = 0.06719
2023-05-15 19:45:32,480 - INFO - Epoch 1318 training loss = 0.07987
2023-05-15 19:45:36,562 - INFO - Epoch 1319 training loss = 0.06925
2023-05-15 19:45:40,639 - INFO - Epoch 1320 training loss = 0.0704
2023-05-15 19:45:41,021 - INFO - Validation loss = 0.4437
2023-05-15 19:45:41,021 - INFO - best model
2023-05-15 19:45:45,120 - INFO - Epoch 1321 training loss = 0.07068
2023-05-15 19:45:49,203 - INFO - Epoch 1322 training loss = 0.06656
2023-05-15 19:45:53,281 - INFO - Epoch 1323 training loss = 0.06586
2023-05-15 19:45:57,361 - INFO - Epoch 1324 training loss = 0.06772
2023-05-15 19:46:01,443 - INFO - Epoch 1325 training loss = 0.0694
2023-05-15 19:46:05,524 - INFO - Epoch 1326 training loss = 0.06998
2023-05-15 19:46:09,605 - INFO - Epoch 1327 training loss = 0.06551
2023-05-15 19:46:13,688 - INFO - Epoch 1328 training loss = 0.06476
2023-05-15 19:46:17,772 - INFO - Epoch 1329 training loss = 0.06418
2023-05-15 19:46:21,852 - INFO - Epoch 1330 training loss = 0.06718
2023-05-15 19:46:22,234 - INFO - Validation loss = 0.442
2023-05-15 19:46:22,234 - INFO - best model
2023-05-15 19:46:26,336 - INFO - Epoch 1331 training loss = 0.06391
2023-05-15 19:46:30,414 - INFO - Epoch 1332 training loss = 0.06476
2023-05-15 19:46:34,494 - INFO - Epoch 1333 training loss = 0.06626
2023-05-15 19:46:38,569 - INFO - Epoch 1334 training loss = 0.06712
2023-05-15 19:46:42,634 - INFO - Epoch 1335 training loss = 0.06346
2023-05-15 19:46:46,702 - INFO - Epoch 1336 training loss = 0.06304
2023-05-15 19:46:50,770 - INFO - Epoch 1337 training loss = 0.06311
2023-05-15 19:46:54,836 - INFO - Epoch 1338 training loss = 0.06238
2023-05-15 19:46:58,901 - INFO - Epoch 1339 training loss = 0.06164
2023-05-15 19:47:02,960 - INFO - Epoch 1340 training loss = 0.06391
2023-05-15 19:47:03,342 - INFO - Validation loss = 0.4324
2023-05-15 19:47:03,343 - INFO - best model
2023-05-15 19:47:07,429 - INFO - Epoch 1341 training loss = 0.06111
2023-05-15 19:47:11,496 - INFO - Epoch 1342 training loss = 0.06196
2023-05-15 19:47:15,567 - INFO - Epoch 1343 training loss = 0.06341
2023-05-15 19:47:19,635 - INFO - Epoch 1344 training loss = 0.06049
2023-05-15 19:47:23,710 - INFO - Epoch 1345 training loss = 0.0576
2023-05-15 19:47:27,790 - INFO - Epoch 1346 training loss = 0.05856
2023-05-15 19:47:31,870 - INFO - Epoch 1347 training loss = 0.05991
2023-05-15 19:47:35,950 - INFO - Epoch 1348 training loss = 0.05854
2023-05-15 19:47:40,029 - INFO - Epoch 1349 training loss = 0.06087
2023-05-15 19:47:44,111 - INFO - Epoch 1350 training loss = 0.06157
2023-05-15 19:47:44,493 - INFO - Validation loss = 0.4337
2023-05-15 19:47:48,577 - INFO - Epoch 1351 training loss = 0.05869
2023-05-15 19:47:52,659 - INFO - Epoch 1352 training loss = 0.05757
2023-05-15 19:47:56,740 - INFO - Epoch 1353 training loss = 0.05806
2023-05-15 19:48:00,821 - INFO - Epoch 1354 training loss = 0.05711
2023-05-15 19:48:04,897 - INFO - Epoch 1355 training loss = 0.05705
2023-05-15 19:48:08,969 - INFO - Epoch 1356 training loss = 0.05694
2023-05-15 19:48:13,044 - INFO - Epoch 1357 training loss = 0.05616
2023-05-15 19:48:17,120 - INFO - Epoch 1358 training loss = 0.0558
2023-05-15 19:48:21,194 - INFO - Epoch 1359 training loss = 0.05823
2023-05-15 19:48:25,267 - INFO - Epoch 1360 training loss = 0.05687
2023-05-15 19:48:25,650 - INFO - Validation loss = 0.4282
2023-05-15 19:48:25,650 - INFO - best model
2023-05-15 19:48:29,744 - INFO - Epoch 1361 training loss = 0.05553
2023-05-15 19:48:33,816 - INFO - Epoch 1362 training loss = 0.05435
2023-05-15 19:48:37,888 - INFO - Epoch 1363 training loss = 0.05442
2023-05-15 19:48:41,962 - INFO - Epoch 1364 training loss = 0.05451
2023-05-15 19:48:46,038 - INFO - Epoch 1365 training loss = 0.05431
2023-05-15 19:48:50,114 - INFO - Epoch 1366 training loss = 0.05412
2023-05-15 19:48:54,186 - INFO - Epoch 1367 training loss = 0.05362
2023-05-15 19:48:58,258 - INFO - Epoch 1368 training loss = 0.05385
2023-05-15 19:49:02,331 - INFO - Epoch 1369 training loss = 0.05373
2023-05-15 19:49:06,404 - INFO - Epoch 1370 training loss = 0.05291
2023-05-15 19:49:06,786 - INFO - Validation loss = 0.4209
2023-05-15 19:49:06,786 - INFO - best model
2023-05-15 19:49:10,882 - INFO - Epoch 1371 training loss = 0.05215
2023-05-15 19:49:14,957 - INFO - Epoch 1372 training loss = 0.05295
2023-05-15 19:49:19,033 - INFO - Epoch 1373 training loss = 0.05328
2023-05-15 19:49:23,106 - INFO - Epoch 1374 training loss = 0.05106
2023-05-15 19:49:27,173 - INFO - Epoch 1375 training loss = 0.05282
2023-05-15 19:49:31,245 - INFO - Epoch 1376 training loss = 0.05481
2023-05-15 19:49:35,319 - INFO - Epoch 1377 training loss = 0.05147
2023-05-15 19:49:39,392 - INFO - Epoch 1378 training loss = 0.05198
2023-05-15 19:49:43,467 - INFO - Epoch 1379 training loss = 0.05138
2023-05-15 19:49:47,543 - INFO - Epoch 1380 training loss = 0.05398
2023-05-15 19:49:47,925 - INFO - Validation loss = 0.4258
2023-05-15 19:49:51,999 - INFO - Epoch 1381 training loss = 0.05233
2023-05-15 19:49:56,071 - INFO - Epoch 1382 training loss = 0.0496
2023-05-15 19:50:00,143 - INFO - Epoch 1383 training loss = 0.05079
2023-05-15 19:50:04,218 - INFO - Epoch 1384 training loss = 0.05025
2023-05-15 19:50:08,291 - INFO - Epoch 1385 training loss = 0.05023
2023-05-15 19:50:12,364 - INFO - Epoch 1386 training loss = 0.04955
2023-05-15 19:50:16,439 - INFO - Epoch 1387 training loss = 0.04985
2023-05-15 19:50:20,513 - INFO - Epoch 1388 training loss = 0.05132
2023-05-15 19:50:24,585 - INFO - Epoch 1389 training loss = 0.04997
2023-05-15 19:50:28,656 - INFO - Epoch 1390 training loss = 0.04954
2023-05-15 19:50:29,038 - INFO - Validation loss = 0.4149
2023-05-15 19:50:29,038 - INFO - best model
2023-05-15 19:50:33,131 - INFO - Epoch 1391 training loss = 0.05037
2023-05-15 19:50:37,204 - INFO - Epoch 1392 training loss = 0.04893
2023-05-15 19:50:41,277 - INFO - Epoch 1393 training loss = 0.04911
2023-05-15 19:50:45,352 - INFO - Epoch 1394 training loss = 0.05007
2023-05-15 19:50:49,428 - INFO - Epoch 1395 training loss = 0.04852
2023-05-15 19:50:53,500 - INFO - Epoch 1396 training loss = 0.04905
2023-05-15 19:50:57,573 - INFO - Epoch 1397 training loss = 0.04812
2023-05-15 19:51:01,646 - INFO - Epoch 1398 training loss = 0.04818
2023-05-15 19:51:05,719 - INFO - Epoch 1399 training loss = 0.04768
2023-05-15 19:51:09,792 - INFO - Epoch 1400 training loss = 0.04798
2023-05-15 19:51:10,174 - INFO - Validation loss = 0.4108
2023-05-15 19:51:10,175 - INFO - best model
2023-05-15 19:51:14,271 - INFO - Epoch 1401 training loss = 0.04791
2023-05-15 19:51:18,346 - INFO - Epoch 1402 training loss = 0.04832
2023-05-15 19:51:22,418 - INFO - Epoch 1403 training loss = 0.04839
2023-05-15 19:51:26,490 - INFO - Epoch 1404 training loss = 0.04809
2023-05-15 19:51:30,562 - INFO - Epoch 1405 training loss = 0.047
2023-05-15 19:51:34,634 - INFO - Epoch 1406 training loss = 0.047
2023-05-15 19:51:38,707 - INFO - Epoch 1407 training loss = 0.04699
2023-05-15 19:51:42,782 - INFO - Epoch 1408 training loss = 0.04748
2023-05-15 19:51:46,857 - INFO - Epoch 1409 training loss = 0.04666
2023-05-15 19:51:50,932 - INFO - Epoch 1410 training loss = 0.04666
2023-05-15 19:51:51,314 - INFO - Validation loss = 0.4103
2023-05-15 19:51:51,314 - INFO - best model
2023-05-15 19:51:55,408 - INFO - Epoch 1411 training loss = 0.04697
2023-05-15 19:51:59,480 - INFO - Epoch 1412 training loss = 0.04664
2023-05-15 19:52:03,553 - INFO - Epoch 1413 training loss = 0.04717
2023-05-15 19:52:07,626 - INFO - Epoch 1414 training loss = 0.04655
2023-05-15 19:52:11,699 - INFO - Epoch 1415 training loss = 0.04603
2023-05-15 19:52:15,776 - INFO - Epoch 1416 training loss = 0.04683
2023-05-15 19:52:19,849 - INFO - Epoch 1417 training loss = 0.04677
2023-05-15 19:52:23,914 - INFO - Epoch 1418 training loss = 0.04648
2023-05-15 19:52:27,985 - INFO - Epoch 1419 training loss = 0.04591
2023-05-15 19:52:32,057 - INFO - Epoch 1420 training loss = 0.04544
2023-05-15 19:52:32,439 - INFO - Validation loss = 0.408
2023-05-15 19:52:32,439 - INFO - best model
2023-05-15 19:52:36,534 - INFO - Epoch 1421 training loss = 0.04556
2023-05-15 19:52:40,605 - INFO - Epoch 1422 training loss = 0.04545
2023-05-15 19:52:44,678 - INFO - Epoch 1423 training loss = 0.04538
2023-05-15 19:52:48,753 - INFO - Epoch 1424 training loss = 0.04561
2023-05-15 19:52:52,825 - INFO - Epoch 1425 training loss = 0.04533
2023-05-15 19:52:56,896 - INFO - Epoch 1426 training loss = 0.04546
2023-05-15 19:53:00,968 - INFO - Epoch 1427 training loss = 0.04555
2023-05-15 19:53:05,041 - INFO - Epoch 1428 training loss = 0.04535
2023-05-15 19:53:09,113 - INFO - Epoch 1429 training loss = 0.04598
2023-05-15 19:53:13,187 - INFO - Epoch 1430 training loss = 0.04531
2023-05-15 19:53:13,569 - INFO - Validation loss = 0.4051
2023-05-15 19:53:13,569 - INFO - best model
2023-05-15 19:53:17,667 - INFO - Epoch 1431 training loss = 0.04485
2023-05-15 19:53:21,739 - INFO - Epoch 1432 training loss = 0.04504
2023-05-15 19:53:25,810 - INFO - Epoch 1433 training loss = 0.04478
2023-05-15 19:53:29,881 - INFO - Epoch 1434 training loss = 0.04498
2023-05-15 19:53:33,953 - INFO - Epoch 1435 training loss = 0.04546
2023-05-15 19:53:38,025 - INFO - Epoch 1436 training loss = 0.04502
2023-05-15 19:53:42,097 - INFO - Epoch 1437 training loss = 0.04465
2023-05-15 19:53:46,171 - INFO - Epoch 1438 training loss = 0.04429
2023-05-15 19:53:50,245 - INFO - Epoch 1439 training loss = 0.04471
2023-05-15 19:53:54,315 - INFO - Epoch 1440 training loss = 0.04443
2023-05-15 19:53:54,698 - INFO - Validation loss = 0.4033
2023-05-15 19:53:54,699 - INFO - best model
2023-05-15 19:53:58,793 - INFO - Epoch 1441 training loss = 0.04453
2023-05-15 19:54:02,865 - INFO - Epoch 1442 training loss = 0.04482
2023-05-15 19:54:06,937 - INFO - Epoch 1443 training loss = 0.04426
2023-05-15 19:54:11,010 - INFO - Epoch 1444 training loss = 0.04468
2023-05-15 19:54:15,085 - INFO - Epoch 1445 training loss = 0.04392
2023-05-15 19:54:19,159 - INFO - Epoch 1446 training loss = 0.04376
2023-05-15 19:54:23,232 - INFO - Epoch 1447 training loss = 0.04429
2023-05-15 19:54:27,302 - INFO - Epoch 1448 training loss = 0.0442
2023-05-15 19:54:31,374 - INFO - Epoch 1449 training loss = 0.04465
2023-05-15 19:54:35,447 - INFO - Epoch 1450 training loss = 0.04346
2023-05-15 19:54:35,829 - INFO - Validation loss = 0.402
2023-05-15 19:54:35,830 - INFO - best model
2023-05-15 19:54:39,924 - INFO - Epoch 1451 training loss = 0.0438
2023-05-15 19:54:43,998 - INFO - Epoch 1452 training loss = 0.04408
2023-05-15 19:54:48,074 - INFO - Epoch 1453 training loss = 0.04384
2023-05-15 19:54:52,145 - INFO - Epoch 1454 training loss = 0.04435
2023-05-15 19:54:56,203 - INFO - Epoch 1455 training loss = 0.04403
2023-05-15 19:55:00,263 - INFO - Epoch 1456 training loss = 0.04395
2023-05-15 19:55:04,321 - INFO - Epoch 1457 training loss = 0.04385
2023-05-15 19:55:08,380 - INFO - Epoch 1458 training loss = 0.04388
2023-05-15 19:55:12,441 - INFO - Epoch 1459 training loss = 0.04371
2023-05-15 19:55:16,503 - INFO - Epoch 1460 training loss = 0.04389
2023-05-15 19:55:16,884 - INFO - Validation loss = 0.3997
2023-05-15 19:55:16,885 - INFO - best model
2023-05-15 19:55:20,969 - INFO - Epoch 1461 training loss = 0.04356
2023-05-15 19:55:25,034 - INFO - Epoch 1462 training loss = 0.04367
2023-05-15 19:55:29,099 - INFO - Epoch 1463 training loss = 0.04401
2023-05-15 19:55:33,165 - INFO - Epoch 1464 training loss = 0.04378
2023-05-15 19:55:37,231 - INFO - Epoch 1465 training loss = 0.04363
2023-05-15 19:55:41,298 - INFO - Epoch 1466 training loss = 0.04389
2023-05-15 19:55:45,375 - INFO - Epoch 1467 training loss = 0.04345
2023-05-15 19:55:49,451 - INFO - Epoch 1468 training loss = 0.04329
2023-05-15 19:55:53,531 - INFO - Epoch 1469 training loss = 0.04393
2023-05-15 19:55:57,611 - INFO - Epoch 1470 training loss = 0.04343
2023-05-15 19:55:57,994 - INFO - Validation loss = 0.3993
2023-05-15 19:55:57,994 - INFO - best model
2023-05-15 19:56:02,087 - INFO - Epoch 1471 training loss = 0.04399
2023-05-15 19:56:06,154 - INFO - Epoch 1472 training loss = 0.0435
2023-05-15 19:56:10,220 - INFO - Epoch 1473 training loss = 0.04368
2023-05-15 19:56:14,289 - INFO - Epoch 1474 training loss = 0.04351
2023-05-15 19:56:18,358 - INFO - Epoch 1475 training loss = 0.04343
2023-05-15 19:56:22,424 - INFO - Epoch 1476 training loss = 0.04369
2023-05-15 19:56:26,489 - INFO - Epoch 1477 training loss = 0.04339
2023-05-15 19:56:30,554 - INFO - Epoch 1478 training loss = 0.04344
2023-05-15 19:56:34,619 - INFO - Epoch 1479 training loss = 0.0433
2023-05-15 19:56:38,686 - INFO - Epoch 1480 training loss = 0.04297
2023-05-15 19:56:39,067 - INFO - Validation loss = 0.3996
2023-05-15 19:56:43,138 - INFO - Epoch 1481 training loss = 0.0437
2023-05-15 19:56:47,209 - INFO - Epoch 1482 training loss = 0.04305
2023-05-15 19:56:51,277 - INFO - Epoch 1483 training loss = 0.04321
2023-05-15 19:56:55,343 - INFO - Epoch 1484 training loss = 0.04314
2023-05-15 19:56:59,409 - INFO - Epoch 1485 training loss = 0.04359
2023-05-15 19:57:03,477 - INFO - Epoch 1486 training loss = 0.04315
2023-05-15 19:57:07,553 - INFO - Epoch 1487 training loss = 0.04343
2023-05-15 19:57:11,631 - INFO - Epoch 1488 training loss = 0.04319
2023-05-15 19:57:15,701 - INFO - Epoch 1489 training loss = 0.04365
2023-05-15 19:57:19,770 - INFO - Epoch 1490 training loss = 0.04252
2023-05-15 19:57:20,151 - INFO - Validation loss = 0.3993
2023-05-15 19:57:24,218 - INFO - Epoch 1491 training loss = 0.04398
2023-05-15 19:57:28,284 - INFO - Epoch 1492 training loss = 0.04354
2023-05-15 19:57:32,351 - INFO - Epoch 1493 training loss = 0.04385
2023-05-15 19:57:36,417 - INFO - Epoch 1494 training loss = 0.04331
2023-05-15 19:57:40,484 - INFO - Epoch 1495 training loss = 0.04311
2023-05-15 19:57:44,553 - INFO - Epoch 1496 training loss = 0.04343
2023-05-15 19:57:48,624 - INFO - Epoch 1497 training loss = 0.04294
2023-05-15 19:57:52,692 - INFO - Epoch 1498 training loss = 0.04355
2023-05-15 19:57:56,759 - INFO - Epoch 1499 training loss = 0.04329
2023-05-15 19:57:57,119 - INFO - Validation loss = 0.3993
